@INPROCEEDINGS{8332632,
author={Liu, He and Huang, Fupeng and Li, Han and Liu, Weiwei and Wang, Tongxun},
booktitle={2017 14th Web Information Systems and Applications Conference (WISA)}, title={A Big Data Framework for Electric Power Data Quality Assessment},
year={2017},
volume={},
number={},
pages={289-292},
abstract={Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.},
keywords={Big Data;Data integrity;Power grids;History;Real-time systems;Sensors;data quality;electric power data;data quality assessment;big data;framework},
doi={10.1109/WISA.2017.29},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8029366,
author={Taleb, Ikbal and Serhani, Mohamed Adel},
booktitle={2017 IEEE International Congress on Big Data (BigData Congress)}, title={Big Data Pre-Processing: Closing the Data Quality Enforcement Loop},
year={2017},
volume={},
number={},
pages={498-501},
abstract={In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.},
keywords={Big Data;Optimization;Data models;Quality assessment;Big Data;Data Quality Evaluation;Data Quality Rules Discovery;Big Data Pre-Processing},
doi={10.1109/BigDataCongress.2017.73},
ISSN={},
month={June},}
@INPROCEEDINGS{9245455,
author={Loetpipatwanich, Sakda and Vichitthamaros, Preecha},
booktitle={2020 1st International Conference on Big Data Analytics and Practices (IBDAP)}, title={Sakdas: A Python Package for Data Profiling and Data Quality Auditing},
year={2020},
volume={},
number={},
pages={1-4},
abstract={Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called “Sakdas” this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.},
keywords={Data integrity;Pipelines;Data visualization;Big Data;Syntactics;Software;Python;Data Quality Management;Data Profiling;Data Quality Auditing;Python Package;Data Pipeline},
doi={10.1109/IBDAP50342.2020.9245455},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8386521,
author={Hongxun, Tian and Honggang, Wang and Kun, Zhou and Mingtai, Shi and Haosong, Li and Zhongping, Xu and Taifeng, Kang and Jin, Li and Yaqi, Cai},
booktitle={2018 IEEE 3rd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)}, title={Data quality assessment for on-line monitoring and measuring system of power quality based on big data and data provenance theory},
year={2018},
volume={},
number={},
pages={248-252},
abstract={Currently, on-line monitoring and measuring system of power quality has accumulated a huge amount of data. In the age of big data, those data integrated from various systems will face big data application problems. This paper proposes a data quality assessment system method for on-line monitoring and measuring system of power quality based on big data and data provenance to assess integrity, redundancy, accuracy, timeliness, intelligence and consistency of data set and single data. Specific assessment rule which conforms to the situation of on-line monitoring and measuring system of power quality will be devised to found data quality problems. Thus it will provide strong data support for big data application of power quality.},
keywords={Data integrity;Power quality;Monitoring;Power measurement;Redundancy;Big Data;Business;power qualiy;data quality;big data;data provenance;data assessment},
doi={10.1109/ICCCBDA.2018.8386521},
ISSN={},
month={April},}
@INPROCEEDINGS{8605945,
author={Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida},
booktitle={2018 International Conference on Innovations in Information Technology (IIT)}, title={Big Data Quality Assessment Model for Unstructured Data},
year={2018},
volume={},
number={},
pages={69-74},
abstract={Big Data has gained an enormous momentum the past few years because of the tremendous volume of generated and processed Data from diverse application domains. Nowadays, it is estimated that 80% of all the generated data is unstructured. Evaluating the quality of Big data has been identified to be essential to guarantee data quality dimensions including for example completeness, and accuracy. Current initiatives for unstructured data quality evaluation are still under investigations. In this paper, we propose a quality evaluation model to handle quality of Unstructured Big Data (UBD). The later captures and discover first key properties of unstructured big data and its characteristics, provides some comprehensive mechanisms to sample, profile the UBD dataset and extract features and characteristics from heterogeneous data types in different formats. A Data Quality repository manage relationships between Data quality dimensions, quality Metrics, features extraction methods, mining methodologies, data types and data domains. An analysis of the samples provides a data profile of UBD. This profile is extended to a quality profile that contains the quality mapping with selected features for quality assessment. We developed an UBD quality assessment model that handles all the processes from the UBD profiling exploration to the Quality report. The model provides an initial blueprint for quality estimation of unstructured Big data. It also, states a set of quality characteristics and indicators that can be used to outline an initial data quality schema of UBD.},
keywords={Big Data;Data integrity;Data mining;Feature extraction;Data models;Measurement;Quality assessment;Big Data;Data Quality;Unstructured Data;Quality of Unstructured Big Data},
doi={10.1109/INNOVATIONS.2018.8605945},
ISSN={2325-5498},
month={Nov},}
@INPROCEEDINGS{8078796,
author={HongJu, Xiao and Fei, Wang and FenMei, Wang and XiuZhen, Wang},
booktitle={2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)}, title={Some key problems of data management in army data engineering based on big data},
year={2017},
volume={},
number={},
pages={149-152},
abstract={This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.},
keywords={Data engineering;Data analysis;Data integration;Big Data;Distributed databases;Data models;Uncertainty;army data engineering;data management;data integration;data analysis;representation of data analysis results;data quality},
doi={10.1109/ICBDA.2017.8078796},
ISSN={},
month={March},}
@INPROCEEDINGS{7364064,
author={Becker, David and King, Trish Dunn and McMullen, Bill},
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, title={Big data, big data quality problem},
year={2015},
volume={},
number={},
pages={2644-2653},
abstract={A USAF sponsored MITRE research team undertook four separate, domain-specific case studies about Big Data applications. Those case studies were initial investigations into the question of whether or not data quality issues encountered in Big Data collections are substantially different in cause, manifestation, or detection than those data quality issues encountered in more traditionally sized data collections. The study addresses several factors affecting Big Data Quality at multiple levels, including collection, processing, and storage. Though not unexpected, the key findings of this study reinforce that the primary factors affecting Big Data reside in the limitations and complexities involved with handling Big Data while maintaining its integrity. These concerns are of a higher magnitude than the provenance of the data, the processing, and the tools used to prepare, manipulate, and store the data. Data quality is extremely important for all data analytics problems. From the study's findings, the "truth about Big Data" is there are no fundamentally new DQ issues in Big Data analytics projects. Some DQ issues exhibit return-s-to-scale effects, and become more or less pronounced in Big Data analytics, though. Big Data Quality varies from one type of Big Data to another and from one Big Data technology to another.},
keywords={Big data;Biomedical imaging;Personnel;Complexity theory;Bioinformatics;Process control;Instruments;Big Data;Data Quality;Returns to Scale},
doi={10.1109/BigData.2015.7364064},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9260067,
author={Wong, Ka Yee. and Wong, Raymond K.},
booktitle={2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)}, title={Big Data Quality Prediction on Banking Applications: Extended Abstract},
year={2020},
volume={},
number={},
pages={791-792},
abstract={Big data has been transformed into knowledge by information systems to add value in businesses. Enterprises relying on it benefit from risk management to a certain extent. The value, however, depends on the quality of data. The quality needs to be verified before any use of the data. Specifically, measuring the quality by simulating the real life situation and even forecast it accurately turns into a hot topic. In recent years, there have been numerous researches on the measurement and assessment of data quality. These are yet to utilize a scientific computational method for the measurement and prediction. Current methods either fail to make an accurate prediction or do not consider the correlation and time sequence factors of the data. To address this, we design a model to extend machine learning technique to business applications predicting this. Firstly, we implement the model to detect data noises from a risk dataset according to an international data quality standard from banking industry and then estimate their impacts with Gaussian and Bayesian methods. Secondly, we direct sequential learning in multiple deep neural networks for the prediction with an attention mechanism. The model is experimented with various network methodologies to show the predictive power of machine learning technique and is evaluated by validation data to confirm the model effectiveness. The model is scalable to apply to any industries utilizing big data other than the banking industry.},
keywords={Big Data;Data models;Banking;Neural networks;Industries;Data integrity;Computational modeling;Big Data;Data Noise;Data Quality;Prediction and Machine Learning},
doi={10.1109/DSAA49011.2020.00119},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8686099,
author={Abdallah, Mohammad},
booktitle={2019 International Conference on Big Data and Computational Intelligence (ICBDCI)}, title={Big Data Quality Challenges},
year={2019},
volume={},
number={},
pages={1-3},
abstract={Big Data, is a growing technique these days. There are many uses of Big Data; Artificial Intelligence, Health Care, Business, and many more. For that reason, it becomes necessary to deal with this massive volume of data with caution and care in a term to make sure that the data used and produced is in high quality. Therefore, the Big Data quality is must, and its rules have to be satisfied. In this paper, the main Big Data Quality Factors, which need to be measured, is presented in the perspective of the data itself, the data management, data processing, and data users. This research highlights the quality factors that may be used later to create different Big Data quality models.},
keywords={Big Data;Quality Measurement;Quality Model;Quality Assurance},
doi={10.1109/ICBDCI.2019.8686099},
ISSN={},
month={Feb},}
@INPROCEEDINGS{7816918,
author={Taleb, Ikbal and Kassabi, Hadeel T. El and Serhani, Mohamed Adel and Dssouli, Rachida and Bouhaddioui, Chafik},
booktitle={2016 Intl IEEE Conferences on Ubiquitous Intelligence Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)}, title={Big Data Quality: A Quality Dimensions Evaluation},
year={2016},
volume={},
number={},
pages={759-765},
abstract={Data is the most valuable asset companies are proud of. When its quality degrades, the consequences are unpredictable, can lead to complete wrong insights. In Big Data context, evaluating the data quality is challenging, must be done prior to any Big data analytics by providing some data quality confidence. Given the huge data size, its fast generation, it requires mechanisms, strategies to evaluate, assess data quality in a fast, efficient way. However, checking the Quality of Big Data is a very costly process if it is applied on the entire data. In this paper, we propose an efficient data quality evaluation scheme by applying sampling strategies on Big data sets. The Sampling will reduce the data size to a representative population samples for fast quality evaluation. The evaluation targeted some data quality dimensions like completeness, consistency. The experimentations have been conducted on Sleep disorder's data set by applying Big data bootstrap sampling techniques. The results showed that the mean quality score of samples is representative for the original data, illustrate the importance of sampling to reduce computing costs when Big data quality evaluation is concerned. We applied the Quality results generated as quality proposals on the original data to increase its quality.},
keywords={Big data;Measurement;Feature extraction;Quality assessment;Social network services;Companies;Context;Big Data;data quality dimensions;data quality evaluation;Big data sampling},
doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0122},
ISSN={},
month={July},}
@INPROCEEDINGS{7364065,
author={Rao, Dhana and Gudivada, Venkat N and Raghavan, Vijay V.},
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, title={Data quality issues in big data},
year={2015},
volume={},
number={},
pages={2654-2660},
abstract={Though the issues of data quality trace back their origin to the early days of computing, the recent emergence of Big Data has added more dimensions. Furthermore, given the range of Big Data applications, potential consequences of bad data quality can be for more disastrous and widespread. This paper provides a perspective on data quality issues in the Big Data context. it also discusses data integration issues that arise in biological databases and attendant data quality issues.},
keywords={Databases;Big data;Measurement;Context;Cleaning;Biology;Computers;Data quality;big data;biological data;information quality},
doi={10.1109/BigData.2015.7364065},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8862267,
author={Juneja, Ashish and Das, Nripendra Narayan},
booktitle={2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)}, title={Big Data Quality Framework: Pre-Processing Data in Weather Monitoring Application},
year={2019},
volume={},
number={},
pages={559-563},
abstract={Big Data has become an imminent part of all industries and business sectors today. All organizations in any sector like energy, banking, retail, hardware, networking, etc all generate huge quantum of heterogenous data which if mined, processed and analyzed accurately can reveal immensely useful patterns for business heads to apply to generate and grow their businesses. Big Data helps in acquiring, processing and analyzing large amounts of heterogeneous data to derive valuable results. Quality of information is affected by size, speed and format in which data is generated. Hence, Quality of Big Data is of great relevance and importance. We propose addressing various aspects of the raw data to improve its quality in the pre-processing stage, as the raw data may not usable as-is. We are exploring process like Cleansing to fix as much data as feasible, Noise filters to remove bad data, as well sub-processes for Integration and Filtering along with Data Transformation/Normalization. We evaluate and profile the Big Data during acquisition stage, which is adapted to expectations to avoid cost overheads later while also improving and leading to accurate data analysis. Hence, it is imperative to improve Data quality even it is absorbed and utilized in an industry's Big Data system. In this paper, we propose a Pre-Processing Framework to address quality of data in a weather monitoring and forecasting application that also takes into account global warming parameters and raises alerts/notifications to warn users and scientists in advance.},
keywords={Big Data;Data integrity;Meteorology;Monitoring;Data mining;Organizations;Big Data;Big Data Quality;Data Quality;preprocessing;pre-processing},
doi={10.1109/COMITCon.2019.8862267},
ISSN={},
month={Feb},}
@INPROCEEDINGS{7840595,
author={Haryadi, Adiska Fardani and Hulstijn, Joris and Wahyudi, Agung and van der Voort, Haiko and Janssen, Marijn},
booktitle={2016 IEEE International Conference on Big Data (Big Data)}, title={Antecedents of big data quality: An empirical examination in financial service organizations},
year={2016},
volume={},
number={},
pages={116-121},
abstract={Big data has been acknowledged for its enormous potential. In contrast to the potential, in a recent survey more than half of financial service organizations reported that big data has not delivered the expected value. One of the main reasons for this is related to data quality. The objective of this research is to identify the antecedents of big data quality in financial institutions. This will help to understand how data quality from big data analysis can be improved. For this, a literature review was performed and data was collected using three case studies, followed by content analysis. The overall findings indicate that there are no fundamentally new data quality issues in big data projects. Nevertheless, the complexity of the issues is higher, which makes it harder to assess and attain data quality in big data projects compared to the traditional projects. Ten antecedents of big data quality were identified encompassing data, technology, people, process and procedure, organization, and external aspects.},
keywords={Big data;Industries;Insurance;Companies;Finance;big data;data quality;big data quality;antecedents;finance},
doi={10.1109/BigData.2016.7840595},
ISSN={},
month={Dec},}
@ARTICLE{8935096,
author={Li, Mingda and Wang, Hongzhi and Li, Jianzhong},
journal={Big Data Mining and Analytics}, title={Mining conditional functional dependency rules on big data},
year={2020},
volume={3},
number={1},
pages={68-84},
abstract={Current Conditional Functional Dependency (CFD) discovery algorithms always need a well-prepared training dataset. This condition makes them difficult to apply on large and low-quality datasets. To handle the volume issue of big data, we develop the sampling algorithms to obtain a small representative training set. We design the fault-tolerant rule discovery and conflict-resolution algorithms to address the low-quality issue of big data. We also propose parameter selection strategy to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate that our method can discover effective CFD rules on billion-tuple data within a reasonable period.},
keywords={Big Data;Training;Data mining;Cleaning;Sampling methods;Heuristic algorithms;Fault tolerance;data mining;conditional functional dependency;big data;data quality},
doi={10.26599/BDMA.2019.9020019},
ISSN={2096-0654},
month={March},}
@INPROCEEDINGS{8397554,
author={Zhang, Pengcheng and Xiong, Fang and Gao, Jerry and Wang, Jimin},
booktitle={2017 IEEE SmartWorld, Ubiquitous Intelligence Computing, Advanced Trusted Computed, Scalable Computing Communications, Cloud Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)}, title={Data quality in big data processing: Issues, solutions and open problems},
year={2017},
volume={},
number={},
pages={1-7},
abstract={With the rapid development of social networks, Internet of things, Cloud computing as well as other technologies, big data age is arriving. The increasing number of data has brought great value to the public and enterprises. Meanwhile how to manage and use big data better has become the focus of all walks of life. The 4V characteristics of big data have brought a lot of issues to the big data processing. The key to big data processing is to solve data quality issue, and to ensure data quality is a prerequisite for the successful application of big data technique. In this paper, we use recommendation systems and prediction systems as typical big data applications, and try to find out the data quality issues during data collection, data preprocessing, data storage and data analysis stages of big data processing. According to the elaboration and analysis of the proposed issues, the corresponding solutions are also put forward. Finally, some open problems to be solved in the future are also raised.},
keywords={Big Data;Data integrity;Data analysis;Social network services;Data preprocessing;Internet;Big Data;Big data processing;Data Quality;Recommendation system;Prediction system},
doi={10.1109/UIC-ATC.2017.8397554},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9006294,
author={Arruda, Darlan and Madhavji, Nazim H.},
booktitle={2019 IEEE International Conference on Big Data (Big Data)}, title={QualiBD: A Tool for Modelling Quality Requirements for Big Data Applications},
year={2019},
volume={},
number={},
pages={5977-5979},
abstract={The development of Big Data applications is not well-explored, to our knowledge. Embracing Big Data in system building, questions arise as to how to elicit, specify, analyse, model, and document Big Data quality requirements. In our ongoing research, we explore a requirements modelling language for Big Data software applications. In this paper, we introduce QualiBD, a modelling tool that implements the proposed goal-oriented requirements language that facilitates the modelling of Big Data quality requirements.},
keywords={Tools;Data models;Containers;Software;Real-time systems;Big Data applications;Big Data Applications;Quality Requirements;Big Data Goal-oriented Requirements Language;Requirements Modelling Tool},
doi={10.1109/BigData47090.2019.9006294},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8465129,
author={Juddoo, Suraj and George, Carlisle},
booktitle={2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)}, title={Discovering Most Important Data Quality Dimensions Using Latent Semantic Analysis},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Big Data quality is a field which is emerging. Many authors nowadays agree that data quality is still very relevant, even for Big Data uses. However, there is a lack of frameworks or guidelines about how to carry out those big data quality initiatives. The starting point of any data quality work is to determine the properties of data quality, termed as data quality dimensions (DQDs). Even those dimensions lack precise rigour in terms of definition from existing literature. This current research aims to contribute towards identifying the most important DQDs for big data in the health industry. It is a continuation of a previous work, which already identified five most important DQDs, using a human judgement based technique known as inner hermeneutic cycle. To remove potential bias coming from the human judgement aspect, this research uses the same set of literature but applies a statistical technique known to extract knowledge from a set of documents known as latent semantic analysis. The results confirm only 2 similar most important DQDs, namely accuracy and completeness.},
keywords={Data integrity;Big Data;Semantics;Industries;Bibliographies;Libraries;Production;Big Data;data quality;health data;data quality dimensions;latent semantic analysis},
doi={10.1109/ICABCD.2018.8465129},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7364060,
author={Nobles, Alicia L. and Vilankar, Ketki and Wu, Hao and Barnes, Laura E.},
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, title={Evaluation of data quality of multisite electronic health record data for secondary analysis},
year={2015},
volume={},
number={},
pages={2612-2620},
abstract={Currently, a large amount of data is amassed in electronic health records (EHRs). However, EHR systems are largely information silos, that is, uses of these systems are often confined to management of patient information and analytics specific to a clinician's practice. A growing trend in healthcare is combining multiple databases to support epidemiological research. The College Health Surveillance Network is the first national data warehouse containing EHR data from 31 different student health centers. Each member university contributes to the data warehouse by uploading select EHR data including patient demographics, diagnoses, and procedures to a common server on a monthly basis. In this paper, we focus on the data quality dimensions from a subsample of the data comprised of over 5.7 million patient visits for approximately 980,000 patients with 4,465 unique diagnoses from 23 of those universities. We examine the data for measures of completeness, consistency, and availability for secondary use for epidemiological research. Additionally, clinical documentation practices and EHR vendor were evaluated as potential contributors to data quality. We found that overall about 70% of the data in the data warehouse is available for secondary use, and identified clinical documentation practices that are correlated to a reduction in data quality. This suggests that automated quality control and proactive clinical documentation support could reduce ad-hoc data cleaning needs resulting in greater data availability for secondary use.},
keywords={Documentation;Data warehouses;Medical services;Big data;Databases;Cleaning;Medical diagnostic imaging;electronic health records;data quality;big data;multiple data vendors;metrics},
doi={10.1109/BigData.2015.7364060},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9006358,
author={Kumar, Jitendra and Crow, Michael C. and Devarakonda, Ranjeet and Giansiracusa, Michael and Guntupally, Kavya and Olatt, Joseph V. and Price, Zach and Shanafield, Harold A. and Singh, Alka},
booktitle={2019 IEEE International Conference on Big Data (Big Data)}, title={Provenance–aware workflow for data quality management and improvement for large continuous scientific data streams},
year={2019},
volume={},
number={},
pages={3260-3266},
abstract={Data quality assessment, management and improvement is an integral part of any big data intensive scientific research to ensure accurate, reliable, and reproducible scientific discoveries. The task of maintaining the quality of data, however, is non-trivial and poses a challenge for a program like the Department of Energy's Atmospheric Radiation Measurement (ARM) that collects data from hundreds of instruments across the world, and distributes thousands of streaming data products that are continuously produced in near-real-time for an archive 1.7 Petabyte in size and growing. In this paper, we present a computational data processing workflow to address the data quality issues via an easy and intuitive web-based portal that allows reporting of any quality issues for any site, facility or instruments at a granularity down to individual variables in the data files. This portal allows instrument specialists and scientists to provide corrective actions in the form of symbolic equations. A parallel processing framework applies the data improvement to a large volume of data in an efficient, parallel environment, while optimizing data transfer and file I/O operations; corrected files are then systematically versioned and archived. A provenance tracking module tracks and records any change made to the data during its entire life cycle which are communicated transparently to the scientific users. Developed in Python using open source technologies, this software architecture enables fast and efficient management and improvement of data in an operational data center environment.},
keywords={Data integrity;Instruments;Atmospheric measurements;Dictionaries;Big Data;Portals;scientific data workflows;data quality;provenance;atmospheric science},
doi={10.1109/BigData47090.2019.9006358},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9323615,
author={Han, Weiguo and Jochum, Matthew},
booktitle={IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium}, title={A Machine Learning Approach for Data Quality Control of Earth Observation Data Management System},
year={2020},
volume={},
number={},
pages={3101-3103},
abstract={In the big data era, innovative technologies like cloud computing, artificial intelligence, and machine learning are increasingly utilized in the large-scale data management systems of many industry sectors to make them more scalable and intelligent. Applying them to automate and optimize earth observation data management is a hot topic. To improve data quality control mechanisms, a machine learning method in combination with built-in quality rules is presented in this paper to evolve processes around data quality and enhance management of earth observation data. The rules of quality check are set up to detect the common issues, including data completeness, data latency, bad data, and data duplication, and the machine learning model is trained, tested, and deployed to address these quality issues automatically and reduce manual efforts.},
keywords={Data integrity;Machine learning;Earth;Satellites;Big Data;Process control;Monitoring;Big Data;Machine Learning;Earth Observation Data;Data management;Data Quality;Random Forest},
doi={10.1109/IGARSS39084.2020.9323615},
ISSN={2153-7003},
month={Sep.},}
@INPROCEEDINGS{9006187,
author={Shrivastava, Shrey and Patel, Dhaval and Bhamidipaty, Anuradha and Gifford, Wesley M. and Siegel, Stuart A. and Ganapavarapu, Venkata Sitaramagiridharganesh and Kalagnanam, Jayant R.},
booktitle={2019 IEEE International Conference on Big Data (Big Data)}, title={DQA: Scalable, Automated and Interactive Data Quality Advisor},
year={2019},
volume={},
number={},
pages={2913-2922},
abstract={Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today's world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.},
keywords={Data integrity;Machine learning;Pipelines;Cleaning;Libraries;Buildings;Data quality;machine learning;data cleaning;scalability;automation;data science},
doi={10.1109/BigData47090.2019.9006187},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9073586,
author={Khaleel, Majida Yaseen and Hamad, Murtadha M.},
booktitle={2019 12th International Conference on Developments in eSystems Engineering (DeSE)}, title={Data Quality Management for Big Data Applications},
year={2019},
volume={},
number={},
pages={357-362},
abstract={Currently, as a result of the continuous increase of data, one of the key issues is the development of systems and applications to deal with storage, management and processing of big numbers of data. These data are found in unstructured ways. Data management with traditional approaches is inappropriate because of the large and complex data sizes. Hadoop is a suitable solution for the continuous increase in data sizes. The important characteristics of the Hadoop are distributed processing, high storage space, and easy administration. Hadoop is better known for distributed file systems. In this paper, we have proposed techniques and algorithms that deal with big data including data collecting, data preprocessing, algorithms for data cleaning, A Technique for Converting Unstructured Data to Structured Data using metadata, distributed data file system (fragmentation algorithm) and Quality assurance algorithms by using the model is the statistical model to evaluate the highest educational institutions. We concluded that Metadata accelerates query response required and facilitates query execution, metadata will be content for reports, fields and descriptions. Total time access for three complex queries in distributed processing it is 00: 03: 00 per second while in nondistributed processing it is at 00: 15: 77 per second, average is approximately five minutes per second. Quality assurance note values (T-test) is 0.239 and values (T-dis) is 1.96, as a result of dealing with scientific sets and humanities sets. In the comparison law, it can be deduced that if the t-test is smaller than the t-dis; so there is no difference between the mean of the scientific and humanities samples, the values of C.V for both scientific is (8.585) and humanities sets is (7.427), using the law of homogeneity know whether any sets are more homogeneous whenever the value of a small C.V was more homogeneous however the humanity set is more homogeneity.},
keywords={Distributed databases;Metadata;Big Data;Standards;File systems;Data integrity;Big Data;data quality;unstructured Data Distributed data file system;and statistical model.},
doi={10.1109/DeSE.2019.00072},
ISSN={2161-1351},
month={Oct},}
@INPROCEEDINGS{7374131,
author={Juddoo, Suraj},
booktitle={2015 International Conference on Computing, Communication and Security (ICCCS)}, title={Overview of data quality challenges in the context of Big Data},
year={2015},
volume={},
number={},
pages={1-9},
abstract={Data quality management systems are thoroughly researched topics and have resulted in many tools and techniques developed by both academia and industry. However, the advent of Big Data might pose some serious questions pertaining to the applicability of existing data quality concepts. There is a debate concerning the importance of data quality for Big Data; one school of thought argues that high data quality methods are essential for deriving higher level analytics while another school of thought argues that data quality level will not be so important as the volume of Big Data would be used to produce patterns and some amount of dirty data will not mask the analytic results which might be derived. This paper aims to investigate various components and activities forming part of data quality management such as dimensions, metrics, data quality rules, data profiling and data cleansing. The result list existing challenges and future research areas associated with Big Data for data quality management.},
keywords={Big data;Context;Quality management;Frequency measurement;Organizations;Big Data;Data quality;Data profiling;Data cleansing;data quality rules;dimensions;metrics},
doi={10.1109/CCCS.2015.7374131},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7347061,
author={Wenlu Yang and Da Silva, Alzennyr and Picard, Marie-Luce},
booktitle={2015 International Workshop on Computational Intelligence for Multimedia Understanding (IWCIM)}, title={Computing data quality indicators on Big Data streams using a CEP},
year={2015},
volume={},
number={},
pages={1-5},
abstract={Big Data is often referred to as the 3Vs: Volume, Velocity and Variety. A 4th V (validity) was introduced to address the quality dimension. Poor data quality can be costly, lead to breaks in processes and invalidate the company's efforts on regulatory compliance. In order to process data streams in real time, a new technology called CEP (complex event processing) was developed. In France, the current deployment of smart meters will generate massive electricity consumption data. In this work, we developed a diagnostic approach to compute generic quality indicators of smart meter data streams on the fly. This solution is based on Tibco StreamBase CEP. Visualization tools were also developed in order to give a better understanding of the inter-relation between quality issues and geographical/temporal dimensions. According to the application purpose, two visualization methods can be loaded: (1) StreamBase LiveView is used to visualize quality indicators in real time; and (2) a Web application provides a posteriori and geographical analysis of the quality indicators which are plotted on a map within a color scale (lighter colors indicate good quality and darker colors indicate poor quality). In future works, new quality indicators could be added to the solution which can be applied in an operational context in order to monitor data quality from smart meters.},
keywords={Smart meters;Data visualization;Smart grids;Real-time systems;Image color analysis;Indexes;Data quality;Big Data;data stream;CEP;smart grids},
doi={10.1109/IWCIM.2015.7347061},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9616552,
author={Li, Congli and Yang, Bin and Chen, Xuezhen and Zhang, Enjie and Huang, Hongjun and Li, Da},
booktitle={2021 International Conference on Wireless Communications and Smart Grid (ICWCSG)}, title={Research on smart grid big data amp;#x2019;s curve mean clustering algorithm for edge-cloud collaborative application},
year={2021},
volume={},
number={},
pages={395-398},
abstract={As the demand for smart grid construction increases, advanced power applications based on edge-cloud collaboration continue to increase. Among them, there are many data-driven artificial intelligence calculations and analyses, all of which are calculated and analyzed based on electric power big data. However, for the massive electric power big data, it is impossible to obtain more internally related information only by observing the data from the surface. To a certain extent, it directly affects the upper-level advanced applications. To solve this problem, this paper studies and proposes a curve-mean clustering algorithm for load big data, which is the most widely used load data in smart grid. By analyzing the advanced measurement infrastructure, the matrix low-rank property of load big data and the calculation of singular value, the curve mean clustering of load big data is realized, and the optimal determination method of cluster number is expounded. Experiments are conducted based on actual resident user load data and compared with the classic mean shift clustering algorithm. By calculating the average distance within the cluster, the average distance between clusters and the DI index, it is verified that the proposed method clustering is more accurate and the selection of cluster number is optimal. The research plays a very good role in basic analysis for improving the big data analysis capability and data quality of smart grid.},
keywords={Wireless communication;Low voltage;Data integrity;Urban areas;Clustering algorithms;Collaboration;Big Data;Smart grid big data;Data quality;Curve mean clustering Algorithm},
doi={10.1109/ICWCSG53609.2021.00085},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8942297,
author={Snineh, Sidi Mohamed and Bouattane, Omar and Youssfi, Mohamed and Daaif, Abdelaziz},
booktitle={2019 Third International Conference on Intelligent Computing in Data Sciences (ICDS)}, title={Towards a multi-agents model for errors detection and correction in big data flows},
year={2019},
volume={},
number={},
pages={1-5},
abstract={The quality of data for decision-making will always be a major factor for companies that want to remain competitors. In addition, the era of Big Data has brought new challenges for the processing, management, storage of data and in particular the challenge represented by the veracity of these data which is one of the 5Vs that characterizes Big Data. This characteristic that defines the quality or reliability of the data and its sources must be verified in the future systems of each company. In this paper, we present an approach that helps to improve the quality of Big Data by the distributed execution of algorithms for detecting and correcting data errors. The idea is to have a multi-agents model for errors detection and correction in big data flow. This model linked to a repository specific to each company. This repository contains the most frequent errors, metadata, error types, error detection algorithms and error correction algorithms. Each agent of this model represents an algorithm and will be deployed in multiple instances when needed. The use of these agents will go through two steps. In the first step, the detection agents and error correction agents manage each flow entering the system in real time. In the second step, all the processed data flows in first step will be a dataset to which the error detection and correction agents are applied in batch in order to process other types of errors. Among architectures who allow this processing type, we have chosen Lambda architecture.},
keywords={Big Data;Multi-agent systems;Companies;Task analysis;Error correction;Data integrity;Real-time systems;Multi-agents;Big Data;Data quality;detection and correction of data errors},
doi={10.1109/ICDS47004.2019.8942297},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8258267,
author={Catarci, Tiziana and Scannapieco, Monica and Console, Marco and Demetrescu, Camil},
booktitle={2017 IEEE International Conference on Big Data (Big Data)}, title={My (fair) big data},
year={2017},
volume={},
number={},
pages={2974-2979},
abstract={Policy making has the strict requirement to rely on quantitative and high quality information. This paper will address the data quality issue for policy making by showing how to deal with Big Data quality in the different steps of a processing pipeline, with a focus on the integration of Big Data sources with traditional sources. In this respect, a relevant role is played by metadata and in particular by ontologies. Integration systems relying on ontologies enable indeed a formal quality evaluation of inaccuracy, inconsistency and incompleteness of integrated data. The paper will finally describe data confidentiality as a Big Data quality dimension, showing the main issues to be faced for its assurance.},
keywords={Big Data;Pipelines;Ontologies;Metadata;Google;Quality-driven policies;Big Data pipeline;ontology-based quality checking;Big Data confidentiality},
doi={10.1109/BigData.2017.8258267},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8596389,
author={Ezzine, Imane and Benhlima, Laila},
booktitle={2018 IEEE 5th International Congress on Information Science and Technology (CiSt)}, title={A Study of Handling Missing Data Methods for Big Data},
year={2018},
volume={},
number={},
pages={498-501},
abstract={Improving data quality is not a recent field but in the context of big data this is a challenging area as there is a crucial need for data quality to, for example, increase the accuracy of big data analytics or avoid storing redundant data. Missing data is one of the major problem that faces the quality of data. There are several methods and approaches that have been used in relational databases to handle missing data most of which have been adapted to big data. This paper aims to provide an overview of some methods and approaches for handling missing data in big data contexts.},
keywords={Data integrity;Big Data;Data models;Dictionaries;Machine learning algorithms;Machine learning;Measurement;data quality;missing data;big data;functional dependancy;master data;machine learning},
doi={10.1109/CIST.2018.8596389},
ISSN={2327-1884},
month={Oct},}
@INPROCEEDINGS{7207219,
author={Taleb, Ikbal and Dssouli, Rachida and Serhani, Mohamed Adel},
booktitle={2015 IEEE International Congress on Big Data}, title={Big Data Pre-processing: A Quality Framework},
year={2015},
volume={},
number={},
pages={191-198},
abstract={With the abundance of raw data generated from various sources, Big Data has become a preeminent approach in acquiring, processing, and analyzing large amounts of heterogeneous data to derive valuable evidences. The size, speed, and formats in which data is generated and processed affect the overall quality of information. Therefore, Quality of Big Data (QBD) has become an important factor to ensure that the quality of data is maintained at all Big data processing phases. This paper addresses the QBD at the pre-processing phase, which includes sub-processes like cleansing, integration, filtering, and normalization. We propose a QBD model incorporating processes to support Data quality profile selection and adaptation. In addition, it tracks and registers on a data provenance repository the effect of every data transformation happened in the pre-processing phase. We evaluate the data quality selection module using large EEG dataset. The obtained results illustrate the importance of addressing QBD at an early phase of Big Data processing lifecycle since it significantly save on costs and perform accurate data analysis.},
keywords={Big data;Data integration;Accuracy;Distributed databases;Data analysis;Business;Big Data;Data Quality;pre-processing},
doi={10.1109/BigDataCongress.2015.35},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{7384166,
author={Chenran, Xiong and Youde, Wu},
booktitle={2015 International Conference on Intelligent Transportation, Big Data and Smart City}, title={The Geographic Environment Analysis of Regional Economic Development of Yunnan Province of China Based on the Big Data Technology},
year={2015},
volume={},
number={},
pages={869-872},
abstract={This paper uses the analysis methods of data mining, data quality and management, semantic engine and prediction in big data to analyze the geographic environment of the Yunnan's economic development from its special location and geographic elements, beholding that the geographic elements lay basic material conditions for its economic development of Yunnan province. Geographic factors on one hand encourage the economic development and on the other hand affect together the economic development of Yunnan province.},
keywords={Transportation;Big data;Smart cities;analysis of big data;Yunnan province of China;regional economic development;geographic environment},
doi={10.1109/ICITBS.2015.220},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9314391,
author={Faroukhi, Abou Zakaria and El Alaoui, Imane and Gahi, Youssef and Amine, Aouatif},
booktitle={2020 IEEE 2nd International Conference on Electronics, Control, Optimization and Computer Science (ICECOCS)}, title={Big Data Value Chain: A Unified Approach for Integrated Data Quality and Security},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Big Data has grown significantly in recent years. This growth has led organizations to adopt Big Data Value Chains (BDVC) as the appropriate framework for unlocking the value to make suitable decisions. Despite its promising opportunities, Big Data raises new concerns such as data quality and security that could radically impact the effectiveness of the BDVC. These two essential aspects have become an urgent need for any Big Data project to provide meaningful datasets and reliable insights. In this contribution, we highlight the importance of considering data quality and security requirements. Then, we propose a coherent, unified framework that extends BDVC with security and quality aspects. Through quality and security reports, the model can self-evaluate and arrange tasks according to orchestration and monitoring process, allowing the BDVC to evolve at the organization pace and to align strategically with its objectives as well as to federate a sustainable ecosystem.},
keywords={Big Data;Security;Data integrity;Data models;Organizations;Decision making;Reliability;Big Data Value Chain;Data Management;Data Quality;Data Security;Process Integration;Orchestration},
doi={10.1109/ICECOCS50124.2020.9314391},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7840935,
author={Haug, Frank S.},
booktitle={2016 IEEE International Conference on Big Data (Big Data)}, title={Bad big data science},
year={2016},
volume={},
number={},
pages={2863-2871},
abstract={As hardware and software technologies have improved, our definition of a “manageable amount of data” has increased in its scope dramatically. The term “big data” can be applied to any of several different projects and technologies sharing the ultimate goal of supporting analysis on these large, heterogeneous, and evolving data sets. The term “data science” refers to the statistical, technical, and domain-specific knowledge required to ensure that the analysis is done properly. Techniques for managing some common causes for bad data and invalid analysis have been used in other areas, such as data warehousing and distributed database. However, big data projects face special challenges when trying to combine big data and data science without producing inaccurate, misleading, or invalid results. This paper discusses potential causes for “bad big data science”, focusing primarily on the data quality of the input data, and suggests methods for minimizing them based on techniques originally developed for data warehousing and distributed database projects.},
keywords={Big data;Metadata;Distributed databases;Stakeholders;Data science;Big Data;Data Quality;Data Warehousing;Distributed Database;Metadata},
doi={10.1109/BigData.2016.7840935},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8457745,
author={Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida},
booktitle={2018 IEEE International Congress on Big Data (BigData Congress)}, title={Big Data Quality: A Survey},
year={2018},
volume={},
number={},
pages={166-173},
abstract={With the advances in communication technologies and the high amount of data generated, collected, and stored, it becomes crucial to manage the quality of this data deluge in an efficient and cost-effective way. The storage, processing, privacy and analytics are the main keys challenging aspects of Big Data that require quality evaluation and monitoring. Quality has been recognized by the Big Data community as an essential facet of its maturity. Yet, it is a crucial practice that should be implemented at the earlier stages of its lifecycle and progressively applied across the other key processes. The earlier we incorporate quality the full benefit we can get from insights. In this paper, we first identify the key challenges that necessitates quality evaluation. We then survey, classify and discuss the most recent work on Big Data management. Consequently, we propose an across-the-board quality management framework describing the key quality evaluation practices to be conducted through the different Big Data stages. The framework can be used to leverage the quality management and to provide a roadmap for Data scientists to better understand quality practices and highlight the importance of managing the quality. We finally, conclude the paper and point to some future research directions on quality of Big Data.},
keywords={Big Data;Data integrity;Quality management;Data visualization;Databases;Sensors;Social network services;Big Data, Data Quality, Quality Management framework, Quality of Big Data},
doi={10.1109/BigDataCongress.2018.00029},
ISSN={},
month={July},}
@INPROCEEDINGS{6755349,
author={Freitas, Patrícia Alves de and Reis, Everson Andrade dos and Michel, Wanderson Senra and Gronovicz, Mauro Edson and Rodrigues, Márcio Alexandre de Macedo},
booktitle={2013 IEEE 16th International Conference on Computational Science and Engineering}, title={Information Governance, Big Data and Data Quality},
year={2013},
volume={},
number={},
pages={1142-1143},
abstract={The value of information as a competitive differential has been taken into consideration in companies all over the world for some time already. In recent years, there has been heated debate about some terms originated from new concepts related to information, such as big data, due to the promise that such topic might revolutionise world trade. Hence, data and information governance and quality have been increasingly discussed in the business world.},
keywords={Companies;Information management;Data handling;Data storage systems;Computer architecture;Information Governance;Big Data;Data Quality},
doi={10.1109/CSE.2013.168},
ISSN={},
month={Dec},}
@ARTICLE{8667300,
author={Lee, Doyoung},
journal={IEEE Access}, title={Big Data Quality Assurance Through Data Traceability: A Case Study of the National Standard Reference Data Program of Korea},
year={2019},
volume={7},
number={},
pages={36294-36299},
abstract={In the era of big data, the scientific and social demand for quality data is aggressive and urgent. This paper sheds light on the expanded role of metrology of verifying validated procedures of data production and developing adequate uncertainty evaluation methods to ensure the trustworthiness of data and information. In this regard, I explore the mechanism of the national standard reference data (SRD) program of Korea, which connects various scientific and social sectors to metrology by applying useful metrological concepts and methods to produce reliable data and convert such data into national standards. In particular, the changing interpretation of metrological key concepts, such as “measurement,” “traceability,” and “uncertainty,” will be explored and reconsidered from the perspective of data quality assurance. As a result, I suggest the concept of “data traceability” with “the matrix of data quality evaluation” according to the elements of a data production system and related evaluation criteria. To conclude, I suggest social and policy implications for the new role of metrology and standards for producing and disseminating reliable knowledge sources from big data.},
keywords={Standards;Uncertainty;Big Data;Metrology;Reliability;Measurement uncertainty;Biomedical measurement;Big data;data quality;data traceability;metrology;standard reference data;uncertainty},
doi={10.1109/ACCESS.2019.2904286},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8276745,
author={Micic, Natasha and Neagu, Daniel and Campean, Felician and Zadeh, Esmaeil Habib},
booktitle={2017 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)}, title={Towards a Data Quality Framework for Heterogeneous Data},
year={2017},
volume={},
number={},
pages={155-162},
abstract={Every industry has significant data output as a product of their working process, and with the recent advent of big data mining and integrated data warehousing it is the case for a robust methodology for assessing the quality for sustainable and consistent processing. In this paper a review is conducted on Data Quality (DQ) in multiple domains in order to propose connections between their methodologies. This critical review suggests that within the process of DQ assessment of heterogeneous data sets, not often are they treated as separate types of data in need of an alternate data quality assessment framework. We discuss the need for such a directed DQ framework and the opportunities that are foreseen in this research area and propose to address it through degrees of heterogeneity.},
keywords={Data integrity;Measurement;Warranties;Metadata;Cleaning;Big Data;Heterogeneous Data Sets;Data Quality;Metadata;Data Cleaning;Data Quality Assessment},
doi={10.1109/iThings-GreenCom-CPSCom-SmartData.2017.28},
ISSN={},
month={June},}
@INPROCEEDINGS{8416208,
author={Blanquer, Ignacio and Meira, Wagner},
booktitle={2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)}, title={EUBra-BIGSEA, A Cloud-Centric Big Data Scientific Research Platform},
year={2018},
volume={},
number={},
pages={47-48},
abstract={This paper describes the achievements of project EUBra-BIGSEA, which has delivered programming models and data analytics tools for the development of distributed Big Data applications. As framework components, multiple data models are supported (e.g. data streams, multidimensional data, etc.) and efficient mechanisms to ensure privacy and security, on top of a QoS-aware layer for the smart and rapid provisioning of resources in a cloud-based environment.},
keywords={Big Data;Data analysis;Data privacy;Quality of service;Data models;Security;Biological system modeling;Cloud-Computing,-Big-Data,-Quality-of-Service,-Data-Analytics},
doi={10.1109/DSN-W.2018.00023},
ISSN={2325-6664},
month={June},}
@INPROCEEDINGS{9297009,
author={Juddoo, Suraj and George, Carlisle},
booktitle={2020 3rd International Conference on Emerging Trends in Electrical, Electronic and Communications Engineering (ELECOM)}, title={A Qualitative Assessment of Machine Learning Support for Detecting Data Completeness and Accuracy Issues to Improve Data Analytics in Big Data for the Healthcare Industry},
year={2020},
volume={},
number={},
pages={58-66},
abstract={Tackling Data Quality issues as part of Big Data can be challenging. For data cleansing activities, manual methods are not efficient due to the potentially very large amount of data.. This paper aims to qualitatively assess the possibilities for using machine learning in the process of detecting data incompleteness and inaccuracy, since these two data quality dimensions were found to be the most significant by a previous research study conducted by the authors. A review of existing literature concludes that there is no unique machine learning algorithm most suitable to deal with both incompleteness and inaccuracy of data. Various algorithms are selected from existing studies and applied against a representative big (healthcare) dataset. Following experiments, it was also discovered that the implementation of machine learning algorithms in this context encounters several challenges for Big Data quality activities. These challenges are related to the amount of data particualar machine learning algorithms can scale to and also to certain data type restrictions imposed by some machine learning algorithms. The study concludes that 1) data imputation works better with linear regression models, 2) clustering models are more efficient to detect outliers but fully automated systems may not be realistic in this context. Therefore, a certain level of human judgement is still needed.},
keywords={Industries;Machine learning algorithms;Data integrity;Clustering algorithms;Medical services;Big Data;Tools;Big Data;Data Quality;Data Inaccuracy;Data incompleteness;Machine Learning},
doi={10.1109/ELECOM49001.2020.9297009},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8029373,
author={Zhou, Lixiao and Huang, Maohai},
booktitle={2017 IEEE International Congress on Big Data (BigData Congress)}, title={Challenges of Software Testing for Astronomical Big Data},
year={2017},
volume={},
number={},
pages={529-532},
abstract={Astronomy has been one of the first areas of science to embrace and learn from big data. The amount of data we have on our universe is doubling every year thanks to big telescopes and better light detectors. Most leading research is based on data from a handful of very expensive telescopes. Undoubtedly, the data quality is the key basis for the leading scientific findings. It is imperative that software testers understand that big data is about far more than simply data volume. This paper will analyze characteristics, types, methods, strategies, problems, challenges and propose some possible solutions of software testing for astronomical big data.},
keywords={Big Data;Software testing;Software;Astronomy;Data analysis;Distributed databases;software testing; big data; astronomical software},
doi={10.1109/BigDataCongress.2017.91},
ISSN={},
month={June},}
@INPROCEEDINGS{9529590,
author={Bhardwaj, Dave and Ormandjieva, Olga},
booktitle={2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)}, title={Toward a Novel Measurement Framework for Big Data (MEGA)},
year={2021},
volume={},
number={},
pages={1579-1586},
abstract={Big Data is quickly becoming a chief part of the decision-making process in both industry and academia. As more and more institutions begin relying on Big Data to make strategic decisions, the quality of the underlying data comes into question. The quality of Big Data isn’t always transparent and large-scale systems may even lack its visibility, which adversely affects the credibility of the Big Data systems. Continuous monitoring and measurement of data quality is therefore paramount in assessing whether the information can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses the need for Big Data quality measurement modeling and automation by proposing a novel conceptual quality measurement framework for Big Data (MEGA) with the purpose of assessing the underlying quality characteristics of Big Data (also known as the V’s of Big Data) at each step of the Big Data Pipelines. The theoretical quality measurement models for four of the Big Data V’s (Volume, Variety, Velocity, Veracity) are currently automated; the remaining 6 V’s (Vincularity, Validity, Value, Volatility, Valence and Vitality) will be tackled in our future work. The approach is illustrated on a case study.},
keywords={Solid modeling;Data integrity;Volume measurement;Pipelines;Big Data;Data models;Software;Big Data quality characteristics;The V’s;Big Data Quality Measurement Framework;Big Data Pipelines},
doi={10.1109/COMPSAC51774.2021.00235},
ISSN={0730-3157},
month={July},}
@INPROCEEDINGS{8258221,
author={Hee, Kim},
booktitle={2017 IEEE International Conference on Big Data (Big Data)}, title={Is data quality enough for a clinical decision?: Apply machine learning and avoid bias},
year={2017},
volume={},
number={},
pages={2612-2619},
abstract={This paper provides a practical guideline for the assurance and (re-)usage of clinical data. It proposes a process which aims to provide a systematic data quality assurance even without involving a medical domain expert. Especially when (re-)using clinical data, data quality is an important topic because clinical data are not purposely collected. Therefore, data driven conclusions might be false, because a given dataset is not representative. These false data driven conclusions could even harm the life of patients. Thus, all researchers should adhere to some basic principles that can prevent false conclusions. Twelve empirical experiments were conducted in order to prove that my process is able to assure data quality with respect to the descriptive and predictive analysis. Descriptive results obtained by applying stratified sampling are conflicting in four out of nine population inputs. Sampling is carried based on the top ranked feature drawn by the Contextual Data Quality Assurance (CDQA). Between datasets these features are confirmed by the Mutual Data Quality Assurance (MDQA). Stratified sampled inputs improve predictive results compared to raw data. Both Area Under the Curve (AUC) scores and accuracy scores increase by three percent.},
keywords={Data mining;Medical diagnostic imaging;Currencies;Tools;Medical services;Measurement;data quality;decision quality;healthcare;clinical data;EHRs;data quality assurance;stratified sampling;bias},
doi={10.1109/BigData.2017.8258221},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9134140,
author={Gan, Wenting},
booktitle={2020 International Conference on E-Commerce and Internet Technology (ECIT)}, title={Design of Network Precision Marketing Based on Big Data Analysis Technology},
year={2020},
volume={},
number={},
pages={77-81},
abstract={In the process of big data processing, big data analysis is the core work content. After obtaining a large amount of data, we use related analysis technology to perform data processing and analysis to obtain knowledge. Its related content includes visual analysis, data mining, predictive analysis, semantic analysis and data quality management. How to obtain big data, classify and store according to data types, mine valuable information from big data, and effectively apply big data in the field of precision marketing are hot topics of research. On the basis of researching the key technologies of big data analysis, this paper uses Hadoop big data to analyze and store the massive online logs generated by users' mobile terminals, and calculates and builds user characteristic databases. We use relevant analysis technology to analyze the user's location information, browsing and usage habits, hobbies, and focus content. At the same time, a precise marketing model is established according to user behavior characteristics and attributes, thereby improving the marketing effect of the enterprise.},
keywords={Visualization;Data analysis;Databases;Data integrity;Semantics;Big Data;Internet;Big Data;E-commerce;Internet marketing;Marketing design},
doi={10.1109/ECIT50008.2020.00026},
ISSN={},
month={April},}
@INPROCEEDINGS{9391025,
author={Wang, Fengling and Wang, Han and Xue, Liang},
booktitle={2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, title={Research on Data Security in Big Data Cloud Computing Environment},
year={2021},
volume={5},
number={},
pages={1446-1450},
abstract={In the big data cloud computing environment, data security issues have become a focus of attention. This paper delivers an overview of conceptions, characteristics and advanced technologies for big data cloud computing. Security issues of data quality and privacy control are elaborated pertaining to data access, data isolation, data integrity, data destruction, data transmission and data sharing. Eventually, a virtualization architecture and related strategies are proposed to against threats and enhance the data security in big data cloud environment.},
keywords={Cloud computing;Data security;Data integrity;Big Data;Maintenance engineering;Virtualization;Information technology;big data;cloud computing;data security;big data cloud computing;security policy},
doi={10.1109/IAEAC50856.2021.9391025},
ISSN={2689-6621},
month={March},}
@INPROCEEDINGS{8787092,
author={Guan, Zhibin and Ji, Tongkai and Qian, Xu and Ma, Yan and Hong, Xuehai},
booktitle={2017 5th Intl Conf on Applied Computing and Information Technology/4th Intl Conf on Computational Science/Intelligence and Applied Informatics/2nd Intl Conf on Big Data, Cloud Computing, Data Science (ACIT-CSII-BCD)}, title={A Survey on Big Data Pre-processing},
year={2017},
volume={},
number={},
pages={241-247},
abstract={In this paper, we briefly introduce some basic concepts and characteristics of big data. We are surrounded by massive amount of data but starving for knowledge. In the era of Big Data, how to quickly obtain high-quality and valuable information from massive amounts of data has become an important research direction. Hence, we focus our attention to the data pre-processing which is a sub-content of the data processing workflow. In this paper, the four phases of data pre-processing, including data cleansing, data integration, data reduction, and data transformation, have been discussed. And different approaches for a variety of purposes have been presented, which show current methods and techniques need to be further modified in order to improve the quality of data before data analysis.},
keywords={Big Data;Data integration;Data analysis;Dimensionality reduction;Feature extraction;Data integrity;big data processing;data pre-processing;data cleansing;dimension reduction;data quality},
doi={10.1109/ACIT-CSII-BCD.2017.49},
ISSN={},
month={July},}
@INPROCEEDINGS{8621924,
author={Guntupally, Kavya and Devarakonda, Ranjeet and Kehoe, Kenneth},
booktitle={2018 IEEE International Conference on Big Data (Big Data)}, title={Spring Boot based REST API to Improve Data Quality Report Generation for Big Scientific Data: ARM Data Center Example},
year={2018},
volume={},
number={},
pages={5328-5329},
abstract={Web application technologies are growing rapidly with continuous innovation and improvements. This paper focuses on the popular Spring Boot [1] java-based framework for building web and enterprise applications and how it provides the flexibility for service-oriented architecture (SOA). One challenge with any Spring-based applications is its level of complexity with configurations. Spring Boot makes it easy to create and deploy stand-alone, production-grade Spring applications with very little Spring configuration. Example, if we consider Spring Model-View-Controller (MVC) framework [2], we need to configure dispatcher servlet, web jars, a view resolver, and component scan among other things. To solve this, Spring Boot provides several Auto Configuration options to setup the application with any needed dependencies. Another challenge is to identify the framework dependencies and associated library versions required to develop a web application. Spring Boot offers simpler dependency management by using a comprehensive, but flexible, framework and the associated libraries in one single dependency, which provides all the Spring related technology that you need for starter projects as compared to CRUD web applications. This framework provides a range of additional features that are common across many projects such as embedded server, security, metrics, health checks, and externalized configuration. Web applications are generally packaged as war and deployed to a web server, but Spring Boot application can be packaged either as war or jar file, which allows to run the application without the need to install and/or configure on the application server. In this paper, we discuss how Atmospheric Radiation Measurement (ARM) Data Center (ADC) at Oak Ridge National Laboratory, is using Spring Boot to create a SOA based REST [4] service API, that bridges the gap between frontend user interfaces and backend database. Using this REST service API, ARM scientists are now able to submit reports via a user form or a command line interface, which captures the same data quality or other important information about ARM data.},
keywords={Springs;Databases;Service-oriented architecture;Data integrity;Tools;Servers;Big Data;auto configuration;CRUD;java framework;service-oriented architecture;REST;spring boot},
doi={10.1109/BigData.2018.8621924},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7498367,
author={Sadiq, Shazia and Papotti, Paolo},
booktitle={2016 IEEE 32nd International Conference on Data Engineering (ICDE)}, title={Big data quality - whose problem is it?},
year={2016},
volume={},
number={},
pages={1446-1447},
abstract={The increased reliance on data driven enterprise has seen an unprecedented investment in big data initiatives. Organizations averaged US$8M in investments in big data-related initiatives and programs in 2014, with 70% of large enterprises and 56% of small and medium enterprises (SMEs) having already deployed, or planning to deploy, big-data projects [1]. As companies intensify their efforts to get value from big data, the growth in the amount of data being managed continues at an exponential rate, leaving organizations with a massive footprint of unexplored, unfamiliar datasets. On February 8th, 2015, a group of global thought leaders from the database research community outlined the grand challenges in getting value from big data [2]. The key message was the need to develop the capacity to `understand how the quality of data affects the quality of the insight we derive from it'.},
keywords={Big data;Cleaning;Computer science;Databases;Investment},
doi={10.1109/ICDE.2016.7498367},
ISSN={},
month={May},}
@INPROCEEDINGS{7363865,
author={Rettig, Laura and Khayati, Mourad and Cudré-Mauroux, Philippe and Piórkowski, Michal},
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, title={Online anomaly detection over Big Data streams},
year={2015},
volume={},
number={},
pages={1113-1122},
abstract={Data quality is a challenging problem in many real world application domains. While a lot of attention has been given to detect anomalies for data at rest, detecting anomalies for streaming applications still largely remains an open problem. For applications involving several data streams, the challenge of detecting anomalies has become harder over time, as data can dynamically evolve in subtle ways following changes in the underlying infrastructure. In this paper, we describe and empirically evaluate an online anomaly detection pipeline that satisfies two key conditions: generality and scalability. Our technique works on numerical data as well as on categorical data and makes no assumption on the underlying data distributions. We implement two metrics, relative entropy and Pearson correlation, to dynamically detect anomalies. The two metrics we use provide an efficient and effective detection of anomalies over high velocity streams of events. In the following, we describe the design and implementation of our approach in a Big Data scenario using state-of-the-art streaming components. Specifically, we build on Kafka queues and Spark Streaming for realizing our approach while satisfying the generality and scalability requirements given above. We show how a combination of the two metrics we put forward can be applied to detect several types of anomalies - like infrastructure failures, hardware misconfiguration or user-driven anomalies - in large-scale telecommunication networks. We also discuss the merits and limitations of the resulting architecture and empirically evaluate its scalability on a real deployment over live streams capturing events from millions of mobile devices.},
keywords={Entropy;Measurement;Correlation;Big data;Data structures;Yttrium;Sparks},
doi={10.1109/BigData.2015.7363865},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9678209,
author={Yalaoui, Mehdi and Boukhedouma, Saida},
booktitle={2021 International Conference on Information Systems and Advanced Technologies (ICISAT)}, title={A survey on data quality: principles, taxonomies and comparison of approaches},
year={2021},
volume={},
number={},
pages={1-9},
abstract={Nowadays, data generation keeps increasing exponentially due to the emergence of the Internet of Things (IoT) and Big data technologies. The manipulation of such Big amount of data becomes more and more difficult because of its size and its variety. For better governance of organizations (decision making, data analysis, earnings increase …), data quality and data governance at present of Big data are two major pillars for the design of any system handling data within the organization. This explains the number of researches conducted as it constitutes a research subject with several gaps and opportunities. Many works were conducted to define and standardize Data Quality (DQ) and its dimensions, others were directed to design and propose data quality assessment and improvement models or frameworks. This work aims to recall the data quality principles starting by the needed background knowledge, then identify and compare the relevant taxonomies existing in the literature, next surveys and compares the available Data quality assessment and improvement approaches. After that, we propose a metamodel highlighting the main concepts of DQ assessment and we describe a generic process for DQ assessment and improvement. Finally, we evoke the main challenges in the field of DQ before and after the emergence of Big Data.},
keywords={Data integrity;Taxonomy;Standards organizations;Decision making;Organizations;Big Data;Data models;Data Quality;Big Data;Quality Dimensions;Quality Metrics;Metamodel;Assessment process;Improvement},
doi={10.1109/ICISAT54145.2021.9678209},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7944952,
author={Xie, Chunli and Gao, Jerry and Tao, Chuanqi},
booktitle={2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)}, title={Big Data Validation Case Study},
year={2017},
volume={},
number={},
pages={281-286},
abstract={With the advent of big data, data is being generated, collected, transformed, processed and analyzed at an unprecedented scale. Since data is created at a fast velocity and with a large variety, the quality of big data is far from perfect. Recent studies have shown that poor quality can bring serious erroneous data costs on the result of big data analysis. Data validation is an important process to recognize and improve data quality. In this paper, a case study that is relevant to big data quality is designed to study original big data quality, data quality dimension, data validation process and tools.},
keywords={Big Data;Tools;Databases;Electronic mail;Reliability;Temperature measurement;Null value;big data quality;big data validation;data checklist;big data tool;case study},
doi={10.1109/BigDataService.2017.44},
ISSN={},
month={April},}
@INPROCEEDINGS{9671672,
author={Poon, Lex and Farshidi, Siamak and Li, Na and Zhao, Zhiming},
booktitle={2021 IEEE International Conference on Big Data (Big Data)}, title={Unsupervised Anomaly Detection in Data Quality Control},
year={2021},
volume={},
number={},
pages={2327-2336},
abstract={Data is one of the most valuable assets of an organization and has a tremendous impact on its long-term success and decision-making processes. Typically, organizational data error and outlier detection processes perform manually and reactively, making them time-consuming and prone to human errors. Additionally, rich data types, unlabeled data, and increased volume have made such data more complex. Accordingly, an automated anomaly detection approach is required to improve data management and quality control processes. This study introduces an unsupervised anomaly detection approach based on models comparison, consensus learning, and a combination of rules of thumb with iterative hyper-parameter tuning to increase data quality. Furthermore, a domain expert is considered a human in the loop to evaluate and check the data quality and to judge the output of the unsupervised model. An experiment has been conducted to assess the proposed approach in the context of a case study. The experiment results confirm that the proposed approach can improve the quality of organizational data and facilitate anomaly detection processes.},
keywords={Data integrity;Decision making;Process control;Quality control;Organizations;Big Data;Data models;data quality control;data quality assessment;unsupervised learning;anomaly detection;automated data quality control},
doi={10.1109/BigData52589.2021.9671672},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9172875,
author={Zhang, Guobao},
booktitle={2020 IEEE Fifth International Conference on Data Science in Cyberspace (DSC)}, title={A data traceability method to improve data quality in a big data environment},
year={2020},
volume={},
number={},
pages={290-294},
abstract={In the actual project of data sharing and data governance, the problems of data heterogeneity and low data quality have not been solved. Data quality detection based on syntax rules can effectively find data quality problems, but it had not improved the data quality, especially in a variety of heterogeneous data source environments. This paper designs a data governance model based on data traceability, and we can get data feedback and revision through this model. The proposed method considers the different ownership of data and may form a closed-loop data service chain including effective data validation, data tracking, and data revision, data release. The analysis of the case shows that the method is effective to improve the data quality and meet the requirements of data security also.},
keywords={Data Governance;Data Credibility;Data Traceability},
doi={10.1109/DSC50466.2020.00051},
ISSN={},
month={July},}
@INPROCEEDINGS{7363987,
author={Priebe, Torsten and Markus, Stefan},
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, title={Business information modeling: A methodology for data-intensive projects, data science and big data governance},
year={2015},
volume={},
number={},
pages={2056-2065},
abstract={This paper discusses an integrated methodology to structure and formalize business requirements in large data-intensive projects, e.g. data warehouses implementations, turning them into precise and unambiguous data definitions suitable to facilitate harmonization and assignment of data governance responsibilities. We place a business information model in the center - used end-to-end from analysis, design, development, testing to data quality checks by data stewards. In addition, we show that the approach is suitable beyond traditional data warehouse environments, applying it also to big data landscapes and data science initiatives - where business requirements analysis is often neglected. As proper tool support has turned out to be inevitable in many real-world settings, we also discuss software requirements and their implementation in the Accurity Glossary tool. The approach is evaluated based on a large banking data warehouse project the authors are currently involved in.},
keywords={Business;Data models;Terminology;Big data;Data warehouses;Wheels;Standards organizations;Data Modeling;Project Methodology;Data Governance;Metadata;Information Catalog},
doi={10.1109/BigData.2015.7363987},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7584971,
author={Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal and Nujum, Alramzana},
booktitle={2016 IEEE International Congress on Big Data (BigData Congress)}, title={An Hybrid Approach to Quality Evaluation across Big Data Value Chain},
year={2016},
volume={},
number={},
pages={418-425},
abstract={While the potential benefits of Big Data adoption are significant, and some initial successes have already been realized, there remain many research and technical challenges that must be addressed to fully realize this potential. The Big Data processing, storage and analytics, of course, are major challenges that are most easily recognized. However, there are additional challenges related for instance to Big Data collection, integration, and quality enforcement. This paper proposes a hybrid approach to Big Data quality evaluation across the Big Data value chain. It consists of assessing first the quality of Big Data itself, which involve processes such as cleansing, filtering and approximation. Then, assessing the quality of process handling this Big Data, which involve for example processing and analytics process. We conduct a set of experiments to evaluate Quality of Data prior and after its pre-processing, and the Quality of the pre-processing and processing on a large dataset. Quality metrics have been measured to access three Big Data quality dimensions: accuracy, completeness, and consistency. The results proved that combination of data-driven and process-driven quality evaluation lead to improved quality enforcement across the Big Data value chain. Hence, we recorded high prediction accuracy and low processing time after we evaluate 6 well-known classification algorithms as part of processing and analytics phase of Big Data value chain.},
keywords={Big data;Metadata;Measurement;Quality assessment;Quality of service;Unified modeling language;Big Data;Quality assessment;Metadata;Quality metrics;quality Metadata;Quality of process;Hybrid quality assessment},
doi={10.1109/BigDataCongress.2016.65},
ISSN={},
month={June},}
@INPROCEEDINGS{8258380,
author={Fu, Qian and Easton, John M.},
booktitle={2017 IEEE International Conference on Big Data (Big Data)}, title={Understanding data quality: Ensuring data quality by design in the rail industry},
year={2017},
volume={},
number={},
pages={3792-3799},
abstract={The railways worldwide are increasingly looking to the integration of their data resources coupled with advanced analytics to enhance traffic management, to provide new insights on the health of infrastructure assets, to provide soft linkages to other transport modes, and ultimately to enable them to better serve their customers. As in many industrial sectors, over the past decade the rail industry has been investing heavily in sensing technologies that record every aspect of the operation of the railway network. However, as any data scientist knows, it does not matter how good an algorithm is, if you put rubbish in, you get rubbish out; and as the traditional industry model of working with data only within the system that it was collected by becomes increasingly fragile, the industry is discovering that it knows less than it thought about the data it is gathering. When coupled with legacy data resources of unknown accuracy, such as design diagrams for assets that in many cases are decades old, the rail industry now faces a crisis in which its data may become essentially worthless due to a poor understanding of the quality of its data. This paper reports the findings of the first phase of a three-phase systematic review of literature about how data quality can be managed and evaluated in the rail domain. It begins by discussing why data quality matters in a rail context, before going on to define the quality, introduce and expand the concept of a data quality schema.},
keywords={Industries;Rails;Data models;Rail transportation;Systematics;Decision making;data quality;rail;quality by design;data quality schema},
doi={10.1109/BigData.2017.8258380},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8257913,
author={Benbernou, Salima and Ouziri, Mourad},
booktitle={2017 IEEE International Conference on Big Data (Big Data)}, title={Enhancing data quality by cleaning inconsistent big RDF data},
year={2017},
volume={},
number={},
pages={74-79},
abstract={We address the problem of dealing with inconsistencies in fusion of big data sources using Resource Description Framework (RDF) and ontologies. We propose a scalable approach ensuring data quality for query answering over big RDF data in a distributed way on a Spark ecosystem. In so doing, the cleaning inconsistent big RDF data approach is built on the following steps (1) modeling consistency rules to detect the inconsistency triples even if it is implicitly hidden including inference and inconsistent rules (2) detecting inconsistency through rule evaluation based on Apache Spark framework to discover the minimally sub-set of inconsistent triples (3) cleaning the inconsistency through finding the best repair for consistent query answering.},
keywords={Resource description framework;Ontologies;Sparks;Cleaning;Inference algorithms;Automobiles;Jacobian matrices;Big data;Data quality;Inconsistency;Logical inference;RDF data cleaning;Apache Spark ecosystems},
doi={10.1109/BigData.2017.8257913},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7502278,
author={Shanmugam, Srinivasan and Seshadri, Gokul},
booktitle={2016 IEEE 2nd International Conference on Big Data Security on Cloud (BigDataSecurity), IEEE International Conference on High Performance and Smart Computing (HPSC), and IEEE International Conference on Intelligent Data and Security (IDS)}, title={Aspects of Data Cataloguing for Enterprise Data Platforms},
year={2016},
volume={},
number={},
pages={134-139},
abstract={As the adoption of enterprise Big Data platforms mature, the necessity to maintain systematic catalogue of data being processed and managed by these platforms becomes imperative. Enterprise data catalogues serve as centralized repositories of storing such metadata about data being handled by such platforms, enabling better data governance, security and control. Standardized approaches, methodologies and tools for data catalogues are being discussed and evolved. This paper introduces some key variables, attributes and indexes that need to be handled in such cataloguing solutions -- such as data contexts, data-system relationships, data quality, reliability, sensitivity and accessibility. The paper also discusses specific approaches on how each of these aspects can be adopted and applied to different enterprise contexts effectively.},
keywords={Context;Business;Metadata;Big data;Electronic mail;Reliability;Indexes;Enterprise data;Data catalogue;Metadata of data;Data context;Data quality;Data governance;Data as a service},
doi={10.1109/BigDataSecurity-HPSC-IDS.2016.52},
ISSN={},
month={April},}
@INPROCEEDINGS{9006446,
author={Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Kahn, Michael G},
booktitle={2019 IEEE International Conference on Big Data (Big Data)}, title={An Interactive Data Quality Test Approach for Constraint Discovery and Fault Detection},
year={2019},
volume={},
number={},
pages={200-205},
abstract={Data quality tests validate heterogeneous data to detect violations of syntactic and semantic constraints. The specification of these constraints can be incomplete because domain experts typically specify them in an ad hoc manner. Existing automated test approaches can generate false alarms and do not explain the constraint violations while reporting faulty data records. In previous work, we proposed ADQuaTe, which is an automated data quality test approach that uses an unsupervised deep learning techni que (1) to discover constraints from big datasets that may have been missed by experts, and (2) to label as suspicious those records that violate the constraints. These records are grouped and explanations for constraint violations are presented to domain experts who determine whether or not the groups are actually faulty. This paper presents ADQuaTe2, which extends ADQuaTe to use an interactive learning technique that incorporates expert feedback to retrain the learning model and improve the accuracy of constraint discovery and fault detection. We evaluate the effectiveness of the approach on real-world datasets from a health data warehouse and a plant diagnosis database. We also use datasets with known faults from the UCI repository to evaluate the improvement in the accuracy of the approach after incorporating ground truth knowledge.},
keywords={Fault detection;Data integrity;Data models;Semantics;Decision trees;Self-organizing feature maps;Inspection;Big Data;Data quality tests;Explainable learning;Interactive learning;Unsupervised learning},
doi={10.1109/BigData47090.2019.9006446},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7474375,
author={Ciancarini, Paolo and Poggi, Francesco and Russo, Daniel},
booktitle={2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService)}, title={Big Data Quality: A Roadmap for Open Data},
year={2016},
volume={},
number={},
pages={210-215},
abstract={Open Data (OD) is one of the most discussed issue of Big Data which raised the joint interest of public institutions, citizens and private companies since 2009. However, the massive amount of freely available data has not yet brought the expected effects: as of today, there is no application that has fully exploited the potential provided by large and distributed information sources in a non-trivial way, nor any service has substantially changed for the better the lives of people. The era of a new generation applications based on OD is far to come. In this context, we observe that OD quality is one of the major threats to achieving the goals of the OD movement. The starting point of this case study is the quality of the OD released by the five Constitutional offices of Italy. Our exploratory case study aims to assess the quality of such releases and the real implementations of OD. The outcome suggests the need of a drastic improvement in OD quality. Finally we highlight some key quality principles for OD, and propose a roadmap for further research.},
keywords={Big data;Metadata;Government;ISO Standards;Distributed databases;Open Data Quality;Information Modeling;E-Government;Big Data Knowledge Extraction},
doi={10.1109/BigDataService.2016.37},
ISSN={},
month={March},}
@ARTICLE{7974765,
author={Khalfi, Besma and de Runz, Cyril and Faiz, Sami and Akdag, Herman},
journal={IEEE Transactions on Big Data}, title={A New Methodology for Storing Consistent Fuzzy Geospatial Data in Big Data Environment},
year={2021},
volume={7},
number={2},
pages={468-482},
abstract={In this era of big data, as relational databases are inefficient, NoSQL databases are a workable solution for data storage. In this context, one of the key issues is the veracity and therefore the data quality. Indeed, as with classic data, geospatial big data are generally fuzzy even though they are stored as crisp data (perfect data). Hence, if data are geospatial and fuzzy, additional complexities appear because of the complex syntax and semantic features of such data. The NoSQL databases do not offer strict data consistency. Therefore, new challenges are needed to be overcome to develop efficient methods that simultaneously ensure the performance and the consistency in storing fuzzy geospatial big data. This paper presents a new methodology that tackles the storage issues and validates the fuzzy spatial entities' consistency in a document-based NoSQL system. Consequently, first, to better express the structure of fuzzy geospatial data in such a system, we present a logical model called Fuzzy GeoJSON schema. Second, for consistent storage, we implement a schema-driven pipeline based on the Fuzzy GeoJSON schema and semantic constraints.},
keywords={Fuzzy sets;Geospatial analysis;Big Data;Data models;NoSQL databases;Spatial databases;Fuzzy set theory;Spatial databases;data storage representations;schema and subschema;fuzzy set;imprecision;consistency;big data;NoSQL systems;JSON},
doi={10.1109/TBDATA.2017.2725904},
ISSN={2332-7790},
month={June},}
@INPROCEEDINGS{7473058,
author={Gao, Jerry and Xie, Chunli and Tao, Chuanqi},
booktitle={2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)}, title={Big Data Validation and Quality Assurance -- Issuses, Challenges, and Needs},
year={2016},
volume={},
number={},
pages={433-441},
abstract={With the fast advance of big data technology and analytics solutions, big data computing and service is becoming a very hot research and application subject in academic research, industry community, and government services. Nevertheless, there are increasing data quality problems resulting in erroneous data costs in enterprises and businesses. Current research seldom discusses how to effectively validate big data to ensure data quality. This paper provides informative discussions for big data validation and quality assurance, including the essential concepts, focuses, and validation process. Moreover, the paper presents a comparison among big data validation tools and several major players in industry are discussed. Furthermore, the primary issues, challenges, and needs are discussed.},
keywords={Big data;Quality assurance;Organizations;Q-factor;Standards organizations;Quality assurance;big data quality assurance;big data validation;data validation},
doi={10.1109/SOSE.2016.63},
ISSN={},
month={March},}
@INPROCEEDINGS{8622229,
author={Austin, Claire C.},
booktitle={2018 IEEE International Conference on Big Data (Big Data)}, title={A Path to Big Data Readiness},
year={2018},
volume={},
number={},
pages={4844-4853},
abstract={"Big Data readiness" begins at the source where data are first created and extends along a path through an organization to the outside world. This paper focuses on practical solutions to common problems experienced when integrating diverse datasets from disparate sources. Following the Introduction, Section 2 situates Big Data in the larger context of open government, open science, science integrity, and Standards, internationally and in Canada. Section 3 analyses the Big Data problem space, while Section 4 proposes a Big Data solution space. Section 5 proposes eight data checklist modules and suggests implementation strategies to effectively meet a variety of organizational needs. Section 6 summarizes conclusions and describes future work.},
keywords={Big Data;Government;Standards organizations;Europe;Tools;Big Data;data quality;data checklist;data repository;open science;open government;data science},
doi={10.1109/BigData.2018.8622229},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9378148,
author={O’Shea, Enda and Khan, Rafflesia and Breathnach, Ciara and Margaria, Tiziana},
booktitle={2020 IEEE International Conference on Big Data (Big Data)}, title={Towards Automatic Data Cleansing and Classification of Valid Historical Data An Incremental Approach Based on MDD},
year={2020},
volume={},
number={},
pages={1914-1923},
abstract={The project Death and Burial Data: Ireland 1864-1922 (DBDIrl) examines the relationship between historical death registration data and burial data to explore the history of power in Ireland from 1864 to 1922. Its core Big Data arises from historical records from a variety of heterogeneous sources, some aspects are pre-digitized and machine readable. A huge data set (over 4 million records in each source) and its slow manual enrichment (ca 7,000 records processed so far) pose issues of quality, scalability, and creates the need for a quality assurance technology that is accessible to non-programmers. An important goal for the researcher community is to produce a reusable, high-level quality assurance tool for the ingested data that is domain specific (historic data), highly portable across data sources, thus independent of storage technology.This paper outlines the step-wise design of the finer granular digital format, aimed for storage and digital archiving, and the design and test of two generations of the techniques, used in the first two data ingestion and cleaning phases.The first small scale phase was exploratory, based on metadata enrichment transcription to Excel, and conducted in parallel with the design of the final digital format and the discovery of all the domain-specific rules and constraints for the syntax and semantic validity of individual entries. Excel embedded quality checks or database-specific techniques are not adequate due to the technology independence requirement. This first phase produced a Java parser with an embedded data cleaning and evaluation classifier, continuously improved and refined as insights grew. The next, larger scale phase uses a bespoke Historian Web Application that embeds the Java validator from the parser, as well as a new Boolean classifier for valid and complete data assurance built using a Model-Driven Development technique that we also describe. This solution enforces property constraints directly at data capture time, removing the need for additional parsing and cleaning stages. The new classifier is built in an easy to use graphical technology, and the ADD-Lib tool it uses is a modern low-code development environment that auto-generates code in a large number of programming languages. It thus meets the technology independence requirement and historians are now able to produce new classifiers themselves without being able to program. We aim to infuse the project with computational and archival thinking in order to produce a robust data set that is FAIR compliant (Free Accessible Inter-operable and Re-useable).},
keywords={Java;Quality assurance;Semantics;Big Data;Tools;Syntactics;Cleaning;Data Collection;Data Analytics;Model-Driven Development;Historical Data;Data Parsing;Data Cleaning;Ethics;Data Assurance;Data Quality},
doi={10.1109/BigData50022.2020.9378148},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7877056,
author={Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Marín-Tordera, Eva},
booktitle={2016 IEEE/ACM 3rd International Conference on Big Data Computing Applications and Technologies (BDCAT)}, title={Towards a Comprehensive Data LifeCycle Model for Big Data Environments},
year={2016},
volume={},
number={},
pages={100-106},
abstract={A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource, however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation. 2. Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity, and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.},
keywords={Data models;Adaptation models;Biological system modeling;Big Data;Organizations;Data acquisition;Computational modeling;Big Data;Data LifeCycle;Data Management;Data Organization;Data Complexity;Vs Challenges},
doi={},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8275101,
author={Setiadi, Yazid and Uluwiyah, Ana},
booktitle={2017 International Workshop on Big Data and Information Security (IWBIS)}, title={Improving data quality through big data: Case study on big data-mobile positioning data in Indonesia tourism statistics},
year={2017},
volume={},
number={},
pages={43-48},
abstract={Big Data is a new concept that has become widely popularised in recent years. The revolutionized meaning of information communication technologies and Internet technologies refers to mobile communications which enable individuals to move and generate, transmit and receive different kinds of information. There are many communication options where users can search, interact and share information with other users such as website, social media, online communities blogs, and email called as Digital transformation. In Digital transformation era, over 95 % of travellers today use digital resources. Digital traveler can be as data source for official statistics. One of methods to capture the number of tourist can use Mobile Positioning Data (MPD). This method is considered able to improve the quality of survey data. This article will discuss further how to improve the quality of survey data through Big Data with case studies of MPD users in Indonesian Tourism Statistics.},
keywords={1/f noise;Mobile communication;Big Data;Accuracy;Mobile Positioning Data;Tourism Statistics},
doi={10.1109/IWBIS.2017.8275101},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9671890,
author={Geronazzo, Angela and Ziegler, Markus},
booktitle={2021 IEEE International Conference on Big Data (Big Data)}, title={QMLEx: Data Driven Digital Transformation in Marketing Analytics},
year={2021},
volume={},
number={},
pages={5900-5902},
abstract={This paper presents a data driven approach to replace expert driven business processes. The QMLEx methodology combines NLP algorithms to improve data quality, ML techniques to perform the task and exploits external data sources to eliminate the need for the expert input. The methodology is applied to our internal process devoted to creating groups of products with similar features, one of the most relevant use case in marketing analytics.},
keywords={Itemsets;Soft sensors;Digital transformation;Data integrity;Conferences;Big Data;Feature extraction;Digital transformation;entity linking;topic extraction;word embedding;pattern search;frequent itemset mining;data quality},
doi={10.1109/BigData52589.2021.9671890},
ISSN={},
month={Dec},}
@ARTICLE{9481251,
author={Yu, Wenjin and Liu, Yuehua and Dillon, Tharam and Rahayu, Wenny and Mostafa, Fahed},
journal={IEEE Internet of Things Journal}, title={An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques},
year={2022},
volume={9},
number={3},
pages={2443-2454},
abstract={With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &#x0026; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.},
keywords={Big Data;Internet of Things;Intelligent sensors;Data analysis;Cloud computing;Smart manufacturing;Sensor phenomena and characterization;Big data;health state monitoring;Internet of Things (IoT);noisy data cleaning;real-time systems;sensor selection},
doi={10.1109/JIOT.2021.3096637},
ISSN={2327-4662},
month={Feb},}
@INPROCEEDINGS{7840769,
author={Ganapathi, Archana and Chen, Yanpei},
booktitle={2016 IEEE International Conference on Big Data (Big Data)}, title={Data quality: Experiences and lessons from operationalizing big data},
year={2016},
volume={},
number={},
pages={1595-1602},
abstract={Data quality issues pose a significant barrier to operationalizing big data. They pertain to the meaning of the data, the consistency of that meaning, the human interpretation of results, and the contexts in which the results are used. Data quality issues arise after organizations have moved past clear-cut technical solutions to early bottlenecks in using data. Left unaddressed, such issues can and have led to high profile missteps, and raise doubts about the data-driven world view altogether. In this paper, we share real-world case studies of tackling data quality challenges across industry verticals. We present initial ideas on how to systematically address data quality issues via technology. The success of operationalizing big data will depend on the quality of data involved, and whether such data causes uncertainty and disruptions, or delivers genuine knowledge and value.},
keywords={Cleaning;Big data;Measurement;Business;Software;Instruments;Industries},
doi={10.1109/BigData.2016.7840769},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7435456,
author={Yang, Sha and Yu, Wei and Hu, Yahui and Wang, Kai and Wang, Jun and Li, Shijun},
booktitle={2015 Third International Conference on Advanced Cloud and Big Data}, title={An Automatic Discovery Framework of Cross-Source Data Inconsistency for Web Big Data},
year={2015},
volume={},
number={},
pages={73-79},
abstract={The vigorous growth of big data has triggered both opportunities and challenges in business and industry. However, Web big data distributed in diverse sources with multiple data structures frequently conflict with each other, i.e. inconsistency in cross-source Web big data. In this paper, we propose a state-of-the-art architecture of auto-discovering inconsistency with Web big data. Our contributions include: (1) we classify the inconsistency features to formalize inconsistency data and establish an algebraic operation system, (2) we propose three algorithms to auto-discover inconsistency, including constraint-based, SDA-based and HPDM-based method and (3) we conduct experiments on real-world dataset to compare aforesaid schemes with Oracle-based inconsistency detection framework. The empirical results show that our methods outperform traditional framework both on accuracy and efficiency under Web big data.},
keywords={Big data;Data models;Computers;Data mining;Industries;Algorithm design and analysis;Distributed databases;Web Big Data;Data Consistency;Web Data Management;Data Quality Assessment;Data Analysis},
doi={10.1109/CBD.2015.22},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8695373,
author={Zuo, Yiming and Vernica, Rares and Lei, Yang and Barcelo, Steven and Rogacs, Anita},
booktitle={2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)}, title={A Big Data Platform for Surface Enhanced Raman Spectroscopy Data with an Application on Image-Based Sensor Quality Control},
year={2019},
volume={},
number={},
pages={463-466},
abstract={Surface-enhanced Raman spectroscopy (SERS) significantly enhances the Raman scattering by molecules, enabling detection and identification of small quantities of relevant bio-/chemical markers in a wide range of applications. In this paper, we present a big data platform with both a local client and cloud server built for acquiring, processing, visualizing and storing SERS sensor data. The local client controls the hardware (i.e., spectrometer and stage) to collect SERS spectra from HP designed sensors, and offers the options to analyze, visualize and save the spectra with meta-data records, including relevant experimental conditions. The cloud server contains remote databases and web interface for centralized data management to users from different locations. Here we describe how this platform was built and demonstrate its use for automated sensor quality control based on sensor images. Sensor quality control is a common practice, employed in sensor production to select high performing sensors. Image-based approach is a natural way to perform sensor quality control without destructing the sensors. Automating this process using the proposed platform can also reduce the time spent and achieve consistent result by avoiding human visual inspection.},
keywords={Quality control;Servers;Cloud computing;Big Data;Visualization;Inspection;Databases;Big data platform, Surface Enhanced Raman Spectroscopy, sensor quality control},
doi={10.1109/MIPR.2019.00093},
ISSN={},
month={March},}
@INPROCEEDINGS{9590700,
author={Du, Jinming},
booktitle={2021 IEEE 4th International Conference on Information Systems and Computer Aided Education (ICISCAE)}, title={Research on the Construction of Educational Data Quality Model Based on Multiple Constraints Model},
year={2021},
volume={},
number={},
pages={363-367},
abstract={With the development of Internet and information technology, data has become an important asset related to the development prospects of society and all walks of life. At present, there are many quality problems in the use of educational data, which has brought great obstacles to exerting the value of educational data. Only by using scientific statistical methods, obtaining real, objective, comprehensive, scientific and effective basic data, and carrying out systematic and comprehensive analysis on the obtained data, can we give full play to its command and decision-making role. The development of big data and artificial intelligence technology provides new ideas for the analysis and evaluation of educational data quality, and is committed to restoring the overall picture of the education system and promoting the change of regional educational ecology. In this paper, an educational data quality analysis model based on multiple constraint model is proposed, which classifies the data in the database, divides the information in the database into several different categories according to the data characteristics, and establishes a quality management system for educational data in universities, so as to effectively improve the quality of educational data.},
keywords={Training;Analytical models;Systematics;Databases;Data integrity;Biological system modeling;Education;Educational data;Data quality;Statistics},
doi={10.1109/ICISCAE52414.2021.9590700},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9209633,
author={Byabazaire, John and O’Hare, Gregory and Delaney, Declan},
booktitle={2020 29th International Conference on Computer Communications and Networks (ICCCN)}, title={Using Trust as a Measure to Derive Data Quality in Data Shared IoT Deployments},
year={2020},
volume={},
number={},
pages={1-9},
abstract={Recent developments in Internet of Things have heightened the need for data sharing across application domains to foster innovation. As most of these IoT deployments are based on heterogeneous sensor types, there is increased scope for sharing erroneous, inaccurate or inconsistent data. This in turn may lead to inaccurate models built from this data. It is important to evaluate this data as it is collected to establish its quality. This paper presents an analysis of data quality as it is represented in Internet of Things (IoT) systems and some of the limitations of this representation. The paper then introduces the use of trust as a heuristic to drive data quality measurements. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework for representing data quality within the big data model. To demonstrate the application of a trust backed framework, we used data collected from a IoT deployment of sensors to measure air quality in which a low cost sensor was co-located with a gold reference sensor. Using data streams modeled based on a dataset from an IoT deployment, our initial results show that the framework's trust score are consistent with the accuracy measure of the machine learning models.},
keywords={Data integrity;Data models;Big Data;Biological system modeling;Measurement;Standards;Internet of Things;Data Quality;Internet of Things (IoT);Trust;Big Data Model;Machine learning},
doi={10.1109/ICCCN49398.2020.9209633},
ISSN={2637-9430},
month={Aug},}
@INPROCEEDINGS{8029349,
author={Kim, Hee Young and Cho, June-Suh},
booktitle={2017 IEEE International Congress on Big Data (BigData Congress)}, title={Data Governance Framework for Big Data Implementation with a Case of Korea},
year={2017},
volume={},
number={},
pages={384-391},
abstract={Big Data governance requires a data governance that can satisfy the needs for corporate governance, IT governance, and ITA/EA. While the existing data governance focuses on the processing of structured data, Big Data governance needs to be established in consideration of a broad sense of Big Data services including unstructured data. To achieve the goals of Big Data, strategies need to be established together with goals that are aligned with the vision and objective of an organization. In addition to the preparation of the IT infrastructure, a proper preparation of the components is required to effectively implement the strategy for Big Data services. We propose the Big Data Governance Framework in this paper. The Big Data governance framework presents criteria different from existing criteria at the data quality level. It focuses on timely, reliable, meaningful, and sufficient data services, focusing on what data attributes should be achieved based on the data attributes of Big Data services. In addition to the quality level of Big Data, the personal information protection strategy and the data disclosure/accountability strategy are also needed to achieve goals and to prevent problems. This paper performed case analysis based on the Big Data Governance Framework with the National Pension Service of South Korea. Big Data services in the public sector are an inevitable choice to improve the quality of people's life. Big Data governance and its framework are the essential components for the realization of Big Data service.},
keywords={Big Data;Government;Data privacy;Reliability;Social network services;Big data;Data governance;Data governance framework;Case analysis},
doi={10.1109/BigDataCongress.2017.56},
ISSN={},
month={June},}
@INPROCEEDINGS{9298378,
author={Desai, Vinod and H A, Dinesha},
booktitle={2020 IEEE International Conference for Innovation in Technology (INOCON)}, title={A Hybrid Approach to Data Pre-processing Methods},
year={2020},
volume={},
number={},
pages={1-4},
abstract={This is an era of big data, as data is growing exponentially and resources are running out of infrastructure, so it is required to accommodate all the data that gets generated. We collect data in enormous amounts to derive meaningful conclusions, perform effective data analytics and improve decision making. As we don't have enough infrastructures to support data storage for huge volumes, it is needed to clean the data in compulsion. It is a mandatory to carry out a step before doing anything with the data. We call it pre-processing of data and this is carried out in various steps. Pre-processing includes data cleaning, data integration, data filtering, and data transformation and so on. As such preprocessing is not limited to the number of steps or a number of methods or definitive methods. We must innovatively preprocess the data before it is being consumed for data analytics. It has become a responsibility for every data analyst or big data researcher to handpick data for his or her analytics. Considering all these techniques in mind we are proposing a hybrid technique to leverage various algorithms available to pre-process our data along with minor modifications such as at the run time, choosing an algorithm or technique wisely based on the data that we have.},
keywords={Big Data;Data integrity;Error analysis;Data mining;Data analysis;Transforms;Time series analysis;Big Data;Data Pre-processing;Data Quality checks},
doi={10.1109/INOCON50539.2020.9298378},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9263667,
author={Xiangwei, Kong},
booktitle={2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)}, title={Evaluation of Flight Test Data Quality Based on Rough Set Theory},
year={2020},
volume={},
number={},
pages={1053-1057},
abstract={With the continuous development of flight test technology, the test system is filled with massive, multi-structured, and multi-dimensional data resources. The value of big data has been fully recognized by the society. How to tap the value of data has become an application in various research fields and industries. The most concerned issue of the field. Whether the data is rubbish or treasure, the most important question is whether the data to be analyzed and mined is of high quality. A low-quality data source will not only fail to reflect the value of the data, but may also run counter to the actual situation, which has side effects. In order to effectively evaluate the quality of flight test data, according to the characteristics of test data in flight test, a test data quality evaluation method based on rough set theory is proposed, standard test methods and test indicators for flight test data quality are proposed, and data quality evaluation is given. The method of rule extraction realizes the quality evaluation of flight test data.},
keywords={Data integrity;Rough sets;Feature extraction;Image color analysis;Shape;Packet loss;Data mining;Flight test;data quality;rough set;quality evaluation},
doi={10.1109/CISP-BMEI51763.2020.9263667},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8532518,
author={Burkhardt, Andrew and Berryman, Sheila and Brio, Ashley and Ferkau, Susan and Hubner, Gloria and Lynch, Kevin and Mittman, Susan and Sonderer, Kathy},
booktitle={2018 IEEE AUTOTESTCON}, title={Measuring Manufacturing Test Data Analysis Quality},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Manufacturing test data volumes are constantly increasing. While there has been extensive focus in the literature on big data processing, less focus has existed on data quality, and considerably less focus has been placed specifically on manufacturing test data quality. This paper presents a fully automated test data quality measurement developed by the authors to facilitate analysis of manufacturing test operations, resulting in a single number used to compare manufacturing test data quality across programs and factories, and focusing effort cost-effectively. The automation enables program and factory users to see, understand, and improve their test data quality directly. Immediate improvements in test data quality speed manufacturing test operation analysis, reducing elapsed time and overall spend in test operations. Data quality has significant financial impacts to businesses [1]. While manufacturing cost models are well understood, data quality cost models are less well understood (see Eppler & Helfert [2] who review manufacturing cost models and create a taxonomy for data quality costs). Kim & Choi [3] discuss measuring data quality costs, and a rudimentary data quality cost calculation is described in [4]. Haug et al. [5] describe a classification of costs for poor data quality, and while they do not provide a cost calculation, they do define optimality for data quality. Laranjeiro et al. [6] have a recent survey of poor data quality classification. Ge & Helfert [7] extend the work in [2], and provide an updated review of data quality costs. Test data is specifically addressed in the context of data processing in [8]. Big data quality efforts are reviewed in [9, 10]. Data quality metrics are discussed in [11], and requirements for data quality metrics are identified in [12]. Data inconsistencies are detailed in [13], while categorical data inconsistencies are explained in [14]. In the current work, manufacturing test data quality is directly correlated to the speed of manufacturing test operations analysis. A measurement for manufacturing test data quality indicates the speed at which analysis can be performed, and increases in the test data quality score have precipitated increases in the speed of analysis, described herein.},
keywords={Data integrity;Manufacturing;Measurement;Decision making;Production facilities;Data models;manufacturing test;data quality;test data quality;cost of data quality},
doi={10.1109/AUTEST.2018.8532518},
ISSN={1558-4550},
month={Sep.},}
@INPROCEEDINGS{8367700,
author={Yu, Bin and Zhang, Chen and Tang, ZhouHua and Sun, JiangYan},
booktitle={2018 IEEE 3rd International Conference on Big Data Analysis (ICBDA)}, title={Verification method of data quality in science and technology cloud in Shaanxi province},
year={2018},
volume={},
number={},
pages={319-323},
abstract={This paper analyzes and summarizes the data quality problems in the Shaanxi Science and Technology Resource Coordination Center “Science and Technology Cloud” project. These two major problems about scientific and technological information data quality are verified. One is data redundancy caused by organizations' name abbreviation and the other is partial scientific and technological information data missing. This paper designs and implements solutions to the problems in “Science and Technology Cloud” project. This paper extracts 15643 data from scientific and technical talent pool and scientific literature library. The experimental results verify the effectiveness and feasibility of the solution of data redundancy and data missing.},
keywords={Data integrity;Redundancy;Databases;Dynamic programming;Education;Remuneration;Organizations;Science and Technology Cloud;data quality;data redundancy;missing value processing},
doi={10.1109/ICBDA.2018.8367700},
ISSN={},
month={March},}
@INPROCEEDINGS{8465132,
author={Segooa, Mmatshuene Anna and Kalema, Billy Mathias},
booktitle={2018 International Conference on Advances in Big Data, Computing and Data Communication Systems (icABCD)}, title={Improve Decision Making Towards Universities Performance Through Big Data Analytics},
year={2018},
volume={},
number={},
pages={1-5},
abstract={The technology Big Bang has seen organizations universities inclusive generating big volumes of data in various formats and at high speed than they used to do. Such data is referred to as Big Data. This voluminous data can be of great significance to organizations if better insights are drawn for management to improve decision making. However, to draw valued insight from Big Data, advanced forms of analytics need to be employed and such techniques are commonly known as Big Data analytics (BDA), This paper sough to report on analysis of factors influencing the leverage of BDA to improve performance in universities.},
keywords={Big Data;Organizations;Decision making;Standards organizations;Learning management systems;Market research;Big Data;Big Data Analytics;universities decision making;data quality},
doi={10.1109/ICABCD.2018.8465132},
ISSN={},
month={Aug},}
@ARTICLE{8950481,
author={Deng, Wei and Guo, Yixiu and Liu, Jie and Li, Yong and Liu, Dingguo and Zhu, Liang},
journal={Chinese Journal of Electrical Engineering}, title={A missing power data filling method based on improved random forest algorithm},
year={2019},
volume={5},
number={4},
pages={33-39},
abstract={Missing data filling is a key step in power big data preprocessing, which helps to improve the quality and the utilization of electric power data. Due to the limitations of the traditional methods of filling missing data, an improved random forest filling algorithm is proposed. As a result of the horizontal and vertical directions of the electric power data are based on the characteristics of time series. Therefore, the method of improved random forest filling missing data combines the methods of linear interpolation, matrix combination and matrix transposition to solve the problem of filling large amount of electric power missing data. The filling results show that the improved random forest filling algorithm is applicable to filling electric power data in various missing forms. What's more, the accuracy of the filling results is high and the stability of the model is strong, which is beneficial in improving the quality of electric power data.},
keywords={Filling;Power systems;Random forests;Interpolation;Data models;Big Data;Data mining;Big data cleaning;missing data filling;data preprocessing;random forest;data quality},
doi={10.23919/CJEE.2019.000025},
ISSN={2096-1529},
month={Dec},}
@INPROCEEDINGS{9545944,
author={Ying, KangHui and Hu, WenYu and Chen, Jin Bo and Li, Guo Nong},
booktitle={2021 International Conference on Artificial Intelligence, Big Data and Algorithms (CAIBDA)}, title={Research on instance-level data cleaning technology},
year={2021},
volume={},
number={},
pages={238-242},
abstract={Effectivedata analysis and data mining are based on data availability and data quality. Data cleaning is a commonly used technique to improve data quality. Instance-level data cleaning is an important part of data cleaning. The focus is on the comparison and analysis of the detection and cleaning methods of attributes and recorded values in the instance-level data cleaning technology, and the experimental analysis of the repeated record cleaning methods. This paper introduces the application field of data cleaning technology represented by the electrical engineering field combined with the application situation, and provides valuable selection suggestions for the characteristics of different data sets and the applicable instancelevel data cleaning technology. Summarizing and analyzing the existing detection and cleaning technology methods, it is concluded that instance-level data cleaning has a lot of research and development space in long text, unstructured data and specific fields. Finally, the challenges and development directions of the instance-level data cleaning technology are prospected.},
keywords={Electrical engineering;Data integrity;Big Data;Cleaning;Data mining;Artificial intelligence;Research and development;instance-level data cleaning;duplicate records cleaning;attribute cleaning},
doi={10.1109/CAIBDA53561.2021.00057},
ISSN={},
month={May},}
@INPROCEEDINGS{8102206,
author={Alqarni, Mohammed A.},
booktitle={2017 14th International Conference on Smart Cities: Improving Quality of Life Using ICT IoT (HONET-ICT)}, title={Benefits of SDN for Big data applications},
year={2017},
volume={},
number={},
pages={74-77},
abstract={Big data applications depend on underlying networks that make the transfer of information possible. These networks may be real (conventional) or virtual (in case of services hosted in data centers). Either way, the responsibility of smooth execution of the application, despite increasing traffic volume, lies with the service provider. The service providers face many challenges with respect to providing a high quality of service. It is therefore in the best interest of the service providers that efficiency of the applications is increased. SDN has the potential to improve big data application performance. In this paper we have a look at the recent advancements in technology that helps improve big data applications using SDN and discuss our observations.},
keywords={Big Data applications;Optimization;Servers;Protocols;Multimedia communication;SDN;Network Virtualization;Software Defined Networks;Big data},
doi={10.1109/HONET.2017.8102206},
ISSN={1949-4106},
month={Oct},}
@INPROCEEDINGS{7051902,
author={Zillner, Sonja and Oberkampf, Heiner and Bretschneider, Claudia and Zaveri, Amrapali and Faix, Werner and Neururer, Sabrina},
booktitle={Proceedings of the 2014 IEEE 15th International Conference on Information Reuse and Integration (IEEE IRI 2014)}, title={Towards a technology roadmap for big data applications in the healthcare domain},
year={2014},
volume={},
number={},
pages={291-296},
abstract={Big Data technologies can be used to improve the quality and efficiency of healthcare delivery. The highest impact of Big Data applications is expected when data from various healthcare areas, such as clinical, administrative, financial, or outcome data, can be integrated. However, as of today, the seamless access to the various healthcare data pools is only possible in a very constrained and limited manner. For enabling the seamless access several technical requirements, such as data digitalization, semantic annotation, data sharing, data privacy and security as well as data quality need to be addressed. In this paper, we introduce a detailed analysis of these technical requirements and show how the results of our analysis lead towards a technical roadmap for Big Data in the healthcare domain.},
keywords={Medical services;Big data;Semantics;Data privacy;Standards;Biomedical imaging;Security;Big Data;technical requirements;data digitalization;semantic annotation;data integration;data privacy and security;data quality},
doi={10.1109/IRI.2014.7051902},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8785536,
author={Auer, Florian and Felderer, Michael},
booktitle={2019 IEEE/ACM 4th International Workshop on Metamorphic Testing (MET)}, title={Addressing Data Quality Problems with Metamorphic Data Relations},
year={2019},
volume={},
number={},
pages={76-83},
abstract={In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.},
keywords={Big Data;Data integrity;Testing;Encyclopedias;Internet;Electronic publishing;metamorphic testing, data quality, big data, quality assessment, metamorphic data relations},
doi={10.1109/MET.2019.00019},
ISSN={},
month={May},}
@ARTICLE{6949519,
author={O'Leary, Daniel E.},
journal={IEEE Intelligent Systems}, title={Embedding AI and Crowdsourcing in the Big Data Lake},
year={2014},
volume={29},
number={5},
pages={70-73},
abstract={Daniel E. O'Leary examines the notion of the Big Data Lake and contrasts it with decision support-based data warehouses. In addition, some of the risks of the emerging Lake concept that ultimately require data governance are analyzed. O'Leary investigates using different AI and crowdsourcing (human intelligence) applications in that lake in order to integrate disparate data sources, facilitate master data management and analyze data quality. Although data governance often is not seen as a technology issue, it is seen as a critical component of making the Big Data Lake "work".},
keywords={Crowdsourcing;Artificial intelligence;Big data;Data warehouses;Decision support systems;Databases;Business;Big Data Lake;data warehouses;artificial intelligence;crowdsourcing;data governance;master data management;intelligent systems},
doi={10.1109/MIS.2014.82},
ISSN={1941-1294},
month={Sep.},}
@INPROCEEDINGS{8258218,
author={Colborne, Adrienne and Smit, Michael},
booktitle={2017 IEEE International Conference on Big Data (Big Data)}, title={Identifying and mitigating risks to the quality of open data in the post-truth era},
year={2017},
volume={},
number={},
pages={2588-2594},
abstract={Big Data analysis often relies on open data, integrating it with large private data sets, using it as ground truth information, or providing it as part of the input to large simulations. Data can be released openly by governments to achieve various objectives: transparency, informing citizen engagement, or supporting private enterprise, to name a few. To the latter objective, Big Data analytics algorithms rely on high-quality, timely access to various data sources, including open data. Examples include retail analytics drawing on open demographic data and weather forecast systems drawing on open weather and climate data. In this paper, we describe the rise of post-truth in society, and the risks this poses to the quality, integrity, and authenticity of open data. We also discuss approaches to identifying, assessing, and mitigating these risks, and suggest future steps to manage this data quality concern.},
keywords={Big Data;Meteorology;Portals;Voting;open data;post-truth;fake news;risk identification;risk mitigation;data quality assurance},
doi={10.1109/BigData.2017.8258218},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8713218,
author={Mylavarapu, Goutam and Thomas, Johnson P and Viswanathan, K Ashwin},
booktitle={2019 IEEE 4th International Conference on Big Data Analytics (ICBDA)}, title={An Automated Big Data Accuracy Assessment Tool},
year={2019},
volume={},
number={},
pages={193-197},
abstract={Data analysis is the most important aspect of any business as it is critical to decision-making. Data quality assessment is a necessary function to be performed before data analysis, as the quality of data has high impact on the outcome of the analysis. Data quality is a multi-dimensional factor that affects the analysis in numerous ways. Among all the dimensions, accuracy is the most important and hardest dimension to assess. With the advent of big data, this problem becomes more complicated. There are only few studies that focus on data accuracy with minimal domain expert dependency. In this paper, we propose an extensive data accuracy assessment tool that uses machine learning to determine the accuracy of data. In addition, our model also addresses the intrinsic and contextual categories of data accuracy. Our model was developed on Apache Spark which serves as the big data environment for handling large datasets.},
keywords={Data integrity;Big Data;Tools;Data models;Standards;Machine learning;Couplings;Data accuracy;contextual accuracy;data quality;word embeddings;record linkage;k-nearest neighbors;logistic regression;decision trees},
doi={10.1109/ICBDA.2019.8713218},
ISSN={},
month={March},}
@INPROCEEDINGS{9620761,
author={Ku, Tai-Yeon and Park, Wan-Ki and Choi, Hoon},
booktitle={2021 International Conference on Information and Communication Technology Convergence (ICTC)}, title={Mechanism of a big-data platform for residential heat energy consumption},
year={2021},
volume={},
number={},
pages={1450-1452},
abstract={Although the solar energy industry is becoming widespread, it is necessary to manage the charging and generating scheduling of solar power generation according to the ever-changing climate environment. In order to do this, a judgment criterion that can give timely charge / discharge instructions is needed and it needs to be actively performed. In this paper, we define a big-data platform for residential heat energy consumption. As a technology to secure thermal energy data of apartment houses, collect thermal energy data by dividing it into supply/equipment/usage. In order to secure standardized thermal energy data from the calorimeter installed. Equipped with data classification and processing, LP storage and management, data quality measurement and analysis functions. Develop a data adapter, from several multiunit dwellings with different calorimeter types. We will collect thermal energy data with an integrated big data system.},
keywords={Energy consumption;Temperature distribution;Water storage;Data integrity;Water heating;Solar energy;Big Data;energy management;energy big data;energy information collection},
doi={10.1109/ICTC52510.2021.9620761},
ISSN={2162-1233},
month={Oct},}
@INPROCEEDINGS{7823519,
author={Li, Tao and He, Yihai and Zhu, Chunling},
booktitle={2016 International Conference on Industrial Informatics - Computing Technology, Intelligent Technology, Industrial Information Integration (ICIICII)}, title={Big Data Oriented Macro-Quality Index Based on Customer Satisfaction Index and PLS-SEM for Manufacturing Industry},
year={2016},
volume={},
number={},
pages={181-186},
abstract={The aim of this paper was to develop a novel macro-quality index driven by big quality data regarding the macro-quality management demands of manufacturing enterprises in Industry 4.0. Firstly, the connotation of big data of macro-quality management in manufacturing industry is expounded, which is the collection of the quality data in product lifecycle including the product quality data, process quality data and organizational ability data. Secondly, referring to the customer satisfaction index theory, a new big data oriented macro-quality index computation model based on the partial least square-structural equation modeling(PLS-SEM) theory is proposed, and the partial least square(PLS) is adopted to estimate the path parameters. Finally, a case study of macro-quality situation evaluation for the manufacturing industry of a city in China is presented. The final result shows that the proposed macro-quality index model is applicable and predictable.},
keywords={Indexes;Mathematical model;Data models;Manufacturing industries;Computational modeling;Big data;Customer satisfaction;macro-quality evaluation;big data;macro-quality index;customer satisfaction index;PLS-SEM},
doi={10.1109/ICIICII.2016.0052},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8740576,
author={Zan, Songting and Zhang, Xu},
booktitle={2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC)}, title={Medical Data Quality Assessment Model Based on Credibility Analysis},
year={2018},
volume={},
number={},
pages={940-944},
abstract={The current main problem of medical data is low accuracy. Although existing data quality evaluation and audit can meet the needs, network security problem will be more serious in the future. As a result, the data quality evaluation model must include data credibility. This paper proposes the medical big data quality evaluation model based on credibility analysis and Analytic Hierarchy Process(AHP). Firstly, analyze the data credibility. After eliminating the unreliable data, calculate the data quality dimensions respectively. Then obtain the data quality evaluation result by integrating all dimensions with AHP. Through simulation, the data quality evaluation model can effectively identify the data that affects the credibility. The data quality evaluation results are improved after removing the untrusted data, besides the amount of data is reduced, which is applicable to the big data scenario.},
keywords={Data models;Data integrity;Big Data;Mathematical model;Analytical models;Analytic hierarchy process;Surgery;data quality evaluation;medical;credibility analysis;analytic hierarchy process},
doi={10.1109/ITOEC.2018.8740576},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7321725,
author={Wang, Jianwu and Tang, Yan and Nguyen, Mai and Altintas, Ilkay},
booktitle={2014 IEEE/ACM International Symposium on Big Data Computing}, title={A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning},
year={2014},
volume={},
number={},
pages={16-25},
abstract={In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.},
keywords={Big data;Engines;Bayes methods;Partitioning algorithms;Accuracy;Algorithm design and analysis;Distributed databases;Big Data;Bayesian network;Distributed computing;Ensemble learning;Scientific workflow;Kepler;Hadoop},
doi={10.1109/BDC.2014.10},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9491629,
author={Hattawi, Waleed and Shaban, Sameeh and Shawabkah, Aon Al and Alzu’bi, Shadi},
booktitle={2021 International Conference on Information Technology (ICIT)}, title={Recent Quality Models in BigData Applications},
year={2021},
volume={},
number={},
pages={811-815},
abstract={In this time the big data became an important part of all areas, it can be used in multiple industrials such as banking, education, government, networking, energy, health care, etc. So, because of that the huge amount of data became have problems or unnecessary data, and so that comes from the difficulty of measure the quality of these data. In this research we show the quality characteristic that can be help to increase the efficiency of quality measurement process of BDA by comparing it with other quality model of BDA and applying it on the 7V's of big data.},
keywords={Analytical models;Area measurement;Government;Energy measurement;Medical services;Big Data;Data models;Big Data;Quality;Quality Models;Data Quality;Big Data Application;The 7v's Of Big Data},
doi={10.1109/ICIT52682.2021.9491629},
ISSN={},
month={July},}
@INPROCEEDINGS{8109143,
author={Chengzan, Li and Yanfei, Hou and Jianhui, Li and Lili, Zhang},
booktitle={2017 IEEE 13th International Conference on e-Science (e-Science)}, title={ScienceDB: A Public Multidisciplinary Research Data Repository for eScience},
year={2017},
volume={},
number={},
pages={248-255},
abstract={Research data repositories are necessary infrastructures that ensure the data generated for research are accessible, stable, reliable, and reusable. Based on years of accumulated data work experience, the Computer Network Information Center of the Chinese Academy of Sciences has built a multi-disciplinary data repository ScienceDB for research users and teams using its big data storage, analysis and computing environments. This paper firstly introduces the motivation to develop ScienceDB and gives a profile to it. Then the overall technical framework of ScienceDB is introduced, and the key technologies such as the support for multidiscipline extensibility, data collaboration and data recommendation are analyzed deeply. And then this paper presents the functions and features of ScienceDB's current version and discusses some issues such as its data policy, data quality assurance measures, and current application status. Finally, it summarizes and puts forward that it needs to carry out more in-depth research and practice of ScienceDB in order to meet the higher requirements of eScience in terms of thorough data association and fusion, data analysis and mining, data evaluation, and so on.},
keywords={Metadata;Business;Data visualization;Data mining;Distributed databases;Collaboration;Computer architecture;research data repository;technical framework;multidiscipline;data recommendation;data collaboration;open data},
doi={10.1109/eScience.2017.38},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6972054,
author={Ludwig, Heiko},
booktitle={2014 IEEE 18th International Enterprise Distributed Object Computing Conference}, title={Managing Big Data Effectively - A Cloud Provider and a Cloud Consumer Perspective},
year={2014},
volume={},
number={},
pages={91-91},
abstract={Summary form only given. Instrumentation of processes and an organization's environment provides vast amounts of data that can be used to drivedecisions. Next to setting up data collection, supervising data quality, and applying proper methods of analysis, organizationsface the challenge to set up an infrastructure and architecture to do so efficiently and cost-effectively. Virtualized platformssuch as private or public clouds are the method of choice for deployment, in particular for data analyses not occurringconstantly. A cloud provider, either a commercial Cloud company or an IT organization within an enterprise, will like to set upa cloud platform such that clients can run big data workloads effectively on. Cloud customers would like to set up big dataapplications in a cost-effective and performant way on their platform.This keynote will walk through a few real life big data analysis scenarios from different industries and discuss thechallenges Cloud providers face making trade-offs. Understanding those challenges and solutions help cloud users choose theright match between their algorithm, big data system and cloud platform.},
keywords={Big data;Cloud computing;Organizations;Face;Conferences;Abstracts;Instruments},
doi={10.1109/EDOC.2014.21},
ISSN={1541-7719},
month={Sep.},}
@INPROCEEDINGS{7321730,
author={Ahnn, Jong Hoon},
booktitle={2014 IEEE/ACM International Symposium on Big Data Computing}, title={A Practical Approach to Scalable Big Data Computing for the Personalization of Services at Samsung},
year={2014},
volume={},
number={},
pages={64-73},
abstract={We observe that the recent advances in big data computing have empowered the personalization of service including model-based services such as speech recognition, face recognition, and context-aware service. Various sources of user's logs can be utilized in remodeling, adapting, and personalizing pretrained models to improve the quality of service. We propose a system that can support store/retrieve data and process them in a scalable manner on top of Samsung' big data infrastructure. An automatic speech recognition (ASR) service such as Samsung's S-Voice, Apple's SIRI is one of the representative examples. Recently advances in ASR married with big data technologies drive more personalized services in many areas of services. A speaker adaptation is now a well-accepted technology that requires huge computation cost in creating a personalized acoustic model and corresponding language model over several billions of Samsung product users. We implement a personalized and scalable ASR system powered by the big data infrastructure which brings data-driven personalized opportunities to voice-enabled services such as voice-to-text transcriber, voice-enabled web search in a peta bytes scale. We verify the feasibility of speaker adaptation based on 107 testers' recordings and obtain about 10% of recognition accuracy. An optimal set of performance optimization is suggested to have the best performance such as workflow compaction, file compression, best file system selection among several distributed file systems.},
keywords={Adaptation models;Speech;Acoustics;Computational modeling;Engines;Big data;Speech recognition;Big Data;Cloud Computing;Hadoop;Speech Recognition;Scalability;Personalization},
doi={10.1109/BDC.2014.11},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8034978,
author={Ding, Junhua and Kang, Xiaojun and Hu, Xin-Hua and Gudivada, Venkat},
booktitle={2017 IEEE International Conference on Services Computing (SCC)}, title={Building a Deep Learning Classifier for Enhancing a Biomedical Big Data Service},
year={2017},
volume={},
number={},
pages={140-147},
abstract={Providing an easily accessible data service with high quality data is important for building big data applications. In this paper, we introduce a big data service for managing and accessing massive-scale biomedical image data. The service includes three major components: a NoSQL database for storing images and data analytics results, a client consisting of a group of query scripts for data access and management, and a data quality enhancement component for improving the performance of data analytics. Low-quality data can result in incorrect analytics results and may lead to no value even harmful conclusions. Therefore, it is important to provide an effective mechanism for ensuring data quality improvement in a big data service. We describe the implement ion of a deep learning classifier to automatically filter low quality data in datasets. To improve the effectiveness of data separation, the classifier is rigorously validated with synthetic data generated by a collection of scientific tools. Design of big data services with data quality improvement as an integral component, along with the best practices collected from this experimental study, will help researchers and practitioners to develop strategies for improving the quality of big data services, building big data applications, and designing machine learning classifiers.},
keywords={Diffraction;Big Data;Machine learning;Morphology;Support vector machines;Three-dimensional displays;Noise measurement;machine learning;deep learning;big data;data quality;cross-validation},
doi={10.1109/SCC.2017.25},
ISSN={2474-2473},
month={June},}
@INPROCEEDINGS{9352886,
author={Qi, Cui and Mingyue, Sun and Na, Mi and Honggang, Wang and Yanhong, Jian and Jing, Zhu},
booktitle={2020 IEEE 3rd International Conference on Electronics and Communication Engineering (ICECE)}, title={Regional Electricity Sales Forecasting Research Based on Big Data Application Service Platform},
year={2020},
volume={},
number={},
pages={229-233},
abstract={Regional monthly electricity sales forecast is an important basis for regional power grid planning and construction, evaluation of regional economic development and operation, and protection of residents' lives. It is also an important work of regional power regulation and management, decision-making of power generation and purchase, improvement of power supply equipment utilization rate and deepening of power system reform. Based on the current situation of power supply enterprise information development, distribution network business status and characteristics, this paper analyzes the factors affecting electricity sales. According to the characteristics of annual changes in electricity sales and data quality factors, the recurrent neural network model is selected based on the big data application service platform. The long short term memory neural network model performs multi-step multivariate prediction on time series, and uses the attention mechanism to combine two independent models for prediction. Experiments conducted on the historical electricity sales data set of a power supply company show that compared with traditional machine learning methods, this method has advantages in accuracy and efficiency.},
keywords={Recurrent neural networks;Power supplies;Time series analysis;Predictive models;Big Data applications;Prediction algorithms;Data models;Recurrent neural network;Long Short-Term Memory neural network;regional monthly electricity sales;electricity sales forecast;big data application service platform},
doi={10.1109/ICECE51594.2020.9352886},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8315370,
author={Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal},
booktitle={2017 IEEE 7th International Symposium on Cloud and Service Computing (SC2)}, title={Quality Profile-Based Cloud Service Selection for Fulfilling Big Data Processing Requirements},
year={2017},
volume={},
number={},
pages={149-156},
abstract={Big data has emerged as promising technology to handle huge and special data. Processing Big data involves selecting the appropriate services and resources thanks to the variety of services offered by different Cloud providers. Such selection is difficult, especially if a set of Big data requirements should be met. In this paper, we propose a dynamic cloud service selection scheme that assess Big data requirements, dynamically map these to the most available cloud services, and then recommend the best match services that fulfill different Big data processing requests. Our selection is conducted in two stages: 1) relies on a Big data task profile that efficiently capture Big data task's requirements and map them to QoS parameters, and then classify cloud providers that best satisfy these requirements, 2) uses the list of selected providers from stage 1 to further select the appropriate Cloud services to fulfill the overall Big Data task requirements. We extend the Analytic Hierarchy Process (AHP) based ranking mechanism to cope with the problem of multi-criteria selection. We conduct a set of experiments using simulated cloud setup to evaluate our selection scheme as well as the extended AHP against other selection techniques. The results show that our selection approach outperforms the others and select efficiently the appropriate cloud services that guarantee Big data task's QoS requirements.},
keywords={Big Data;Quality of service;Task analysis;Cloud computing;Data models;Analytic hierarchy process;Mathematical model;Big Data Task profile;Cloud service selection;QoS},
doi={10.1109/SC2.2017.30},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7507493,
author={Rathore, Purva and Shukla, Deepak},
booktitle={2015 International Conference on Communication Networks (ICCN)}, title={Analysis and performance improvement of K-means clustering in big data environment},
year={2015},
volume={},
number={},
pages={43-46},
abstract={The big data environment is used to support the huge amount of data processing. In this environment tons (i.e. Giga bytes, Tera bytes) of data is processed. Therefore the various online applications where the huge data request are generated are treated using the big data i.e. facebook, google. In this presented work the big data environment is studied and investigated how the data is consumed using the big data and how the supporting tools are working with the Hadoop storage. Furthermore, for keen understanding and investigation, a cluster analysis technique more specifically the K-mean clustering algorithm is implemented through the Hadoop and MapReduce. The clustering is a part of big data analytics where the unlabelled data is processed and utilized to make groups of the data. In addition of that it is observed the traditional k-mean algorithm is not much suitably works with the Hadoop and MapReduce thus small amount of modification is performed on the data processing technique. In addition of that during cluster analysis various issues are found in traditional k-means i.e. fluctuating accuracy, outliers and empty cluster. Therefore a new clustering algorithm with modification on traditional approach of k-means clustering is proposed and implemented. That approach first enhances the data quality by removing the outlier points in datasets and then the bi-part method is used to perform the clustering. The proposed clustering technique implemented using the JAVA, Hadoop and MapReduce finally the performance of the proposed clustering approach is evaluated and compared with the traditional k-means clustering algorithm. The obtained performance shows the effective results and enhanced accuracy of cluster formation with the removal of the de-efficiency. Thus the proposed work is adoptable for the big data environment with improving the performance of clustering.},
keywords={Image recognition;Data visualization;Breast;Image segmentation;data mining;clustering;big data;performance improvement;implementation},
doi={10.1109/ICCN.2015.9},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9232648,
author={Hossen, Md Ismail and Goh, Michael and Hossen, Abid and Rahman, Md. Armanur},
booktitle={2020 11th IEEE Control and System Graduate Research Colloquium (ICSGRC)}, title={A Study on the Aspects of Quality of Big Data on Online Business and Recent Tools and Trends Towards Cleaning Dirty Data},
year={2020},
volume={},
number={},
pages={209-213},
abstract={The reliability, efficiency, and accuracy of e-business depend on the quality of data that is associated with a buyer, seller, brokers, e-business portals, admins, managers, decision-makers and so on. However, maintaining the quality of data in e-business is very challenging. It is because e-business data typically comes from different communication channels and sources. Integrating and managing the data quality of different sources is generally much troublesome than dealing with traditional business data. Even though there are several data cleaning methods and tools exist those methods and tools have some constraints. None of them directly working, particularly on e-business data that motivates to do research to highlight the aspects of big data quality related to e-business. Therefore, this research demonstrates the problems related to data quality related to online business, discusses the existing literature of data quality, the current tools and techniques that are being used for data quality and provides a research finding highlighting the weaknesses of current tools to address the problem of online business.},
keywords={Data integrity;Tools;Companies;Cleaning;Task analysis;Machine learning;Regulation;E-business;Big data;data quality;dirty data;machine learning},
doi={10.1109/ICSGRC49013.2020.9232648},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9497187,
author={Adnan, Kiran and Akbar, Rehan and Wang, Khor Siak},
booktitle={2021 International Conference on Computer Information Sciences (ICCOINS)}, title={Towards Improved Data Analytics Through Usability Enhancement of Unstructured Big Data},
year={2021},
volume={},
number={},
pages={1-6},
abstract={A high volume of unstructured data is being generated from diverse and heterogeneous sources. The unstructured data analytics process is used to extract valuable insights from these unstructured data sources but unlocking useful and usable information is critical for analytics. Despite advancements in technologies, data preparation requires an inordinate amount of time in unstructured data manipulation into a usable form. Although several data manipulation and preparation techniques have been proposed for unstructured big data, relatively limited research has addressed the usability issues of unstructured data. This study identifies the usability issues of unstructured big data for the analytical process to bridge the identified gap. The usability enhancement model has been proposed for unstructured big data to facilitate the subjective and objective efficacy of unstructured big data for data preparation and manipulation activities. Moreover, concept mapping is an essential element to improve the usability of unstructured big data incorporated in the proposed model with usability rules. These rules reduce the usability gap between data availability and its usefulness for an intended purpose. The proposed research model will help to improve the efficiency of unstructured big data analytics.},
keywords={Bridges;Analytical models;Data analysis;Data integrity;Decision making;Data visualization;Big Data;Unstructured data;data usability;unstructured data analytics;pragmatic data quality},
doi={10.1109/ICCOINS49721.2021.9497187},
ISSN={},
month={July},}
@INPROCEEDINGS{8997699,
author={Pan, Lingling and Liu, Jun and Li, Feng},
booktitle={2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)}, title={Multi-dimensional Index Construction of Electric Power Multi-source Measurement Data considering Spatio-temporal Correlation},
year={2019},
volume={1},
number={},
pages={2386-2390},
abstract={The operation of complex AC/DC power grid changes rapidly and dynamically, which objectively puts forward higher requirements for on-line analysis, and it is urgent to improve the basic data quality of power grid. Because of low quality and poor synchronization of the basic data of power grid, it is impossible to accurately map the actual operation of the power grid. At the same time, the cross-system data matching degree is low and the data correlation is poor, so it can not support the multi-scale data analysis for all kinds of applications. In this paper, the associated method of multi-source heterogeneous data in the power grid is studied. Combined with big data's access characteristics, big data storage, big data retrieval and artificial intelligence technology, the high-speed data storage and index architecture of power big data are constructed, and a multi-dimensional index reflecting the associated relationship of operating data is established from the dimensions of time, space, application, device and so on. It is easier to analyze multi-source data, to improve the basic data quality of power grid, which provides effective support for accurate data analysis and evaluation of power grid.},
keywords={Big Data;Power grids;Indexes;Power measurement;Time measurement;Memory;power big data;multi-source heterogeneous data;spatio-temporal correlation;data storage;multi-dimensional index},
doi={10.1109/IAEAC47372.2019.8997699},
ISSN={2381-0947},
month={Dec},}
@INPROCEEDINGS{8836982,
author={Islam Sarker, Md Nazirul and Wu, Min and Chanthamith, Bouasone and Yusufzada, Shaheen and Li, Dan and Zhang, Jie},
booktitle={2019 2nd International Conference on Artificial Intelligence and Big Data (ICAIBD)}, title={Big Data Driven Smart Agriculture: Pathway for Sustainable Development},
year={2019},
volume={},
number={},
pages={60-65},
abstract={Increasing agricultural production is top most solution in the face of rapid population growth through digitalization of agriculture by using most developed technology like big data. There is a long debate on the application of big data in agriculture. This study is an attempt to explore the suitability of the big data technologies for increasing production and improving quality in agriculture. The study uses an extensive review of current research works and studies in agriculture for exploring the best and compatible practices which can help farmers at field level for increasing production and improving quality. This study reveals a number of available big data technologies and practices in agriculture for solving the current problems and challenges at field level. A conceptual model is developed for proper implementation of available big data technologies at farmer's field level. The study highlights data generation procedure, availability of technology, availability of hardware, software, data collection techniques, method of analysis and suitability of application of big data technologies for smart agriculture. The article explores that there are still some challenges exists in this field as a new domain in agriculture like privacy of data, data quality, availability, initial investment, infrastructure and related expertise. The study suggests that government initiatives, public-private partnership, openness of data, financial investment and regional basis research work are necessary for implementing the big data technologies in agriculture at large scale.},
keywords={Agriculture;Big Data;Production;Sensors;Systematics;Sociology;Statistics;big data;smart agriculture;data driven;precision agriculture;smart farming},
doi={10.1109/ICAIBD.2019.8836982},
ISSN={},
month={May},}
@INPROCEEDINGS{8406658,
author={Palacio, Ana León and López, Óscar Pastor},
booktitle={2018 12th International Conference on Research Challenges in Information Science (RCIS)}, title={From big data to smart data: A genomic information systems perspective},
year={2018},
volume={},
number={},
pages={1-11},
abstract={During the last two decades, data generated by Next Generation Sequencing Technologies have revolutionized our understanding of human biology and improved the study on how changes (variations) in the DNA are involved in the risk of suffering a certain disease. A huge amount of genomic data is publicly available and frequently used by the research community in order to extract meaningful and reliable gene-disease relationships. However, management of this exponential growth of data has become a challenge for biologists; under such a big data problem perspective, they are forced to delve into a lake of complex data spread in over thousand heterogeneous repositories, represented in multiple formats and with different levels of quality; but when data are used to solve a concrete problem only a small part of that “data lake” is really significant; this is what we call the “smart” data perspective. Using conceptual models and the principles of data quality management, adapted to the genomic domain, we propose a systematic approach to move from a big data to a smart data perspective. The aim of this approach is to populate an Information System with genomic data which must be accessible, informative and actionable enough to extract valuable knowledge.},
keywords={Genomics;Bioinformatics;Big Data;Data integrity;Diseases;Data models;Conceptual Modelling;Data Quality;Big Data;Smart Data;Genomics},
doi={10.1109/RCIS.2018.8406658},
ISSN={2151-1357},
month={May},}
@ARTICLE{7937830,
author={Benbernou, Salima and Huang, Xin and Ouziri, Mourad},
journal={IEEE Transactions on Big Data}, title={Semantic-Based and Entity-Resolution Fusion to Enhance Quality of Big RDF Data},
year={2021},
volume={7},
number={2},
pages={436-450},
abstract={Within an organisation, the quality in big data is a cornerstone to operational, transactional processes and to the reliability of business analytics for decision making. In fact, as organizations are harnessing multi-sources data to rise the benefits of their business, the quality of data becomes important and crucial. This paper presents a new approach to query big data sources using Resource Description Framework (RDF) representation to ensure data quality by harvesting more relevant and complete query results. Our approach handles two important types of heterogeneity over multiple data sources: semantic heterogeneity and URI-based entity identification. It proposes (1) a semantic entity resolution method based on inference mechanism using rules to manage the misunderstanding of data, in real world entities (2) Data Quality enhancement using MapReduce-based query rewriting approach includes the entity resolution results to infer and adds implicit data into query results (3) a parallel combination of MapReduce jobs of saturation and query rewriting inferences to handle transitive and cyclic rules for a richer rules' expression language (4) experiments to assess the efficiency of the proposed approach over real big RDF data originating from insurance and synthetic data sets.},
keywords={Big Data;Resource description framework;Semantics;Joining processes;Erbium;Organizations;Data quality;big data fusion;inferences;entity resolution;query rewriting},
doi={10.1109/TBDATA.2017.2710346},
ISSN={2332-7790},
month={June},}
@INPROCEEDINGS{9599192,
author={Zhang, Zhenwei and Wu, Wenyan and Wu, Dongjie},
booktitle={2021 2nd International Symposium on Computer Engineering and Intelligent Communications (ISCEIC)}, title={A Multi-Mode Learning Behavior Real-time Data Acquisition Method Based on Data Quality},
year={2021},
volume={},
number={},
pages={64-69},
abstract={With the rapid development of new technologies such as artificial intelligence, big data, and the Internet of Things, many researchers have probed into the study of learning analysis, trying to solve the problems of teaching by analyzing the learning behavior data from learning process. And in many learning behavior research, the sensor network usually consists of a host of mutually independent data sources, which can be used to monitor measured objects from multiple dimensions thereby obtaining the multi-source multi-modal sensory data. However, there still exist false negative readings, false positive readings and environmental interference, etc. Therefore, we propose a multi-source multimode sensory data acquisition method based on Date Quality(DQ). We first define the data quality in terms of four aspects-accuracy, integrity, consistency and instantaneity. Then, by the modeling there aspects respectively, we propose metrics to estimate the comprehensive data quality method of multi-source multi-mode sensory data. Finally, a data acquisition method is presented based on data quality, which selects a part of data sources for data transmission according to the given precision. This method aims at reducing the consumption of the sensory network on the premise of the data quality guarantee. An extensive experimental evaluation demonstrates the efficiency and effectiveness of the algorithm.},
keywords={Measurement;Data integrity;Data acquisition;Learning (artificial intelligence);Interference;Data models;Real-time systems;multi-mode Data;Learning behavior;data quality;data acquisition},
doi={10.1109/ISCEIC53685.2021.00021},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8603594,
author={Farooqi, M. Mashab and Ali Khattak, Hasan and Imran, Muhammad},
booktitle={2018 14th International Conference on Emerging Technologies (ICET)}, title={Data Quality Techniques in the Internet of Things: Random Forest Regression},
year={2018},
volume={},
number={},
pages={1-4},
abstract={Internet of Things (IoTs) is one of the most promising fields in computer science. It consists of physical devices, automobiles, home appliances, embedded hardware, sensors and actuators which empowers these objects to interface and share information with other devices over the network. The data gathered from these devices is used to make intelligent decisions. If the data quality is poor, decisions are likely to be flawed. A little work has been carried out regarding data quality in the Internet of Things, but there is no scheme which is experimentally proved. In this paper we will identify data quality challenges in the Internet of Things domain and propose a model which ensure data quality standards provided by ISO 8000. We evaluated our model on the weather dataset and used the random forest prediction method to calculate the accuracy of our data. Results show that when compared with the baseline model the proposed system improves accuracy by 38.88%.},
keywords={Data integrity;Data models;Internet of Things;Cleaning;Meteorology;Sensors;Predictive models;data quality;internet of things;big data;machine learning},
doi={10.1109/ICET.2018.8603594},
ISSN={},
month={Nov},}
@ARTICLE{8361574,
author={Meng, Qianyu and Wang, Kun and He, Xiaoming and Guo, Minyi},
journal={Big Data Mining and Analytics}, title={QoE-driven big data management in pervasive edge computing environment},
year={2018},
volume={1},
number={3},
pages={222-233},
abstract={In the age of big data, services in the pervasive edge environment are expected to offer end-users better Quality-of-Experience (QoE) than that in a normal edge environment. However, the combined impact of the storage, delivery, and sensors used in various types of edge devices in this environment is producing volumes of high-dimensional big data that are increasingly pervasive and redundant. Therefore, enhancing the QoE has become a major challenge in high-dimensional big data in the pervasive edge computing environment. In this paper, to achieve high QoE, we propose a QoE model for evaluating the qualities of services in the pervasive edge computing environment. The QoE is related to the accuracy of high-dimensional big data and the transmission rate of this accurate data. To realize high accuracy of high-dimensional big data and the transmission of accurate data through out the pervasive edge computing environment, in this study we focused on the following two aspects. First, we formulate the issue as a high-dimensional big data management problem and test different transmission rates to acquire the best QoE. Then, with respect to accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on deep learning, which is suitable for high-dimensional big data analysis in the pervasive edge computing environment. Our simulation results reveal that our proposed algorithm can achieve high QoE performance.},
keywords={Big Data;Quality of experience;Edge computing;Quality of service;Computational modeling;Training;Streaming media;Quality-of-Experience (QoE); high-dimensional big data management; deep learning; pervasive edge},
doi={10.26599/BDMA.2018.9020020},
ISSN={2096-0654},
month={Sep.},}
@INPROCEEDINGS{8258543,
author={Liu, Lixin and Chen, Jun},
booktitle={2017 IEEE International Conference on Big Data (Big Data)}, title={The influences of deep-sea vision data quality on observational analysis},
year={2017},
volume={},
number={},
pages={4789-4791},
abstract={Deep-sea study by optical observation method is an interdisciplinary subject and faces plenty of difficulties. To find out the influences of vision data quality, characteristic of vision data for deep-sea observation is analyzed, and a deep-sea landing experiment has been implemented. Data quality analyzing based on real deep-sea vision data that collected in the in-situ observation platform is actualized. It is expected that the research on influence mechanism of deep-sea vision quality is beneficial to the detection of region of interest, the judging of animal existence, the classification of species, and the trajectories labeling. Further analyzing on unsupervised deep-sea vision data quality control is necessary.},
keywords={Scattering;Optical sensors;Optical imaging;Fish;Backscatter;deep-sea observation;vision data quality;automatic judging},
doi={10.1109/BigData.2017.8258543},
ISSN={},
month={Dec},}
@ARTICLE{9320548,
author={Luo, Xin and Chen, Minzhi and Wu, Hao and Liu, Zhigang and Yuan, Huaqiang and Zhou, MengChu},
journal={IEEE Transactions on Automation Science and Engineering}, title={Adjusting Learning Depth in Nonnegative Latent Factorization of Tensors for Accurately Modeling Temporal Patterns in Dynamic QoS Data},
year={2021},
volume={18},
number={4},
pages={2142-2155},
abstract={A nonnegative latent factorization of tensors (NLFT) model precisely represents the temporal patterns hidden in multichannel data emerging from various applications. It often adopts a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. However, learning depth in this algorithm is not adjustable, resulting in frequent training fluctuation or poor model convergence caused by overshooting. To address this issue, this study carefully investigates the connections between the performance of an NLFT model and its learning depth via SLF-NMUT to present a joint learning-depth-adjusting scheme for it. Based on this scheme, a Depth-adjusted Multiplicative Update on tensor algorithm is innovatively proposed, thereby achieving a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Empirical studies on two industrial data sets demonstrate that compared with the state-of-the-art NLFT models, a DNL model achieves significant accuracy gain when performing missing data estimation on a high-dimensional and incomplete tensor with high efficiency. Note to Practitioners—Multichannel data are often encountered in various big-data-related applications. It is vital for a data analyzer to correctly capture the temporal patterns hidden in them for efficient knowledge acquisition and representation. This article focuses on analyzing temporal QoS data, which is a representative kind of multichannel data. To correctly extract their temporal patterns, an analyzer should correctly describe their nonnegativity. Such a purpose can be achieved by building a nonnegative latent factorization of tensors (NLFT) model relying on a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. But its learning depth is not adjustable, making an NLFT model frequently suffer from severe fluctuations in its training error or even fail to converge. To address this issue, this study carefully investigates the learning rules for an NLFT model’s decision parameters using an SLF-NMUT and proposes a joint learning-depth-adjusting scheme. This scheme manipulates the multiplicative terms in SLF-NMUT-based learning rules linearly and exponentially, thereby making the learning depth adjustable. Based on it, this study builds a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Compared with the existing NLFT models, a DNL model better represents multichannel data. It meets industrial needs well and can be used to achieve high performance in data analysis tasks like temporal-aware missing data estimation},
keywords={Tensors;Big Data;Quality of service;Computational efficiency;Machine learning;Web services;Algorithm;big data;dynamics;high-dimensional and incomplete (HDI) data;machine learning;missing data estimation;multichannel data;nonnegative latent factorization of tensors (NLFT);temporal pattern;quality of service (QoS);web service},
doi={10.1109/TASE.2020.3040400},
ISSN={1558-3783},
month={Oct},}
@INPROCEEDINGS{7363818,
author={Bonner, Stephen and McGough, Andrew Stephen and Kureshi, Ibad and Brennan, John and Theodoropoulos, Georgios and Moss, Laura and Corsar, David and Antoniou, Grigoris},
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, title={Data quality assessment and anomaly detection via map/reduce and linked data: A case study in the medical domain},
year={2015},
volume={},
number={},
pages={737-746},
abstract={Recent technological advances in modern healthcare have lead to the ability to collect a vast wealth of patient monitoring data. This data can be utilised for patient diagnosis but it also holds the potential for use within medical research. However, these datasets often contain errors which limit their value to medical research, with one study finding error rates ranging from 2.3%-26.9% in a selection of medical databases. Previous methods for automatically assessing data quality normally rely on threshold rules, which are often unable to correctly identify errors, as further complex domain knowledge is required. To combat this, a semantic web based framework has previously been developed to assess the quality of medical data. However, early work, based solely on traditional semantic web technologies, revealed they are either unable or inefficient at scaling to the vast volumes of medical data. In this paper we present a new method for storing and querying medical RDF datasets using Hadoop Map / Reduce. This approach exploits the inherent parallelism found within RDF datasets and queries, allowing us to scale with both dataset and system size. Unlike previous solutions, this framework uses highly optimised (SPARQL) joining strategies, intelligent data caching and the use of a super-query to enable the completion of eight distinct SPARQL lookups, comprising over eighty distinct joins, in only two Map / Reduce iterations. Results are presented comparing both the Jena and a previous Hadoop implementation demonstrating the superior performance of the new methodology. The new method is shown to be five times faster than Jena and twice as fast as the previous approach.},
keywords={Resource description framework;Medical diagnostic imaging;Medical services;Big data;Sensors;Biomedical monitoring;RDF;Medical Data;Map / Reduce;Joins},
doi={10.1109/BigData.2015.7363818},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7840906,
author={Angryk, Rafal A. and Galarus, Douglas E.},
booktitle={2016 IEEE International Conference on Big Data (Big Data)}, title={The SMART approach to comprehensive quality assessment of site-based spatial-temporal data},
year={2016},
volume={},
number={},
pages={2636-2645},
abstract={There is a need for comprehensive solutions to address the challenges of spatio-temporal data quality assessment. Emphasis is often placed on the quality assessment of individual observations from sensors but not on the sensors themselves nor upon site metadata such as location and timestamps. The focus of this paper is on the development and evaluation of a representative and comprehensive, interpolation-based methodology for assessment of spatio-temporal data quality. We call our method the SMART method, short for Simple Mappings for the Approximation and Regression of Time series. When applied to a real-world, meteorological data set, we show that our method outperforms standard interpolators and we identify numerous problematic sites that otherwise would not have been flagged as bad. We further identify sites for which metadata is incorrect. We believe that there are many problems with real data sets like these and, in the absence of an approach like ours, these problems have largely gone unidentified. Our results bring into question the validity of provider-based quality control indicators. In addition to providing a comprehensive solution, our approach is novel for the simple but effective way that it accounts for spatial and temporal variation.},
keywords={Sensors;Quality control;Metadata;Meteorology;Interpolation;Quality assessment;Time series analysis;data quality;data stream processing;spatial-temporal data;quality control;interpolation},
doi={10.1109/BigData.2016.7840906},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7809598,
author={Kläs, Michael and Putz, Wolfgang and Lutz, Tobias},
booktitle={2016 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement (IWSM-MENSURA)}, title={Quality Evaluation for Big Data: A Scalable Assessment Approach and First Evaluation Results},
year={2016},
volume={},
number={},
pages={115-124},
abstract={High-quality data is a prerequisite for most types of analysis provided by software systems. However, since data quality does not come for free, it has to be assessed and managed continuously. The increasing quantity, diversity, and velocity that characterize big data today make these tasks even more challenging. We identified challenges that are specific for big data quality assessments with particular emphasis on their usage in smart ecosystems and make a proposal for a scalable cross-organizational approach that addresses these challenges. We developed an initial prototype to investigate scalability in a multi-node test environment using big data technologies. Based on the observed horizontal scalability behavior, there is an indication that the proposed approach also allows dealing with increasing volumes of heterogeneous data.},
keywords={Big data;Quality assessment;Metadata;Instruments;Ecosystems;Companies;big data quality assessment;quality measurement;velocity;volume;variety;SQA4BD;QUAMOCO;smart ecosystems;SPARK;HADOOP},
doi={10.1109/IWSM-Mensura.2016.026},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7818902,
author={Lakshen, Guma Abdulkhader and Vraneš, Sanja and Janev, Valentina},
booktitle={2016 24th Telecommunications Forum (TELFOR)}, title={Big data and quality: A literature review},
year={2016},
volume={},
number={},
pages={1-4},
abstract={Big Data refers to data volumes in the range of Exabyte (1018) and beyond. Such volumes exceed the capacity of current on-line storage and processing systems. With characteristics like volume, velocity and variety big data throws challenges to the traditional IT establishments. Computer assisted innovation, real time data analytics, customer-centric business intelligence, industry wide decision making and transparency are possible advantages, to mention few, of Big Data. There are many issues with Big Data that warrant quality assessment methods. The issues are pertaining to storage and transport, management, and processing. This paper throws light into the present state of quality issues related to Big Data. It provides valuable insights that can be used to leverage Big Data science activities.},
keywords={Big data;Benchmark testing;Quality assessment;Databases;Software;Sparks;Engines;Big Data;Quality assessment;stream processing;survey;Big Data frameworks},
doi={10.1109/TELFOR.2016.7818902},
ISSN={},
month={Nov},}
@ARTICLE{8658160,
author={Singh, Nitin and Lai, Kee-Hung and Vejvar, Markus and Cheng, T. C. E.},
journal={IEEE Engineering Management Review}, title={Big Data Technology: Challenges, Prospects, and Realities},
year={2019},
volume={47},
number={1},
pages={58-66},
abstract={We attempt to demonstrate the value of big data to enterprises by interweaving the perceptions, challenges, and opportunities of big data for businesses. While enterprises are aware of the value of big data to their businesses, there are challenges of exploiting big data in terms of data quality and usage. Executives might lack knowledge on how applications are related to one another in the big data ecosystem and the business benefits to reap. We summarize the results of a research study that explores emerging business perceptions of big data. We examine the current practice in 20 large enterprises, each having an annual revenue of more than USD 0.5 billion and present our findings related to executive perceptions. We provide insights on how firms can develop their big data expertise along various dimensions and identify critical ideas to be further investigated to better understand the issues that practitioners and researchers might be equally grappling with.},
keywords={Big Data;Data integrity;Data analysis;Software;Investment;Companies;Big data;business analytics;Hadoop;people;talent gap;implementation},
doi={10.1109/EMR.2019.2900208},
ISSN={1937-4178},
month={Firstquarter},}
@INPROCEEDINGS{9006422,
author={Grueneberg, K. and Calo, S. and Dewan, P. and Verma, D. and O’Gorman, Tristan},
booktitle={2019 IEEE International Conference on Big Data (Big Data)}, title={A Policy-based Approach for Measuring Data Quality},
year={2019},
volume={},
number={},
pages={4025-4031},
abstract={With the growing importance of data in all aspects of the functioning of an enterprise, having good quality of data is crucial in support of business processes. However, there do not exist good metrics to measure the quality of data that is available within an enterprise. While there are several data quality standards, their complexity and their required customization makes them difficult to use in real-world industrial scenarios. In this paper, we discuss the challenges encountered in measuring data quality within asset management systems. We propose a policy-based approach for measuring data quality, and show how such an approach can be customized and interpreted easily by practitioners in the field.},
keywords={Data integrity;Standards;Measurement;Asset management;Data models;Complexity theory;Data Quality;Asset Management Systems;Policy based Data Management},
doi={10.1109/BigData47090.2019.9006422},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8421858,
author={Jiang, Wei and Ning, Xiuli and Xu, Yingcheng},
booktitle={2018 5th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2018 4th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom)}, title={Review on Big Data Fusion Methods of Quality Inspection for Consumer Goods},
year={2018},
volume={},
number={},
pages={95-102},
abstract={Quality big data comes from a wide range, the problem is how to eliminate the structure barriers between various types of data from different sources, to achieve the effective integration for isolated or fragmented data, and then to mine the information, knowledge and wisdom according to the actual needs. It is urgent for the quality inspection departments to solve the problem of making a decision the first time and take preventive measures. This paper introduced the research status of named entity recognition, solid unified detection, data conflict resolution, data fusion method from the characteristics of quality inspection data. The existence problems are analyzed and the research direction is looking forward of the research on data fusion in this paper.},
keywords={Big Data;Inspection;Data integration;Semantics;Hidden Markov models;Thesauri;Support vector machines;Big data of quality inspection;data fusion;named entity recognition;entity resolution;data conflict resolution},
doi={10.1109/CSCloud/EdgeCom.2018.00025},
ISSN={},
month={June},}
@INPROCEEDINGS{7364058,
author={Feng, Tao and Zhuang, Zhenyun and Pan, Yi and Ramachandra, Haricharan},
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, title={A memory capacity model for high performing data-filtering applications in Samza framework},
year={2015},
volume={},
number={},
pages={2600-2605},
abstract={Data quality is essential in big data paradigm as poor data can have serious consequences when dealing with large volumes of data. While it is trivial to spot poor data for small-scale and offline use cases, it is challenging to detect and fix data inconsistency in large-scale and online (real-time or near-real time) big data context. An example of such scenario is spotting and fixing poor data using Apache Samza, a stream processing framework that has been increasingly adopted to process near-real-time data at LinkedIn. To optimize the deployment of Samza processing and reduce business cost, in this work we propose a memory capacity model for Apache Samza to allow denser deployments of high performing data-filtering applications built on Samza. The model can be used to provision just-enough memory resource to applications by tightening the bounds on the memory allocations. We apply our memory capacity model on Linkedln's real use cases in production, which significantly increases the deployment density and saves business costs. We will share key learning in this paper.},
keywords={Containers;LinkedIn;Data models;Big data;Java;Measurement;Real-time systems;Apache Samza;capacity model;data filtering;performance},
doi={10.1109/BigData.2015.7364058},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9079066,
author={Mohammad, Banan and Alzyadat, Wael and Al-Fayoumi, Mohammad and EL Hawi, Ruba and Alhroob, Aysh},
booktitle={2020 11th International Conference on Information and Communication Systems (ICICS)}, title={An Approach to Improve Data Quality from Big Data Aspect by Sensitive Cost and Time},
year={2020},
volume={},
number={},
pages={022-026},
abstract={Big data is term of dataset with characteristic volume, value and veracity that lead to challenges unable proceed using traditional techniques to extract value, project management perspective is dynamic processing that utilizes the appropriate resources of organization in many phases by measuring in four-factor scope, time, cost and quality. In this research aim improve data quality from big data via project management scope depends on high trust which is getting high accuracy from confidence level in volume of data, confidence get with context and value of data which lead to determine accuracy deeply in it and finally choose from data depending on veracity of it, the experiment using three main factors time, cost and scope, strongest relation arranging between them start by project scope as strongest one then cost, product and finally time is weakest between them, in the final when select best quality use two sides generally from quality degree and be middle-quality interval and especially from relative distance with the strongest factor.},
keywords={Costs;Correlation;Statistical analysis;Data integrity;Volume measurement;Project management;Big Data;Big Data;Project Management;Sensitive Rule;Quality},
doi={10.1109/ICICS49469.2020.239526},
ISSN={2573-3346},
month={April},}
@INPROCEEDINGS{9251206,
author={Jiang, Yushan and Liu, Yongxin and Liu, Dahai and Song, Houbing},
booktitle={2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)}, title={Applying Machine Learning to Aviation Big Data for Flight Delay Prediction},
year={2020},
volume={},
number={},
pages={665-672},
abstract={Flight delay has been a serious and widespread problem that needs to be solved. One promising solution is the flight delay prediction. Although big data analytics and machine learning have been applied successfully in many domains, their applications in aviation are limited. This paper presents a comprehensive study of flight delay spanning data pre-processing, data visualization and data mining, in which we develop several machine learning models to predict flight arrival delays. Two data sets were used, namely Airline On-Time Performance (AOTP) Data and Quality Controlled Local Climatological Data (QCLCD). This paper aims to recognize useful patterns of the flight delay from aviation data and perform accurate delay prediction. The best result for flight delay prediction (five classes) using machine learning models is 89.07% (Multilayer Perceptron). A Convolution neural network model is also built which is enlightened by the idea of pattern recognition and success of neural network method, showing a slightly better result with 89.32% prediction accuracy.},
keywords={Atmospheric modeling;Neural networks;Machine learning;Predictive models;Big Data;Delays;Quantum cascade lasers;Flight Delay;Machine Learning;Aviation Data Analytics},
doi={10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00114},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9102068,
author={Wang, Jiye and Li, Yang and Guo, Jian and Cao, Junwei and Hua, Haochen and Xing, Chunxiao and Qi, Caijuan and Pi, Zhixian},
booktitle={2020 IEEE 7th International Conference on Industrial Engineering and Applications (ICIEA)}, title={Data Quality Analysis Framework and Evaluation Methods for Power System Operation with High Proportion of Renewable Energy Penetration},
year={2020},
volume={},
number={},
pages={687-692},
abstract={Global climate crisis in 21st century pushed countries to move towards energy transformation in generation and consumption. To achieve green and low-carbon energy transformation goals, it is necessary that a large number of renewable energy resources such as wind and solar to be consumed. Renewable energy with intermittent fluctuations in time dimension and agglomerations in spatial dimension increases the complexity of green energy consumption friendly. Therefore, comprehensive data and advanced predictive analysis methods are required to guarantee safety of operation and transactions for renewable energy plants and stations. We can even say that quality of renewable energy data determines the accuracy of prediction and analysis. Firstly, this article analyzes the operation and transaction characteristics of distributed renewable energy plants, and data quality analysis framework for distributed renewable energy operations and transactions was built on the new energy cloud platform. Data information were classified into model parameter and status instance, which are related to dispatching and energy power transaction businesses such as equipment model management, operation monitoring and security analysis, measurement statistics etc. The importance between them is determined according to pairwise comparison. Finally, analytic hierarchy process (AHP) theory was applied to calculate weights for data integrity, accuracy, consistency and timeliness, data quality assessment process and calculation methods were designed, and load series data was used to verify its correctness.},
keywords={Data integrity;Renewable energy sources;Data models;Analytical models;Production;Monitoring;Clouds;renewable energy cloud;data quality analysis;AHP;evaluation metrics},
doi={10.1109/ICIEA49774.2020.9102068},
ISSN={},
month={April},}
@INPROCEEDINGS{9378401,
author={Makkar, Himanshu and Toshniwal, Durga and Jangra, Shalini},
booktitle={2020 IEEE International Conference on Big Data (Big Data)}, title={Closed Itemset based Sensitive Pattern Hiding for Improved Data Utility and Scalability},
year={2020},
volume={},
number={},
pages={4026-4035},
abstract={Frequent itemset mining is used to extract interesting associations and correlations between the itemsets present in transactional datasets. The frequently appearing patterns are used for various business decision making policies, for instance to increase co-purchase of products, price optimization, cross promotion etc. However, there are some sensitive patterns present in datasets that can reveal individual or organisation's specific confidential information that they would not prefer to be known since it can cause them huge social and monetary loss. Privacy Preserving Data Mining (PPDM) approaches are used to hide these sensitive patterns with maintaining the utility of the data. Heuristics-based PPDM approaches are widely adopted sensitive pattern hiding approaches due to their simplicity and lesser computational time as compared to the border-based and exact approaches. However, these approaches causes high side effects concerning the quality of datasets. In this paper, two heuristics-based algorithms, Removal of Closed Sensitive Itemsets with Maximum Support (MaxRCSI) and Removal of Closed Sensitive Itemsets with Minimum Support (MinRCSI), are proposed. In these algorithms, data sanitization is performed over closed sensitive itemsets to improve the utility of sanitized data. The proposed algorithms are parallelized on Spark parallel computing framework to deal with the massive amount of data i.e. big data. Experiments performed on real and synthetic datasets show that the proposed algorithms preserve the privacy of datasets with substantially better utility as compared to the traditional algorithms with less execution time.},
keywords={Data privacy;Itemsets;Heuristic algorithms;Scalability;Big Data;Parallel processing;Sparks;Privacy Preserving Data Mining;Data Sanitisation;Reduced Sensitive itemset;Data Quality},
doi={10.1109/BigData50022.2020.9378401},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8539254,
author={Hossen, J. and Jesmeen H, M. Z. and Sayeed, Shohel},
booktitle={2018 7th International Conference on Computer and Communication Engineering (ICCCE)}, title={Modifying Cleaning Method in Big Data Analytics Process using Random Forest Classifier},
year={2018},
volume={},
number={},
pages={208-213},
abstract={Accurate data is a key success factor influencing the performance of data analytics results, especially for the detection and prediction purpose. Nowadays, Big Data analytics (BDA) is used to analyze the sheer volume of data available in an organization. These data quality must be maintained in order to obtain correct alert and valuable insights from the rapidly changing data of high volume, velocity, variety, veracity, and value. This paper aim is to modify existing framework of big data analytics by improving an important step in pre-processing (i.e. Data Cleaning). Initially, feature selection based on Random Forest is used to extract effective features. Then, two classifier algorithms (i.e. Random Forest classifier and Linear SVM classifier) are applied to train using the dataset to classify data quality and to develop an intelligent model. In evaluation, our experimental results show a consistent accuracy of Random Forest and Linear Regression around 90%. Using this approach, we expect to provide a set of cleaned data for further processing. Besides, analysts can benefit from this system in data analytical process in cleaning stage and conclude that the data is cleaned. Finally, a comparison is presented between available functions which are used to handle missing values with the developed system.},
keywords={Forestry;Support vector machines;Data models;Cleaning;Feature extraction;Training;Big Data;Big data;Data Analytics;missing data;data cleaning;Machine learning;Random Forest;Gini Index},
doi={10.1109/ICCCE.2018.8539254},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7751629,
author={Li, Mingxin and Wei, Heng and Liao, Hongxi},
booktitle={2016 16th International Symposium on Communications and Information Technologies (ISCIT)}, title={Mobile terminal quality of experience analysis based on big data},
year={2016},
volume={},
number={},
pages={241-245},
abstract={In this paper, we proposes a method to analyze and evaluate the quality of experience (QoE) in mobile terminals using “big data”. The feature parameters of key quality indicator (KQI) are obtained from operators and quantized through the use of a scoring system. Then the scores of customer experience indicator (CEI) and QoE are calculated based on our proposed analytical model. In combination with the data of market operation, the terminal QoE evaluation scores contribute to offer effective suggestions on the promotion of mobile terminal.},
keywords={Indexes;Mobile communication;Mobile computing;Big data;Internet;Delays;big data;terminal;QoE;variable coefficient method},
doi={10.1109/ISCIT.2016.7751629},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7273312,
author={Wang, May D.},
booktitle={2015 IEEE 39th Annual Computer Software and Applications Conference}, title={Biomedical Big Data Analytics for Patient-Centric and Outcome-Driven Precision Health},
year={2015},
volume={3},
number={},
pages={1-2},
abstract={Rapid advancements in biotechnologies such as -omic (genomics, proteomics, metabolomics, lipidomics etc.), next generation sequencing, bio-nanotechnologies, molecular imaging, and mobile sensors etc. accelerate the data explosion in biomedicine and health wellness. Multiple nations around the world have been seeking novel effective ways to make sense of "big data" for evidence-based, outcome-driven, and affordable 5P (Patient-centric, Predictive, Preventive, Personalized, and Precise) healthcare. My main research focus is on multi-modal and multi-scale (i.e. molecular, cellular, whole body, individual, and population) biomedical data analytics for discovery, development, and delivery, including translational bioinformatics in biomarker discovery for personalized care; imaging informatics in histopathology for clinical diagnosis decision support; bionanoinformatics for minimally-invasive image-guided surgery; critical care informatics in ICU for real-time evidence-based decision making; and chronic care informatics for patient-centric health. In this talk, first, I will highlight major challenges in biomedical and health informatics pipeline consisting of data quality control, information feature extraction, advanced knowledge modeling, decision making, and proper action taking through feedback. Second, I will present informatics methodological research in (i) data integrity and integration; (ii) case-based reasoning for individualized care; and (iii) streaming data analytics for real-time decision support using a few mobile health case studies (e.g. Sickle Cell Disease, asthma, pain management, rehabilitation, diabetes etc.). Last, there is big shortage of data scientists and engineers who are capable of handling Big Data. In addition, there is an urgent need to educate healthcare stakeholders (i.e. patients, physicians, payers, and hospitals) on how to tackle these grant challenges. I will discuss efforts such as patient-centric educational intervention, community-based crowd sourcing, and Biomedical Data Analytics MOOC development. Our research has been supported by NIH, NSF, Georgia Research Alliance, Georgia Cancer Coalition, Emory-Georgia Tech Cancer Nanotechnology Center, Children's Health Care of Atlanta, Atlanta Clinical and Translational Science Institute, and industrial partners such as Microsoft Research and HP.},
keywords={Biomedical imaging;Informatics;Bioinformatics;Big data;Cancer;Decision making},
doi={10.1109/COMPSAC.2015.343},
ISSN={0730-3157},
month={July},}
@INPROCEEDINGS{9378296,
author={Shrivastava, Shrey and Patel, Dhaval and Zhou, Nianjun and Iyengar, Arun and Bhamidipaty, Anuradha},
booktitle={2020 IEEE International Conference on Big Data (Big Data)}, title={DQLearn : A Toolkit for Structured Data Quality Learning},
year={2020},
volume={},
number={},
pages={1644-1653},
abstract={Data Quality (DQ) has been one of the key focuses as Data Analytics and Artificial Intelligence (AI) fields continue to grow. Yet, data quality analysis has mostly been a disjointed, ad-hoc, and cumbersome process in the overall data analysis workflow. There have been ongoing attempts to formalize this process, but the solutions that have come out are not universally applicable. Most of the proposed solutions try to address the problem of data quality from a limited perspective and suc-cessfully address only a subset of all challenges. These solutions fail to translate to other domains due to a lack of structure. In this paper, we present DQLearn, a toolkit for structured data quality learning. We start by presenting the core principle on which we build our library and introduce the four components that provide a solid base to address the needs of the data quality problem. Then, we showcase our automation structure - "Workflows", and the two optimization techniques equipped with it, that help the users to structure their learning problem very easily. Next, we discuss four important scenarios of the DQ Workflows in the overall life-cycle. Finally, we demonstrate the utility of the proposed toolkit with public datasets and show benchmark results from optimization experiments.},
keywords={Data analysis;Automation;Data integrity;Optimization methods;Big Data;Solids;Task analysis},
doi={10.1109/BigData50022.2020.9378296},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7363743,
author={Libes, Don and Shin, Seungjun and Woo, Jungyub},
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, title={Considerations and recommendations for data availability for data analytics for manufacturing},
year={2015},
volume={},
number={},
pages={68-75},
abstract={Data analytics is increasingly becoming recognized as a valuable set of tools and techniques for improving performance in the manufacturing enterprise. However, data analytics requires data and a lack of useful and usable data has become an impediment to research in data analytics. In this paper, we describe issues that would help aid data availability including data quality, reliability, efficiency, and formats specific to data analytics in manufacturing. To encourage data availability, we present recommendations and requirements to guide future data contributions. We also describe the need for data for challenge problems in data analytics. A better understanding of these needs, recommendations, and requirements may improve the ability of researchers and other practitioners to improve research and more rapidly deploy data analytics in manufacturing.},
keywords={Data analysis;Manufacturing;Sensors;Encryption;NIST;Synchronization;big data;challenge problems;data analytics;data quality;requirements;smart manufacturing},
doi={10.1109/BigData.2015.7363743},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6816764,
author={Saha, Barna and Srivastava, Divesh},
booktitle={2014 IEEE 30th International Conference on Data Engineering}, title={Data quality: The other face of Big Data},
year={2014},
volume={},
number={},
pages={1294-1297},
abstract={In our Big Data era, data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Recent studies have shown that poor quality data is prevalent in large databases and on the Web. Since poor quality data can have serious consequences on the results of data analyses, the importance of veracity, the fourth `V' of big data is increasingly being recognized. In this tutorial, we highlight the substantial challenges that the first three `V's, volume, velocity and variety, bring to dealing with veracity in big data. Due to the sheer volume and velocity of data, one needs to understand and (possibly) repair erroneous data in a scalable and timely manner. With the variety of data, often from a diversity of sources, data quality rules cannot be specified a priori; one needs to let the “data to speak for itself” in order to discover the semantics of the data. This tutorial presents recent results that are relevant to big data quality management, focusing on the two major dimensions of (i) discovering quality issues from the data itself, and (ii) trading-off accuracy vs efficiency, and identifies a range of open problems for the community.},
keywords={Information management;Data handling;Data storage systems;Databases;Maintenance engineering;Quality management;Cleaning},
doi={10.1109/ICDE.2014.6816764},
ISSN={2375-026X},
month={March},}
@INPROCEEDINGS{8258222,
author={Lazar, Alina and Jin, Ling and Spurlock, C. Anna and Wu, Kesheng and Sim, Alex},
booktitle={2017 IEEE International Conference on Big Data (Big Data)}, title={Data quality challenges with missing values and mixed types in joint sequence analysis},
year={2017},
volume={},
number={},
pages={2620-2627},
abstract={The goal of this paper is to investigate the impact of missing values in categorical time series sequences on common data analysis tasks. Being able to more effectively identify patterns in socio-demographic longitudinal data is an important component in a number of social science settings. However, performing fundamental analytical operations, such as clustering for grouping these data based on similarity patterns, is challenging due to the categorical and multi-dimensional nature of the data, and their corruption by missing and inconsistent values. To study these data quality issues, we employ longitudinal sequence data representations, a similarity measure designed for categorical and longitudinal data, together with state-of-the art clustering methodologies reliant on hierarchical algorithms. The key to quantifying the similarity and difference among data records is a distance metric. Given the categorical nature of our data, we employ an “edit” type distance using Optimal Matching (OM). Because each data record has multiple variables of different types, we investigate the impact of mixing these variables in a single similarity measure. Between variables with binary values and those with multiple nominal values, we find that the ability to overcome missing data problems is harder in the nominal domain versus the binary domain. Additionally, artificial clusters introduced by the alignment of leading missing values can be resolved by tuning the missing value substitution cost parameter.},
keywords={Trajectory;Sequences;Time series analysis;Education;Employment;joint sequence analysis;optimal matching;missing values;time series clustering;data quality},
doi={10.1109/BigData.2017.8258222},
ISSN={},
month={Dec},}
@ARTICLE{8809689,
author={Fiore, Sandro and Elia, Donatello and Pires, Carlos Eduardo and Mestre, Demetrio Gomes and Cappiello, Cinzia and Vitali, Monica and Andrade, Nazareno and Braz, Tarciso and Lezzi, Daniele and Moraes, Regina and Basso, Tania and Kozievitch, Nádia P. and Fonseca, Keiko Verônica Ono and Antunes, Nuno and Vieira, Marco and Palazzo, Cosimo and Blanquer, Ignacio and Meira, Wagner and Aloisio, Giovanni},
journal={IEEE Access}, title={An Integrated Big and Fast Data Analytics Platform for Smart Urban Transportation Management},
year={2019},
volume={7},
number={},
pages={117652-117677},
abstract={Smart urban transportation management can be considered as a multifaceted big data challenge. It strongly relies on the information collected into multiple, widespread, and heterogeneous data sources as well as on the ability to extract actionable insights from them. Besides data, full stack (from platform to services and applications) Information and Communications Technology (ICT) solutions need to be specifically adopted to address smart cities challenges. Smart urban transportation management is one of the key use cases addressed in the context of the EUBra-BIGSEA (Europe-Brazil Collaboration of Big Data Scientific Research through Cloud-Centric Applications) project. This paper specifically focuses on the City Administration Dashboard, a public transport analytics application that has been developed on top of the EUBra-BIGSEA platform and used by the Municipality stakeholders of Curitiba, Brazil, to tackle urban traffic data analysis and planning challenges. The solution proposed in this paper joins together a scalable big and fast data analytics platform, a flexible and dynamic cloud infrastructure, data quality and entity matching algorithms as well as security and privacy techniques. By exploiting an interoperable programming framework based on Python Application Programming Interface (API), it allows an easy, rapid and transparent development of smart cities applications.},
keywords={Urban areas;Big Data;Data analysis;Transportation;Cloud computing;Data mining;Europe;Big data;cloud computing;data analytics;data privacy;data quality;distributed environment;public transport management;smart city},
doi={10.1109/ACCESS.2019.2936941},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9148148,
author={Bai, Zhongxian and Zhuo, Rongqing},
booktitle={2020 International Conference on Computer Information and Big Data Applications (CIBDA)}, title={Quality Management of Crowd Sensing Data Based on Machine Learning},
year={2020},
volume={},
number={},
pages={185-188},
abstract={Recently, research on crowd sensing data quality management is a new subject area developed based on wireless sensor network related concepts. Crowd sensing data has also experienced many links in the process of network propagation, so it is inevitable that there are abnormal data in the database. Therefore, how to filter these unreliable data to get more real data is particularly important. This paper takes somatosensory temperature as an example, solves the problem of calculating the similarity of unequal long-term sequences by using DTW technology, and then clusters and compares the data in the database to find out the abnormal data. Thereby, the accuracy of the somatosensory temperature database is improved, and relevant users can obtain more accurate information. The experimental results show that when the minimum DTW value exceeds the threshold t = 0.7, the more the number of simulation sequences, the more stable the accuracy rate, and the faster the growth rate of the running time.},
keywords={Time series analysis;Temperature sensors;Mobile handsets;Temperature distribution;Databases;Data models;Machine Learning;Crowd Sensing;Big Data Analysis;Abnormal Data Detection Management;Clustering Method},
doi={10.1109/CIBDA50819.2020.00049},
ISSN={},
month={April},}
@INPROCEEDINGS{9672007,
author={Zhou, Xiantian and Ordonez, Carlos},
booktitle={2021 IEEE International Conference on Big Data (Big Data)}, title={Programming Languages in Data Science: a Comparison from a Database Angle},
year={2021},
volume={},
number={},
pages={3147-3154},
abstract={In a typical Data Science project, the analyst uses many programming languages to explore and analyze big data coming from diverse data sources. A major challenge is managing and pre-processing so much data, with potentially inconsistent content, significant redundancy, in diverse formats, with varying data quality. Database systems research has tackled such problems for a long time, but mostly on relational databases. With such motivation in mind, this paper compares strengths and weaknesses of popular languages used nowadays from a database pespective: Python, R and SQL. We discuss the entire analytic pipeline, going from data integration, cleaning and pre-processing to model application and tuning. From a database systems perspective, we present a comprehensive survey of storage mechanisms, data processing algorithms, external algorithms, run-time memory management, consistency, optimizations and parallel processing. From a programming languages angle, we consider elegance, expressiveness, abstraction, composability, interactive behavior and automatic code optimization. We present a short experimental evaluation comparing the performance of the three languages on typical data exploration and pre-processing tasks. Our conclusion: there is no winner.},
keywords={Soft sensors;Redundancy;Relational databases;Big Data;Data science;Database systems;Task analysis},
doi={10.1109/BigData52589.2021.9672007},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8289792,
author={El-Ghafar, Randa M. Abd and Gheith, Mervat H. and El-Bastawissy, Ali H. and Nasr, Eman S.},
booktitle={2017 13th International Computer Engineering Conference (ICENCO)}, title={Record linkage approaches in big data: A state of art study},
year={2017},
volume={},
number={},
pages={224-230},
abstract={Record Linkage aims to find records in a dataset that represent the same real-world entity across many different data sources. It is a crucial task for data quality. With the evolution of Big Data, new difficulties appeared to deal mainly with the 5Vs of Big Data properties; i.e. Volume, Variety, Velocity, Value, and Veracity. Therefore Record Linkage in Big Data is more challenging. This paper investigates ways to apply Record Linkage algorithms that handle the Volume property of Big Data. Our investigation revealed four major issues. First, the techniques used to resolve the Volume property of Big Data mainly depend on partitioning the data into a number of blocks. The processing of those blocks is parallelly distributed among many executers. Second, MapReduce is the most famous programming model that is designed for parallel processing of Big Data. Third, a blocking key is usually used for partitioning the big dataset into smaller blocks; it is often created by the concatenation of the prefixes of chosen attributes. Partitioning using a blocking key may lead to unbalancing blocks, which is known as data skew, where data is not evenly distributed among blocks. An uneven distribution of data degrades the performance of the overall execution of the MapReduce model. Fourth, to the best of our knowledge, a small number of studies has been done so far to balance the load between data blocks in a MapReduce framework. Hence more work should be dedicated to balancing the load between the distributed blocks.},
keywords={Couplings;Big Data;Programming;Data integration;Task analysis;Data models;Databases;Big Data;Big Data Integration;blocking;entity matching;entity resolution;Hadoop;machine learning;MapReduce;Record Linkage},
doi={10.1109/ICENCO.2017.8289792},
ISSN={2475-2320},
month={Dec},}
@INPROCEEDINGS{8121809,
author={Liu, He and Chen, Jiangqi and Huang, Fupeng and Li, Han},
booktitle={2017 14th International Symposium on Pervasive Systems, Algorithms and Networks 2017 11th International Conference on Frontier of Computer Science and Technology 2017 Third International Symposium of Creative Computing (ISPAN-FCST-ISCC)}, title={An Electric Power Sensor Data Oriented Data Cleaning Solution},
year={2017},
volume={},
number={},
pages={430-435},
abstract={With the development of Smart Grid Technology, more and more electric power sensor data are utilized in various electric power systems. To guarantee the effectiveness of such systems, it is necessary to ensure the quality of electric power sensor data, especially when the scale of electric power sensor data is large. In the field of large-scale electric power sensor data cleaning, the computational efficiency and accuracy of data cleaning are two vital requirements. In order to satisfy these requirements, this paper presents an electric power sensor data oriented data cleaning solution, which is composed of a data cleaning framework and a data cleaning method. Based on Hadoop, the given framework is able to support large-scale electric power sensor data acquisition, storage and processing. Meanwhile, the proposed method which achieves outlier detection and reparation is implemented on the basis of a time-relevant k-means clustering algorithm in Spark. The feasibility and effectiveness of the proposed method is evaluated on a data set which originates from charging piles. Experimental results show that the proposed data cleaning method is able to improve the data quality of electric power sensor data by finding and repairing most outliers. For large-scale electric power sensor data, the proposed data cleaning method has high parallel performance and strong scalability.},
keywords={Power systems;Cleaning;Clustering algorithms;Big Data;Sparks;Data acquisition;Algorithm design and analysis;electric power senser data;data cleaning;k-means clustering;outlier;Spark},
doi={10.1109/ISPAN-FCST-ISCC.2017.29},
ISSN={2375-527X},
month={June},}
@INPROCEEDINGS{9397990,
author={Labeeb, Kashshaf and Chowdhury, Kuraish Bin Quader and Riha, Rabea Basri and Abedin, Mohammad Zoynul and Yesmin, Sarmila and Khan, Mohammad Nasfikur Rahman},
booktitle={2020 IEEE International Women in Engineering (WIE) Conference on Electrical and Computer Engineering (WIECON-ECE)}, title={Pre-Processing Data In Weather Monitoring Application By Using Big Data Quality Framework},
year={2020},
volume={},
number={},
pages={284-287},
abstract={In this research, we are working with Big Data for obtaining, preparing and analyzing data-based information to make use of the data retrieved which will benefit any organization. It is a progressing part of all divisions of industry and business. All organizations in any field, for example, oil, money, fabricating hardware and so forth produce big data, which can show incredibly helpful designs to business directors to make and develop their organizations, when the information is gathered and analyzed accurately. It permits us to gather, store, and decipher immense measures of big data to produce useful outcomes. Data quality is affected by the information that is gathered to be analyzed as that data will make sure whether in the long run a specific method of conducting the ongoing process is useful or not. Consequently, the consistency of big data very important. Here, we propose that the various types of raw information should be analyzed to expand its precision in the pre-handling stage, as those pieces of information are not utilized later in the process. During investments, we break down and model the big data to decrease overhead expenses to create and add to a solid understanding of results to improve information consistency.},
keywords={Solid modeling;Oils;Organizations;Big Data;Solids;Monitoring;Meteorology;heterogeneous data;noise filter;convergence;climate change},
doi={10.1109/WIECON-ECE52138.2020.9397990},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8748630,
author={Saha, Ajitesh Kumar and Kumar, Ashwini and Tyagi, Vishu and Das, Sanjoy},
booktitle={2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)}, title={Big Data and Internet of Things: A Survey},
year={2018},
volume={},
number={},
pages={150-156},
abstract={In this digital era, here the sum of data is generate with store has prolonged inside a less period of time. The data in this era generated high speed leads to many challenges. The size is primarily and periodically, just the measurement that bounces in the big data position. In this survey, we have attempt to give a broad explanation of big data that captures its other unique and important features. We have discussed 4V's model. Also, the latest technologies uses in big data, like Hadoop, HDFS, MapReduce and different type of methods used by big data. Finally various benefits of using big data analysis and include some feature of cloud computing.},
keywords={Big Data;Reliability;Cloud computing;Social networking (online);Sensors;Videos;Data integrity;Big Data;Big Data Analytics;Big Data Quality;Data Reliability;IOT;Hadoop;HDFS;Cloud Computing},
doi={10.1109/ICACCCN.2018.8748630},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8241335,
author={Mejía-Lavalle, Manuel and Meusel, Winfrid and Tavira, Jonathan Villanueva and Cruz, Mirian Calderón},
booktitle={2017 International Conference on Mechatronics, Electronics and Automotive Engineering (ICMEAE)}, title={Effective Data Quality Diagnostic Schema for Big Data},
year={2017},
volume={},
number={},
pages={163-168},
abstract={Big Data environment is a computing area with a great growth. Today it is common that we hear about databases with huge volumes of information and also we hear about Data Mining and Business Intelligence projects related with these huge databases. However, in general, little attention has been given to the quality of the data. Here we propose and present innovative metrics and schema designed to perform a basic task related to the Data Quality issue, this is, the diagnostic. The preliminary results that we obtained when we apply our approaches to Big Data encourage us to continue this work.},
keywords={Databases;Big Data;Measurement;Cleaning;Data mining},
doi={10.1109/ICMEAE.2017.29},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9284399,
author={Mystakidis, Aristeidis and Tjortjis, Christos},
booktitle={2020 11th International Conference on Information, Intelligence, Systems and Applications (IISA}, title={Big Data Mining for Smart Cities: Predicting Traffic Congestion using Classification},
year={2020},
volume={},
number={},
pages={1-8},
abstract={This paper provides an analysis and proposes a methodology for predicting traffic congestion. Several machine learning algorithms and approaches are compared to select the most appropriate one. The methodology was implemented using Data Mining and Big Data techniques along with Python, SQL, and GIS technologies and was tested on data originating from one of the most problematic, regarding traffic congestion, streets in Thessaloniki, the 2nd most populated city in Greece. Evaluation and results have shown that data quality and size were the most critical factors towards algorithmic accuracy. Result comparison showed that Decision Trees were more accurate than Logistic Regression.},
keywords={Machine learning algorithms;Smart cities;Big Data;Prediction algorithms;Data mining;Traffic congestion;Regression tree analysis;Data Mining;Big Data;Machine learning;Smart Cities;Prediction;Classification;Traffic Congestion},
doi={10.1109/IISA50023.2020.9284399},
ISSN={},
month={July},}
@INPROCEEDINGS{9378483,
author={Hossain, Md Monir and Sebestyen, Mark and Mayank, Dhruv and Ardakanian, Omid and Khazaei, Hamzeh},
booktitle={2020 IEEE International Conference on Big Data (Big Data)}, title={Large-scale Data-driven Segmentation of Banking Customers},
year={2020},
volume={},
number={},
pages={4392-4401},
abstract={This paper presents a novel big data analytics framework for creating explainable personas for retail and business banking customers. These personas are essential to better tailor financial products and improve customer retention. This framework is comprised of several components including anomaly detection, binning and aggregation of contextual data, clustering of transaction time series, and mining association rules that map contextual data to cluster identifiers. Leveraging rich transaction and contextual data available from nearly 60,000 retail and 90,000 business customers of a financial institution, we empirically evaluate this framework and describe how the identified association rules can be used to explain and refine existing customer classes, and identify new customer classes and various data quality issues. We also analyze the performance of the proposed framework and show that it can easily scale to millions of banking customers.},
keywords={Data integrity;Conferences;Time series analysis;Banking;Big Data;Anomaly detection;Business;customer segmentation;clustering;association rules mining;anomaly detection},
doi={10.1109/BigData50022.2020.9378483},
ISSN={},
month={Dec},}
@ARTICLE{9082104,
author={Darwish, Tasneem S. J. and Bakar, Kamalrulnizam Abu and Kaiwartya, Omprakash and Lloret, Jaime},
journal={IEEE Transactions on Vehicular Technology}, title={TRADING: Traffic Aware Data Offloading for Big Data Enabled Intelligent Transportation System},
year={2020},
volume={69},
number={7},
pages={6869-6879},
abstract={Todays' Intelligent Transportation System (ITS) applications majorly depend on either limited neighbouring traffic data or crowd sourced stale traffic data. Enabling big traffic data analytics in ITS environments is a step closer towards utilizing significant traffic patterns and trends for making more precise and intelligent decisions particularly in connected autonomous vehicular environments. Towards this end, this paper presents a Traffic Aware Data Offloading (TRADING) approach for big traffic data centric ITS applications in connected autonomous vehicular environments. Specifically, TRADING balances offloading data traffic among gateways focusing on vehicular traffic and network status in the vicinity of gateways. In addition, TRADING mitigates the effect of gateway advertisement overhead to liberate the transmission channels for traffic big data transmission. The performance of TRADING is comparatively evaluated in a realistic simulation environment by considering gateway access overhead, load distribution among gateways, data offloading delay, and data offloading success ratio. The comparative performance evaluation results show some significant developments towards enabling big traffic data centric ITS.},
keywords={Logic gates;Big Data;Quality of service;Delays;Real-time systems;Safety;Roads;Big data;gateway;intelligent transportation systems;VANET;vehicle-to-internet},
doi={10.1109/TVT.2020.2991372},
ISSN={1939-9359},
month={July},}
@ARTICLE{8667006,
author={Xu, Xuefang and Lei, Yaguo and Li, Zeda},
journal={IEEE Transactions on Industrial Electronics}, title={An Incorrect Data Detection Method for Big Data Cleaning of Machinery Condition Monitoring},
year={2020},
volume={67},
number={3},
pages={2326-2336},
abstract={The presence of incorrect data leads to the decrease of condition-monitoring big data quality. As a result, unreliable or misleading results are probably obtained by analyzing these poor-quality data. In this paper, to improve the data quality, an incorrect data detection method based on an improved local outlier factor (LOF) is proposed for data cleaning. First, a sliding window technique is used to divide data into different segments. These segments are considered as different objects and their attributes consist of time-domain statistical features extracted from each segment, such as mean, maximum and peak-to-peak value. Second, a kernel-based LOF (KLOF) is calculated using these attributes to evaluate the degree of each segment being incorrect data. Third, according to these KLOF values and a threshold value, incorrect data are detected. Finally, a simulation of vibration data generated by a defective rolling element bearing and three real cases concerning a fixed-axle gearbox, a wind turbine, and a planetary gearbox are used to verify the effectiveness of the proposed method, respectively. The results demonstrate that the proposed method is able to detect both missing segments and abnormal segments, which are two typical incorrect data, effectively, and thus is helpful for big data cleaning of machinery condition monitoring.},
keywords={Big Data;Machinery;Feature extraction;Condition monitoring;Data integrity;Fault diagnosis;Cleaning;Condition-monitoring big data;data cleaning;data quality;incorrect data;local outlier factor (LOF)},
doi={10.1109/TIE.2019.2903774},
ISSN={1557-9948},
month={March},}
@INPROCEEDINGS{7584934,
author={Yin, Jianwei and Tang, Yan and Lo, Wei and Wu, Zhaohui},
booktitle={2016 IEEE International Congress on Big Data (BigData Congress)}, title={From Big Data to Great Services},
year={2016},
volume={},
number={},
pages={165-172},
abstract={Big Data is increasingly adopted by a wide range of service industries to improve the quality and value of their services, e.g., inventory that matches well the supply and demand, and pricing that reflects well the market needs. Customers benefit from higher quality of service enabled by Big Data. Service providers get higher profits from more precise control of costs and accurate knowledge of customer needs. In this paper, we define the next generation high quality services as Great Services, characterized by 4P Quality-of-Service (QoS) dimensions: Panorama, Penetration, Prediction and Personalization, which go much further than current services. The transformation of Big Data into Great Services would be difficult and expensive without methodical techniques and software tools. We call the intermediate step Deep Knowledge, which is generated by Big Data (with the 4V challenges - Volume, Velocity, Variety, and Veracity) and used in the creation of Great Services. Deep Knowledge is distinguished from traditional Big Data by 4C properties (Complexity, Cross-domain, Customization, and Convergence). In order to achieve the 4P QoS dimensions of Great Services, we need Deep Knowledge with 4C properties. In this paper, we describe an informal characterization of Great Services with 4P QoS dimensions with examples, and outline the techniques and tools that facilitate the transformation of Big Data into Deep Knowledge with 4C properties, and then the use of Deep Knowledge in Great Services.},
keywords={Big data;Quality of service;Public transportation;Vehicles;Industries;Next generation networking;Complexity theory;Great Services;4P QoS},
doi={10.1109/BigDataCongress.2016.28},
ISSN={},
month={June},}
@INPROCEEDINGS{8991080,
author={Guo, Peng and Yang, Guosheng and Wang, Wenhuan and Yan, Zhoutian and Zhang, Lie and Zhang, Hanfang},
booktitle={2019 International Conference on Electronic Engineering and Informatics (EEI)}, title={Relay Protection Data Integrity Check Method Based on Big Data Association Algorithm},
year={2019},
volume={},
number={},
pages={506-508},
abstract={Relay protection big data creates good conditions for the improvement of professional applications, and data integrity is an important aspect that reflects data quality. The association of relay protection big data is intense. This paper applies Apriori algorithm to mine data relevance and generate association rules. Based on this, the integrity of relay protection data is checked, and the incomplete data is predicted. Taking the relay protection defect data as an example, the paper explores the correlation among 251 items of the six dimensions of protection relay defect data such as type of protection, the severity of the defect, whether the protection is out of operation, the defect location, the cause of the defect, and the equipment manufacturer, completing the processing of incomplete data with great application results.},
keywords={Apriori algorithm;relay protection;defect data;integrity check},
doi={10.1109/EEI48997.2019.00115},
ISSN={},
month={Nov},}
@ARTICLE{9496639,
author={Khan, Abudul Wahid and Khan, Maseeh Ullah and Khan, Javed Ali and Ahmad, Arshad and Khan, Khalil and Zamir, Muhammad and Kim, Wonjoon and Ijaz, Muhammad Fazal},
journal={IEEE Access}, title={Analyzing and Evaluating Critical Challenges and Practices for Software Vendor Organizations to Secure Big Data on Cloud Computing: An AHP-Based Systematic Approach},
year={2021},
volume={9},
number={},
pages={107309-107332},
abstract={Recently, its becomes easy to track down the data due to its availability in a large number. Although for data management, processing, and obtainability, cloud computing is considered a well-known approach for organizational development on the internet. Despite many advantages, cloud computing has still numerous security challenges that can affect the big-data usage on cloud computing. To find the security issues/challenges that are faced by software vendors’ organizations we conducted a systematic literature review (SLR) through which we have find out 103 relevant research publications by developing a search string that is inspired by the research questions. This relevant data was comprised from different databases e.g. Google Scholar, IEEE Explore, ScienceDirect, ACM Digital Library, and SpringerLink. Furthermore, for the detailed literature review, we have accomplished all the steps in SLR, for example, development of SLR protocol, Initials and final assortment of the relevant data, data extraction, data quality assessment, and data synthesis. We identified fifteen (15) critical security challenges which are: data secrecy, geographical data location, unauthorized data access, lack of control, lack of data management, network-level issues, data integrity, data recovery, lack of trust, data sharing, data availability, asset issues, legal amenabilities, lack of quality, and lack of consistency. Furthermore, sixty four (64) standard practices are identified for these critical security challenges using the proposed SLR that could help vendor organizations to overcome the security challenges for big data. The findings of our research study demonstrate the resemblances and divergences in the identified security challenges in different periods, continents, databases, and methods. The proposed SLR will also support software vendor organizations for securing big data on the cloud computing platforms. This paper has the following content: in Section II, we have describe the Literature review; in Section III, research methodology is specified; in Section IV, the findings of the SLR and the analysis of result are discussed; in Section V, the limitations of this research are given; in Section VI, we discussed our conclusions and future work.},
keywords={Cloud computing;Security;Big Data;Software;Organizations;Social networking (online);STEM;Security challenges;big data;cloud computing;SLR;vendor;SPSS},
doi={10.1109/ACCESS.2021.3100287},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7371861,
author={Laranjeiro, Nuno and Soydemir, Seyma Nur and Bernardino, Jorge},
booktitle={2015 IEEE 21st Pacific Rim International Symposium on Dependable Computing (PRDC)}, title={A Survey on Data Quality: Classifying Poor Data},
year={2015},
volume={},
number={},
pages={179-188},
abstract={Data is part of our everyday life and an essential asset in numerous businesses and organizations. The quality of the data, i.e., the degree to which the data characteristics fulfill requirements, can have a tremendous impact on the businesses themselves, the companies, or even in human lives. In fact, research and industry reports show that huge amounts of capital are spent to improve the quality of the data being used in many systems, sometimes even only to understand the quality of the information in use. Considering the variety of dimensions, characteristics, business views, or simply the specificities of the systems being evaluated, understanding how to measure data quality can be an extremely difficult task. In this paper we survey the state of the art in classification of poor data, including the definition of dimensions and specific data problems, we identify frequently used dimensions and map data quality problems to the identified dimensions. The huge variety of terms and definitions found suggests that further standardization efforts are required. Also, data quality research on Big Data appears to be in its initial steps, leaving open space for further research.},
keywords={Standards organizations;Industries;Training;Companies;Decision making;Poor data quality;dirty data;poor data classification;data quality problems},
doi={10.1109/PRDC.2015.41},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8444581,
author={Zhang, Mingming and Wo, Tianyu and Xie, Tao},
booktitle={2018 IEEE International Conference on Pervasive Computing and Communications (PerCom)}, title={A Platform Solution of Data-Quality Improvement for Internet-of-Vehicle Services},
year={2018},
volume={},
number={},
pages={1-7},
abstract={Interconnection and intelligence have become the latest trends of the new generation of vehicle and transportation technologies. Applications built upon platforms of cloud-centered vehicle networking, i.e., Internet-of-Vehicles (IoVs), have been increasingly developed and deployed to provide data-centric services (e.g., driving assistance). Because these services are often safety critical, assuring service dependability has become an important requirement. In this paper, we propose DQI, a platform-level solution of Data-Quality Improvement designed to assure service dependability for Internet-of-Vehicle services. As an example, DQI is deployed in CarStream, an industrial system of big data processing designed for chauffeured car services. Via CarStream, over 30,000 vehicles are organized in a virtual vehicle network by sharing vehicle-status data in a near real-time manner. Such data often have low-quality issues and compromise the dependability of data-centric services. DQI includes techniques of data-quality improvement, including detecting outliers, extracting frequent patterns, and interpolating sequences. DQI enhances the dependability of data-centric services in IoVs by addressing the common data-quality requirements at the platform level. Upper-level services can benefit from DQI for data-quality improvement and reduce the complexity of service logic. We evaluate DQI by using a three-year dataset of vehicles and real applications deployed in CarStream. The result shows that compared with existing approaches, DQI can effectively restore missing data and correct anomalies with more than 30.0% improvement in precision. By studying multiple real applications, we also show that this data-quality improvement can indeed enhance the dependability of IoV services.},
keywords={Interpolation;Task analysis;Inspection;Cloud computing;Roads;Data integrity;Conferences;Dependability;Interpolation;Sequence Matching;Data Quality;Big Data;Internet-of-Vehicles},
doi={10.1109/PERCOM.2018.8444581},
ISSN={2474-249X},
month={March},}
@INPROCEEDINGS{9263243,
author={Yousfi, Aola and El Yazidi, Moulay Hafid and Zellou, Ahmed},
booktitle={2020 International Conference on Advanced Computer Science and Information Systems (ICACSIS)}, title={HASSO: A Highly-Automated Source Selection and Ordering System Based on Data Quality Factors},
year={2020},
volume={},
number={},
pages={155-164},
abstract={Big data integration gives access to a large number of data sources through a unified user interface. Answers include high-quality data, medium-quality data, and low-quality data. Selecting a subset of high accurate and consistent data sources and ordering them appropriately is critical to obtain as many high-quality answers as possible right after querying few data sources. However, the process of selecting and ordering data sources can be quite complicated and can present several challenges. The main challenge faced during that process is identifying the most adequate data quality factors to consider. In this paper, we present HASSO, a Highly-Automated Source Selection and Ordering System based on data quality factors. To produce consistent and high accurate answers, HASSO identifies, for each data source, its domain, data consistency and data accuracy using the schema matches. To maximize the total number of complete and non-redundant answers returned right after querying a small number of data sources, HASSO orders data sources in terms of their data overlap and in a decreasing order of their overall coverage. Experimental results in real-world domains show that HASSO produces high-quality answers at high speed.},
keywords={Computer science;Data integrity;User interfaces;Big Data;Information systems;Source Selection;Source Ordering;Data Quality;Big Data Integration;Semantic Similarity},
doi={10.1109/ICACSIS51025.2020.9263243},
ISSN={2330-4588},
month={Oct},}
@INPROCEEDINGS{7029280,
author={Earley, Seth},
booktitle={2014 IT Professional Conference}, title={Presentation 1. Information governance in the age of big data},
year={2014},
volume={},
number={},
pages={1-3},
abstract={Organizations have understood the value of their structured data — mostly financial transactions — since the first mainframes were developed in the 40's. Data quality issues have always been a challenge and the increasing numbers of applications consuming and producing structured transactional data has grown exponentially. Unstructured information has been given less significance and strategic importance and therefore fewer resources and less attention on the part of leadership. All of that has changed and is changing faster than anyone imagined. Unstructured content is what humans produce. They create the documents: strategies, proposals, support documents, marketing content, white papers, engineering specifications, etc. that form the intelligence and core knowledge capital of the enterprise. Many organizations have left business units to fend for themselves and “go figure it out” with little guidance or support. These has led to terabytes of content that people cannot find their way through and that leaves the organization open to risks, liabilities and costs of e discovery. According to the Minnesota Journal of Law, Science & Technology, a gig of data costs $30,000 in e discovery costs. The cost of storage is ten cents. The problem is that the enterprise does not understand the hidden costs of not making data accessible and usable — lost time, lost IP, inefficiency, poor customer service that can lead to lost customers, slower growth, etc. As newer collaboration technologies are deployed, they expose the bad habits and sins of the past. Deploying a new search engine shows that the content is not curated. Standing up a new content management application like SharePoint reveals the haphazard shanty town of an information architecture with inconsistencies in models, terminology and applications. Today's landscape of marketing and customer experience technologies is complex and interconnected and requires those upstream knowledge processes that produce unstructured content as the fuel. Customer experience entails everything that happens before you purchase (marketing, education and outreach), when you purchase (e commerce with product content and data), and after you purchase (self-service systems and knowledge bases that support the call center). This is the customer lifecycle and at each step in the process systems and tools need to be harmonized as they gather information about the users using attribute models that are consistent and that serve the business and the customer. They take content and data as input and then output more data. One applications exhaust is another applications fuel. Organizations are also purchasing data streams to enrich their internal information sources. Social media is an enormous virtually untapped reservoir of data about customers and what they think about organizations. This can be mined for sentiment and to gauge marketing effectiveness. Increasingly, much of this is being placed into the hands of the marketing organization. In fact, a study by Gartner Group said that by 2017 the CMO will spend more money on IT than the CIO. What all of this means is that information, content and data governance need to be considered as part of a whole and not as separate initiatives. Elements of good information governance include: • Deployment and Operationalization • Alignment with User Needs • Business Value • Buy-In and Change Management • Sponsorship and Accountability Leads to the following outcomes: • Manages conflicts in business priorities (between initiatives, business units, drivers, etc) • Allows for ongoing input from various stakeholders and constituencies in order to evolve capabilities with the needs of the business • Prioritizes efforts and allocation of resources • Assigns roles and responsibilities with accountability to critical functions • Takes into consideration various levels of maturity in the organization — no one size fits all • Ensures that investments in systems, processes and tools are providing sufficient return to the business • Balances centralized standards with decentralized decision making • Aligns incentives to use a system with business goals This session will review governance concepts, discuss how they apply to various types of data and content and provide a framework for developing governance processes and structures.},
keywords={Organizations;Standards organizations;Information architecture;Fuels;Information management;Big data},
doi={10.1109/ITPRO.2014.7029280},
ISSN={},
month={May},}
@INPROCEEDINGS{9357200,
author={Ezzine, Imane and Benhlima, Laila},
booktitle={2020 6th IEEE Congress on Information Science and Technology (CiSt)}, title={Technology against COVID-19 A Blockchain-based framework for Data Quality},
year={2020},
volume={},
number={},
pages={84-89},
abstract={The effects of COVID-19 have quickly spread around the world, testing the limits of the population and the public health sector. High demand on medical services are offset by disruptions in daily operations as hospitals struggle to function in the face of overcapacity, understaffing and information gaps. Faced with these problems, new technologies are being deployed to fight this pandemic and help medical staff governments to reduce its spread. Among these technologies, we find blockchains and Big Data which have been used in tracking, prediction applications and others. However, despite the help that these new technologies have provided, they remain limited if the data with which they are fed are not of good quality. In this paper, we highlight some benefits of using BIG Data and Blockchain to deal with this pandemic and some data quality issues that still present challenges to decision making. Finally we present a general Blockchain-based framework for data governance that aims to ensure a high level of data trust, security, and privacy.},
keywords={COVID-19;Pandemics;Data integrity;Blockchain;Big Data;Statistics;Testing;Covid-19;Blockchain;Big Data;Data Quality;data governance},
doi={10.1109/CiSt49399.2021.9357200},
ISSN={2327-1884},
month={June},}
@INPROCEEDINGS{8900190,
author={Han, Weiguo and Jochum, Matthew},
booktitle={IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium}, title={Practices and Experiences in High Volumes of Satellite Data Management},
year={2019},
volume={},
number={},
pages={4364-4367},
abstract={High volumes of satellite data management within an organization is still challenging and daunting in the era of big data. The increasing information technology costs and limited budgets, growing satellite data needs, data availability across multiple teams and projects, strategic goals of organization, and expected project outcomes require better satellite data management mechanism and system to facilitate research and development activities. An organization level centralized satellite data repository is a practical solution to satisfy these requirements. This paper describes the best practices and experiences from building such a central satellite data repository within our organization, including data management strategy and policy, scalable and extensible system infrastructure, comprehensive data management system, and technical support and user assistance. These practices can be borrowed and applied in other organizations with similar requirements.},
keywords={Satellites;Organizations;Research and development;Databases;Monitoring;Buildings;Data integrity;Satellite Data Management;Big Data;Data Quality;Central Data Repository},
doi={10.1109/IGARSS.2019.8900190},
ISSN={2153-7003},
month={July},}
@INPROCEEDINGS{8456103,
author={Liu, Kai-Cheng and Kuo, Chuan-Wei and Liao, Wen-Chiuan and Wang, Pang-Chieh},
booktitle={2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)}, title={Optimized Data de-Identification Using Multidimensional k-Anonymity},
year={2018},
volume={},
number={},
pages={1610-1614},
abstract={In the globalized knowledge economy, big data analytics have been widely applied in diverse areas. A critical issue in big data analysis on personal information is the possible leak of personal privacy. Therefore, it is necessary to have an anonymization-based de-identification method to avoid undesirable privacy leak. Such method can prevent published data form being traced back to personal privacy. Prior empirical researches have provided approaches to reduce privacy leak risk, e.g. Maximum Distance to Average Vector (MDAV), Condensation Approach and Differential Privacy. However, previous methods inevitably generate synthetic data of different sizes and is thus unsuitable for general use. To satisfy the need of general use, k-anonymity can be chosen as a privacy protection mechanism in the de-identification process to ensure the data not to be distorted, because k-anonymity is strong in both protecting privacy and preserving data authenticity. Accordingly, this study proposes an optimized multidimensional method for anonymizing data based on both the priority weight-adjusted method and the mean difference recommending tree method (MDR tree method). The results of this study reveal that this new method generate more reliable anonymous data and reduce the information loss rate.},
keywords={Data privacy;Privacy;Numerical models;Big Data;Measurement;Data models;Greedy algorithms;privacy preserving;k-anonymity;de-identification;data quality;information loss},
doi={10.1109/TrustCom/BigDataSE.2018.00235},
ISSN={2324-9013},
month={Aug},}
@INPROCEEDINGS{9179628,
author={Tsoumakos, Dimitrios and Giannakopoulos, Ioannis},
booktitle={2020 IEEE Sixth International Conference on Big Data Computing Service and Applications (BigDataService)}, title={Content-Based Analytics: Moving beyond Data Size},
year={2020},
volume={},
number={},
pages={33-40},
abstract={Efforts on Big Data technologies have been highly directed towards the amount of data a task can access or crunch. Yet, for content-driven decision making, it is not (only) about the size, but about the "right" data: The number of available datasets (a different type of volume) can reach astronomical sizes, making a thorough evaluation of each input prohibitively expensive. The problem is exacerbated as data sources regularly exhibit varying levels of uncertainty and velocity/churn. To date, there exists no efficient method to quantify the impact of numerous available datasets over different analytics tasks and workflows. This visionary work puts the spotlight on data content rather than size. It proposes a novel modeling, planning and processing research bundle that assesses data quality in terms of analytics performance. The main expected outcome is to provide efficient, continuous and intelligent management and execution of content-driven data analytics. Intelligent dataset selection can achieve massive gains on both accuracy and time required to reach a desired level of performance. This work introduces the notion of utilizing dataset similarity to infer operator behavior and, consequently, be able to build scalable, operator-agnostic performance models for Big Data tasks over different domains. We present an overview of the promising results from our initial work with numerical and graph data and respective operators. We then describe a reference architecture with specific areas of research that need to be tackled in order to provide a data-centric analytics ecosystem.},
keywords={Data models;Analytical models;Task analysis;Predictive models;Uncertainty;Biological system modeling;Numerical models;modeling;data quality;big data;Machine Learning;scheduling},
doi={10.1109/BigDataService49289.2020.00013},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9403739,
author={Wei, Li and Dawei, Wang and Lixia, Wang},
booktitle={2020 International Conference on Big Data Artificial Intelligence Software Engineering (ICBASE)}, title={Research on data Traceability Method Based on blockchain Technology},
year={2020},
volume={},
number={},
pages={45-49},
abstract={Energy Internet is a major innovation to deal with the environmental crisis and efficient energy management and use in the current society. The important condition to achieve this goal is to summarize, integrate, process and apply the data of various industries in the energy field, and then support the relevant management and decision-making. In this process, how to ensure the authenticity and credibility of data is one of the keys in the construction of energy Internet. Therefore, this paper will study the application scenarios of blockchain technology in data traceability. With the help of the natural characteristics of blockchain, such as decentralized, distributed storage, tamper proof, open and transparent, combined with relevant national standards and international theoretical models, based on the needs of energy Internet data integration and management, this paper will develop a data traceability method suitable for the energy industry, and build a covering energy Data life cycle model of Internet. Through the research of this paper, we can help all localities to establish data traceability mechanism in the energy Internet, to help users to accurately grasp where the data is created, what systems have been transferred, which users have carried out query and modification, and so on, so as to realize the monitoring and control of the whole process of data flow, which helps to improve the credibility of data, and also helps to ensure the safety and quality of data and promote the construction of energy Internet huge data application.},
keywords={Industries;Technological innovation;Distributed databases;Blockchain;Data models;Internet;Safety;blockchain;data traceability;data quality;data security;data governance;energy Internet;huge data},
doi={10.1109/ICBASE51474.2020.00017},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8622989,
author={Micic, Natasha and Neagu, Daniel and Torgunov, Denis and Campean, Felician},
booktitle={2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)}, title={Exploring Methods for Comparing Similarity of Dimensionally Inconsistent Multivariate Numerical Data},
year={2018},
volume={},
number={},
pages={1528-1535},
abstract={When developing multivariate data classification and clustering methodologies for data mining, it is clear that most literature contributions only really consider data that contain consistently the same attributes. There are however many cases in current big data analytics applications where for same topic and even same source data sets there are differing attributes being measured, for a multitude of reasons (whether the specific design of an experiment or poor data quality and consistency). We define this class of data a dimensionally inconsistent multivariate data, a topic that can be considered a subclass of the Big Data Variety research. This paper explores some classification methodologies commonly used in multivariate classification and clustering tasks and considers how these traditional methodologies could be adapted to compare dimensionally inconsistent data sets. The study focuses on adapting two similarity measures: Robinson-Foulds tree distance metrics and Variation of Information; for comparing clustering of hierarchical cluster algorithms (such clusters are derived from the raw multivariate data). The results from experiments on engineering data highlight that adapting pairwise measures to exclude non-common attributes from the traditional distance metrics may not be the best method of classification. We suggest that more specialised metrics of similarity are required to address challenges presented by dimensionally inconsistent multivariate data, with specific applications for big engineering data analytics.},
keywords={Measurement;Time series analysis;Feature extraction;Phylogeny;Mutual information;Big Data;Data mining;Similarity measures;Robinson Foulds;Variation of Information;Engineering data;Multivariate numerical data;Dimensional inconsistency},
doi={10.1109/HPCC/SmartCity/DSS.2018.00251},
ISSN={},
month={June},}
@INPROCEEDINGS{8113071,
author={Yu, Weiqing and Zhu, Wendong and Liu, Guangyi and Kan, Bowen and Zhao, Ting and Liu, He},
booktitle={2017 3rd International Conference on Big Data Computing and Communications (BIGCOM)}, title={Cluster-Based Best Match Scanning for Large-Scale Missing Data Imputation},
year={2017},
volume={},
number={},
pages={232-238},
abstract={High-quality data are the prerequisite for analyzing and using big data to guarantee the value of the data. Missing values in data is a common yet challenging problem in data analytics and data mining, especially in the era of big data. Amount of missing values directly affects the data quality. Therefore, it is critical to properly recover missing values in the dataset. This paper presents a new imputation algorithm called Cluster-based Best Match Scanning (CBMS) designed for Big Data. It is a modification of k-NN imputation. CBMS focuses on recovering continuous numeric missing values, and aims at balancing computational complexity and accuracy. As an imputation algorithm, it can potentially reduce the time complexity of k-NN from O(n^2*d) to O(n^1.5*d), and also reduce the space/memory usage, while perform no worse than k-NN imputation. On top of that CBMS is highly parallelizable.Simulation of CBMS is conducted on smart meter reading data. Data is manually divided into training set and testing set, and testing accuracy is evaluated by computing the mean absolute deviation. Comparison with linear interpolation and k-NN imputation is made to demonstrate the power and effectiveness of our proposed CBMS algorithm.},
keywords={Time complexity;Clustering algorithms;Algorithm design and analysis;Estimation;Big Data;Data mining;big data;cluster-based best match scanning;data imputation;k-NN},
doi={10.1109/BIGCOM.2017.48},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9377900,
author={Tawakuli, Amal and Kaiser, Daniel and Engel, Thomas},
booktitle={2020 IEEE International Conference on Big Data (Big Data)}, title={Synchronized Preprocessing of Sensor Data},
year={2020},
volume={},
number={},
pages={3522-3531},
abstract={Sensor data whether collected for machine learning, deep learning or other applications must be preprocessed to fit input requirements or improve performance and accuracy. Data preparation is an expensive, resource consuming and complex phase often performed centrally on raw data for a specific application. The dataflow between the edge and the cloud can be enhanced in terms of efficiency, reliability and lineage by preprocessing the datasets closer to their data sources. We propose a dedicated data preprocessing framework that distributes preprocessing tasks between a cloud stage and two edge stages to create a dataflow with progressively improving quality. The framework handles heterogenous data and dynamic preprocessing plans simultaneously targeting diverse applications and use cases from different domains. Each stage autonomously executes sensor specific preprocessing plans in parallel while synchronizing the progressive execution and dynamic updates of the preprocessing plans with the other stages. Our approach minimizes the workload on central infrastructures and reduces the resources used for transferring raw data from the edge. We also demonstrate that preprocessing data can be sensor specific rather than application specific and thus can be performed prior to knowing a specific application.},
keywords={Deep learning;Cloud computing;Data preprocessing;Big Data;Synchronization;Reliability;Task analysis;Data Quality;Data Preprocessing;Sensor Data;Edge Computing;Data Management},
doi={10.1109/BigData50022.2020.9377900},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9671538,
author={Feric, Zlatan and Agostini, Nicolas Bohm and Beene, Daniel and Signes-Pastor, Antonio J. and Halchenko, Yuliya and Watkins, Deborah and MacKenzie, Debra and Karagas, Margaret and Manjourides, Justin and Alshawabkeh, Akram and Kaeli, David},
booktitle={2021 IEEE International Conference on Big Data (Big Data)}, title={A Secure and Reusable Software Architecture for Supporting Online Data Harmonization},
year={2021},
volume={},
number={},
pages={2801-2812},
abstract={Retrospective data harmonization across multiple research cohorts and studies is frequently done to increase statistical power, provide comparison analysis, and create a richer data source for data mining. However, when combining disparate data sources, harmonization projects face data management and analysis challenges. These include differences in the data dictionaries and variable definitions, privacy concerns surrounding health data representing sensitive populations, and lack of properly defined data models. With the availability of mature open-source web-based database technologies, developing a complete software architecture to overcome the challenges associated with the harmonization process can alleviate many roadblocks. By leveraging state-of-the-art software engineering and database principles, we can ensure data quality and enable cross-center online access and collaboration.This paper outlines a complete software architecture developed and customized using the Django web framework, leveraged to harmonize sensitive data collected from three NIH-support birth cohorts. We describe our framework and show how we successfully overcame challenges faced when harmonizing data from these cohorts. We discuss our efforts in data cleaning, data sharing, data transformation, data visualization, and analytics, while reflecting on what we have learned to date from these harmonized datasets.},
keywords={Dictionaries;Software architecture;Databases;Soft sensors;Data visualization;Big Data;Natural language processing},
doi={10.1109/BigData52589.2021.9671538},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8075416,
author={Xie, Huan and Tong, Xiaohua and Meng, Wen and Wang, Fang and Xu, Xiong},
booktitle={2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)}, title={Multiple stratified sampling strategy for assessing the big remote sensing products},
year={2015},
volume={},
number={},
pages={1-4},
abstract={The number and volume of remote sensing data and its derived products, which are regarded as typical “big data”, have grown exponentially. How to assess the quality of these big remote sensing products become a challenge. As an importance technique, spatial sampling is regarded to be necessary for the quality assessment of remote sensing derived products. This paper proposes an approach of multiple stratified spatial sampling for assessing the remote sensing products, with the aim of resolving the issue of the quality inspection of big remote sensing products. The proposed method improves the sampling accuracy without increasing the sampling size, and the whole procedure is repeatable and easily adopted for the quality inspection of remote sensing derived products.},
keywords={Remote sensing;Inspection;Sampling methods;Sociology;Big Data;Quality assessment;multiple stratified;spatial sampling;quality assessment;remote sensing products;big data},
doi={10.1109/WHISPERS.2015.8075416},
ISSN={2158-6276},
month={June},}
@INPROCEEDINGS{8622249,
author={Huang, Yu and Milani, Mostafa and Chiang, Fei},
booktitle={2018 IEEE International Conference on Big Data (Big Data)}, title={PACAS: Privacy-Aware, Data Cleaning-as-a-Service},
year={2018},
volume={},
number={},
pages={1023-1030},
abstract={Data cleaning consumes up to 80% of the data analysis pipeline. This is a significant overhead for organizations where data cleaning is still a manually driven process requiring domain expertise. Recent advances have fueled a new computing paradigm called Database-as-a-Service, where data management tasks are outsourced to large service providers. We propose a new Data Cleaning-as-a-Service model that allows a client to interact with a data cleaning provider who hosts curated, and sensitive data. We present PACAS: a Privacy-Aware data Cleaning-As-a-Service framework that facilitates communication between the client and the service provider via a data pricing scheme where clients issue queries, and the service provider returns clean answers for a price while protecting her data. We propose a practical privacy model in such interactive settings called (X,Y,L)-anonymity that extends existing data publishing techniques to consider the data semantics while protecting sensitive values. Our evaluation over real data shows that PACAS effectively safeguards semantically related sensitive values, and provides improved accuracy over existing privacy-aware cleaning techniques.},
keywords={Cleaning;Data privacy;Pricing;Data models;Semantics;Osteoarthritis;Maintenance engineering;data quality;data cleaning;data privacy},
doi={10.1109/BigData.2018.8622249},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9101464,
author={Swami, Arun and Vasudevan, Sriram and Huyn, Joojay},
booktitle={2020 IEEE 36th International Conference on Data Engineering (ICDE)}, title={Data Sentinel: A Declarative Production-Scale Data Validation Platform},
year={2020},
volume={},
number={},
pages={1579-1590},
abstract={Many organizations process big data for important business operations and decisions. Hence, data quality greatly affects their success. Data quality problems continue to be widespread, costing US businesses an estimated $600 billion annually. To date, addressing data quality in production environments still poses many challenges: easily defining properties of high-quality data; validating production-scale data in a timely manner; debugging poor quality data; designing data quality solutions to be easy to use, understand, and operate; and designing data quality solutions to easily integrate with other systems. Current data validation solutions do not comprehensively address these challenges. To address data quality in production environments at LinkedIn, we developed Data Sentinel, a declarative production-scale data validation platform. In a simple and well-structured configuration, users declaratively specify the desired data checks. Then, Data Sentinel performs these data checks and writes the results to an easily understandable report. Furthermore, Data Sentinel provides well-defined schemas for the configuration and report. This makes it easy for other systems to interface or integrate with Data Sentinel. To make Data Sentinel even easier to use, understand, and operate in production environments, we provide Data Sentinel Service (DSS), a complementary system to help specify data checks, schedule, deploy, and tune data validation jobs, and understand data checking results. The contributions of this paper include the following: 1) Data Sentinel, a declarative production-scale data validation platform successfully deployed at LinkedIn 2) A generic design to build and deploy similar systems for production environments 3) Experiences and lessons learned that can benefit practitioners with similar objectives.},
keywords={Data integrity;Production;LinkedIn;Big Data;Business;Debugging;Schedules},
doi={10.1109/ICDE48307.2020.00140},
ISSN={2375-026X},
month={April},}
@ARTICLE{8756123,
author={Zhang, Xi and Zhu, Qixuan},
journal={IEEE Journal on Selected Areas in Communications}, title={Information-Centric Virtualization for Software-Defined Statistical QoS Provisioning Over 5G Multimedia Big Data Wireless Networks},
year={2019},
volume={37},
number={8},
pages={1721-1738},
abstract={The multimedia transmission represents a typical big data application in the fifth-generation (5G) wireless networks. However, supporting multimedia big data transmission over 5G wireless networks imposes many new and open challenges because multimedia big data services are both time-sensitive and bandwidth-intensive over time-varying wireless channels with constrained wireless resources. To overcome these difficulties, in this paper we propose the information-centric virtualization architectures for software-defined statistical delay-bounded quality of service (QoS) provisioning over 5G multimedia big data wireless networks. In particular, our proposed schemes integrate the three 5G-promising candidate techniques to guarantee the statistical delay-bounded QoS for multimedia big data transmissions: 1) information-centric network (ICN), to derive the optimal in-network caching locations for multimedia big data; 2) network functions virtualization (NFV), to abstract the PHY-layer infrastructures into several virtualized networks to derive the optimal multimedia data contents delivery paths; and 3) software-defined networks (SDNs), to dynamically reconfigure wireless resources allocation architectures through the SDN-control plane. Under our proposed architectures, to jointly optimize the implementations of NFV and SDN techniques under ICN architectures, we develop the three virtual network selection and transmit-power allocation schemes to: 1) maximize single user's effective capacity; 2) jointly optimize the aggregate effective capacity and allocation fairness over all users; and 3) coordinate non-cooperative gaming among all users, respectively. By simulations and numerical analyses, we show that our proposed architectures and schemes significantly outperform the other existing schemes in supporting the statistical delay-bounded QoS provisioning over the 5G multimedia big data wireless networks.},
keywords={Big Data;Quality of service;Wireless networks;5G mobile communication;Resource management;Wireless sensor networks;5G multimedia big data wireless networks;ICN;NFV;SDN;optimal transmit power;statistical delay-bounded QoS;effective capacity;relay selection},
doi={10.1109/JSAC.2019.2927088},
ISSN={1558-0008},
month={Aug},}
@INPROCEEDINGS{8622349,
author={Sinha, Shweta and Seys, Marcia},
booktitle={2018 IEEE International Conference on Big Data (Big Data)}, title={HL7 Data Acquisition amp; Integration: Challenges and Best Practices},
year={2018},
volume={},
number={},
pages={2453-2457},
abstract={Lack of interoperability between health data systems is a leading challenge for healthcare in the United States. This paper describes the challenges and lessons learned in the process of incorporating HL7 data and integration with Electronic Health Records and Health Information Exchanges from the perspective of a midsized Health Plan (Payer). As a Health Care Payer, Premera has a unique perspective regarding how health plans can provide the necessary data to complete the picture of care. This paper shares some of the best practices and focus areas for successful implementation of healthcare data integrations. This paper also focuses on integrating claims and clinical data using a master patient index as well as challenges faced in that process.Note that going forward `Health Plan' and `Payer' will be used interchangeably. Also, `Provider(s)', `Hospitals', `Healthcare Providers', `Clinics', `Provider Organizations' will be used interchangeably and in the context of this paper may mean the same.},
keywords={Medical services;Organizations;Standards organizations;Engines;Monitoring;Best practices;Interoperability;HL7;big data;integration;acquisition;master patient index;payer;interoperability;data quality},
doi={10.1109/BigData.2018.8622349},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9659660,
author={Mohammad, Banan and Alzyadat, Wael and Al-Fayoumi, Mohammad and EL Hawi, Ruba and AyshAlhroob},
booktitle={2021 International Conference on Engineering and Emerging Technologies (ICEET)}, title={An Improve The Quality Of Data Considering Big Data Aspect Based On Sensitive Of Cost Time},
year={2021},
volume={},
number={},
pages={1-6},
abstract={Big data is term of dataset with characteristic volume, value and veracity that lead to confrontation unable proceed using traditional techniques to extract value, project management perspective is dynamic processing that utilizes the suitable resources of organization in many stages by measuring in four-factor scope, time, cost and quality. In this research aim improve data quality from big data via project management scope consist on high trust which is bring high accuracy from confidence level in volume of data, confidence get with context and value of data which lead to determine accuracy deeply in it and finally select from data depending on veracity of it, the experiment using three main factors time, cost and scope, strongest relation organizing between them start by project scope as strongest one then cost, product and Last one time is weakest between them, in the final when select best quality use two sides mostly from quality degree and be center of quality interval and in particular from closest distance with the strongest factor.},
keywords={Costs;Correlation;Data integrity;Volume measurement;Project management;Big Data;Time measurement;Big Data;Project Management;Sensitive Rule;Quality},
doi={10.1109/ICEET53442.2021.9659660},
ISSN={2409-2983},
month={Oct},}
@ARTICLE{8641478,
author={Zhang, Meifan and Wang, Hongzhi and Li, Jianzhong and Gao, Hong},
journal={IEEE Access}, title={One-Pass Inconsistency Detection Algorithms for Big Data},
year={2019},
volume={7},
number={},
pages={22377-22394},
abstract={Data in the real world is often dirty. Inconsistency is an important kind of dirty data; before repairing inconsistency, we need to detect them first. The time complexities of the current inconsistency detection algorithms are super-linear to the size of data and not suitable for the big data. For the inconsistency detection of big data, we develop an algorithm that detects inconsistency within the one-pass scan of the data according to both the functional dependency (FD) and the conditional functional dependency (CFD) in our previous work. In this paper, we propose inconsistency detection algorithms in terms of FD, CFD, and Denial Constraint (DC). DCs are more expressive than FDs and CFDs. Developing the algorithm to detect the violation of DCs increases the applicability of our inconsistency detection algorithms. We compare the performance of our algorithm with the performance of implementing SQL queries in MySQL and BigQuery. The experimental results indicate the high efficiency of our algorithms.},
keywords={Big Data;Data integrity;Detection algorithms;Databases;Time complexity;Hazards;Business;Inconsistency detection;big data;one-pass algorithm;data quality;denial constraint},
doi={10.1109/ACCESS.2019.2898707},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8771733,
author={Radhakrishnan, Asha and Das, Sarasij},
booktitle={2018 20th National Power Systems Conference (NPSC)}, title={Quality Assessment of Smart Grid Data},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Enormous amount of data gets generated in the Smart Grids (SGs) due to the large number of measuring devices, higher measurement rates and various types of sensors. Smart grid data contains important and critical information about the grid. Data driven applications are being developed for better planning, monitoring and operation of SGs. The outcome of data analytics heavily depends on the quality of SG data. However, not much work has been reported on the quality assessment of SG data. This paper addresses the objective assessment of SG data quality. Various dimensions of SG data quality are identified in this paper. Mathematical formulations are proposed to quantify the SG data quality. Proposed data quality metrics have been applied on the SCADA and PMU measurements collected from the Southern Regional Grid of India to demonstrate their effectiveness.},
keywords={Data integrity;Smart grids;Phasor measurement units;Big Data;Quality assessment;Big data;data quality;power system;smart grid},
doi={10.1109/NPSC.2018.8771733},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7724441,
author={Aggarwal, Ankur},
booktitle={2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)}, title={Identification of quality parameters associated with 3V's of Big Data},
year={2016},
volume={},
number={},
pages={1135-1140},
abstract={Big Data approach uses an empirical process that does not lie on the understanding of underlying mechanisms, but lies on the observation of facts. Achieving high quality in Big Data is a critical issue for both the database researchers and practitioners. More explicit consideration must be given to data quality since data increasingly outlives the application for which it was initially designed. In this paper, identification of quality parameters is done which are compatible to the 3V's of big data which will further provide enhancement in achieving quality data to be stored in the repository. Good utilization of Big Data strengthens the performance and competitiveness of the firms by enabling better and faster results to its customer needs. In this paper GQM (Goal Question Metric) methodology is proposed to measure quality using metrics. It describes how to include data quality metrics to project, progress and maintain levels of quality in an organization. It helps to make a decision whether or not our current data satisfies our quality prospects.},
keywords={Big data;Organizations;Conferences;Social network services;Electronic mail;Databases;Big Data;Quality;Volume;Variety;Velocity},
doi={},
ISSN={},
month={March},}
@INPROCEEDINGS{8029838,
author={Wang, Haiyan and Zhang, Han},
booktitle={2017 IEEE International Conference on Web Services (ICWS)}, title={User Requirements Based Service Identification for Big Data},
year={2017},
volume={},
number={},
pages={800-807},
abstract={Service identification meets with new challenges with overwhelming rise of categories and numbers of services in big data scenarios. Most of the current service identification approaches have paid little attention to the granularity of indicator for service identification, neither do they provide with any trustworthy monitoring mechanism during the process of service identification. To address the problems above, we propose a user requirements based service identification approach for big data (URBSI-BD). In the proposed URBSI-BD, we firstly cluster massive services with BIRCH clustering algorithm to obtain a number of service sets. We then employ PSO algorithm with MapReduce mechanism to achieve a fine-grained evaluation of indicator for service identification. Based on the integration, candidate services which can better meet with user requirements will be selected. Finally, we use Beth trust model on the quality of experience of users and set up a monitoring mechanism to better obtain required services. Simulation results and analysis demonstrate that the proposed approach has better performance in service identification compared with other current approaches in big data scenarios.},
keywords={Clustering algorithms;Big Data;Quality of service;Monitoring;Reliability;Optimization;Service Identification;User Requirements;PSO Algorithm;Quality of Experience (QoE)},
doi={10.1109/ICWS.2017.11},
ISSN={},
month={June},}
@INPROCEEDINGS{9378192,
author={Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Gondalia, Shlok and Duggan, Jerry and Kahn, Michael G.},
booktitle={2020 IEEE International Conference on Big Data (Big Data)}, title={An Autocorrelation-based LSTM-Autoencoder for Anomaly Detection on Time-Series Data},
year={2020},
volume={},
number={},
pages={5068-5077},
abstract={Data quality significantly impacts the results of data analytics. Researchers have proposed machine learning based anomaly detection techniques to identify incorrect data. Existing approaches fail to (1) identify the underlying domain constraints violated by the anomalous data, and (2) generate explanations of these violations in a form comprehensible to domain experts. We propose IDEAL, which is an LSTM-Autoencoder based approach that detects anomalies in multivariate time-series data, generates domain constraints, and reports subsequences that violate the constraints as anomalies. We propose an automated autocorrelation-based windowing approach to adjust the network input size, thereby improving the correctness and performance of constraint discovery over manual and brute-force approaches. The anomalies are visualized in a manner comprehensible to domain experts in the form of decision trees extracted from a random forest classifier. Domain experts can then provide feedback to retrain the learning model and improve the accuracy of the process. We evaluate the effectiveness of IDEAL using datasets from Yahoo servers, NASA Shuttle, and Colorado State University Energy Institute. We demonstrate that IDEAL can detect previously known anomalies from these datasets. Using mutation analysis, we show that IDEAL can detect different types of injected faults. We also demonstrate that the accuracy improves after incorporating domain expert feedback.},
keywords={NASA;Manuals;Big Data;Servers;Decision trees;Anomaly detection;Random forests;Anomaly detection;Autocorrelation;Data quality tests;Explainability;LSTM-Autoencoder;Time series},
doi={10.1109/BigData50022.2020.9378192},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8592556,
author={Hao, Jiao and Jinming, Chen and Yajuan, Guo},
booktitle={2018 China International Conference on Electricity Distribution (CICED)}, title={Data-driven lean Management for Distribution Network},
year={2018},
volume={},
number={},
pages={701-705},
abstract={This paper proposes a concept of “data-driven, lean-oriented and closed-loop” management for distribution network and explain how to implement this kind of management, as shown in fig.1 Firstly, a big data platform is constructed to integrate and combine the multi-source data. Secondly, big data analysis technologies such as data mining, machine learning and data visualization are applied to solve problems in distribution network production. For example, accurate location of the fault can be found with help of multisource information from different devices and systems. And we can also be aware of the risk points in distribution network through history data analysis. Finally, this Paper explains how to promote lean management of distribution network in the fields of asset, operation, maintenance and investment based on the big data platform and big data analysis methods. In addition, the feedback procedure sets up a bridge between application and data collecting, which further improve the data quality. Those management measure have been piloted in several cities in Jiangsu. The result proves that they can improve power supply reliability and reduce operating costs significantly. Two practical cases are given to show how they work.},
keywords={Big Data;Maintenance engineering;Data models;Investment;Fault diagnosis;Poles and towers;data-driven;lean management;closed-loop;big data analysis},
doi={10.1109/CICED.2018.8592556},
ISSN={2161-749X},
month={Sep.},}
@ARTICLE{8715359,
author={Ramzan, Shabana and Bajwa, Imran Sarwar and Ramzan, Bushra and Anwar, Waheed},
journal={IEEE Access}, title={Intelligent Data Engineering for Migration to NoSQL Based Secure Environments},
year={2019},
volume={7},
number={},
pages={69042-69057},
abstract={In an era of super computing, data is increasing exponentially requiring more proficiency from the available technologies of data storage, data processing, and analysis. Such continuous massive growth of structured and unstructured data is referred to as a “Big data”. The processing and storage of big data through a conventional technique is not possible. Due to improved proficiency of Big Data solution in handling data, such as NoSQL caused the developers in the previous decade to start preferring big data databases, such as Apache Cassandra, Oracle, and NoSQL. NoSQL is a modern database technology that is designed to provide scalability to support voluminous data, leading to the rise of NoSQL as the most viable database solution. These modern databases aim to overcome the limitations of relational databases such as unlimited scalability, high performance, data modeling, data distribution, and continuous availability. These days, the larger enterprises need to shift NoSQL databases due to their more flexible models. It is a great challenge for business organizations and enterprises to transform their existing databases to NoSQL databases considering heterogeneity and complexity in relational data. In addition, with the emergence of big data, data cleansing has become a great challenge. In this paper, we proposed an approach that has two modules: data transformation and data cleansing module. The first phase is the transformation of a relational database to Oracle NoSQL database through model transformation. The second phase provides data cleansing ability to improve data quality and prepare it for big data analytics. The experiments show the proposed approach successfully transforms the relational database to a big data database and improve data quality.},
keywords={Big Data;NoSQL databases;Transforms;Scalability;Servers;Tools;Relational databases;NoSQL;big data;data cleansing},
doi={10.1109/ACCESS.2019.2916912},
ISSN={2169-3536},
month={},}
@ARTICLE{8415743,
author={Yao, Le and Ge, Zhiqiang},
journal={IEEE Transactions on Industrial Electronics}, title={Scalable Semisupervised GMM for Big Data Quality Prediction in Multimode Processes},
year={2019},
volume={66},
number={5},
pages={3681-3692},
abstract={In this paper, a novel variational inference semisupervised Gaussian mixture model (VI-S2GMM) model is first proposed for semisupervised predictive modeling in multimode processes. Parameters of Gaussian components are identified more accurately with extra unlabeled samples, which improve the prediction performance of the regression model. Since all labeled and unlabeled data samples are involved in each iteration of parameter updating, intractable computing problems occur when facing high-dimension datasets. To tackle this problem, a scalable stochastic VI-S2GMM (SVI-S2GMM) is further proposed. Through taking advantage of a stochastic gradient optimization algorithm to maximize the evidence of lower bound, the VI-based algorithm becomes scalable. In the SVI-S2GMM, only one or a minibatch of samples is randomly selected to update parameters in each iteration, which is more efficient than the VI-S2GMM. Since the whole dataset is divided and transferred to iterations batch by batch, the scalable SVI-S2GMM algorithm can easily handle the big data modeling issue. In this way, a large number of unlabeled data can be useful in the modeling, which will further benefit the prediction performance. The SVI-S2GMM is then exploited for the prediction of a quality-related key performance index. Two examples demonstrate the feasibility and effectiveness of the proposed algorithms.},
keywords={Data models;Big Data;Predictive models;Inference algorithms;Prediction algorithms;Semisupervised learning;Computational modeling;Big data;Gaussian mixture model (GMM);multimode process modeling;quality prediction;semisupervised modeling;stochastic variational inference (SVI)},
doi={10.1109/TIE.2018.2856200},
ISSN={1557-9948},
month={May},}
@INPROCEEDINGS{9107851,
author={Dehui, Fu and Feng, Wang and Shuai, Yuan and Guangzhen, Wang and Mingxin, Shao},
booktitle={2019 6th International Conference on Information Science and Control Engineering (ICISCE)}, title={Fuzzy Comprehensive Evaluation Method for On-line Monitoring Data Quality of Substation Equipment},
year={2019},
volume={},
number={},
pages={753-757},
abstract={This paper analyses the existing problems in on-line monitoring and data quality evaluation of substation equipment, and proposes a multi-dimensional fuzzy comprehensive evaluation method for on-line monitoring data quality of substation equipment. The evaluation index set of online monitoring data quality of substation equipment with 5 dimensions and 11 secondary indexes is established. The weight is determined by combining subjective and objective methods. The fuzzy transformation is completed based on membership function and a multi-dimensional fuzzy comprehensive evaluation model is established. Finally, the evaluation grade of online monitoring data of substation equipment is obtained. Finally, compared with other methods, the validity and accuracy of this method are verified.},
keywords={big data;data quality;subordination;fuzzy comprehensive evaluation;On-line monitoring},
doi={10.1109/ICISCE48695.2019.00154},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7009043,
author={Pankowska, Malgorzata},
booktitle={International Conference on Information Society (i-Society 2014)}, title={Service science facing Big Data},
year={2014},
volume={},
number={},
pages={207-212},
abstract={The Big Data is a modification of the traditional view of information organization, particularly view of the data warehouses and databases. Nowadays, business organizations must address a mix of structured, unstructured and streaming data that supports queries and reports. Business recognized the wealth of untapped information in open social media data. Therefore, the goal of this paper is to present the procedural approach on how to cope with massive data sets' management. The proposal included in this paper covers service science application.},
keywords={Big data;Quality of service;Monitoring;Decision making;Organizations;Data analysis;Big Data;information governance;service science;Service Level Management;service quality},
doi={10.1109/i-Society.2014.7009043},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8901326,
author={Zeyong, Wang and Yutian, Hong and Zhongzheng, Tong},
booktitle={2019 International Conference on Smart Grid and Electrical Automation (ICSGEA)}, title={Risk Assessment Model and Experimental Analysis of Electric Power Production Based on Big Data},
year={2019},
volume={},
number={},
pages={88-91},
abstract={This paper studies the characteristics of big data of power, and aims at the data quality problems faced by power system. It puts forward an assessment method of power system data quality. Based on the characteristics of large power data, a series of indicators influencing the data are analyzed and hierarchically divided to determine the measurement standard of power production data during the process of risk management, namely, the risk index system. Then, the risk assessment model of power data is established by referring to the assessment model in other fields or the rules of deduction and induction in data mining. It can be used to evaluate the quality of power system data, and find a framework and solution suitable for large data quality assessment. Finally, the model is implemented on Hadoop platform, which proves that it takes into account the completeness of the index system, the objectivity of the assessment method and the rapidity of the calculation method.},
keywords={risk assessment;electric power;fuzzy comprehensive evaluation;Hadoop;index},
doi={10.1109/ICSGEA.2019.00028},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7432708,
author={Zhang, Lu and Chen, Yanxia and Zhu, Jie and Pan, Mingyu and Sun, Zhou and Wang, Weixian},
booktitle={2015 5th International Conference on Electric Utility Deregulation and Restructuring and Power Technologies (DRPT)}, title={Data quality analysis and improved strategy research on operations management system for electric vehicles},
year={2015},
volume={},
number={},
pages={2715-2720},
abstract={It is very important for Operations Management System (OMS) and big data analysis application to improve the data quality of Electric Vehicle (EV) charging service. This paper focuses on the charging transaction record data from the Beijing EV charging OMS, and analyzes error types and distributed locations of the abnormal data. Based on the mathematical logic among various kinds of operation data, the data selection rules and processing method are proposed, and the system improved scheme is given. Through the design and application of data selection and processing module, the abnormal data can be timely detected and corrected. It is also beneficial for the system operation and maintenance to improve the acquisition data quality. The comparative analysis results verify the feasibility and effectivity of the proposed scheme. This research is a necessary guarantee for the big data technology application.},
keywords={Big data;Distributed databases;Decision support systems;Power industry;Electric vehicles;Systems operation;Maintenance engineering;electric vehicle;operations management system;big data;data quality;data selection and processing},
doi={10.1109/DRPT.2015.7432708},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8121916,
author={Sattart, Farook and McQuay, Colter and Driessen, Peter F.},
booktitle={2017 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)}, title={Marine mammal sound anomaly and quality detection using multitaper spectrogram and hydrophone big data},
year={2017},
volume={},
number={},
pages={1-6},
abstract={This paper proposes a novel method for anomaly and quality detection of marine mammal sounds using multitaper spectrogram and hydrophone big data. The proposed method is aimed to automatically detect anomaly, such as high-frequency vessel noise, Doppler noise, in sperm whale (SPW) sound as well as the quality of the sound. A new signature function derived from a multi-taper spectrogram is able to detect the anomaly in the data and a new anomaly distortion measure can detect the sound quality into good/bad. The proposed method, is tested with 1905 minutes of data spanning a single year, and using a human operator's annotations. The experimental results reveal that the proposed multitaper spectrogram based approach is efficient in detecting anomaly as well as sperm whale sound quality for hydrophone big data and high detection accuracy (>85%) is achieved for raw input hydrophone data.},
keywords={Spectrogram;Whales;Sonar equipment;Big Data;Acoustic distortion;Anomaly detection;Anomaly detection;Hydrophone Big data;Quality detection;Multitaper spectrogram;Marine mammal sound;Sperm whale},
doi={10.1109/PACRIM.2017.8121916},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8972078,
author={Borrison, Reuben and Kloepper, Benjamin and Mullen, Jennifer},
booktitle={2019 IEEE 17th International Conference on Industrial Informatics (INDIN)}, title={Data Preparation for Data Mining in Chemical Plants using Big Data},
year={2019},
volume={1},
number={},
pages={1185-1191},
abstract={Data preparation for data mining in industrial applications is a key success factor which requires considerable repeated efforts. Although the required activities need to be repeated in very similar fashion across many projects, details of their implementation differ and require both application understanding and experience. As a result, data preparation is done by data mining experts with a strong domain background and a good understanding of the characteristics of the data to be analyzed. Experts with these profiles usually have an engineering background and no strong expertise in distributed programming or big data technology. Unfortunately, the amount of data can be so large that distributed algorithms are required to allow for inspection of results and iteration of preparation steps. This contribution introduces an interactive data preparation workflow for signal data from chemical plants enabling domain experts without background in distributed computing and extensive programming experience to leverage the power of big data technologies.},
keywords={Data quality;Soft sensors;Big data},
doi={10.1109/INDIN41052.2019.8972078},
ISSN={2378-363X},
month={July},}
@ARTICLE{9302878,
author={Song, Shaoxu and Gao, Fei and Huang, Ruihong and Wang, Chaokun},
journal={IEEE Transactions on Knowledge and Data Engineering}, title={Data Dependencies over Big Data: A Family Tree},
year={2020},
volume={},
number={},
pages={1-1},
abstract={Besides the conventional schema-oriented tasks, data dependencies are recently revisited for data quality applications, such as violation detection. To address the variety and veracity issues of big data, data dependencies have been extended as data quality rules to adapt to various data types, ranging from (1)categorical data with equality relationships to (2)heterogeneous data with similarity relationships, and (3)numerical data with order relationships. In this survey, we briefly review the recent proposals on data dependencies categorized into the aforesaid types of data. In addition to (a)the concepts of these data dependency notations, we investigate (b)the extension relationships between data dependencies, e.g., conditional functional dependencies (CFDs) extend the conventional functional dependencies (FDs). It forms a family tree of extensions, mostly rooted in FDs, helping us understand the expressive power of various data dependencies. Moreover, we summarize (c)the discovery of dependencies from data, since data dependencies are often unlikely to be manually specified in a traditional way, given the huge volume and high variety of big data. We further outline (d)the applications of the extended data dependencies, in particular in data quality practice. It guides users to select proper data dependencies with sufficient expressive power and reasonable discovery cost. Finally, we conclude with several directions of future studies on the emerging data.},
keywords={Big Data;Phase frequency detectors;Lakes;Picture archiving and communication systems;Databases;Task analysis;Proposals;Integrity constraints;data dependencies},
doi={10.1109/TKDE.2020.3046443},
ISSN={1558-2191},
month={},}
@INPROCEEDINGS{6984221,
author={Bruballa, Eva and Taboada, Manel and Cabrera, Eduardo and Rexachs, Dolores and Luque, Emilio},
booktitle={2014 International Conference on Future Internet of Things and Cloud}, title={Simulation and Big Data: A Way to Discover Unusual Knowledge in Emergency Departments: Work-in-Progress Paper},
year={2014},
volume={},
number={},
pages={367-372},
abstract={Here a work in progress is reported on within research that aims to obtain knowledge about variables which may influence a hospital emergency department's performance and quality of service. Knowledge discovery will be achieved through the analysis of intensive data generated by the simulation of any possible scenario in the real system. The challenge is to provide knowledge of critical, non-usual or extreme situations. Simulation is the only way to obtain information about these kinds of situations, as it is not possible to test such scenarios in the real system. We show how simulation of the real system through advanced computing is a source of big data, as it allows rapid and massive data generation. The potential of high performance computing makes it possible to generate a very large amount of data within a reasonable time, store this data, then process and analyze it to obtain knowledge. We describe the methodology proposed for this goal, which is based on the use of the simulator as a sensor of the real system, and so as the main source of data. The application of data mining techniques will open the doors to knowledge. To verify that the proposed methodology works, we propose a case study in which the aim is to obtain knowledge from a set of data already available, obtained from the simulation of a reduced set of scenarios of the real system.},
keywords={Data models;Data mining;Computational modeling;Hospitals;Analytical models;Big data;Agent-Based Modeling and Simulation (ABMS);Big Data;Data Mining (DM);Decision Support Systems (DSS);Emergency Department (ED);Knowledge Discovery},
doi={10.1109/FiCloud.2014.65},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7545062,
author={Tu, Shouzhong and Huang, Minlie},
booktitle={2016 IEEE Second International Conference on Multimedia Big Data (BigMM)}, title={Scalable Functional Dependencies Discovery from Big Data},
year={2016},
volume={},
number={},
pages={426-431},
abstract={Functional dependencies (FDs) represent potentially novel and interesting patterns existent in relational databases. The discovery of functional dependencies has a wide range of applications such as database design, knowledge discovery, data quality assessment, etc. There has been growing interest in the problem of functional dependencies discovery in the last ten years. However, existing functional dependencies discovery algorithms are mainly applied to centralized small data. It is far more challenging to discover functional dependencies from big data. In this paper, we propose an efficient functional dependencies discovery algorithm, for mining functional dependencies from distributed big data. We prune candidate FDs at each node by local fragmented data and batch verify candidate FDs in parallel. Load balance is taken into account when discovering functional dependencies. Experiments show that the proposed algorithm is effective on real dataset and synthetic dataset.},
keywords={Distributed databases;Big data;Lattices;Algorithm design and analysis;Partitioning algorithms;Knowledge discovery;Functional dependencies;Discovering functional dependencies;Knowledge discovery;Big data},
doi={10.1109/BigMM.2016.63},
ISSN={},
month={April},}
@INPROCEEDINGS{9480634,
author={Elsahlamy, Ebtsam and Eshra, Abeer and Eshra, Nadia and El-Fishawy, Nawal},
booktitle={2021 International Conference on Electronic Engineering (ICEEM)}, title={Empowering GIS with Big Data: A review of recent advances},
year={2021},
volume={},
number={},
pages={1-7},
abstract={In the past few decades, the use of geographic information systems (GIS) was efficient with servers that could handle the amount of data used. However, as geographical big data grows in size and complexity, storing, managing, processing, analyzing, visualizing, and confirming data quality becomes more difficult. Academia, industry, government, and other institutions are increasingly interested in this information. It's known as Big Data. Since that kind of data recently became massive, there was a need to develop methods to deal with big data and analyze it to keep pace with development. In this paper, we review the previous studies that involve both Big Data and GIS in different applications. Moreover, we focus on the field of agriculture, which is considered one of the most important sources of the economy. Produced results in this research area help decision-makers to make sound executive steps to reach better production.},
keywords={Government;Data visualization;Production;Big Data;Agriculture;Mobile handsets;Servers;GIS;Big-Data;Geospatial;Agriculture;map-reduce},
doi={10.1109/ICEEM52022.2021.9480634},
ISSN={},
month={July},}
@INPROCEEDINGS{8882761,
author={Ullah, Faheem and Ali Babar, M.},
booktitle={2019 24th International Conference on Engineering of Complex Computer Systems (ICECCS)}, title={QuickAdapt: Scalable Adaptation for Big Data Cyber Security Analytics},
year={2019},
volume={},
number={},
pages={81-86},
abstract={Big Data Cyber Security Analytics (BDCA) leverages big data technologies for collecting, storing, and analyzing a large volume of security events data to detect cyber-attacks. Accuracy and response time, being the most important quality concerns for BDCA, are impacted by changes in security events data. Whilst it is promising to adapt a BDCA system's architecture to the changes in security events data for optimizing accuracy and response time, it is important to consider large search space of architectural configurations. Searching a large space of configurations for potential adaptation incurs an overwhelming adaptation time, which may cancel the benefits of adaptation. We present an adaptation approach, QuickAdapt, to enable quick adaptation of a BDCA system. QuickAdapt uses descriptive statistics (e.g., mean and variance) of security events data and fuzzy rules to (re) compose a system with a set of components to ensure optimal accuracy and response time. We have evaluated QuickAdapt for a distributed BDCA system using four datasets. Our evaluation shows that on average QuickAdapt reduces adaptation time by 105× with a competitive adaptation accuracy of 70% as compared to an existing solution.},
keywords={Big Data;Quality of service;Time factors;Feature extraction;Computer crime;Computer architecture;big data, cyber security, adaptation, accuracy},
doi={10.1109/ICECCS.2019.00016},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9140534,
author={Molinari, Andrea and Nollo, Giandomenico},
booktitle={2020 IEEE 20th Mediterranean Electrotechnical Conference ( MELECON)}, title={The quality concerns in health care Big Data},
year={2020},
volume={},
number={},
pages={302-305},
abstract={Health information technology is showing an impressive growing interest towards Big Data. Big Data Analytics is expected to bring important achievements for building sophisticated models, methods and tools that are expected to improve healthcare services and citizen health and wellbeing. In spite of these expectations data quality and analytics methods are not getting the attention they deserve. In this short paper, we aimed to highlight the issues of data quality in the context of Big Data Healthcare Analytics. The common sources of errors, the consequence of these errors, and potential solutions that should be considered to mitigate errors and pitfalls are discussed in the healthcare context.},
keywords={Big Data;Medical services;Data integrity;Biomedical monitoring;Business;Big Data;Analytics;healthcare quality;entity reconciliation},
doi={10.1109/MELECON48756.2020.9140534},
ISSN={2158-8481},
month={June},}
@INPROCEEDINGS{8126976,
author={Han, Weiguo and Jochum, Matthew},
booktitle={2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)}, title={Latency analysis of large volume satellite data transmissions},
year={2017},
volume={},
number={},
pages={384-387},
abstract={A wide array of time-sensitive satellite data is required in the research and development activities for natural hazard assessment, storms and weather prediction, hurricane tracking, disaster and emergency response, and so on. Identifying and analyzing the latencies of large volumes of real-time and near real-time satellite data is very useful and helpful for detecting transmission issues, managing IT resources, and configuring and optimizing data management systems. This paper introduces how to monitor and collect important timestamps of data transmissions, organize them in a NoSQL database, and explore data latency via a user-friendly dashboard. Taking Sentinel series satellite data as an example, data transmission issues are illustrated and investigated further. Latency analysis and explorations help data providers and managers improve data transmission and enhance data management.},
keywords={Satellites;Data communication;Real-time systems;Databases;Browsers;Big Data;Big Data;Satellite Data;Data Latency;Data Quality;NoSQL;MongoDB},
doi={10.1109/IGARSS.2017.8126976},
ISSN={2153-7003},
month={July},}
@INPROCEEDINGS{7011535,
author={Liu, Wanting and Peng, Yonghong and Tobin, Desmond J},
booktitle={2014 IEEE Symposium on Computational Intelligence in Big Data (CIBD)}, title={Integrated analytics of microarray big data reveals robust gene signature},
year={2014},
volume={},
number={},
pages={1-7},
abstract={The advance of high throughput biotechnology enables the generation of large amount of biomedical data. The microarray is increasingly a popular approach for the detection of genome-wide gene expression. Microarray data have thus increased significantly in public accessible database repositories, which provide valuable big data for scientific research. To deal with the challenge of microarray big data collected in different research labs using different experimental set-ups and on different bio-samples, this paper presents a primary study to evaluate the impact of two important factors (the origin of bio-samples and the quality of microarray data) on the integrated analytics of multiple microarray data. The aim is to enable the extraction of reliable and robust gene biomarkers from microarray big data. Our work showed that in order to enhance biomarker discovery from microarray big data (i) it is necessary to treat the microarray data differently in terms of their quality, (ii) it is recommended to stratifying (i.e., sub-group) the data according to the origin of bio-samples in the analytics.},
keywords={Accuracy;Diseases;Big data;Robustness;Educational institutions;Bioinformatics;Malignant tumors;Microarray;integrated analytics;biomarkers},
doi={10.1109/CIBD.2014.7011535},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7364061,
author={Abboura, Asma and Sahrl, Soror and Ouziri, Mourad and Benbernou, Salima},
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, title={CrowdMD: Crowdsourcing-based approach for deduplication},
year={2015},
volume={},
number={},
pages={2621-2627},
abstract={Matching dependencies (MDs) were recently introduced as quality rules for data cleaning and entity resolution. They are rules that specify what values should be considered duplicates, and have to be matched. Defining such quality rules on a database instance, is a very expensive and a time consuming process, and requires huge efforts to analyse the whole database. In this demo paper, we present CrowdMD, a hybrid machine-crowd system for generating MDs. It first asks the crowd to determine whether a given pair, from training sample pairs, match or not. Then, it uses data mining techniques to generate attributes constituting an MD. Using a Restaurant database, we will show how the crowders can help to generate MDs by labelling the training sample through the CrowdMD user interface and how MDs can be mined from this training set.},
keywords={Big data;Databases;Labeling;Hybrid power systems;Crowdsourcing;Training;Cleaning;matching rules;deduplication;entity resolution;big data quality},
doi={10.1109/BigData.2015.7364061},
ISSN={},
month={Oct},}
@ARTICLE{6527249,
author={Wigan, Marcus R. and Clarke, Roger},
journal={Computer}, title={Big Data's Big Unintended Consequences},
year={2013},
volume={46},
number={6},
pages={46-53},
abstract={Businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons. The Web extra at http://youtu.be/TvXoQhrrGzg is a video in which author Marcus Wigan expands on his article "Big Data's Big Unintended Consequences" and discusses how businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons.},
keywords={Information management;Data handling;Data storage systems;Government policies;Databases;Business;Legal aspects;Data privacy;policy;privacy;data;social impact;big data;private data commons},
doi={10.1109/MC.2013.195},
ISSN={1558-0814},
month={June},}
@ARTICLE{9409047,
author={Yahia, Nesrine Ben and Hlel, Jihen and Colomo-Palacios, Ricardo},
journal={IEEE Access}, title={From Big Data to Deep Data to Support People Analytics for Employee Attrition Prediction},
year={2021},
volume={9},
number={},
pages={60447-60458},
abstract={In the era of data science and big data analytics, people analytics help organizations and their human resources (HR) managers to reduce attrition by changing the way of attracting and retaining talent. In this context, employee attrition presents a critical problem and a big risk for organizations as it affects not only their productivity but also their planning continuity. In this context, the salient contributions of this research are as follows. Firstly, we propose a people analytics approach to predict employee attrition that shifts from a big data to a deep data context by focusing on data quality instead of its quantity. In fact, this deep data-driven approach is based on a mixed method to construct a relevant employee attrition model in order to identify key employee features influencing his/her attrition. In this method, we started thinking `big' by collecting most of the common features from the literature (an exploratory research) then we tried thinking `deep' by filtering and selecting the most important features using survey and feature selection algorithms (a quantitative method). Secondly, this attrition prediction approach is based on machine, deep and ensemble learning models and is experimented on a large-sized and a medium-sized simulated human resources datasets and then a real small-sized dataset from a total of 450 responses. Our approach achieves higher accuracy (0.96, 0.98 and 0.99 respectively) for the three datasets when compared previous solutions. Finally, while rewards and payments are generally considered as the most important keys to retention, our findings indicate that `business travel', which is less common in the literature, is the leading motivator for employees and must be considered within HR policies to retention.},
keywords={Big Data;Organizations;Radio frequency;Predictive models;Support vector machines;Data models;Analytical models;Deep people analytics;employee attrition;retention;prediction;interpretation;policies recommendation},
doi={10.1109/ACCESS.2021.3074559},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{6602615,
author={Tien, James M.},
booktitle={2013 10th International Conference on Service Systems and Service Management}, title={Big Data: Unleashing information},
year={2013},
volume={},
number={},
pages={4-4},
abstract={Summary form only given. At present, it is projected that about 4 zettabytes (or 10**21 bytes) of electronic data are being generated per year by everything from underground physics experiments to retail transactions to security cameras to global positioning systems. In the U. S., major research programs are being funded to deal with big data in all five economic sectors (i.e., services, manufacturing, construction, agriculture and mining) of the economy. Big Data is a term applied to data sets whose size is beyond the ability of available tools to undertake their acquisition, access, analytics and/or application in a reasonable amount of time. Whereas Tien (2003) forewarned about the data rich, information poor (DRIP) problems that have been pervasive since the advent of large-scale data collections or warehouses, the DRIP conundrum has been somewhat mitigated by the Big Data approach which has unleashed information in a manner that can support informed - yet, not necessarily defensible or knowledgeable - decisions or choices. Thus, by somewhat overcoming data quality issues with data quantity, data access restrictions with on-demand cloud computing, causative analysis with correlative data analytics, and model-driven with evidence-driven applications, appropriate actions can be undertaken with the obtained information. New acquisition, access, analytics and application technologies are being developed to further Big Data as it is being employed to help resolve the 14 grand challenges (identified by the National Academy of Engineering in 2008), underpin the 10 breakthrough technologies (compiled by the Massachusetts Institute of Technology in 2013) and support the Third Industrial Revolution of mass customization.},
keywords={Information management;Data handling;Data storage systems;Educational institutions;Physics;Security;Cameras},
doi={10.1109/ICSSSM.2013.6602615},
ISSN={2161-1904},
month={July},}
@INPROCEEDINGS{9215250,
author={Peethambaran, Geetha and Naikodi, Chandrakant and Suresh, L},
booktitle={2020 International Conference on Smart Electronics and Communication (ICOSEC)}, title={An Ensemble Learning Approach for Privacy–Quality–Efficiency Trade-Off in Data Analytics},
year={2020},
volume={},
number={},
pages={228-235},
abstract={Privacy is an issue of concern in the electronic era where data has become a primary source of investment for businesses and organizations. The value generated from data is put to use in a number of ways for economic benefit. Customer profiling is one such instance, where data collected is used for targeted marketing, personalized purchase recommendations and customized product deliveries. In such applications, the risk of individual sensitive information disclosure always prevails, affecting the privacy of individuals involved. Hence privacy preserving analysis demands suppressing or transforming data before it is published for analysis, thus curbing data leak. Subsequently, data quality degrades, and operative analytics is affected. With Big data, algorithms that offer a reasonable qualityprivacy trade off need enhancements in terms of efficiency and scalability. In this paper, the work proposed uses a privacy based composite classifier model to analyze the accuracy of classification. The diverse characteristics of algorithms in the composite classifier are found to balance the classification accuracy that is likely to get affected by privacy model. Further, the model's performance with respect to execution time is then evaluated using the parallel computing framework Spark.},
keywords={Data privacy;Privacy;Support vector machines;Big Data;Classification algorithms;Data models;Data integrity;Privacy;Scalability;Big Data;Spark;Analytics;Privacy Preserving;Performance;Utility;UCI;Composite;Efficiency;Anonymization},
doi={10.1109/ICOSEC49089.2020.9215250},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9491733,
author={Alzyadat, Wael and AlHroob, Aysh and Almukahel, Ikhlas Hassan and Muhairat, Mohammad and Abdallah, Mohammad and Althunibat, Ahmad},
booktitle={2021 International Conference on Information Technology (ICIT)}, title={Big Data, Classification, Clustering and Generate Rules: An inevitably intertwined for Prediction},
year={2021},
volume={},
number={},
pages={149-155},
abstract={Big Data filed is an unsettled standard comparing with a traditional database, data mining, or data warehouse. Stability measure aims to acquire the quality dataset which encourages to use of preprocessing data method to handle instability that miniaturization missing data. Therefore, to increase the data quality in order to achieve an accurate prediction, significant rules are used to provide value and meaningful data. Through, three measures by support, confidence, and the lift to acquire frequently rules. These rules are used to conduct the objective extracting pattern, to estimate each browsing customer's likelihood of making a purchase, and to choose meaningful patterns from the discovered association rules.},
keywords={Databases;Data integrity;Big Data;Data warehouses;Data mining;Information technology;Standards;classification;marketing;association rules;Big Data;k-mean;prediction;preprocess},
doi={10.1109/ICIT52682.2021.9491733},
ISSN={},
month={July},}
@ARTICLE{9407288,
author={Hampson, Gary and Hargreaves, Neil and Jakubowicz, Helmut and Williams, Gareth and Hatton, Les},
journal={IEEE Software}, title={Open Collaboration, Data Quality, and COVID-19},
year={2021},
volume={38},
number={3},
pages={137-141},
abstract={The flavor of this "Impact" department is somewhat different. In a pandemic, everybody has to come together. In April 2020, a call went out in the United Kingdom for groups to informally form and collaborate to study this brutal pathogen in whatever way they could. The five authors of this article, old friends from the geophysical industry with decades of experience in numerical modeling and big data, formed such a group.},
keywords={Industries;Pathogens;Pandemics;Data integrity;Collaboration;Data models;Numerical models},
doi={10.1109/MS.2021.3056642},
ISSN={1937-4194},
month={May},}
@INPROCEEDINGS{7979931,
author={Wen, Hongsheng and Chen, Zhiqiang and Gu, Jianping and Zhu, Qiangqiang},
booktitle={2016 7th International Conference on Cloud Computing and Big Data (CCBD)}, title={Big Data Analysis on Radiographic Image Quality},
year={2016},
volume={},
number={},
pages={341-346},
abstract={Mass data generated from in-service radiographic product contain assignable information on Image Quality (IQ). Analyzing data from routine work might supplement the time-consuming Image Quality Assurance Test Procedure (IQATP) to evaluate IQ and to know product type performance on site, which can also locate risks and give manufacturer directions for the further actions as well. This article illustrates methodologies of extracting IQ information from mass data and visual quality track, analysis, control, and risk mitigation in Big Data environments.},
keywords={Detectors;Image edge detection;Radiography;Standards;Image quality;X-ray imaging;Indexes;image quality;in-service;radiographic product;routine data;quality control},
doi={10.1109/CCBD.2016.073},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6597123,
author={Ramaswamy, Lakshmish and Lawson, Victor and Gogineni, Siva Venkat},
booktitle={2013 IEEE International Congress on Big Data}, title={Towards a Quality-centric Big Data Architecture for Federated Sensor Services},
year={2013},
volume={},
number={},
pages={86-93},
abstract={As the Internet of Things (IoT) paradigm gains popularity, the next few years will likely witness 'servitization' of domain sensing functionalities. We envision a cloud-based eco-system in which high quality data from large numbers of independently-managed sensors is shared or even traded in real-time. Such an eco-system will necessarily have multiple stakeholders such as sensor data providers, domain applications that utilize sensor data (data consumers), and cloud infrastructure providers who may collaborate as well as compete. While there has been considerable research on wireless sensor networks, the challenges involved in building cloud-based platforms for hosting sensor services are largely unexplored. In this paper, we present our vision for data quality (DQ)-centric big data infrastructure for federated sensor service clouds. We first motivate our work by providing real-world examples. We outline the key features that federated sensor service clouds need to possess. This paper proposes a big data architecture in which DQ is pervasive throughout the platform. Our architecture includes a markup language called SDQ-ML for describing sensor services as well as for domain applications to express their sensor feed requirements. The paper explores the advantages and limitations of current big data technologies in building various components of the platform. We also outline our initial ideas towards addressing the limitations.},
keywords={Feeds;Clouds;Wireless sensor networks;Computer architecture;Fluid flow measurement;Markup languages;Data models;Internet of Things;Federated Sensor Clouds;Data Quality;Sensor Virtualization},
doi={10.1109/BigData.Congress.2013.21},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{8669595,
author={Jiang, Ying and Zhang, Na and Fang, Ying},
booktitle={2019 International Conference on Intelligent Transportation, Big Data Smart City (ICITBS)}, title={The Analysis and Design of Ship Monitoring System Based on Hybrid Replication Technology},
year={2019},
volume={},
number={},
pages={456-459},
abstract={As the core of informatization, data has a huge significance to the development of information-based enterprises. Data replication technology is an important approach to solve the problem of enterprise data sharing based on distributed database system. It plays a crucial role in promoting business integration of enterprises and institutions, improving data quality, enhancing data sharing and improving the application level of back-end big data analysis [1]. It is necessary to do research for making a good data management of the distributed database application system, synchronizing the data to the data, preventing data conflicting and being able to synchronize or asynchronous replication. Combining with the database design model of the ship monitoring and control system, this paper mainly described how to complete the construction of distributed database system using Oracle, based on advanced replication technology named as the combination of multi-agent replication and materialized views hybrid replication technology.},
keywords={Distributed databases;Database systems;Synchronization;Business;Monitoring;Marine vehicles;Distributed database;advanced replication;materialized view;data conflict},
doi={10.1109/ICITBS.2019.00118},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8035055,
author={Albertoni, Riccardo and De Martino, Monica and Quarati, Alfonso},
booktitle={2017 International Conference on High Performance Computing Simulation (HPCS)}, title={Linked Thesauri Quality Assessment and Documentation for Big Data Discovery},
year={2017},
volume={},
number={},
pages={37-44},
abstract={Thesauri are knowledge systems which may ease Big Data access, fostering their integration and re-use. Currently several Linked Data thesauri covering multi-disciplines are available. They provide a semantic foundation to effectively support cross-organization and cross-disciplinary management and usage of Big Data. Thesauri effectiveness is affected by their quality. Diverse quality measures are available taking into account different facets. However, an overall measure is needed to compare several thesauri and to identify those more qualified for a proper reuse. In this paper, we propose a Multi Criteria Decision Making based methodology for the documentation of the quality assessment of linked thesauri as a whole. We present a proof of concept of the Analytic Hierarchy Process adoption to the set of Linked Data thesauri for the Environment deployed in LusTRE. We discuss the step-by-step practice to document the overall quality measurements, generated by the quality assessment, with the W3C promoted Data Quality Vocabulary.},
keywords={Thesauri;Metadata;Vocabulary;Measurement;Quality assessment;Big Data;quality;linked data;thesauri;AHP;metadata;DQV},
doi={10.1109/HPCS.2017.16},
ISSN={},
month={July},}
@INPROCEEDINGS{7336197,
author={Shi, Weiwei and Zhu, Yongxin and Zhang, Jinkui and Tao, Xiang and Sheng, Gehao and Lian, Yong and Wang, Guoxing and Chen, Yufeng},
booktitle={2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems}, title={Improving Power Grid Monitoring Data Quality: An Efficient Machine Learning Framework for Missing Data Prediction},
year={2015},
volume={},
number={},
pages={417-422},
abstract={Big data techniques has been applied to power grid for the evaluation and prediction of grid conditions. However, the raw data quality rarely can meet the requirement of precise data analytics since raw data set usually contains samples with missing data to which the common data mining models are sensitive. Though classic interpolation or neural network methods can been used to fill the gaps of missing data, their predicted data often fail to fit the rules of power grid conditions. This paper presents a machine learning framework (OR_MLF) to improve the prediction accuracy for datasets with missing data points, which mainly combines preprocessing, optimizing support vector machine (OSVM) and refining SVM (RSVM). On top of the OSVM engine, the scheme introduces dedicated data training strategies. First, the original data originating from data generation facilities is preprocessed through standardization. Traditional SVM is then trained to obtain a preliminary prediction model. Next, the optimized SVM predictors are achieved with new training data set, which is extracted based on the preliminary prediction model. Finally, the missing data prediction result depending on OSVM is selectively inputted into the traditional SVM and the refined SVM is lastly accomplished. We test the OR_MLF framework on missing data prediction of power transformers in power grid system. The experimental results show that the predictors based on the proposed framework achieve lower mean square error than traditional ones. Therefore, the framework OR_MLF would be a good candidate to predict the missing data in power grid system.},
keywords={Support vector machines;Data models;Predictive models;Training;Data mining;Feature extraction;Power grids;missing data prediction;machine learning;support vector machine (SVM);power transformer},
doi={10.1109/HPCC-CSS-ICESS.2015.16},
ISSN={},
month={Aug},}
@ARTICLE{7809119,
author={Hildebrandt, Kai and Panse, Fabian and Wilcke, Niklas and Ritter, Norbert},
journal={IEEE Transactions on Big Data}, title={Large-Scale Data Pollution with Apache Spark},
year={2020},
volume={6},
number={2},
pages={396-411},
abstract={Because of the increasing volume of autonomously collected data objects, duplicate detection is an important challenge in today's data management. To evaluate the efficiency of duplicate detection algorithms with respect to big data, large test data sets are required. Existing test data generation tools, however, are either not able to produce large test data sets or are domain-dependent which limits their usefulness to a few cases. In this paper, we describe a new framework that can be used to pollute a clean, homogeneous and large data set from an arbitrary domain with duplicates, errors and inhomogeneities. To prove its concept, we implemented a prototype which is built upon the cluster computing framework Apache Spark and evaluate its performance in several experiments.},
keywords={Big data;Pollution;Databases;Generators;Prototypes;Gold;Standards;Data quality;duplicate detection;data pollution;Apache Spark},
doi={10.1109/TBDATA.2016.2637378},
ISSN={2332-7790},
month={June},}
@INPROCEEDINGS{9732098,
author={Wrembel, Robert},
booktitle={2021 Eighth International Conference on Social Network Analysis, Management and Security (SNAMS)}, title={Still Open Problems in Data Warehouse and Data Lake Research: extended abstract},
year={2021},
volume={},
number={},
pages={01-03},
abstract={During recent years, we observe a widespread of new data sources, especially all types of social media and IoT devices, which produce huge data volumes, whose content ranges from fully structured to totally unstructured. All these types of data are commonly referred to as big data. They are typically described by the three most important characteristics, called 3V [1], namely: an extremely large volume, a variety of data models and structures (data representations), as well as a high velocity at which data are generated. We argue that out of these three Vs, the most challenging is variety [2]. Such data need to be integrated and transformed into a common representation, which is suitable for analysis, in a similar manner as traditional (mainly table-like) data.},
keywords={Social networking (online);Soft sensors;Transforms;Data warehouses;Big Data applications;Data models;Security;data integration;data warehouse;data lake;big data;extract transform load;data processing workflow;data processing pipeline;data quality;ETL optimization;data source evolution;metadata},
doi={10.1109/SNAMS53716.2021.9732098},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7159309,
author={Subhashini, R. and Akila, G},
booktitle={2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015]}, title={Valence arousal similarity based recommendation services},
year={2015},
volume={},
number={},
pages={1-4},
abstract={Web Services play a vital role in e-commerce and e-business applications. A WS (Web Service) application is interoperable and can work on any platform i.e.; platform independent, large scale distributed systems can be established easily. A Recommender System is a precious tool for providing appropriate recommendations to all users in a Hotel Reservation Website. User based, Top k and profile based approaches are used in collaborative filtering algorithm which does not provide personalized results to the users and inefficiency and scalability problem also occurs due to the increase in the size of large datasets. To address the above mentioned challenges, a Valence-Arousal Similarity based Recommendation Services, called VAS based RS, is proposed. Our proposed mechanism aims to presents a personalized service recommendation list and recommending the most suitable service to the end users. Moreover, it classifies the positive and negative preferences of the users from their reviews to improve the prediction accuracy. For improve its efficiency and scalability in big data environment, VAS based RS is implemented using collaborative filtering algorithm on MapReduce parallel processing paradigm in Hadoop, a widely-adopted distributed computing platform.},
keywords={Web services;Collaboration;Big data;Quality of service;Recommender systems;Scalability;Web Service;Big Data;Recommender System;MapReduce;Hadoop},
doi={10.1109/ICCPCT.2015.7159309},
ISSN={},
month={March},}
@INPROCEEDINGS{8622378,
author={Kaplunovich, Alex and Yesha, Yelena},
booktitle={2018 IEEE International Conference on Big Data (Big Data)}, title={Consolidating billions of Taxi rides with AWS EMR and Spark in the Cloud : Tuning, Analytics and Best Practices},
year={2018},
volume={},
number={},
pages={4501-4507},
abstract={Saving nature using Big Data Analytics is a very noble goal. Using New York taxi rides data, we decided to learn how many rides could be consolidated. It was a journey we would like to share. First, we had to choose the platform for calculation between Amazon Athena, Serverless Microservices, SQL or NoSql databases, Hadoop and Spark. Then, we had to find an optimal solution for the platform using assorted tuning and optimization techniques. Although the problem seems to be straight forward, it turned out that the solution is quite challenging because of the input size, data quality, calculation complexities and numerous EMR/Spark tuning options. We have been using New York taxi data from 2009 to 2017 to quantify the rides that can be joined together. The taxi rides were consolidated based on pickup location, pickup time and drop-off location. We have been calculating the percentage of taxi rides that can be joined. The benchmark originally set was rides within five minutes with a pickup and drop-off locations within half a kilometer. Then we started experimenting with different times and locations. We have been using parquet format, parallel Scala collections, compression, filtering, new column introduction, tuning parameters, I/O overhead tuning, bucketing, timeouts and partitioning. Over 1.2 billion rides were processed using Amazon EMR with Spark. We have been optimizing calculation time and processing price. Spark has hundreds of parameters, EMR has over fifty instances to choose from. It was challenging to process our data within reasonable time. We were able to find the optimal Spark queries (plans), tested different types of joins and compared their performances. Also, we were able to compare I/O and in-memory operations during partitioning and large files manipulation (the input file sizes were hundreds of Gigabytes). The results were amazing - we could consolidate around thirty five percent of total rides, saving tons of gas and improving environment and traffic in New York City.},
keywords={Sparks;Public transportation;Servers;Tuning;Big Data;Structured Query Language;Tools;Analytics;Spark;EMR;Cloud;BigData;Best Practices;Parquet;AWS;Optimization;Tuning},
doi={10.1109/BigData.2018.8622378},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9035250,
author={Mylavarapu, Goutam and Viswanathan, K. Ashwin and Thomas, Johnson P.},
booktitle={2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA)}, title={Assessing Context-Aware Data Consistency},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Data analysis is a demanding task that involves extracting deep insights hidden in data. Many businesses enforce data analysis irrespective of the domain, as it is crucial in minimizing developmental risks. Raw data cannot be used to perform any analysis as poor-quality data leads to erroneous decision-making. This makes data quality assessment a necessary function before data analysis. Data quality is a multi-dimensional factor that affects the analysis in multiple ways. Among all the dimensions, consistency is one of the most critical dimensions to assess. Context of data plays an important role in consistency assessment, as the records are inherently related within a dataset. Existing studies are computationally expensive and do not consider the context of data. In this paper, we propose a comprehensive context-aware data consistency assessment tool that uses machine learning to evaluate the consistency of data. Our model was developed on Apache Hadoop and Apache Spark to support big data, as well as to boost some computationally intensive algorithms.},
keywords={Feature extraction;Data analysis;Data integrity;Data models;Machine learning algorithms;Context modeling;Task analysis;Data analysis;data context;data quality;data consistency;machine learning;word embeddings;approximate dependencies;mutual information;apache hadoop;apache spark},
doi={10.1109/AICCSA47632.2019.9035250},
ISSN={2161-5330},
month={Nov},}
@INPROCEEDINGS{8859426,
author={He, Tieke and Chen, Shenghao and Hao, Lian and Liu, Jia},
booktitle={2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, title={Quality Driven Judicial Data Governance},
year={2019},
volume={},
number={},
pages={66-70},
abstract={With the development of Smart Court 3.0, the amount of judicial data that can be stored and processed by the computer is increasing rapidly. People gradually realize that judicial data contains tremendous social and business value. However, we need stronger ability to handle with and apply massive, multi-source and heterogeneous judicial data. A complete data governance system should be built in order to make full use of the value of data assets. In such a data governance system, data quality control is one of the key steps of data governance, and also the bottleneck of data service development, because data quality determines the upper limit of data application. This paper proposes a judicial data quality measurement framework by analyzing some judicial business data, followed by a data governance method driven by it.},
keywords={Data integrity;Big Data;Decision making;Organizations;Standards organizations;data quality;judicial data governance;quality measurement},
doi={10.1109/QRS-C.2019.00026},
ISSN={},
month={July},}
@INPROCEEDINGS{8258223,
author={Müller, Daniel and Te, Yiea-Funk and Jain, Pratiksha},
booktitle={2017 IEEE International Conference on Big Data (Big Data)}, title={Improving data quality through high precision gender categorization},
year={2017},
volume={},
number={},
pages={2628-2636},
abstract={First name to gender mappings have been widely recognized as a critical tool to complete, study and validate data records in a range of different areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 6 million people, provided by a car insurance. We then study how naming conventions have changed over time and how they differ by nationality. Second, we build a probabilistic first name to gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in a two label and three label setting and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies' outcomes and find that our mapping produces high precision results. We validate that the additional information of nationality and year of birth improve the recall scores of name to gender mappings. Therefore, it constitutes an efficient process to improve data quality of organizations' records, whenever the attribute gender is missing or unreliable.},
keywords={Organizations;Patents;Databases;Systematics;Pragmatics;data quality improvement;hot deck imputation;record completion;gender name mapping;patenting},
doi={10.1109/BigData.2017.8258223},
ISSN={},
month={Dec},}
@ARTICLE{8293821,
author={Li, Xiaoyong and Yuan, Jie and Ma, Huadong and Yao, Wenbin},
journal={IEEE Transactions on Information Forensics and Security}, title={Fast and Parallel Trust Computing Scheme Based on Big Data Analysis for Collaboration Cloud Service},
year={2018},
volume={13},
number={8},
pages={1917-1931},
abstract={Providing high trustworthy service is the most fundamental task for any cloud computing platform. Users are willing to deliver their computing tasks and the most sensitive data to cloud data centers, which is based on the trust relationship established between users and cloud service providers. However, with the development of collaboration cloud computing, how to provider fast response for a large number of users' service requests becomes a challenging problem. In order to quickly provide highly trustworthy services, the service platform must efficiently and quickly reply tens of millions of service requests, and automatically match-make tens of thousands of service resources. In this context, lightweight and fast (high-speed, low-overhead) trust computing schemes become the fundamental demand for implementing a trustworthy and collaborative cloud service. In this paper, we propose an innovative and parallel trust computing scheme based on big data analysis for the trustworthy cloud service environment. First, a distributed and modular perceiving architecture for large-scale virtual machines' service behavior is proposed relying on distributed monitoring agents. Then, an adaptive, lightweight, and parallel trust computing scheme is proposed for big monitored data. To the best of our knowledge, this paper is the first to use a blocked and parallel computing mechanism, the speed of trust calculation is greatly accelerated, which makes this trust computing scheme very suitable for a large-scale cloud computing environment. Performance analysis and experimental results verify feasibility and effectiveness of the proposed scheme.},
keywords={Cloud computing;Collaboration;Monitoring;Security;Big Data;Quality of service;Computer architecture;Cloud computing;service behavior monitoring;trust computing;big data analysis},
doi={10.1109/TIFS.2018.2806925},
ISSN={1556-6021},
month={Aug},}
@INPROCEEDINGS{7840737,
author={Huang, Zhichuan and Xie, Tiantian and Zhu, Ting and Wang, Jianwu and Zhang, Qingquan},
booktitle={2016 IEEE International Conference on Big Data (Big Data)}, title={Application-driven sensing data reconstruction and selection based on correlation mining and dynamic feedback},
year={2016},
volume={},
number={},
pages={1322-1327},
abstract={As sensors spread across almost every industry, the Internet of Things (IoT) is going to trigger an era of big data. However, the abundance of available sensing data causes new challenges when building IoT applications. One main challenge is how to select proper data from large amount of sensing data for learning useful information efficiently. Existing approaches require developers to manage data for each specific application, which is very time consuming since the developers may not have enough knowledge about the dynamic changing data quality of different sensors. In this paper, we propose a data management middleware to learn the correlations between time series sensor data without prior knowledge. The learned correlation is then applied to select the useful sensor and reconstruct the incorrect data. To generalize the correlation models for each application, we utilize the dynamic feedback from the application to update the data selection and reconstruction. We evaluate our data management middleware in smart grids. The evaluation results show that our middleware can achieve better application performance with the help of dynamic feedback, data reconstruction and data selection.},
keywords={},
doi={10.1109/BigData.2016.7840737},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8117153,
author={ur Rehman, Shafiq and Hark, Andre and Gruhn, Volker},
booktitle={2017 8th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)}, title={A framework to handle big data for cyber-physical systems},
year={2017},
volume={},
number={},
pages={72-78},
abstract={The use of big data for cyber-physical systems (CPS) is gaining more importance due to the ever-increasing amount of collectable data. Due to the decreasing cost of sensors and the growth of embedded systems, which are increasingly used in the industries as well as in the private sectors, new methods are needed to evaluate and process the collected data. Therefore, in this paper we proposed a framework to handle big data for cyber-physical systems. The framework considered the possible solutions that would be standardization, cloud computing, online and data stream learning, a methodology to process data and multi-agent systems for CPS. Furthermore, we examine the security challenges and big data issues of cyber-physical systems.},
keywords={Big Data;Cyber-physical systems;Safety;Real-time systems;Sensors;Big data;cyber-physical system (CPS);security;real-time;standardization;infrastructure;data quality},
doi={10.1109/IEMCON.2017.8117153},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9107804,
author={Ju, Xingang and Lian, Feiyu and Zhang, Yuan},
booktitle={2019 6th International Conference on Information Science and Control Engineering (ICISCE)}, title={Data Cleaning Optimization for Grain Big Data Processing using Task Merging},
year={2019},
volume={},
number={},
pages={225-233},
abstract={Data quality has exerted important influence over the application of grain big data, so data cleaning is a necessary and important work. In MapReduce frame, we can use parallel technique to execute data cleaning in high scalability mode, but due to the lack of effective design there are amounts of computing redundancy in the process of data cleaning, which results in lower performance. In this research, we found some tasks often are carried out multiple times on same input files, or require same operation results in the process of data cleaning. For this problem, we proposed a new optimization technique that is based on task merge. By merging simple or redundancy computations on same input files, the number of the loop computation in MapReduce can be reduced greatly. The experiment shows, by this means, the overall system runtime is significantly reduced, which proves that the process of data cleaning is optimized. In this paper, we optimized several modules of data cleaning such as entity identification, inconsistent data restoration, and missing value filling. Experimental results show that the proposed method in this paper can increase efficiency for grain big data cleaning.},
keywords={grain big data;data cleaning;task merging;hadoop;mapReduce},
doi={10.1109/ICISCE48695.2019.00053},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9604548,
author={Hasan, Forat Falih and Bakar, Muhamad Shahbani Abu},
booktitle={2021 5th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)}, title={Data Transformation from SQL to NoSQL MongoDB Based on R Programming Language},
year={2021},
volume={},
number={},
pages={399-403},
abstract={Owing to their high availability and scalability, NoSQL databases are becoming more popular for Big data applications in web analytics and supporting large websites. Moreover, each NoSQL system has its API which does not support industry standards like SQL and JDBC, integrating these systems with other enterprise and reporting software takes more time. The main requirements of Big data and data analytics are transforming the data from SQL databases to NoSQL data structures to represent the data. In this work, we presented a method to transform the data from different types of SQL databases to the desired NoSQL database based on the R programming language. The proposed work is based on the R environment used to handle the data from the source system to the target databases and meet the data quality requirements in data transformation. The results confirmed that the development provided a good solution for the data transformation from SQL to NoSQL by taking into account the data quality requirements.},
keywords={Structured Query Language;Computer languages;Data analysis;NoSQL databases;Data integrity;Scalability;Transforms;Big Data;Data Transformation;SABR Algorithm;NoSQL;ETL},
doi={10.1109/ISMSIT52890.2021.9604548},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8780271,
author={Yu, Wenjin and Dillon, Tharam and Mostafa, Fahed and Rahayu, Wenny and Liu, Yuehua},
booktitle={2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS)}, title={Implementation of Industrial Cyber Physical System: Challenges and Solutions},
year={2019},
volume={},
number={},
pages={173-178},
abstract={The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data & AI Innovation Awards.},
keywords={Big Data;Manufacturing;Industries;Sensors;Data integrity;Real-time systems;Cloud computing;Cyber-Physical System;Internet of Things;Industry 4.0;cloud computing;big data ecosystem;data quality},
doi={10.1109/ICPHYS.2019.8780271},
ISSN={},
month={May},}
@INPROCEEDINGS{9319350,
author={Iyengar, Arun and Patel, Dhaval and Shrivastava, Shrey and Zhou, Nianjun and Bhamidipaty, Anuradha},
booktitle={2020 IEEE Second International Conference on Cognitive Machine Intelligence (CogMI)}, title={Real-Time Data Quality Analysis},
year={2020},
volume={},
number={},
pages={101-108},
abstract={Data quality is critically important for big data and machine learning applications. Data quality systems can analyze data sets for quality and detection of potential errors. They can also provide remediation to fix problems encountered in analyzing data sets. This paper discusses key features that of data quality analysis systems. We also present new algorithms for efficiently maintaining updated data quality metrics on changing data sets. Our algorithms consider anomalies in data regions in determining how much different regions of data contribute to overall data metrics. We also make intelligent choices of which data metrics to update and how frequently to do so in order to limit the overhead for data quality metric updates.},
keywords={Data integrity;Measurement;Prediction algorithms;Anomaly detection;Machine learning algorithms;Task analysis;Interpolation;data quality;data analytics;real time data analytics},
doi={10.1109/CogMI50398.2020.00022},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7536334,
author={Lawson, Victor J. and Ramaswamy, Lakshmish},
booktitle={2016 International Conference on Distributed Computing in Sensor Systems (DCOSS)}, title={TAU-FIVE: A Multi-tiered Architecture for Data Quality and Energy-Sustainability in Sensor Networks},
year={2016},
volume={},
number={},
pages={169-176},
abstract={Current research on wireless sensor networks "WSNs" in the Internet of Things "IoT" has focused on performance, scalability and energy efficiency. Innovations in these areas have many challenges due to the increasing volume of smart device data streams in the internet of Everything "IoE". Data feeds from future IoE systems such as the internet of vehicles, smart homes and smart-cities will need real time consolidation. This merger of technologies will require innovative big data algorithms and architectures that authenticate the data streams. A primary concern is in dynamically quantifying the data quality "DQ" of the streams while constructing real-time metrics to assess the energy efficiency "EE" of these IoE devices. In order to define the relationship between sensor stream DQ and EE, we propose our multi-tiered cloud-service architecture TAU-FIVE. The technical contributions of our framework includes data quality and energy efficiency models based on 7 DQ attributes and multiple reprogrammable smart sensors that dynamically modify and regulate the DQ and EE of a WSN. Our research maintains that WSN's can balance sustainability with quality of service by creating real-time metrics that merge energy usage with data stream integrity. This equilibrium will impact energy awareness in the IoT as the multitude of batch device data streams are integrated with the variety of social and professional networks and evolve into the IoE.},
keywords={Measurement;Feeds;Computer architecture;Wireless sensor networks;Energy efficiency;Clouds;Data models;Data quality;cloud computing;energy model;applications to sensing;green networks},
doi={10.1109/DCOSS.2016.42},
ISSN={2325-2944},
month={May},}
@ARTICLE{7914196,
author={Ding, Xiaoou and Wang, Hongzhi and Gao, Yitong and Li, Jianzhong and Gao, Hong},
journal={Tsinghua Science and Technology}, title={Efficient currency determination algorithms for dynamic data},
year={2017},
volume={22},
number={3},
pages={227-242},
abstract={Data quality is an important aspect in data application and management, and currency is one of the major dimensions influencing its quality. In real applications, datasets timestamps are often incomplete and unavailable, or even absent. With the increasing requirements to update real-time data, existing methods can fail to adequately determine the currency of entities. In consideration of the velocity of big data, we propose a series of efficient algorithms for determining the currency of dynamic datasets, which we divide into two steps. In the preprocessing step, to better determine data currency and accelerate dataset updating, we propose the use of a topological graph of the processing order of the entity attributes. Then, we construct an Entity Query B-Tree (EQB-Tree) structure and an Entity Storage Dynamic Linked List (ES-DLL) to improve the querying and updating processes of both the data currency graph and currency scores. In the currency determination step, we propose definitions of the currency score and currency information for tuples referring to the same entity and use examples to discuss methods and algorithms for their computation. Based on our experimental results with both real and synthetic data, we verify that our methods can efficiently update data in the correct order of currency.},
keywords={Heuristic algorithms;Remuneration;Real-time systems;Databases;Big Data;data quality management; data currency; dynamic determining},
doi={10.23919/TST.2017.7914196},
ISSN={1007-0214},
month={June},}
@INPROCEEDINGS{9378181,
author={Srivastava, Divesh},
booktitle={2020 IEEE International Conference on Big Data (Big Data)}, title={Towards High-Quality Big Data: Lessons from FIT},
year={2020},
volume={},
number={},
pages={4-4},
abstract={Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Data are being generated, collected, and analyzed today at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. As the use of big data has grown, so too have concerns that poor-quality data, prevalent in large data sets, can have serious adverse consequences on data-driven decision making. Responsible data science thus requires a recognition of the importance of veracity, the fourth "V" of big data. In this talk, we first present a vision of high-quality big data and highlight the substantial challenges that the first three V’s, volume, velocity, and variety, bring to dealing with veracity in big data. We then present the FIT Family of adaptive, data-driven statistical tools that we have designed, developed, and deployed at AT&T for continuous data quality monitoring of a large and diverse collection of continuously evolving data. These tools monitor data movement to discover missing, partial, duplicated, and delayed data; identify changes in the content of spatiotemporal streams; and pinpoint anomaly hotspots based on persistence, pervasiveness, and priority. We conclude with lessons from FIT relevant to big data quality that are cause for optimism.},
keywords={},
doi={10.1109/BigData50022.2020.9378181},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7976069,
author={Vieira, Vanessa and Pedrosa, Isabel and Soares, Bruno Horta},
booktitle={2017 12th Iberian Conference on Information Systems and Technologies (CISTI)}, title={Big data amp; analytics: An approach using audit experts' interviews},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Big Data is one of the great trends in the short and medium term in organizations. There is a growing concern in provid solutions to address this trend, to methodical analysis of data and to do better decisions. The amount of data becomes less relevant when there is efficiency in Analytics. Internal auditors need to be in compliance with technology evolution. This research main objective is to understand how are internal auditors perceiving Big Data & Analytics' and which are the opportunities and difficulties pointed to address that challenge. To achieve this main goal, semi-structured interviews were conducted focused on internal auditors group. Those interviews intend to analyze and classify respondents' contributions in order to provide more insights for the present research. As a result, the main opportunities listed were greater information security and greater efficiency in data processing. Pointed obstacles were data quality, security and users' training.},
keywords={Big Data;Interviews;Market research;Organizations;Surges;Software;Big Data;organizations;information;Audit;technology},
doi={10.23919/CISTI.2017.7976069},
ISSN={},
month={June},}
@INPROCEEDINGS{8890376,
author={Chen, Chengling and Su, Zhou and Li, Weiwei and Wang, Yuntao},
booktitle={2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)}, title={Big Data Driven Computing Offloading Scheme with Driverless Vehicles Assistance},
year={2019},
volume={},
number={},
pages={412-416},
abstract={In the era of big data, edge computing is emerged as a promising paradigm to alleviate the pressure on the backbone network and facilitate vehicular services on the road. As edge nodes deployed for vehicular applications, roadside units (RSUs) need to undertake a large number of local computing tasks. However, due to the uncertainty of the vehicular network topology, static RSU deployments are subject to short-term overload and cannot handle various delay-sensitive computing tasks concurrently. To address the problem, we propose a big data driven computing offloading scheme to dispatch idle driverless vehicles to enhance the capacities of RSUs dynamically. First, we present a trust assessment model to evaluate the credibility of driverless vehicles. Then, a multi-attribute reverse auction is applied to maximize the utilities of RSUs and driverless vehicles. In addition, a secure forwarding method is developed to protect the privacy of computing tasks.},
keywords={Task analysis;Computational modeling;Automobiles;Big Data;Quality of service;Privacy;Bandwidth;Vehicular ad-hoc networks, reverse auction, computing offloading, driverless vehicles},
doi={10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00084},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9671443,
author={Mecati, Mariachiara and Vetrò, Antonio and Torchiano, Marco},
booktitle={2021 IEEE International Conference on Big Data (Big Data)}, title={Detecting Discrimination Risk in Automated Decision-Making Systems with Balance Measures on Input Data},
year={2021},
volume={},
number={},
pages={4287-4296},
abstract={Bias in the data used to train decision-making systems is a relevant socio-technical issue that emerged in recent years, and it still lacks a commonly accepted solution. Indeed, the "bias in-bias out" problem represents one of the most significant risks of discrimination, which encompasses technical fields, as well as ethical and social perspectives. We contribute to the current studies of the issue by proposing a data quality measurement approach combined with risk management, both defined in ISO/IEC standards. For this purpose, we investigate imbalance in a given dataset as a potential risk factor for detecting discrimination in the classification outcome: specifically, we aim to evaluate whether it is possible to identify the risk of bias in a classification output by measuring the level of (im)balance in the input data. We select four balance measures (the Gini, Shannon, Simpson, and Imbalance ratio indexes) and we test their capability to identify discriminatory classification outputs by applying such measures to protected attributes in the training set. The results of this analysis show that the proposed approach is suitable for the goal highlighted above: the balance measures properly detect unfairness of software output, even though the choice of the index has a relevant impact on the detection of discriminatory outcomes, therefore further work is required to test more in-depth the reliability of the balance measures as risk indicators. We believe that our approach for assessing the risk of discrimination should encourage to take more conscious and appropriate actions, as well as to prevent adverse effects caused by the "bias in-bias out" problem.},
keywords={Training;Ethics;Decision making;Big Data;Software;Software reliability;Software measurement;Data quality;Data bias;Data ethics;Algorithm fairness;Automated decision-making},
doi={10.1109/BigData52589.2021.9671443},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7802106,
author={Kalan, Reza Shokri and Ünalir, Murat Osman},
booktitle={2016 6th International Conference on Computer and Knowledge Engineering (ICCKE)}, title={Leveraging big data technology for small and medium-sized enterprises (SMEs)},
year={2016},
volume={},
number={},
pages={1-6},
abstract={Wisdom aligns with technology is the key factor for sustainable business development. By increasing amount of public and private data, organizations need to find new solutions to manage data and information which lead to knowledge, better decision making, and value. In the big data-bang, smart organization surfing on-line technology and start planning big data strategy. However, many organizations do not yet have a big data strategy. A challenge facing SMEs is that they may not have the same capacity as large companies to analysis new data sets. Also, traditional data processing tools are not capable for SMEs decision making because of volume, velocity and variety if data. For address this problem we need new leveraging technology, tools and talent. SMEs which have risen to leveraging the value of big data are using advantage of cloud computing and open-source software to realize various goals. The main goal of this investment is about value as a new concept in a big data era. In this study, we focus on emerging trends and future requirement: technology and tools for SMEs.},
keywords={Organizations;Parallel processing;Computer architecture;Security;Privacy;big data;data analytic;data quality;SMEs;business intelligence;cloud computing},
doi={10.1109/ICCKE.2016.7802106},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7510775,
author={Liu, Yunshu and Chen, Xuanyu and Chen, Cailian and Guan, Xingping},
booktitle={2016 IEEE International Conference on Communications (ICC)}, title={Traffic big data analysis supporting vehicular network access recommendation},
year={2016},
volume={},
number={},
pages={1-6},
abstract={With the explosive growth of Internet of Vehicles (IoV), it is undoubted that vehicular demands for real-time Internet access would get a surge in the near future. Therefore, it is foreseeable that the cars within the IoV will generate enormous data. On the one hand, the huge volume of data mean we could get much information (e.g., vehicle's condition and real-time traffic distribution) through the big data analysis. On the other hand, the huge volume of data will overload the cellular network since the cellular infrastructure still represents the dominant access methods for ubiquitous connections. The vehicular ad hoc network (VANET) offloading is a promising solution to alleviate the conflict between the limited capacity of cellular network and big data collection. In a vehicular heterogeneous network formed by cellular network and VANET, an efficient network selection is crucial to ensure vehicles' quality of service. To address this issue, we develop an intelligent network recommendation system supported by traffic big data analysis. Firstly, the traffic model for network recommendation is built through big data analysis. Secondly, vehicles are recommended to access an appropriate network by employing the analytic framework which takes traffic status, user preferences, service applications and network conditions into account. Furthermore an Android application is developed, which enables individual vehicle to access network automatically based on the access recommender. Finally, extensive simulation results show that our proposal can effectively select the optimum network for vehicles, and network resource is fully utilized at the same time.},
keywords={Vehicles;Vehicular ad hoc networks;Roads;Big data;Quality of service;Internet;Real-time systems},
doi={10.1109/ICC.2016.7510775},
ISSN={1938-1883},
month={May},}
@INPROCEEDINGS{9671677,
author={Qi, Wenting and Chelmis, Charalampos},
booktitle={2021 IEEE International Conference on Big Data (Big Data)}, title={Improving Algorithmic Decision–Making in the Presence of Untrustworthy Training Data},
year={2021},
volume={},
number={},
pages={1102-1108},
abstract={Although data quality is of paramount importance in algorithmic decision–making, most existing methods for supervised classification use training data without ever questioning their fidelity. At the same time, counterfactual explanation approaches widely used for post–hoc explanation of algorithmic decisions may result in unrealistic recommendations when left unconstrained. This work highlights a significant research problem, and introduces a novel framework to improve supervised classification in the presence of untrustworthy data, while offering actionable suggestions when an undesirable decision has been made (e.g., loan application rejection). Evaluation results spanning datasets from different domains demonstrate the superiority of the proposed approach, and its comparative advantage as the percentage of mislabeled instances increases.},
keywords={Data integrity;Conferences;Supervised learning;Training data;Machine learning;Big Data;Data models;counterfactual explanations;data quality;data science;supervised learning},
doi={10.1109/BigData52589.2021.9671677},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6906820,
author={Papageorgiou, Apostolos and Zahn, Manuel and Kovacs, Ernö},
booktitle={2014 IEEE International Congress on Big Data}, title={Auto-configuration System and Algorithms for Big Data-Enabled Internet-of-Things Platforms},
year={2014},
volume={},
number={},
pages={490-497},
abstract={Internet of Things (IoT) platforms that handle Big Data might perform poorly or not according to the goals of their operator (in terms of costs, database utilization, data quality, energy-efficiency, throughput) if they are not configured properly. The latter configuration refers mainly to system parameters of the data-collecting gateways, e.g., polling intervals, capture intervals, encryption schemes, used protocols etc. However, re-configuring the platform appropriately upon changes of the system context or the operator targets is currently not taking place. This happens because of the complexity or unawareness of the synergies between system configurations and various aspects of the Big Data-handling IoT platform, but also because of the human resources that an efficient re-configuration would require. This paper presents an auto-configuration solution based on interpretable configuration suggestions, focusing on the algorithms for computing the mentioned suggested configurations. Five such algorithms are contributed, while a thorough evaluation reveals which of these algorithms should be used in different operation scenarios in order to achieve high fulfillment of the operator's targets.},
keywords={Logic gates;Big data;Measurement;Heuristic algorithms;Complexity theory;Standards;Optimization;M2M;IoT;configuration;gateway;autonomic;self-management},
doi={10.1109/BigData.Congress.2014.78},
ISSN={2379-7703},
month={June},}
@ARTICLE{7978034,
author={Xu, Xiaolong and Liu, Xinxin and Liu, Xiaoxiao and Sun, Yanfei},
journal={Journal of Systems Engineering and Electronics}, title={Truth finder algorithm based on entity attributes for data conflict solution},
year={2017},
volume={28},
number={3},
pages={617-626},
abstract={The Internet now is a large-scale platform with big data. Finding truth from a huge dataset has attracted extensive attention, which can maintain the quality of data collected by users and provide users with accurate and efficient data. However, current truth finder algorithms are unsatisfying, because of their low accuracy and complication. This paper proposes a truth finder algorithm based on entity attributes (TFAEA). Based on the iterative computation of source reliability and fact accuracy, TFAEA considers the interactive degree among facts and the degree of dependence among sources, to simplify the typical truth finder algorithms. In order to improve the accuracy of them, TFAEA combines the one-way text similarity and the factual conflict to calculate the mutual support degree among facts. Furthermore, TFAEA utilizes the symmetric saturation of data sources to calculate the degree of dependence among sources. The experimental results show that TFAEA is not only more stable, but also more accurate than the typical truth finder algorithms.},
keywords={Algorithm design and analysis;Reliability;Internet;Telecommunications;Big Data;Data models;truth finder;data reliability;entity attribute;data conflict},
doi={10.21629/JSEE.2017.03.21},
ISSN={1004-4132},
month={June},}
@INPROCEEDINGS{8784484,
author={Zhang, Xupeng and Liang, Du},
booktitle={2019 IEEE 9th International Conference on Electronics Information and Emergency Communication (ICEIEC)}, title={Construction of Elevator Inspection Quality Evaluation System Based on Big Data},
year={2019},
volume={},
number={},
pages={238-242},
abstract={Elevator inspection information has typical big data characteristics. This paper points out that the elevator inspection data introduces the method of elevator inspection big data analysis. Taking elevator inspection as an example, it lists several kinds of big data analysis methods for inspection data, including the risk points describing the basic information of the elevator, the scanning inspection process and the inspection quality. Based on frequency analysis of active factors, outlier test, quality assessment, correlation analysis. Using big data technology, it can make statistical analysis on the data obtained by elevator inspection, make the inspection situation more intuitive, help the management organization to understand the overall elevator quality and elevator inspection, and build an elevator inspection quality evaluation system to make the work more transparent and management more precise. Find more accurate questions, deeper supervision, and more scientific government decisions.},
keywords={Elevators;Inspection;Big Data;Safety;Market research;Testing;Monitoring;Elevator;inspection;big data;quality evaluation},
doi={10.1109/ICEIEC.2019.8784484},
ISSN={2377-844X},
month={July},}
@INPROCEEDINGS{7951876,
author={Liwei Zheng},
booktitle={2017 IEEE 2nd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)}, title={SNSQ ontology: A domain ontology for SNSs data quality},
year={2017},
volume={},
number={},
pages={11-18},
abstract={the advent of online social networks has been one of the most exciting events in this decade. Many popular online social networks such as Twitter, Wechat, Weibo, LinkedIn, and Facebook have become increasingly popular. The consequences of the poor quality of data in a social network are often experienced in everyday life. This paper gives a domain ontology model, SNSQ Ontology, for data quality in the area of social networks. It could be a knowledge base for the quality assessment of the rich and linkage data in the social network. High-quality data would be relevant in the data searching, analyzing and mining. Based on the SNSQ Ontology the strategy for data quality assessment and repair is given. And the co-influence among the four quality dimensions, completeness, consistency, currency, and accuracy, are discussed to guarantee an effective assessment process.},
keywords={Ontologies;Social network services;Artificial neural networks;Synchronization;Maintenance engineering;ontology;social network;data quality assessment},
doi={10.1109/ICCCBDA.2017.7951876},
ISSN={},
month={April},}
@ARTICLE{7452294,
author={Cherubini, Giovanni and Jelitto, Jens and Venkatesan, Vinodh},
journal={Computer}, title={Cognitive Storage for Big Data},
year={2016},
volume={49},
number={4},
pages={43-51},
abstract={Storage system efficiency can be significantly improved by determining the value of data. A key concept is cognitive storage, or optimizing storage systems by better comprehending the relevance of data to user needs and preferences. The Web extra at https://youtu.be/P-ZxlTLwzTI is a video of authors Giovanni Cherubini and Vinodh Venkatesan of IBM Research--Zurich discussing the concepts, applications, and benefits of cognitive storage for big data.},
keywords={Energy efficiency;Performance evaluation;Storage area networks;Big data;Quality of service;Context modeling;Data storage aystems;mass storage;big data;storage management;value of information;repositories;autonomous systems;content analysis;indexing},
doi={10.1109/MC.2016.117},
ISSN={1558-0814},
month={Apr},}
@INPROCEEDINGS{9727023,
author={Xing, Xiaobo},
booktitle={2021 International Symposium on Advances in Informatics, Electronics and Education (ISAIEE)}, title={Financial Big Data Reconciliation Method},
year={2021},
volume={},
number={},
pages={260-263},
abstract={For data errors in distributed financial system caused by multi-system interaction, asynchronous processing and system bug, this paper proposes offline and quasi real-time data reconciliation methods based on the combination of Alibaba big data processing platform and accounting theory. In offline data reconciliation, Full data reconciliation and hour level incremental data reconciliation are introduced. And in quasi real-time data reconciliation, single system and distributed multi-system reconciliation models are introduced. These data reconciliation methods are then verified against 7 million pieces of daily data of the distributed loan system in a financial company. Results show that these methods can complete the financial big data processing, discover the data quality problems timely, and minimize the financial system capital loss.},
keywords={Costs;Data integrity;Education;Distributed databases;Big Data;Maintenance engineering;Real-time systems;financial big data;full data reconciliation;incremental data reconciliation;quasi real time data reconciliation},
doi={10.1109/ISAIEE55071.2021.00071},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9407474,
author={Huang, Haiyan and Wei, Bizhong and Dai, Jian and Ke, Wenlong},
booktitle={2020 16th International Conference on Computational Intelligence and Security (CIS)}, title={Data Preprocessing Method For The Analysis Of Incomplete Data On Students In Poverty},
year={2020},
volume={},
number={},
pages={248-252},
abstract={Data mining is the focus of big data applications in various fields. Data pre-processing is a crucial step in the data mining process. With the development of the information society and the application of databases, the educational data has seen explosive growth, and the data on poor students has become informative. However, the actual student financial aid management system collects the data on poor students which generally has problems such as missing values, attributes redundancy, and noise. To solve this problem, we proposed a novel method called DPBP to preprocess data. The proposed DPBP approach consists of four stages: the preparation of data, the scoping of characteristics, the combination of characteristics, and the filtering of missing number. Firstly, we prepare the dataset by extracting data. Next, the characteristic range is limited by choosing experimental results of feature selection algorithm. Then, third stage performs feature combination to obtain the feature decomposition sets. Finally, based on accuracy and missing number, we gain the optimal dataset. Series of experiments result show that our proposed method significantly improves the data quality and stability.},
keywords={Filtering;Databases;Data integrity;Redundancy;Data preprocessing;Feature extraction;Stability analysis;data mining;data preprocessing;feature selection},
doi={10.1109/CIS52066.2020.00060},
ISSN={},
month={Nov},}
@ARTICLE{9457165,
author={Manogaran, Gunasekaran and Nguyen, Tu N.},
journal={IEEE Transactions on Intelligent Transportation Systems}, title={Displacement-Aware Service Endowment Scheme for Improving Intelligent Transportation Systems Data Exchange},
year={2021},
volume={},
number={},
pages={1-11},
abstract={Intelligent Transportation Systems (ITS) is a smart-transportation system for road-side assistance and data exchange support by integrating cloud and wireless networks. ITS facilitates vehicle-to-vehicle and vehicle-to-anything (V2X) data exchanges for satisfying user demands. The rate of big data granting to the vehicular users is interrupted by the fundamental attributes such as mobility and link instability of the vehicles. To address the issues in vehicular data exchange big data, this article introduces displacement-aware service endowment scheme with the benefits of data offloading. Displacement-aware big data endowment ensures responsive availability of vehicle request information despite unfavorable location and density factors. The time congruency in V2V and V2X data exchanges are adopted for minimizing data exchange dropouts. In the data offloading phase, extraneous information and big data responses are detained based on data exchange relevance to improve congestion free big data endowment. The distinct methods work in a co-operative manner to improve big data quality of fast configuring smart vehicles to provide reliable big data in smart city environments.},
keywords={Big Data;Quality of service;Delays;Data models;Vehicular ad hoc networks;Vehicle-to-everything;Optimization;ITS;mobility prediction;time synchronized data exchanges;data offloading;V2X data exchange.},
doi={10.1109/TITS.2021.3078753},
ISSN={1558-0016},
month={},}
@INPROCEEDINGS{8422106,
author={Meng, Qianyu and Wang, Kun and Liu, Bo and Miyazaki, Toshiaki and He, Xiaoming},
booktitle={2018 IEEE International Conference on Communications (ICC)}, title={QoE-Based Big Data Analysis with Deep Learning in Pervasive Edge Environment},
year={2018},
volume={},
number={},
pages={1-6},
abstract={In the age of big data, the services in pervasive edge environment are expected to offer end-users better Quality of Experience (QoE) than that in a normal edge environment. Nevertheless, various types of edge devices with storage, delivery, and sensing are coming into our environment and produce the high-dimensional big data accompanied by a volume of pervasive big data increasingly with a lot of redundancy. Therefore, the satisfaction of QoE becomes the primary challenge in high dimensional big data on the basis of pervasive edge environment. In this paper, we first propose a QoE model to evaluate the quality of service in pervasive edge environment. The value of QoE does not only include the accurate data, but also the transmission rate. Then, on the basis of the accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on Deep Learning, which is suitable for pervasive edge environment with high-dimensional big data analysis. Simulation results reveal that our proposals could achieve high QoE performance.},
keywords={Big Data;Quality of experience;Training;Tensile stress;Machine learning;Data models;Quality of service},
doi={10.1109/ICC.2018.8422106},
ISSN={1938-1883},
month={May},}
@ARTICLE{9086142,
author={Ding, Xiaoou and Wang, Hongzhi and Su, Jiaxuan and Wang, Muxian and Li, Jianzhong and Gao, Hong},
journal={IEEE Transactions on Knowledge and Data Engineering}, title={Leveraging Currency for Repairing Inconsistent and Incomplete Data},
year={2022},
volume={34},
number={3},
pages={1288-1302},
abstract={Data quality plays a key role in big data management today. With the explosive growth of data from a variety of sources, the quality of data is faced with multiple problems. Motivated by this, we study the multiple data cleaning on incompleteness and inconsistency with currency reasoning and determination in this paper. We introduce a 4-step framework, named ${\sf Imp3C}$Imp3C, for errors detection and quality improvement in incomplete and inconsistent data without timestamps. We achieve an integrated currency determining method to compute the currency orders among tuples, according to currency constraints. Thus, the inconsistent data and missing values are repaired effectively considering the temporal impact. For both effectiveness and efficiency consideration, we carry out inconsistency repair ahead of incompleteness repair. A currency-related consistency distance metric is defined to measure the similarity between dirty tuples and clean ones more accurately. In addition, currency orders are treated as an important feature in the missing imputation training process. The solution algorithms are introduced in detail with case studies. A thorough experiment on three real-life datasets verifies our method ${\sf Imp3C}$Imp3C improves the performance of data repairing with multiple quality problems. ${\sf Imp3C}$Imp3C outperforms the existing advanced methods, especially in the datasets with complex currency orders.},
keywords={Currencies;Maintenance engineering;Cleaning;Urban areas;Remuneration;Databases;Companies;Data cleaning;data quality management;currency determining;temporal data repairing},
doi={10.1109/TKDE.2020.2992456},
ISSN={1558-2191},
month={March},}
@INPROCEEDINGS{7207308,
author={Lawson, Victor and Ramaswamy, Lakshmish},
booktitle={2015 IEEE International Congress on Big Data}, title={Data Quality and Energy Management Tradeoffs in Sensor Service Clouds},
year={2015},
volume={},
number={},
pages={749-752},
abstract={Cloud-based sensor data collection services are becoming an essential part of the Internet of Things (IoT). As the consumer demand grows for these services, the data quality (DQ) of the stream becomes an increasingly vital issue. Of particular interest is the inherent tradeoff between the DQ and the energy consumption of the sensor. Unfortunately, there has been very little research on the management of this tradeoff that allows data consumers to receive high quality data while simultaneously conserving energy. Our work seeks to explore this tradeoff in detail by combining DQ services for the data stream consumer with customizable energy efficient "EE" throttling algorithms for the data feed producers. These energy management services provide cost reduction rewards for consumers who would otherwise make poor DQ/EE decisions. Our primary contributions include cloud-based services for monitoring the tradeoff, an architecture that adjusts to DQ needs and a producer/consumer data stream best matching cloud service. We envision that our services architecture will reward energy efficiency decisions and profoundly affect consumer choices.},
keywords={Feeds;Big data;Computer architecture;Clouds;Wireless sensor networks;Software;Conferences;data quality;cloud service;energy management;sensor network},
doi={10.1109/BigDataCongress.2015.124},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{7129549,
author={Tang, Nan},
booktitle={2015 31st IEEE International Conference on Data Engineering Workshops}, title={Big RDF data cleaning},
year={2015},
volume={},
number={},
pages={77-79},
abstract={Without a shadow of a doubt, data cleaning has played an important part in the history of data management and data analytics. Possessing high quality data has been proven to be crucial for businesses to do data driven decision making, especially within the information age and the era of big data. Resource Description Framework (RDF) is a standard model for data interchange on the semantic web. However, it is known that RDF data is dirty, since many of them are automatically extracted from the web. In this paper, we will first revisit data quality problems appeared in RDF data. Although many efforts have been put to clean RDF data, unfortunately, most of them are based on laborious manual evaluation. We will also describe possible solutions that shed lights on (semi-)automatically cleaning (big) RDF data.},
keywords={Resource description framework;Cleaning;Ontologies;Data mining;Knowledge based systems;Conferences;Databases},
doi={10.1109/ICDEW.2015.7129549},
ISSN={},
month={April},}
@INPROCEEDINGS{9179610,
author={Chouhan, Ashish and Prabhune, Ajinkya and Prabhuraj, Paneesh and Chaudhari, Hitesh},
booktitle={2020 IEEE Sixth International Conference on Big Data Computing Service and Applications (BigDataService)}, title={DWreck: A Data Wrecker Framework for Generating Unclean Datasets},
year={2020},
volume={},
number={},
pages={78-87},
abstract={In this paper, we present DWreck, a data wrecker framework for generating unclean datasets by counterproductively applying different data quality dimensions. In a typical data-analysis pipeline, data cleaning is the most cost-intensive, laborious, and time-consuming step. Unclean dataset or partially cleaned dataset can lead to incorrect training of machine learning models and result in wrong conclusions. Generally, data-scientists examine null, missing, or duplicate values, and the dataset is cleaned by removing the entire record or imputing the values. However, deleting the records, or imputing the values cannot be termed as comprehensive cleaning, as these cleaning techniques may result in a reduction in the population of data, and increased error in estimation due to biased values. For systematically cleaning an unclean dataset, it is necessary to comply with the data quality dimensions such as completeness, validity, consistency, accuracy, and conformity. The errors described as violations of expectations for completeness, accuracy, timeliness, consistency and other dimensions of data quality often impede the successful completion of information processing streams and consequently degrade the dependent business processes. Therefore, educating a data-scientist for comprehensively cleaning a raw-dataset acquired for analysis is an incremental learning process. Moreover, for extensive training on cleaning a dataset on different quality dimensions, it is necessary to provide a variety of datasets that are unclean on various data quality dimensions. Hence, in this paper, we present DWreck, a data wrecker framework for generating unclean datasets by counterproductively applying different data quality dimensions. The DWreck framework is designed on the principles of microservices architecture pattern. For allowing function-specific extensibility, the DWreck comprises four groups of microservices: (a) Dataset Profiling, (b) Data type Processing, (c) Counterproductive Dimensions, and (d) Miscellaneous. The orchestrator coordinates the different microservices in a complex workflow that is further split into three sub-workflows to generate an unclean (wrecked) dataset as an output. Finally, we evaluate the DWreck framework on twenty seed-datasets to generate corresponding wrecked datasets.},
keywords={Generators;Data integrity;XML;Cleaning;Tools;Databases;Pipelines;data generators;data quality dimensions;data cleaning;microservice architectures;data management},
doi={10.1109/BigDataService49289.2020.00020},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9353794,
author={Xia, Hong and Zhang, YongKang and Wang, Han and Chen, YanPing and Wang, ZhongMin},
booktitle={2020 International Conference on Networking and Network Applications (NaNA)}, title={Crowdsourcing Answer Integration Algorithm For Big Data Environment},
year={2020},
volume={},
number={},
pages={335-341},
abstract={Crowdsourcing is an emerging distributed computing model that is widely used. Aiming at the uneven quality of crowdsourcing answers due to different workers' capabilities and attitudes, it is necessary to effectively study the hotspot issue of crowdsourcing answer integration. A crowdsourced answer integration algorithm based on “filter-evaluate-vote” is proposed. This algorithm is implemented using MapReduce parallel programming model in the Hadoop platform, and experiments are performed on multiple data sets. The results show that the proposed algorithm can be effective. It improves the accuracy of crowdsourced answers, and has high computing performance and horizontal scalability, which is suitable for answer integration in a big data environment.},
keywords={Crowdsourcing;Computational modeling;Scalability;Big Data;Quality assessment;Time factors;Task analysis;Crowdsourcing;quality assessment;answer integration;MapReduce},
doi={10.1109/NaNA51271.2020.00064},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9149151,
author={Guo, Weisi},
booktitle={ICC 2020 - 2020 IEEE International Conference on Communications (ICC)}, title={Partially Explainable Big Data Driven Deep Reinforcement Learning for Green 5G UAV},
year={2020},
volume={},
number={},
pages={1-7},
abstract={UAV enabled terrestrial wireless networks enables targeted user-centric service provisioning to en-richen both deep urban coverage and target various rural challenge areas. However, UAVs have to balance the energy consumption of flight with the benefits of wireless capacity delivery via a high dimensional optimisation problem. Classic reinforcement learning (RL) cannot meet this challenge and here, we propose to use deep reinforcement learning (DRL) to optimise both aggregate and minimum service provisioning. In order to achieve a trusted autonomy, the DRL agents have to be able to explain its actions for transparent human-machine interrogation. We design a Double Dueling Deep Q-learning Neural Network (DDDQN) with Prioritised Experience Replay (PER) and fixed Q-targets to achieve stable performance and avoid over-fitting, offering performance gains over naive DQN algorithms. We then use a big data driven case study and found that UAVs battery size determines the nature of its autonomous mission, ranging from an efficient exploiter of one hotspot (100% reward gain) to a stochastic explorer of many hotspots (60-150% reward gain). Using a variety of telecom and social media data, we infer driving Quality-of-Experience (QoE) and Quality-of-Service (QoS) metrics that are in contention with UAV power and communication constraints. Our greener UAVs (30-40% energy saved) address both quantitative QoS and qualitative QoE issues. Partial interpretability in the reinforcement learning is achieved using data features extracted in the hidden layers, offering an initial step for explainable AI (XAI) connecting machine intelligence with human expertise.},
keywords={5G mobile communication;Batteries;Wireless communication;Big Data;Machine learning;Optimization;big data;machine learning;deep reinforcement learning;radio resource management;UAV;energy efficiency;XAI},
doi={10.1109/ICC40277.2020.9149151},
ISSN={1938-1883},
month={June},}
@INPROCEEDINGS{9687865,
author={Yan, Peipei and Li, Feng and Xiang, Zhiwei and Li, Mingxuan and Fan, Shuming},
booktitle={2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)}, title={Research and application of power data management key technology},
year={2021},
volume={2},
number={},
pages={98-102},
abstract={With the intensified application of power information systems and the advent of the “big data” era, higher requirements are put forward for power data resource management and power data security. Electric power companies have carried out research on key technologies for data management, established a three-level management system at the provincial, prefectural and county levels, built a panoramic view of data resources, a data operation management platform, a data negative list sharing mechanism, and a data security protection mechanism, which were applied to all aspects of data management and data governance. Through the support of business processes, data standards, data quality, etc., it has effectively improved the management efficiency of power data, improved the company's data management level, promoted business collaboration and efficiency.},
keywords={Visualization;Data security;Process control;Collaboration;Big Data;Power grids;Resource management;Electric power data;Data management;Data sharing;Data security},
doi={10.1109/ICIBA52610.2021.9687865},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9145071,
author={Byabazaire, John and O'Hare, Gregory and Delaney, Declan},
booktitle={2020 IEEE International Conference on Communications Workshops (ICC Workshops)}, title={Data Quality and Trust : A Perception from Shared Data in IoT},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Internet of Things devices and data sources areseeing increased use in various application areas. The pro-liferation of cheaper sensor hardware has allowed for widerscale data collection deployments. With increased numbers ofdeployed sensors and the use of heterogeneous sensor typesthere is increased scope for collecting erroneous, inaccurate orinconsistent data. This in turn may lead to inaccurate modelsbuilt from this data. It is important to evaluate this data asit is collected to determine its validity. This paper presents ananalysis of data quality as it is represented in Internet of Things(IoT) systems and some of the limitations of this representation. The paper discusses the use of trust as a heuristic to drive dataquality measurements. Trust is a well-established metric that hasbeen used to determine the validity of a piece or source of datain crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework forrepresenting data quality effectively within the big data modeland why a trust backed framework is important especially inheterogeneously sourced IoT data streams.},
keywords={Data integrity;Data models;Big Data;Biological system modeling;Ecosystems;Standards},
doi={10.1109/ICCWorkshops49005.2020.9145071},
ISSN={2474-9133},
month={June},}
@INPROCEEDINGS{9474741,
author={Mavrogiorgou, Argyro and Kleftakis, Spyridon and Mavrogiorgos, Konstantinos and Zafeiropoulos, Nikolaos and Menychtas, Andreas and Kiourtis, Athanasios and Maglogiannis, Ilias and Kyriazis, Dimosthenis},
booktitle={2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)}, title={beHEALTHIER: A Microservices Platform for Analyzing and Exploiting Healthcare Data},
year={2021},
volume={},
number={},
pages={283-288},
abstract={The era of big data is surrounded by plenty of challenges, concerning aspects related to data quality, data management, and data analysis. Plenty of these challenges are met in several domains, such as the healthcare domain, where the corresponding healthcare platforms not only have to deal with managing and/or analyzing a tremendous quantity of health data, but also have to accomplish these actions in the most efficient and secure way possible. Towards this direction, medical institutions are paying attention to the replacement of traditional approaches such as the Monolithic and Service Oriented Architecture (SOA), which deal with many difficulties for handling the increasing amount of healthcare data. This paper presents a platform for overcoming these issues, by adopting the Microservice Architecture (MSA), being able to efficiently manage and analyze these vast amounts of data. More specifically, the proposed platform, namely beHEALTHIER, offers the ability to construct health policies out of data of collective knowledge, by utilizing a newly proposed kind of electronic health records (i.e., eXtended Health Records (XHRs)) and their corresponding networks, through the efficient analysis and management of ingested healthcare data. In order to achieve that, beHEALTHIER is architected based upon four (4) discrete and interacting pillars, namely the Data, the Information, the Knowledge and the Actions pillars. Since the proposed platform is based on MSA, it fully utilizes MSA's benefits, achieving fast response times and efficient mechanisms for healthcare data collection, processing, and analysis.},
keywords={Machine learning algorithms;Data analysis;Software architecture;Medical services;Computer architecture;Big Data;Service-oriented architecture;Healthcare;Electronic Health Records;Data Collection;Data Analysis;Health Policies;Microservices},
doi={10.1109/CBMS52027.2021.00078},
ISSN={2372-9198},
month={June},}
@INPROCEEDINGS{7403678,
author={Tekieh, Mohammad Hossein and Raahemi, Bijan},
booktitle={2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)}, title={Importance of data mining in healthcare: A survey},
year={2015},
volume={},
number={},
pages={1057-1062},
abstract={In this survey, we collect the related information that demonstrate the importance of data mining in healthcare. As the amount of collected health data is increasing significantly every day, it is believed that a strong analysis tool that is capable of handling and analyzing large health data is essential. Analyzing the health datasets gathered by electronic health record (EHR) systems, insurance claims, health surveys, and other sources, using data mining techniques is very complex and is faced with very specific challenges, including data quality and privacy issues. However, the applications of data mining in healthcare, advantages of data mining techniques over traditional methods, special characteristics of health data, and new health condition mysteries have made data mining very necessary for health data analysis.},
keywords={Data mining;Diseases;Insurance;Data analysis;Organizations;Sociology;data mining;health data analysis;data quality;predictive modelling;health big data;data mining applications},
doi={10.1145/2808797.2809367},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7870183,
author={Clarke, Roger},
booktitle={2016 European Intelligence and Security Informatics Conference (EISIC)}, title={Quality Assurance for Security Applications of Big Data},
year={2016},
volume={},
number={},
pages={1-8},
abstract={The quality of inferences drawn from data, big or small, is heavily dependent on the quality of the data and the quality of the processes applied to it. Big data analytics is emerging from laboratories and being applied to intelligence and security needs. To achieve confidence in the outcomes of these applications, a quality assurance framework is needed. This paper outlines the challenges, and draws attention to the consequences of misconceived and misapplied projects. It presents key aspects of the necessary risk assessment and risk management approaches, and suggests opportunities for research.},
keywords={Big data;Q-factor;Sociology;Statistics;Security;Reliability;Quality assurance;risk assessment;risk management;information quality;data semantics;data scrubbing;decision quality;transparency},
doi={10.1109/EISIC.2016.010},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7996546,
author={Gu, Liqiu and Wang, Kun and Liu, Xiulong and Guo, Song and Liu, Bo},
booktitle={2017 IEEE International Conference on Communications (ICC)}, title={A reliable task assignment strategy for spatial crowdsourcing in big data environment},
year={2017},
volume={},
number={},
pages={1-6},
abstract={With the ubiquitous deployment of the mobile devices with increasingly better communication and computation capabilities, an emerging model called spatial crowdsourcing is proposed to solve the problem of unstructured big data by publishing location-based tasks to participating workers. However, massive spatial data generated by spatial crowdsourcing entails a critical challenge that the system has to guarantee quality control of crowdsourcing. This paper first studies a practical problem of task assignment, namely reliability aware spatial crowdsourcing (RA-SC), which takes the constrained tasks and numerous dynamic workers into consideration. Specifically, the worker confidence is introduced to reflect the completion reliability of the assigned task. Our RA-SC problem is to perform task assignments such that the reliability under budget constraints is maximized. Then, we reveal the typical property of the proposed problem, and design an effective strategy to achieve a high reliability of the task assignment. Besides the theoretical analysis, extensive experimental results also demonstrate that the proposed strategy is stable and effective for spatial crowdsourcing.},
keywords={Crowdsourcing;Big Data;Reliability theory;Sensors;Computational modeling;Measurement;Big data;crowdsourcing;task assignment},
doi={10.1109/ICC.2017.7996546},
ISSN={1938-1883},
month={May},}
@INPROCEEDINGS{7490949,
author={Yuan Gao and Hong Ao and Kang Wang and Weigui Zhou and Yi Li},
booktitle={2015 4th International Conference on Computer Science and Network Technology (ICCSNT)}, title={The diagnosis of wired network malfunctions based on big data and traffic prediction: An overview},
year={2015},
volume={01},
number={},
pages={1204-1208},
abstract={The increasing demand on higher transmission speed and shorter delay in wired networks becomes critical in recent communication networks. However, the capacity of transmission link is limited by the method of transmission. In this paper, aiming at the situation of large scale networks, an overview of the network optimization based on big data and traffic prediction is given in our proposed work. In wired networks, how to make full use of the transmission bandwidth and provide more reliable QoS is in great demand. Based on the network topology in our facility, we make a summary of current diagnosis method of the network and then propose the future possible way to solve the network malfunction based on big data through network log and complex monitors, then we make an overview of the diagnosis method based on traffic prediction, which could effectively make full use of bandwidth and avoid collision of the network.},
keywords={Big data;Quality of service;Market research;Monitoring;Satellites;Delays;Prediction methods;Network Diagnosis;Big Data;Traffic Prediction;Large Scale Network;Complex Network},
doi={10.1109/ICCSNT.2015.7490949},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9378343,
author={Moon, Aekyeung and Woo Son, Seung and Jung, Jiuk and Jeong Song, Yun},
booktitle={2020 IEEE International Conference on Big Data (Big Data)}, title={Understanding Bit-Error Trade-off of Transform-based Lossy Compression on Electrocardiogram Signals},
year={2020},
volume={},
number={},
pages={3494-3499},
abstract={The growing demand for recording longer ECG signals to improve the effectiveness of IoT-enabled remote clinical healthcare is contributing large amounts of ECG data. While lossy compression techniques have shown potential in significantly lowering the amount of data, investigation on how to trade-off between data reduction and data fidelity on ECG data received relatively less attention. This paper gives insight into the power of lossy compression to ECG signals by balancing between data quality and compression ratio. We evaluate the performance of transformed-based lossy compressions on the ECG datasets collected from the Biosemi ActiveTwo devices. Our experimental results indicate that ECG data exhibit high energy compaction property through transformations like DCT and DWT, thus could improve compression ratios significantly without hurting data fidelity much. More importantly, we evaluate the effect of lossy compression on ECG signals by validating the R-peak in the QRS complex. Our method can obtain low error rates measured in PRD (as low as 0.3) and PSNR (up to 67) using only 5% of the transform coefficients. Therefore, R-peaks in the reconstructed ECG signals are almost identical to ones in the original signals, thus facilitating extended ECG monitoring.},
keywords={Performance evaluation;Measurement uncertainty;Transforms;Medical services;Electrocardiography;Big Data;Monitoring;Transform coding;Lossy compression;IoT;Health care;R-peak;data fidelity},
doi={10.1109/BigData50022.2020.9378343},
ISSN={},
month={Dec},}
@ARTICLE{8847467,
author={Zhao, Cong and Yang, Shusen and McCann, Julie A.},
journal={IEEE Transactions on Mobile Computing}, title={On the Data Quality in Privacy-Preserving Mobile Crowdsensing Systems with Untruthful Reporting},
year={2021},
volume={20},
number={2},
pages={647-661},
abstract={The proliferation of mobile smart devices with ever improving sensing capacities means that human-centric Mobile Crowdsensing Systems (MCSs) can economically provide a large scale and flexible sensing solution. The use of personal mobile devices is a sensitive issue, therefore it is mandatory for practical MCSs to preserve private information (the user's true identity, precise location, etc.) while collecting the required sensing data. However, well intentioned privacy protection techniques also conceal autonomous, or even malicious, behaviors of device owners (termed as self-interested), where the objectivity and accuracy of crowdsensing data can therefore be severely threatened. The issue of data quality due to untruthful reporting in privacy-preserving MCSs has been yet to produce solutions. Bringing together game theory, algorithmic mechanism design, and truth discovery, we develop a mechanism to guarantee and enhance the quality of crowdsensing data without jeopardizing the privacy of MCS participants. Together with solid theoretical justifications, we evaluate the performance of our proposal with extensive real-world MCS trace-driven simulations. Experimental results demonstrate the effectiveness of our mechanism on both enhancing the quality of the crowdsensing data and eliminating the motivation of MCS participants, even when their privacy is well protected, to report untruthfully.},
keywords={Sensors;Data integrity;Data privacy;Task analysis;Mobile handsets;Roads;Monitoring;Mobile crowdsensing systems;privacy preservation;data quality;untruthful reporting},
doi={10.1109/TMC.2019.2943468},
ISSN={1558-0660},
month={Feb},}
@INPROCEEDINGS{9364425,
author={Neves, Ricardo A. and Cruvinel, Paulo E.},
booktitle={2021 IEEE 15th International Conference on Semantic Computing (ICSC)}, title={Ontology for Structuring a Digital Databases for Decision Making in Grain Production},
year={2021},
volume={},
number={},
pages={386-392},
abstract={This paper presents an ontology for the structuring of digital databases with the objective of acting in a cloud environment and meeting big data sources in the agricultural context of grain production. Its conception is structured in three stages: the first stage presents an ontological architecture aimed at public and private cloud environments, the second stage deals with a semantic model at process level, and a pseudocode for ontological application is elaborated in the third stage, considering the technologies applied to the cloud. This work combines advanced features to support decision making from Data Lake storage solutions, semantic treatment of big data, as well as the presentation of strategies based on machine learning and data quality analysis to obtain data and metadata organized for application in a decision model. The configuration of the ontology presented meets the diversity of big data projects in the grain production context, the characteristics of which are based on interoperability in the use of heterogeneous data and its integration, elasticity of computational resources, and high availability of cloud access.},
keywords={Cloud computing;Databases;Semantics;Decision making;Production;Ontologies;Big Data;Ontology;Agriculture;Digital Database;Cloud Computing;Big Data;Decision Making},
doi={10.1109/ICSC50631.2021.00071},
ISSN={2325-6516},
month={Jan},}
@INPROCEEDINGS{9071249,
author={Liu, Hong and Sang, Zhenhua and Karali, Sameer},
booktitle={2019 International Conference on Computational Science and Computational Intelligence (CSCI)}, title={Approximate Quality Assessment with Sampling Approaches},
year={2019},
volume={},
number={},
pages={1306-1311},
abstract={Data is useful to the extent that it can be quickly analyzed to reveal valuable information. With high-quality data, we can increase revenue, reduce cost, and reduce risk. On the other hand, the consequences of poor-quality data can be severe. It has been estimated that poor quality customer data costs U.S. businesses $611 billion annually in postage, printing, and staff overhead. These issues make data quality assessment a necessary and critical step in any data-related systems. Big data brings new challenges to data quality assessment due to the scale of data, streaming data, and different forms of data. Therefore, we proposed a sampling-based approximate quality assessment model on large data. Sampling large datasets can make all quality assessment processes cheaper and more feasible because of data reduction. The protocol of this work: First, the sample size is determined for estimating a large dataset. Next, sampling techniques are applied to collect samples. Then, these samples are used to estimate the quality of the large dataset. The objective of quality assessment in this work is to evaluate the completeness, accuracy, and timeliness of data and to return fast and approximate scores. Using different sample sizes and different sampling methods, we obtained 72 sets of data and compared them. These results show that the proposed approach is efficient and provides some insight into the quality assessment with samples.},
keywords={Data integrity;Quality assessment;Writing;Gaussian distribution;Big Data;Sampling methods;Time-frequency analysis;Data Quality, Quality Assessment, Sampling},
doi={10.1109/CSCI49370.2019.00244},
ISSN={},
month={Dec},}
@ARTICLE{8372637,
author={Dong, Yongquan and Dragut, Eduard C. and Meng, Weiyi},
journal={IEEE Transactions on Knowledge and Data Engineering}, title={Normalization of Duplicate Records from Multiple Sources},
year={2019},
volume={31},
number={4},
pages={769-782},
abstract={Data consolidation is a challenging issue in data integration. The usefulness of data increases when it is linked and fused with other data from numerous (Web) sources. The promise of Big Data hinges upon addressing several big data integration challenges, such as record linkage at scale, real-time data fusion, and integrating Deep Web. Although much work has been conducted on these problems, there is limited work on creating a uniform, standard record from a group of records corresponding to the same real-world entity. We refer to this task as record normalization. Such a record representation, coined normalized record, is important for both front-end and back-end applications. In this paper, we formalize the record normalization problem, present in-depth analysis of normalization granularity levels (e.g., record, field, and value-component) and of normalization forms (e.g., typical versus complete). We propose a comprehensive framework for computing the normalized record. The proposed framework includes a suit of record normalization methods, from naive ones, which use only the information gathered from records themselves, to complex strategies, which globally mine a group of duplicate records before selecting a value for an attribute of a normalized record. We conducted extensive empirical studies with all the proposed methods. We indicate the weaknesses and strengths of each of them and recommend the ones to be used in practice.},
keywords={Data integration;Standards;Task analysis;Databases;Google;Data mining;Terminology;Record normalization;data quality;data fusion;web data integration;deep web},
doi={10.1109/TKDE.2018.2844176},
ISSN={1558-2191},
month={April},}
@ARTICLE{9299499,
author={Liu, Jie and Cao, Yijia and Li, Yong and Guo, Yixiu and Deng, Wei},
journal={CSEE Journal of Power and Energy Systems}, title={A big data cleaning method based on improved CLOF and Random Forest for distribution network},
year={2020},
volume={},
number={},
pages={1-10},
abstract={In order to improve the data quality, the big data cleaning method of distribution network was studied in this paper. First, the Local Outlier Factor (LOF) algorithm based on DBSCAN clustering was used to detect outliers. However, due to the difficulty in determining the LOF threshold, a method of dynamically calculating the threshold based on the transformer districts and time was proposed. Besides, the LOF algorithm combines the statistical distribution method to reduce the "misjudgment rate". Aiming at the diversity and complexity of data missing forms in power big data, this paper improved the Random Forest imputation algorithm, which can be applied to various forms of missing data, especially the blocked missing data and even some horizontal or vertical data completely missing. The data in this paper were from real data of 44 transformer districts of a certain 10kV line in distribution network. Experimental results showed that outlier detection was accurate and suitable for any shape and multidimensional power big data. The improved Random Forest imputation algorithm was suitable for all missing forms, with higher imputation accuracy and better model stability. By comparing the network loss prediction between the data using this data cleaning method and the data removing outliers and missing values, it was found that the accuracy of network loss prediction had been improved by nearly 4 percentage points using the data cleaning method mentioned in this paper. Additionally, as the proportion of bad data increased, the difference between the prediction accuracy of cleaned data and that of uncleaned data was greater.},
keywords={Cleaning;Distribution networks;Big Data;Prediction algorithms;Clustering algorithms;Data models;Anomaly detection;Data cleaning;Outliers detection;missing data imputation;LOF;DBSCAN;Random Forest},
doi={10.17775/CSEEJPES.2020.04080},
ISSN={2096-0042},
month={},}
@INPROCEEDINGS{8622185,
author={Wijerathna, Nadeesha and Matsubara, Masaki and Morishima, Atsuyuki},
booktitle={2018 IEEE International Conference on Big Data (Big Data)}, title={Finding Evidences by Crowdsourcing},
year={2018},
volume={},
number={},
pages={3560-3563},
abstract={Crowdsourcing is a promising tool involving multiple people in completing tasks that are difficult to complete by an individual, a small team or a computer. Ensuring the quality of the results is also one of the primary problems in crowdsourcing. One of the major approaches to improve the data quality to aggregate answers from more than one workers. This study explores a different approach - we ask workers to prove facts. We devise a general framework for collecting and ranking evidence-based proofs. The experiments results show that the proposed framework works and how diverse the collected proofs are. Our results clearly indicate that the crowd-based approach to prove facts is promising.},
keywords={Task analysis;Crowdsourcing;Libraries;Media;Uniform resource locators;Web pages;Big Data;Evidence;Assumption;Crowdsourcing},
doi={10.1109/BigData.2018.8622185},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7518511,
author={Ahmad, Awais and Paul, Anand and Rathore, M. Mazhar and Rho, Seungmin},
booktitle={2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)}, title={Big Data Analytical Architecture Using Divide-and-Conquer Approach in Machine-to-Machine Communication},
year={2015},
volume={},
number={},
pages={1819-1824},
abstract={Machine-to-Machine (M2M) technology unremittingly motivates any time-place-objects connectivity of the devices in and around the world. Every day, a rapid growth of large M2M networks and digital storage technology, lead to a massive heterogeneous data depository, in which the M2M data are captured and warehoused in the diverse database frameworks as a magnitude of heterogeneous data sources. Hence, the M2M that handles Big Data might perform poorly or not according to the goals of their operator due to massive heterogeneous data sources may face various incompatibilities, such as data quality, processing and computational efficiency, analysis and feature extraction applications. Therefore, to address the aforementioned constraints, this paper presents a Big Data Analytical architecture based on Divide-and-Conquer approach. The designed system architecture exploits divide-and-conquer approach, where big data sets are first transformed into a several data blocks that can be quickly processed, then it classifies and reorganizes these data blocks from the same source. In addition, the data blocks are aggregated in a sequential manner based on a machine ID, and equally partitions the data using filtration and load balancing algorithms. The feasibility and efficiency of the proposed system architecture are implemented on Hadoop single node setup. The results show that the proposed system architecture efficiently extract various features (such as River) from the massive volume of satellite data.},
keywords={Servers;Satellites;Big data;Computer architecture;Algorithm design and analysis;Decision making;Feature extraction;Big Data;divide-and-conquer;machine ID;efficiency},
doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.330},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9345133,
author={Yuan, Fang and Hong, Xianbin and Yuan, Cheng and Fei, Xiang and Guan, Sheng-Uei and Liu, Dawei and Wang, Wei},
booktitle={2020 IEEE 6th International Conference on Computer and Communications (ICCC)}, title={Keywords-oriented Data Augmentation for Chinese},
year={2020},
volume={},
number={},
pages={2006-2012},
abstract={In natural language processing tasks, data is very important, but data collection is not cheap. Large volume data can well serve a series of tasks, especially for deep learning tasks. Data augmentation methods are solutions to data problems, which can work well on rising data quality and quantity, such as generating text without meaning changing and expanding the diversity of data distribution. A user-friendly method of the data augmentation is to sample words in a text then augmenting them. The sampling method is often implemented by a random probability. Although the performance of this solution has been proved over the past few years, random sampling is not the best choice for the data augmentation as it has a chance of randomly introducing some noise into initial data, like stop words. The generated data could interfere with the subsequent tasks and drop the accuracy of the tasks' solutions. Hence, this paper aims to introduce a novel data augmentation method that could avoid involving such noisy data. The strategy is keywords-oriented data augmentation for Chinese (KDA). The KDA proposed in this paper indicates a method of extracting keywords based on category labels, and an augmenting method based on the keywords. In contrast to randomness, the proposed technique firstly selects the key information data, then expands the selected data. The experimental section is compared with another two typical data augmentation techniques on three Chinese data sets for text classification tasks. The result shows that the KDA technique has a better performance in the data augmentation task than the compared two.},
keywords={Deep learning;Data integrity;Text categorization;Sampling methods;Natural language processing;Noise measurement;Task analysis;Data Augmentation;Chinese;Classification},
doi={10.1109/ICCC51575.2020.9345133},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7872993,
author={Pu, Dong-Mei and Gao, Da-Qi and Yuan, Yu-Bo},
booktitle={2016 International Conference on Machine Learning and Cybernetics (ICMLC)}, title={A dynamic data correction algorithm based on polynomial smooth support vector machine},
year={2016},
volume={2},
number={},
pages={820-824},
abstract={Data quality plays an important role in modern intelligent information system and is crucial to any data analysis task. Many imperfection-handling techniques avoid overfitting or simply remove offending portions of the data. Data correction can help to retain and recover as much information as possible from the original data resources. In this paper, we proposed a novel technique based on polynomial smooth support vector machine. The quadratic polynomial and the first degree of polynomial as the support vector machine smooth functions are investigated. At the same time, the function was used as smooth function to calculate compensation values. In order to show the procedures of our algorithm, some necessary steps need to be considered. Firstly, the original data are normalized, so as to eliminate experimental effects of dimensional problems. Secondly, the three different kinds of smooth functions need to be analysed mathematically. The difference measure are calculated to make sure the results of correction through different data correction models. The results of given noised data sets can show that the proposed the data correction method based on polynomial smooth support vector machine is effectiveness.},
keywords={Support vector machines;Heuristic algorithms;Data analysis;Aerodynamics;Cybernetics;Big data;Machine learning algorithms;Data analysis;Data correction;Support vector machine;Data Mining},
doi={10.1109/ICMLC.2016.7872993},
ISSN={2160-1348},
month={July},}
@INPROCEEDINGS{8622786,
author={Zheng, Ningxin and Chen, Quan and Chen, Chen and Guo, Minyi},
booktitle={2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)}, title={CLIBE: Precise Cluster-Level I/O Bandwidth Enforcement in Distributed File System},
year={2018},
volume={},
number={},
pages={124-131},
abstract={A distributed file system (DFS) is a core component to implement big data applications. On the one hand, a DFS is capable of managing a large volume of data with desirable properties that strike the balance between high availability, reliability, and so on. On the other hand, a DFS relies on underlying storage systems (e.g., hard drives, solid state drives, etc.) and suffer from slow read/write operations. In big data era, large-scale data processing applications start to leverage the in-memory processing to improve the performance by reducing the inhibitive cost of I/O operations. However, it is still inevitable to read input data from or write outputs to the storage system. Slow I/O operations are often the main bottleneck of emerging big data applications. In particular, while these applications often use DFSs to store their results for the high availability and reliability, the unmanaged I/O bandwidth contention results in the QoS violation of high priority applications when multiple applications share the same DFS. To enable I/O management and allocation on big-data platforms, we propose a Cluster-Level I/O Bandwidth Enforcement (CLIBE) approach that consists of a cluster-level I/O bandwidth quota manager, multiple node-level I/O bandwidth controllers, and a feedback-based quota reallocator. The quota manager splits and distributes the I/O bandwidth quota of an application to the active nodes that are serving this application. The bandwidth controller on a node ensures that the I/O bandwidth used by an application would not exceed its bandwidth quota on the node. For an application affected by slow or overloaded nodes, the quota reallocator reallocates the idle I/O bandwidth on underloaded nodes to this application to guarantee its throughput. Our experiment on a real-system cluster shows that CLIBE is able to precisely control the I/O bandwidth used by an application at the cluster level, with the deviation smaller than 2.51%.},
keywords={Bandwidth;Quality of service;Big Data applications;Throughput;Writing;Reliability;Distributed file system;I/O bandwidth enforcement;HDFS},
doi={10.1109/HPCC/SmartCity/DSS.2018.00048},
ISSN={},
month={June},}
@INPROCEEDINGS{8625275,
author={Canbek, Gürol and Sagiroglu, Seref and Taskaya Temizel, Tugba},
booktitle={2018 International Congress on Big Data, Deep Learning and Fighting Cyber Terrorism (IBIGDELFT)}, title={New Techniques in Profiling Big Datasets for Machine Learning with a Concise Review of Android Mobile Malware Datasets},
year={2018},
volume={},
number={},
pages={117-121},
abstract={As the volume, variety, velocity aspects of big data are increasing, the other aspects such as veracity, value, variability, and venue could not be interpreted easily by data owners or researchers. The aspects are also unclear if the data is to be used in machine learning studies such as classification or clustering. This study proposes four techniques with fourteen criteria to systematically profile the datasets collected from different resources to distinguish from one another and see their strong and weak aspects. The proposed approach is demonstrated in five Android mobile malware datasets in the literature and in security industry namely Android Malware Genome Project, Drebin, Android Malware Dataset, Android Botnet, and Virus Total 2018. The results have shown that the proposed profiling methods reveal remarkable insight about the datasets comparatively and directs researchers to achieve big but more visible, qualitative, and internalized datasets.},
keywords={Malware;Big Data;Machine learning;Mobile applications;Genomics;Bioinformatics;Aerospace electronics;data profiling;data quality;big data;malware detection;mobile malware;machine learning;classification;Android;feature engineering},
doi={10.1109/IBIGDELFT.2018.8625275},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7379507,
author={Krawczyk, Bartosz and Wozniak, Michal},
booktitle={2015 IEEE International Conference on Systems, Man, and Cybernetics}, title={Weighted Naïve Bayes Classifier with Forgetting for Drifting Data Streams},
year={2015},
volume={},
number={},
pages={2147-2152},
abstract={Mining massive data streams in real-time is one of the contemporary challenges for machine learning systems. Such a domain encompass many of difficulties hidden beneath the term of Big Data. We deal with massive, incoming information that must be processed on-the-fly, with lowest possible response delay. We are forced to take into account time, memory and quality constraints. Our models must be able to quickly process large collection of data and swiftly adapt themselves to occurring changes (shifts and drifts) in data streams. In this paper, we propose a novel version of simple, yet effective Naïve Bayes classifier for mining streams. We add a weighting module, that automatically assigns an importance factor to each object extracted from the stream. The higher the weight, the bigger influence given object exerts on the classifier training procedure. We assume, that our model works in the non-stationary environment with the presence of concept drift phenomenon. To allow our classifier to quickly adapt its properties to evolving data, we imbue it with forgetting principle implemented as weight decay. With each passing iteration, the level of importance of previous objects is decreased until they are discarded from the data collection. We propose an efficient sigmoidal function for modeling the forgetting rate. Experimental analysis, carried out on a number of large data streams with concept drift prove that our weighted Naïve Bayes classifier displays highly satisfactory performance in comparison with state-of-the-art stream classifiers.},
keywords={Training;Adaptation models;Data mining;Memory management;Detectors;Data models;Probability;machine learning;data stream;concept drift;big data;incremental learning;forgetting},
doi={10.1109/SMC.2015.375},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9574505,
author={Zeng, Hui and Zhao, Xiaoyong and Wang, Lei},
booktitle={2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI)}, title={Multivariate Time Series Anomaly Detection On Improved HTM Model},
year={2021},
volume={},
number={},
pages={759-763},
abstract={In recent years, industrial big data has attracted much attention as the key technical support of “Intelligent Manufacturing” and “Industrial Internet”. And as the dependence of intelligent manufacturing on digitalization continues to increase, data quality problems caused by device and system failures, harsh environment, improper scheduling and management, duplication or missing of data fields, etc., have more significant impacts on industrial processes. Therefore, the anomaly detection of industrial big data is particularly important. Among the methods onto time series data for anomaly detection, HTM(Hierarchical Temporal Memory) algorithm performs well in the unsupervised univariate time series data anomaly detection, but the capability of original HTM model for detecting multivariate time series anomaly data is insufficient. However, the multivariate data anomaly detection is common in industry and the performance requirements for data anomaly detection are relatively high. Thus, this paper proposes an improved HTM algorithm model - MSP-HTM(Multiple Spatial Poolers HTM) model. The MSP-HTM model respectively encode the value of different dimensions at the same time, and then put the result from encoder into spatial pooler respectively, finally the temporal memory layer merge result from spatial poolers, and predict future data. Experiments show that the MSP-HTM model can improve performance by processing the multivariate time series data in parallel and improve the effect of data anomaly detection.},
keywords={Performance evaluation;Job shop scheduling;Computational modeling;Time series analysis;Big Data;Predictive models;Prediction algorithms;Multivariate time series;Anomaly detection;Hierarchical temporal memory;Industrial bigdata},
doi={10.1109/CEI52496.2021.9574505},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9421436,
author={Wang, Xiaofeng and Jiang, Yong and Zhan, Gaofeng and Zhao, Tong},
booktitle={2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)}, title={Quality Analysis and Evaluation Method for Multisource Aggregation Data based on Structural Equation Model},
year={2020},
volume={},
number={},
pages={1279-1282},
abstract={In the era of big data, how to evaluate the data quality of multi-source aggregation data is very important. The reason is that uneven data quality will directly lead to inaccurate or ambiguous data in the database, and indirectly lead to the deviation of subsequent data mining and decision-making. In this paper, structural equation model(SEM) is introduced to explore the effectiveness of various data quality evaluation indicators in data aggregation and finding out internal relationship between them. A new quality evaluation method of multi-source aggregation data is proposed, based on the regression's significance analysis and factor loads of each observation index in the SEM model. The case analysis shows that the proposed method is feasible and can be used to evaluate the quality of multi-source aggregation data adaptively for a long time.},
keywords={Analytical models;Adaptation models;Numerical analysis;Data integrity;Computational modeling;Urban areas;Data aggregation;Data Aggregation;Data Analysis;Quality Evaluation;Structural Equation Model},
doi={10.1109/ICMCCE51767.2020.00280},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7818437,
author={Jain, Shashwat and Khandelwal, Manish and Katkar, Ashutosh and Nygate, Joseph},
booktitle={2016 12th International Conference on Network and Service Management (CNSM)}, title={Applying big data technologies to manage QoS in an SDN},
year={2016},
volume={},
number={},
pages={302-306},
abstract={Managing QoS in a telecommunications network is a complex process. Effective network design and sizing in conjunction with load balancing, access control and traffic prioritization need to be orchestrated to optimize CAPEX investment, maximize network utilization and ensure that performance metrics and SLAs are met. This work shows how big data analytics were used to improve the management of QoS in an SDN by performing multi-dimensional analysis of Key Performance Indicators (KPIs) and applying machine learning algorithms to discover new correlations, perform root cause analysis and predict traffic congestion.},
keywords={Delays;Correlation;Quality of service;Jitter;Big data;Ports (Computers)},
doi={10.1109/CNSM.2016.7818437},
ISSN={2165-963X},
month={Oct},}
@INPROCEEDINGS{9724934,
author={Feng, Xinyi},
booktitle={2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM)}, title={Power Data Quality Optimization and Evaluation Based on BPNN},
year={2021},
volume={},
number={},
pages={505-509},
abstract={With the continuous improvement of the information technology and communications of Smart Grid, the electric power big data environment has been formed. The data shows diversity and multi-source characteristics. How to ensure the quality of power data in the computer organization under the condition of heterogeneity is the premise of making relevant decisions. This paper firstly gives the definition of Data Space of power enterprises, analyzes the factors affecting the quality of data in the computer environment, and gives the relevant architecture of processing power data in the data space. Secondly, based on business flow and Petri net in the computer environment, this paper constructs the data flow and quality control model of the front and back platforms. The former represents the data flow in the power business and abstracts it to form Petri net computer information flow, so that the data can achieve the effect of cleaning while flowing in the business process. Finally, an evaluation index system is built and back-propagation neural network (BPNN) is used to determine the weight, a case study is given to verify the effectiveness of the proposed method.},
keywords={Data integrity;Petri nets;Computer architecture;Aerospace electronics;Data processing;Data models;Cleaning;data space;data quality;business flow;petri net;BPNN},
doi={10.1109/AIAM54119.2021.00106},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9006289,
author={Rocha, Lais M. A. and Bessa, Aline and Chirigati, Fernando and OFriel, Eugene and Moro, Mirella M. and Freire, Juliana},
booktitle={2019 IEEE International Conference on Big Data (Big Data)}, title={Understanding Spatio-Temporal Urban Processes},
year={2019},
volume={},
number={},
pages={563-572},
abstract={Increasingly, decisions are based on insights and conclusions derived from the results of data analysis. Thus, determining the validity of these results is of paramount importance. In this paper, we take a step towards helping users identify potential issues in spatio-temporal data and thus gain trust in the results they derived from these data. We focus on processes that are captured by relationships among datasets that serve as the data exhaust for different components of urban environments. In this scenario, debugging data involves two important challenges: the inherent complexity of spatio-temporal data, and the number of possible relationships. We propose a framework for profiling spatio-temporal relationships that automatically identifies data slices that present a significant deviation from what is expected, and thus, helps focus a user's attention on slices of the data that may have quality issues and/or that may affect the conclusions derived from the analysis' results. We describe the profiling methodology and how it derives relationships, identifies candidate deviations, assesses their statistical significance, and measures their magnitude. We also present a series of cases studies using real datasets from New York City which demonstrate the usefulness of spatio-temporal profiling to build trust on data analysis' results.},
keywords={Urban areas;Correlation;Spatial resolution;Data analysis;Mathematical model;Public transportation;Standards;data quality;data profiling;urban data},
doi={10.1109/BigData47090.2019.9006289},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9276830,
author={Zhang, Huaxin and Liu, Yu and Wang, Zituo and Li, Tiansong and Cao, Keyin},
booktitle={2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)}, title={Research on Film Data Preprocessing and Visualization},
year={2020},
volume={1},
number={},
pages={946-952},
abstract={Data is the core of information, and good data quality is a prerequisite for many data analysis. Data cleaning is to increase the fault tolerance rate by correcting the error value of detected data. This paper aims to solve the problem of data set processing and visualization in the recommendation algorithm, so as to better apply in the field of recommendation algorithm. The recommendation algorithm and data sets Movielens and IMDB are analyzed theoretically. First, data set A was processed from data reading and movie score calculation; Again, the IMDB is processed in four steps to make it more suitable for the recommendation algorithm field; Finally, the plot function is used to visualize the key information. experiment shows: The data set sorted out by the above methods can effectively improve the quality and availability of data and provide relevant basis for better application in the algorithm.},
keywords={Motion pictures;Data visualization;Visualization;Prediction algorithms;Market research;Arrays;Electronic commerce;recommended algorithm;Dataset;data processing;data visualization},
doi={10.1109/ICIBA50161.2020.9276830},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8982563,
author={Yoo, Yeisol and Yoo, Jin Soung},
booktitle={2019 IEEE International Conferences on Ubiquitous Computing Communications (IUCC) and Data Science and Computational Intelligence (DSCI) and Smart Computing, Networking and Services (SmartCNS)}, title={RFID Data Warehousing and OLAP with Hive},
year={2019},
volume={},
number={},
pages={476-483},
abstract={Radio Frequency Identification (RFID) technology is used in many applications for monitoring object movement. The use of RFID in supply chain management systems enables to track the movement of products from suppliers to warehouses, store backrooms, and eventually points of sale. The vast amount of data resulting from the proliferation of RFID readers and tags poses challenges for data management and analytics. RFID data warehousing can enhance data quality and consistency, and give great potential benefits for Online Analytical Processing (OLAP) applications. Traditional data warehouses are built primarily on relational database management systems. However, the size of RFID data being collected and analyzed in the industry for business intelligence is growing rapidly, making traditional warehousing solutions prohibitively expensive. Hive is an open-source data warehousing solution built on top of Hadoop which is a popular Big Data computing framework. This paper presents alternative RFID data warehouse designs which can handle a large amount of RFID data and support a variety of OLAP queries. The proposed approaches are implemented on Hive and evaluated for query performance in cloud computing environment.},
keywords={RFID data warehousing;OLAP;Hive;Cloud computing},
doi={10.1109/IUCC/DSCI/SmartCNS.2019.00105},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8463091,
author={Bantug, Derek and Franklin, Paul and Boone, Ted},
booktitle={2018 Annual Reliability and Maintainability Symposium (RAMS)}, title={Product Reliability and Databases: Lessons Learned},
year={2018},
volume={},
number={},
pages={1-5},
abstract={This experience paper describes some lessons learned using “big data.” Managers want to make data driven decisions, and many companies spend substantial effort and resources to develop collection methods, record facts, and store records in large data warehouses. Additional resources take this collection of data and produce reports, which are then used to support decision making. We work with customer premises equipment, and our databases track nearly 50 million serial numbers. As reliability engineers, we use this data as the basis of analysis to assess field performance of the equipment the databases track. In 2016, we began to use a standard tool to serve as a definitive repository and an engine to do preliminary postprocessing. This database uses data dimensions, where each dimension is an array. It is convenient to think of the lefthand column as a set of labels and the cells to the right as measures (either collected data or computed metrics) for each label. The advantage of creating dimensions is that-rather than working with individual data items and the relationships between them-a dimension preprocesses data into a set that has relationships with other sets. This models the “real world” more closely. A cube is just a set of one or more dimensions. Using a cube allows complex questions to be asked and answered in ways that relational databases do not. We have been using this data structure to support analysis of customer premises equipment, typically set top boxes, modems, and similar equipment that is leased by the provider to customers at their residences and businesses. Tools that support cubes offer several advantages. It is possible to do analyses in a cube that are difficult in a relational database that does not support the “logical ecology” that a cube does By moving up and down the data hierarchy, it is possible to see relationships on the screen, and outputs can be saved to other more powerful post processing tools for more detailed analysis Cubes support faster and less error prone analysis This paper describes these points and illustrate them with a simple example. Our objective is to illustrate the concepts rather than work through a detailed problem. Our work to date suggests that it is critical to manage data quality in a broad sense so that the resulting reports and analysis are trustworthy. We have had a generally positive experience with this technology and found that it benefits the business by allowing processes to be modeled. This refines our understanding of the meaning of the various process metrics, and in some cases, we have been able to recommend changes to policy.},
keywords={Databases;Tools;Measurement;Maintenance engineering;Reliability;Engines;Arrays;Databases;data cube;big data;lessons learned},
doi={10.1109/RAM.2018.8463091},
ISSN={2577-0993},
month={Jan},}
@INPROCEEDINGS{9574197,
author={Xue, Lian},
booktitle={2021 IEEE International Conference on Advances in Electrical Engineering and Computer Applications (AEECA)}, title={Competency Evaluation System of English Teaching Post based on K-Means Clustering Algorithm},
year={2021},
volume={},
number={},
pages={887-891},
abstract={With the rapid development of big data, user data information is increasingly perfect, data warehouse integration is more reasonable, and data quality is constantly improving, so the value of data is increasing. Based on parallel processing of K-means clustering algorithm, this paper extracts ability constraint information, integrates K-means clustering algorithm, clusters and integrates various index parameters of post competency. From the final experimental results, this method improves the execution efficiency and accuracy compared with other methods, and can be used in practice.},
keywords={Electrical engineering;Data integrity;Education;Clustering algorithms;Big Data;Parallel processing;Data warehouses;English Teaching;Post Competency;Clustering Algorithm;System Assessment},
doi={10.1109/AEECA52519.2021.9574197},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9060352,
author={Xiao, Yunlong and Gu, Yang and Wang, Jiwei and Wu, Tong},
booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence Computing, Advanced Trusted Computing, Scalable Computing Communications, Cloud Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)}, title={A Collaborative Multi-modality Selection Method Based on Data Utility Assessment},
year={2019},
volume={},
number={},
pages={454-459},
abstract={Multimodal fusion is more and more widely used in the field of machine learning, but it faces a prominent problem in practical application: data utility is not stable. The data of different modalities may be missing and noisy randomly, which will interfere the machine learning model of multi-modal fusion. Most of the existing multi-modal fusion methods neglect data utility problems or only adopt simple data denoising methods to improve data utility. To solve the problem of unstable data utility, we propose a data selection method based on the evaluation of data utility. By training a special machine learning model, the optimal modal combination is predicted according to the quality evaluation of multi-modal data samples to accomplish the dynamic selection of data modalities. The experimental results show that the proposed method can effectively improve the accuracy of multi-modal recognition under low data utility.},
keywords={Data integrity;Machine learning;Data models;Training;Task analysis;Mathematical model;Gesture recognition;data selection;multimodal;data utility;data quality assessment},
doi={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00120},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9534570,
author={Jie, Lu and Zheng, Su and Qi, Wang and Xiya, Chen},
booktitle={2021 2nd International Conference on Artificial Intelligence and Education (ICAIE)}, title={Analysis of Employment Status and Countermeasures of Biology Graduates in Local Normal Universities Based on Big Data Technology—Take the Graduates of Guangxi Normal University From 20l6 to 2020 as an Example},
year={2021},
volume={},
number={},
pages={572-578},
abstract={The data of annual reports on the employment quality f local normal universities from 2016 to 2020 were captured from the network with the help of information technology. The data related to basic situation and employment destination of biology graduates in Guangxi Normal University were analyzed and collected by data analytic technology such as Data Mining Algorithms, Data Quality Master Data Management and Predictive Analytic Capabilities. In view of the problems of single employment structure, insufficient employment skills, vague employment planning and low achievement of Innovation and Entrepreneurship, measures such as the employment policy interpretation, the talent training programs adjustment, the employment guidance services system improvement and teaching mode innovation are needed in the purpose of promoting employment, providing useful reference to the further progress of teaching reform and optimizing talent training modes and methods for graduates of biology.},
keywords={Training;Technological innovation;Employment;Big Data;Prediction algorithms;Biology;Planning;data analysis;majors concerning biology;graduates;employment quality;local normal universities},
doi={10.1109/ICAIE53562.2021.00127},
ISSN={},
month={June},}
@INPROCEEDINGS{9188195,
author={Demchenko, Yuri and Cushing, Reggie and Los, Wouter and Grosso, Paola and de Laat, Cees and Gommans, Leon},
booktitle={2019 International Conference on High Performance Computing Simulation (HPCS)}, title={Open Data Market Architecture and Functional Components},
year={2019},
volume={},
number={},
pages={1017-1021},
abstract={This paper discusses the principles of organisation and infrastructure components of Open Data Markets (ODM) that would facilitate secure and trusted data exchange between data market participants, and other cooperating organisations. The paper provides a definition of the data properties as economic goods and identifies the generic characteristics of ODM as a Service. This is followed by a detailed description of the generic data market infrastructure that can be provisioned on demand for a group of cooperating parties. The proposed data market infrastructure and its operation are employing blockchain technologies for securing data provenance and providing a basis for data monetisation. Suggestions for trust management and data quality assurance are discussed.},
keywords={Economics;Cloud computing;Data models;Contracts;Big Data;Computer architecture;Open Data Market;Data Marketplace;Trusted Data Market;Industrial Data Space;Data Economics;STREAM Data Properties},
doi={10.1109/HPCS48598.2019.9188195},
ISSN={},
month={July},}
@INPROCEEDINGS{9006489,
author={Park, Hyunseop and Ko, Hyunwoong and Lee, Yung-Tsun T. and Cho, Hyunbo and Witherell, Paul},
booktitle={2019 IEEE International Conference on Big Data (Big Data)}, title={A Framework for Identifying and Prioritizing Data Analytics opportunities in Additive Manufacturing},
year={2019},
volume={},
number={},
pages={3458-3467},
abstract={Many industries, including manufacturing, are adopting data analytics (DA) in making decisions to improve quality, cost, and on-time delivery. In recent years, more research and development efforts have applied DA to additive manufacturing (AM) decision-making problems such as part design and process planning. Though there are many AM decision-making problems, not all benefit greatly from DA. This may be due to insufficient AM data, unreliable data quality, or the fact that DA is not cost effective when it is applied to some AM problems. This paper proposes a framework to investigate DA opportunities in a manufacturing operation, specifically AM. The proposed framework identifies and prioritizes AM potential opportunities where DA can make impact. The proposed framework is presented in a five-tier architecture, including value, decision-making, data analytics, data, and data source tiers. A case study is developed to illustrate how the proposed framework identifies DA opportunities in AM.},
keywords={Decision making;Solid modeling;Mechanical variables measurement;Electric variables measurement;Shape measurement;Data analysis;Analytical models;Data analytics;opportunity identification and prioritization;architecture;additive manufacturing},
doi={10.1109/BigData47090.2019.9006489},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8248534,
author={Zhang, Heng and Liu, Guohua and Zhao, Wenfeng and Ni, Mengfei},
booktitle={2017 4th International Conference on Systems and Informatics (ICSAI)}, title={Incomplete relation revision method based on template},
year={2017},
volume={},
number={},
pages={1563-1567},
abstract={Data quality is the core problem in the field of big data application. In practical applications, data are often derived from multi-source databases, which can cause named conflict and similar duplicate record problems. This paper proposed a method to solve the named conflict problem and similar duplicate record based on a given template. In order to find the relationship between the template data and the data to be unified, we segmented the data that to be unified, then built an extensional B tree based on these data. At the same time, we construct a NFA for each template value. By using these NFA, we can get the language pattern for each column in the template relation. Finally, we search each template value in the extensional B tree, if the template value can be found and the corresponding data to be unified can be accepted by the NFA based on the template value, we can use the template value to replace the data that need to be recovered. Then, the data can be consolidated and integrated to ensure the consistency and integrity of the data.},
keywords={Algorithm design and analysis;Cleaning;Maintenance engineering;Feature extraction;Sorting;Classification algorithms;data quality;template;NFA;extensional B tree},
doi={10.1109/ICSAI.2017.8248534},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9302543,
author={Zada, Muhammad Sadiq Hassan and Yuan, Bo and Anjum, Ashiq and Azad, Muhammad Ajmal and Khan, Wajahat Ali and Reiff-Marganiec, Stephan},
booktitle={2020 IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT)}, title={Large-scale Data Integration Using Graph Probabilistic Dependencies (GPDs)},
year={2020},
volume={},
number={},
pages={27-36},
abstract={The diversity and proliferation of Knowledge bases have made data integration one of the key challenges in the data science domain. The imperfect representations of entities, particularly in graphs, add additional challenges in data integration. Graph dependencies (GDs) were investigated in existing studies for the integration and maintenance of data quality on graphs. However, the majority of graphs contain plenty of duplicates with high diversity. Consequently, the existence of dependencies over these graphs becomes highly uncertain. In this paper, we proposed graph probabilistic dependencies (GPDs) to address the issue of uncertainty over these large-scale graphs with a novel class of dependencies for graphs. GPDs can provide a probabilistic explanation for dealing with uncertainty while discovering dependencies over graphs. Furthermore, a case study is provided to verify the correctness of the data integration process based on GPDs. Preliminary results demonstrated the effectiveness of GPDs in terms of reducing redundancies and inconsistencies over the benchmark datasets.},
keywords={Probabilistic logic;Uncertainty;Data integration;Data integrity;Redundancy;Scalability;Erbium;data integration;information retrieval;NoSQL databases;graph probabilistic dependencies;data science},
doi={10.1109/BDCAT50828.2020.00028},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7167407,
author={Casale, Giuliano and Ardagna, Danilo and Artac, Matej and Barbier, Franck and Di Nitto, Elisabetta and Henry, Alexis and Iuhasz, Gabriel and Joubert, Christophe and Merseguer, Jose and Munteanu, Victor Ion and Perez, Juan Fernando and Petcu, Dana and Rossi, Matteo and Sheridan, Craig and Spais, Ilias and Vladuic, Daniel},
booktitle={2015 IEEE/ACM 7th International Workshop on Modeling in Software Engineering}, title={DICE: Quality-Driven Development of Data-Intensive Cloud Applications},
year={2015},
volume={},
number={},
pages={78-83},
abstract={Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.},
keywords={Unified modeling language;Big data;Data models;Computational modeling;Analytical models;Reliability;Software;Big Data;quality assurance;model-driven engineering},
doi={10.1109/MiSE.2015.21},
ISSN={2156-7891},
month={May},}
@INPROCEEDINGS{9655853,
author={Lawson, Victor J. and Banerjee, Madhushri},
booktitle={2021 IEEE 5th International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE)}, title={Measuring the Impact of an IoT Temperature Sensor Framework for Tracking Contagious Diseases},
year={2021},
volume={},
number={},
pages={332-337},
abstract={Due to the COVID-19 pandemic, much computer science research has been dedicated to utilizing sensor readings for medical purposes. Throughout this period, the need for virus symptom tracking has become a promising area for remotely deployed sensor networks and platforms. Our research goal is to prove that the temperature readings from these sensor network platforms can be statistically linked to public record, medical case study data. The expected outcome of our project is to prove the correlation between sensor network tracking of remote human temperature data and medical records for COVID cases. The results of this study will prove that tracking human temperature can assist in tracking disease outbreaks in various populations. Our framework platform is comprised of four main modules: (1) Temperature Collection, (2) Internal Data Validation (3) Internal-External data merger, (4) Data Analytics. The temperature data are collected from internal databases, mobile sensing devices and medical health professionals. After collection, the internal data are validated by our software, TAU-FIVE, a multi-tier data quality validation system, then merged with external data sources into a data analytic based data warehouse. The data mart queries are designed to compare the location and date of temperature sensor data with known data sets from government officials. Once blended into a fully operational data warehouse, these data marts produce high quality data analysis linking remotely sensed human temperature readings to sources of disease outbreaks.},
keywords={Temperature sensors;COVID-19;Temperature measurement;Temperature distribution;Data analysis;Correlation;Databases;Data Quality;Data Analytics;Information Quality;Big Data;Data Integration},
doi={10.1109/ICITISEE53823.2021.9655853},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9095877,
author={Li, Song and Ning, Sun and Yezhou, Yao and Jingjing, Tian and Wenxue, Zhang and Liang, Chi},
booktitle={2019 2nd International Conference on Safety Produce Informatization (IICSPI)}, title={Application of Data Mining Technology in the Recall of Defective Automobile Products in China ——A Typical Case of the Construction of Digital China},
year={2019},
volume={},
number={},
pages={541-545},
abstract={According to multisource quality safety data of defective automobile products, key quality safety factors of defective automobile products are extracted, a defect information indicator system for automobile products is systematically constructed and a correlated graph is established between quality safety factors. Based on the optimization and correlation of the quality safety factor indicator system, Big Data technology is used to design a data structure for multisource quality safety information cluster, develop a data platform for the defect information analysis of automobile products and achieve information clustering and correlation analysis based on multisource quality safety data, providing technical support for the recall management of defective automobile products.},
keywords={Automobiles;Safety;Big Data;Government;Personnel;Data mining;automobile recall;data mining;information cluster},
doi={10.1109/IICSPI48186.2019.9095877},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6969470,
author={Buyya, Rajkumar and Murray, Derek},
booktitle={2014 IEEE International Parallel Distributed Processing Symposium Workshops}, title={HPGC Keynotes},
year={2014},
volume={},
number={},
pages={850-851},
abstract={These keynote speeches discuss the following: Market-oriented cloud computing and Big Data applications; and Low-latency distributed analytics in Naiad.},
keywords={Cloud computing;Computational modeling;Big data;Quality of service;Distributed processing;Conferences;Educational institutions},
doi={10.1109/IPDPSW.2014.234},
ISSN={},
month={May},}
@INPROCEEDINGS{7471171,
author={Amato, Flora and Moscato, Francesco},
booktitle={2016 30th International Conference on Advanced Information Networking and Applications Workshops (WAINA)}, title={Automatic Cloud Services Composition for Big Data Management},
year={2016},
volume={},
number={},
pages={46-51},
abstract={Every-Day lives are becoming increasingly instrumented by electronic devices and any kind of computer-based (distributed) service. As a result, organizations need to analyse an enormous amounts of data in order to increase their incomings or to improve their services. Anyway, setting-up a private infrastructure to execute analytics over Big Data is still expensive. The exploitation of Cloud infrastructure in Big Data management is appealing because of costs reductions and potentiality of storage, network and computing resources. The Cloud can consistently reduce the cost of analysis of data from different sources, opening analytics to big storages in a multi-cloud environment. Anyway, creating and executing this kind of service is very complex since different resources have to be provisioned and coordinated depending on users' needs. Orchestration is a solution to this problem, but it requires proper languages and methodologies for automatic composition and execution. In this work we propose a methodology for composition of services used for analyses of different Big Data sources: in particular an Orchestration language is reported able to describe composite services and resources in a multi-cloud environment.},
keywords={Cloud computing;Computer architecture;Semantics;Big data;Quality of service;Meteorology;NIST;Cloud Computing;Orchestration;Formal Semantics;Availability},
doi={10.1109/WAINA.2016.169},
ISSN={},
month={March},}
@ARTICLE{7588229,
author={Kong, Linghe and Zhang, Daqiang and He, Zongjian and Xiang, Qiao and Wan, Jiafu and Tao, Meixia},
journal={IEEE Communications Magazine}, title={Embracing big data with compressive sensing: a green approach in industrial wireless networks},
year={2016},
volume={54},
number={10},
pages={53-59},
abstract={New-generation industries heavily rely on big data to improve their efficiency. Such big data are commonly collected by smart nodes and transmitted to the cloud via wireless. Due to the limited size of smart node, the shortage of energy is always a critical issue, and the wireless data transmission is extremely a big power consumer. Aiming to reduce the energy consumption in wireless, this article introduces a potential breach from data redundancy. If redundant data are no longer collected, a large amount of wireless transmissions can be cancelled and their energy saved. Motivated by this breach, this article proposes a compressive-sensing-based collection framework to minimize the amount of collection while guaranteeing data quality. This framework is verified by experiments and extensive real-trace-driven simulations.},
keywords={Big data;Compressed sensing;Wireless sensor networks;Wireless communication;Redundancy;Industrial plants;Green design},
doi={10.1109/MCOM.2016.7588229},
ISSN={1558-1896},
month={October},}
@ARTICLE{8469815,
author={Mi, Jun and Wang, Kun and Li, Peng and Guo, Song and Sun, Yanfei},
journal={IEEE Communications Magazine}, title={Software-Defined Green 5G System for Big Data},
year={2018},
volume={56},
number={11},
pages={116-123},
abstract={The 5G system has been recognized as the most promising technology to provide high-quality network services. As a huge number of networking and computing equipments that generate big data are integrated into the 5G system, energy efficiency becomes the major challenge in building a green 5G system. In this article, we propose a software-defined green 5G system for big data, which consists of three planes: the control plane, the data plane and the energy plane. The data plane contains networking and computing equipments, which can be powered by both traditional grid and renewable energy sources in the energy plane. The control plane monitors the system status and configures the corresponding equipments to achieve energy efficiency and quality-of-service. Furthermore, to reduce the overhead of this software- defined architecture, we investigate a FRS to eliminate redundant system monitoring information. To integrate features in software-defined architecture, we propose an AIFS to mine latent rules among features. Simulation results indicate that our proposals achieve higher efficiency in the green 5G system.},
keywords={Monitoring;5G mobile communication;Green products;Big Data;Renewable energy sources;Quality of service;Computer architecture},
doi={10.1109/MCOM.2017.1700048},
ISSN={1558-1896},
month={November},}
@INPROCEEDINGS{9617238,
author={Bergès, Corinne and Bird, Jim and Shroff, Mehul D. and Rongen, René and Smith, Chris},
booktitle={2021 IEEE International Symposium on the Physical and Failure Analysis of Integrated Circuits (IPFA)}, title={Data analytics and machine learning: root-cause problem-solving approach to prevent yield loss and quality issues in semiconductor industry for automotive applications},
year={2021},
volume={},
number={},
pages={1-10},
abstract={Quality requirements in the semiconductor industry for automotive products are increasing rapidly with the movement to autonomous vehicles and higher levels of safety. It is no longer possible to express maximum failure requirements in parts per million (ppm). Individual failing parts observed in the field and reported by customers can trigger a significant quality response. ‘Zero-defect’ (ZD) is no longer considered a utopian ideal, but a required potentially reachable goal for semiconductor manufacturers. Projects and studies that include artificial intelligence and big data, are seen as key drivers to reach a ZD level of quality. Competing objectives targeted in any industrial project, such as quality improvement and gross margin, must also be considered. Initial projects in machine learning (ML), focusing on yield-loss issues, are being deployed within the manufacturing sites. These projects interconnect typical internal data collected from the manufacturing and assembly lines with engineering, qualification and reliability data. For a specific case study of unexpected abnormally high variability on some parameters, this paper presents a problem-solving approach in a big-data environment. Models implemented and results obtained towards root-cause problem solving for this issue, are discussed. This overall approach may be replicated in other ML projects.},
keywords={Solid modeling;Electronics industry;Predictive models;Reliability engineering;Manufacturing;Safety;Problem-solving;Automotive semiconductor industry;root-cause problem-solving;failure prevention;data analytics;machine learning;big data},
doi={10.1109/IPFA53173.2021.9617238},
ISSN={1946-1550},
month={Sep.},}
@INPROCEEDINGS{9662594,
author={Lu, Jianfei and Li, Suxiu and Zhang, Xinsheng},
booktitle={2021 5th International Conference on Power and Energy Engineering (ICPEE)}, title={A Study on the Business Data Evaluation Method of the Power Grid Value-Added Service},
year={2021},
volume={},
number={},
pages={288-292},
abstract={This research focuses on the data application basic technology for value-added services. In this research, the data quality and data value evaluation methods are studied. The data quality management system from data collection, storage, management and application is formed. Quality and power marketing data quality are analyzed and data value evaluation methods are established. As big data, artificial intelligence and other technologies continue to make breakthroughs, the value of data will become more and more important. Based on the State Grid’s full-service data, the full-scale data analysis across the professional, cross-business, and cross-system will promote the company’s power grid lean and intelligent management level, and will provide more value-added services for the company, government and society.},
keywords={Systematics;Data analysis;Data integrity;Storage management;Government;Data models;Power grids;Date Evaluation Method;Power Grid;Value- Added Service},
doi={10.1109/ICPEE54380.2021.9662594},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9174457,
author={Khokhlov, Igor and Reznik, Leon},
booktitle={2020 IEEE Systems Security Symposium (SSS)}, title={What is the Value of Data Value in Practical Security Applications},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Data value (DV) is a novel concept that is introduced as one of the Big Data phenomenon features. While continuing an investigation of the DV ontology and its relationship with the data quality (DQ) on the conceptual level, this paper researches possible applications and use of the DV in the practical design of security and privacy protection systems and tools. We present a novel approach to DV evaluation that maps DQ metrics into DV value. Developed methods allow DV and DQ use in a wide range of application domains. To demonstrate DQ and DV concept employment in real tasks we present two real-life scenarios. The first use case demonstrates the DV use in crowdsensing application design. It shows up how DV can be calculated by integrating various metrics characterizing data application functionality, accuracy, and security. The second one incorporates the privacy consideration into DV calculus by exploring the relationship between privacy, DQ, and DV in the defense against web-site fingerprinting in The Onion Router (TOR) networks. These examples demonstrate how our methods of the DV and DQ evaluation may be employed in the design of real systems with security and privacy consideration.},
keywords={Data quality;data value;security evaluation;privacy protection},
doi={10.1109/SSS47320.2020.9174457},
ISSN={},
month={July},}
@INPROCEEDINGS{8647366,
author={Hussain, Bilal and Du, Qinghe and Ren, Pinyi},
booktitle={2018 IEEE Global Communications Conference (GLOBECOM)}, title={Deep Learning-Based Big Data-Assisted Anomaly Detection in Cellular Networks},
year={2018},
volume={},
number={},
pages={1-6},
abstract={5G is envisioned to have an artificial intelligence (AI)-empowerment to efficiently plan, manage and optimize the extremely complex network by leveraging colossal amount of data (big data) generated at different levels of the network architecture. Cell outages and congestion pose serious threat to the network management. Sleeping cell is a special case of cell outage in which the cell provides inferior services to its users. This peculiar behavior of the cell is particularly challenging to detect as it disguises itself from the network monitoring entity. Inadequate accuracy and high false alarms are two major constraints of state-of-the-art approaches for the anomaly-sleeping cell and surge in user traffic activity that may lead to congestion-detection in cellular networks. This implies squandering of scarce resources which ultimately results in increased operational expenditure (OPEX) while disrupting network's quality of service (QoS) and user's quality of experience (QoE). Inspired from the prominent success of deep learning (DL) technology in machine learning domain, this is the first study that applies DL for the detection of abovementioned anomalies. We utilized, and did a comprehensive study of, L-layer deep feedforward neural network fueled by real call detail record (CDR) dataset (big data) and achieved 94.6% accuracy with 1.7% false positive rate (FPR), that are remarkable improvements and overcome the limitations of the previous studies. The preliminary results elucidate the feasibility and preeminence of our proposed anomaly detection framework.},
keywords={Computer architecture;Microprocessors;Big Data;Quality of service;Anomaly detection;Cellular networks;Quality of experience},
doi={10.1109/GLOCOM.2018.8647366},
ISSN={2576-6813},
month={Dec},}
@INPROCEEDINGS{6927666,
author={Ba, Huafeng and Gao, Xiaoming and Zhang, Xiaofeng and He, Zhenyu},
booktitle={2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)}, title={Protecting Data Privacy from Being Inferred from High Dimensional Correlated Data},
year={2014},
volume={2},
number={},
pages={495-502},
abstract={In the era of big data, privacy becomes a challenging issue which already attracts a good number of research efforts. In the literature, most of existing privacy preserving algorithms focus on protecting users' privacy from being disclosed by making the set of designated semi-id features indiscriminate. However, how to automatically determine the appropriate semi-id features from high-dimensional correlated data is seldom studied. Therefore, in this paper we first theoretically study the problem and propose the IPFS algorithm to find all possible features forming the candidate semi-id feature set which can infer users' privacy. Then, the KIPFS algorithm is proposed to find the key features from the candidate semi-id feature set. By anonymizing the key feature set, called as key inferring privacy features (KIPFS), users' privacy is protected. To evaluate the effectiveness and the efficacy of the proposed approach, two state-of-the-art algorithms, i.e., K-anonymity and t-closeness, applied on the designated semi-id feature set are chose as the baseline algorithms and their revised versions are applied on the KIPFS for the performance comparison. The promising results showed that by anonymizing the identified KIPFS, both aforementioned algorithms can achieve better performance than the original ones in terms of efficiency and data quality.},
keywords={Privacy;Data privacy;Algorithm design and analysis;Educational institutions;Equations;Computer science;Data analysis;privacy preserving data mining;data publishing;algorithm},
doi={10.1109/WI-IAT.2014.139},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8590192,
author={Fernández-Cerero, Damian and Fernández-Montes, Alejandro and Kolodziej, Joanna and Lefèvre, Laurent},
booktitle={2018 11th International Conference on the Quality of Information and Communications Technology (QUATIC)}, title={Quality of Cloud Services Determined by the Dynamic Management of Scheduling Models for Complex Heterogeneous Workloads},
year={2018},
volume={},
number={},
pages={210-219},
abstract={The quality of services in Cloud Computing (CC) depends on the scheduling strategies selected for processing of the complex workloads in the physical cloud clusters. Using the scheduler of the single type does not guarantee of the optimal mapping of jobs onto cloud resources, especially in the case of the processing of the big data workloads. In this paper, we compare the performances of the cloud schedulers for various combinations of the cloud workloads with different characteristics. We define several scenarios where the proper types of schedulers can be selected from a list of scheduling models implemented in the system, and used to schedule the concrete workloads based on the workloads' parameters and the feedback on the efficiency of the schedulers. The presented work is the first step in the development and implementation of an automatic intelligent scheduler selection system. In our simple experimental analysis, we confirm the usefulness of such a system in today's data-intensive cloud computing.},
keywords={Cloud computing;Task analysis;Processor scheduling;Dynamic scheduling;Big Data;Job shop scheduling;Servers;Big Data;Quality of Big Data;Scheduling;Cloud scheduling;Dynamic cloud scheduling},
doi={10.1109/QUATIC.2018.00039},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8784671,
author={Xinmei, Liang and Luqin},
booktitle={2019 IEEE 9th International Conference on Electronics Information and Emergency Communication (ICEIEC)}, title={Research on Web Service Selection Based on Parallel Skyline Algorithm},
year={2019},
volume={},
number={},
pages={1-5},
abstract={With the continuous development of the Internet, there are many web services with the same functional attributes but different functional attributes. It is urgent to find a web service that can satisfy itself quickly and efficiently from the massive web service data. This paper improves the traditional Skyline algorithm, divides the web service data set into regions, greatly reduces the data points without dominance, and saves memory usage. The improved Skyline algorithm can significantly improve the speed of Web service selection. However, the improved Skyline algorithm will still have insufficient computing resources when processing massive Web service data, resulting in a significant decrease in computing speed and even computer jam. In view of the above situation, this paper will parallelize the improved Skyline algorithm and parallelize the improved Skyline algorithm through the Spark platform. Experiments show that the parallelized Skyline algorithm can better handle massive Web service data.},
keywords={Sparks;Web services;Clustering algorithms;Big Data;Quality of service;Computer science;Parallel processing;Skyline;big data;Spark;Hadoop;parallelization},
doi={10.1109/ICEIEC.2019.8784671},
ISSN={2377-844X},
month={July},}
@ARTICLE{7835169,
author={Zong, Wei and Wu, Feng and Jiang, Zhengrui},
journal={IEEE Transactions on Engineering Management}, title={A Markov-Based Update Policy for Constantly Changing Database Systems},
year={2017},
volume={64},
number={3},
pages={287-300},
abstract={In order to maximize the value of an organization's data assets, it is important to keep data in its databases up-to-date. In the era of big data, however, constantly changing data sources make it a challenging task to assure data timeliness in enterprise systems. For instance, due to the high frequency of purchase transactions, purchase data stored in an enterprise resource planning system can easily become outdated, affecting the accuracy of inventory data and the quality of inventory replenishment decisions. Despite the importance of data timeliness, updating a database as soon as new data arrives is typically not optimal because of high update cost. Therefore, a critical problem in this context is to determine the optimal update policy for database systems. In this study, we develop a Markov decision process model, solved via dynamic programming, to derive the optimal update policy that minimizes the sum of data staleness cost and update cost. Based on real-world enterprise data, we conduct experiments to evaluate the performance of the proposed update policy in relation to benchmark policies analyzed in the prior literature. The experimental results show that the proposed update policy outperforms fixed interval update policies and can lead to significant cost savings.},
keywords={Database systems;Data models;Organizations;Markov processes;Big data;Data quality;data timeliness;enterprise resource planning (ERP);Markov decision process;update policy},
doi={10.1109/TEM.2017.2648516},
ISSN={1558-0040},
month={Aug},}
@INPROCEEDINGS{8271966,
author={Hassanein, Hossam S. and Oteafy, Sharief M. A.},
booktitle={2017 13th International Conference on Distributed Computing in Sensor Systems (DCOSS)}, title={Big Sensed Data Challenges in the Internet of Things},
year={2017},
volume={},
number={},
pages={207-208},
abstract={Internet of Things (IoT) systems are inherently built on data gathered from heterogeneous sources. In the quest to gather more data for better analytics, many IoT systems are instigating significant challenges. First, the sheer volume and velocity of data generated by IoT systems are burdening our networking infrastructure, especially at the edge. The mobility and intermittent connectivity of edge IoT nodes are further hampering real-time access and reporting of IoT data. As we attempt to synergize IoT systems to leverage resource discovery and remedy some of these challenges, the rising challenges of Quality of Information (QoI) and Quality of Resource (QoR) calibration, render many IoT interoperability attempts far-fetched. We survey a number of challenges in realizing IoT interoperability, and advocate for a uniform view of data management in IoT systems. We delve into three planes that encompass Big Sensed Data (BSD) research directions, presenting a building block for future research efforts in IoT data management.},
keywords={Sensors;Calibration;Internet of Things;Data integration;Conferences;Interoperability;Standards;Internet of Things;Big Sensed Data;Next Generation Networks;Quality of Data;Quality of Information},
doi={10.1109/DCOSS.2017.35},
ISSN={2325-2944},
month={June},}
@INPROCEEDINGS{7751640,
author={Mao, Xu and Su, Fei},
booktitle={2016 16th International Symposium on Communications and Information Technologies (ISCIT)}, title={Standards compliance testing on generic data of telecom operators},
year={2016},
volume={},
number={},
pages={302-306},
abstract={The big data is bringing new opportunities to the world. Every traditional telecom operator is exploring new ways to increase revenues and profits from the explosive growth of data traffic, but few have demonstrated the data quality needed for data applications. Standards compliance testing on generic data of telecom operators is the key means to improve competitive quality of comprehensive telecom information services. With analysis of current contradiction between data supply and data demand of telecom operators, this paper defines the generic data of telecom operators and its standards compliance testing, presents the testing procedures and the testing applications, and points out a series of testing research directions.},
keywords={C# languages;Data quality;Telecom operators;Generic data;Standards compliance testing},
doi={10.1109/ISCIT.2016.7751640},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8638638,
author={Xing, Xin and Dong, Bin and Ajo-Franklin, Jonathan and Wu, Kesheng},
booktitle={2018 IEEE/ACM Machine Learning in HPC Environments (MLHPC)}, title={Automated Parallel Data Processing Engine with Application to Large-Scale Feature Extraction},
year={2018},
volume={},
number={},
pages={37-46},
abstract={As new scientific instruments generate ever more data, we need to parallelize advanced data analysis algorithms such as machine learning to harness the available computing power. The success of commercial Big Data systems demonstrated that it is possible to automatically parallelize many algorithms. However, these Big Data tools have trouble handling the complex analysis operations from scientific applications. To overcome this difficulty, we have started to build an automated parallel data processing engine for science, known as ARRAYUDF. This paper provides an overview of this data processing engine, and a use case involving a feature extraction task from a large-scale seismic recording technology, called distributed acoustic sensing (DAS). The key challenge associated with DAS data sets is that they are vast in volume and noisy in data quality. The existing methods used by the DAS team for extracting useful signals like traveling seismic waves are complex and very time-consuming. Our parallel data processing engine reduces the job execution time from 10s of hours to 10s of seconds, and achieves 95% parallelization efficiency. ARRAYUDF could be used to implement more advanced data processing algorithms including machine learning, and could work with many more applications.},
keywords={Arrays;Sensors;Kernel;Data analysis;Engines;Feature extraction;ArrayUDF;distributed acoustic sensing;local similarity},
doi={10.1109/MLHPC.2018.8638638},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9605783,
author={Franklin, Paul},
booktitle={2021 Annual Reliability and Maintainability Symposium (RAMS)}, title={Solving Problems with Rapid Data Discovery},
year={2021},
volume={},
number={},
pages={1-3},
abstract={Summary &#x0026; ConclusionsThis paper describes an approach to extracting reliability data from transaction data and performing analysis on it. Organizations typically collect significant amounts of data that could be used for reliability analysis but is not.Data science is a field that offers reliability engineers insights when faced with analyzing so-called "big data." One subset of data science that can be helpful in this regard is the idea of using data cubes as the basis for analysis. Data cubes use many techniques, such as slicing, aggregation, drill-downs, and pivots [1]. These concepts are widely implemented, and most engineers use them, even if they do not explicitly name them. For example, drill-down would occur when only the data from a particular equipment model is examined; aggregation reverses this. Slicing occurs with all but two dimensions (defined below) are held constant. Pivots occur when data is "rotated" by changing the way rows and columns are selected and displayed.This paper will report on the results of using data cubes to support and drive the culture of reliability engineering:&#x2022;Rapidly model large datasets to confirm or deny reliability and process measures&#x2022;Drive data quality improvements&#x2022;Build confidence in the way business rules are modeled&#x2022;Develop metrics&#x2022;Support decision making in the face of uncertaintyThe paper describes the work done and offers some recommendations for implementation.},
keywords={Analytical models;Decision making;Measurement uncertainty;Process control;Random access memory;Organizations;Reliability engineering;Data cubes;Metrics;Decision support},
doi={10.1109/RAMS48097.2021.9605783},
ISSN={2577-0993},
month={May},}
@ARTICLE{7885523,
author={Bronselaer, Antoon and De Mol, Robin and De Tré, Guy},
journal={IEEE Transactions on Fuzzy Systems}, title={A Measure-Theoretic Foundation for Data Quality},
year={2018},
volume={26},
number={2},
pages={627-639},
abstract={In this paper, a novel framework for data quality measurement is proposed by adopting a measure-theoretic treatment of the problem. Instead of considering a specific setting in which quality must be assessed, our approach departs more formally from the concept of measurement. The basic assumption of the framework is that the highest possible quality can be described by means of a set of predicates. Quality of data is then measured by evaluating those predicates and by combining their evaluations. This combination is based on a capacity function (i.e., a fuzzy measure) that models for each combination of predicates the capacity with respect to the quality of the data. It is shown that expression of quality on an ordinal scale entails a high degree of interpretation and a compact representation of the measurement function. Within this purely ordinal framework for measurement, it is shown that reasoning about quality beyond the ordinal level naturally originates from the uncertainty about predicate evaluation. It is discussed how the proposed framework is positioned with respect to other approaches with particular attention to aggregation of measurements. The practical usability of the framework is discussed for several well known dimensions of data quality and demonstrated in a use-case study about clinical trials.},
keywords={Current measurement;Urban areas;Context;Big Data;Uncertainty;Decision making;Data quality;fuzzy measure;uncertainty modeling},
doi={10.1109/TFUZZ.2017.2686807},
ISSN={1941-0034},
month={April},}
@ARTICLE{8198798,
author={He, Ying and Yu, F. Richard and Zhao, Nan and Leung, Victor C. M. and Yin, Hongxi},
journal={IEEE Communications Magazine}, title={Software-Defined Networks with Mobile Edge Computing and Caching for Smart Cities: A Big Data Deep Reinforcement Learning Approach},
year={2017},
volume={55},
number={12},
pages={31-37},
abstract={Recent advances in networking, caching, and computing have significant impacts on the developments of smart cities. Nevertheless, these important enabling technologies have traditionally been studied separately in the existing works on smart cities. In this article, we propose an integrated framework that can enable dynamic orchestration of networking, caching, and computing resources to improve the performance of applications for smart cities. Then we present a novel big data deep reinforcement learning approach. Simulation results with different system parameters are presented to show the effectiveness of the proposed scheme.},
keywords={Smart cities;Streaming media;Cloud computing;Big Data;Quality of service;Mobile communication;Urban areas},
doi={10.1109/MCOM.2017.1700246},
ISSN={1558-1896},
month={Dec},}
@INPROCEEDINGS{8616529,
author={Kong, Weichang and Qiao, Fei and Wu, Qidi},
booktitle={2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, title={Real Manufacturing Oriented Data Process Techniques with Domain Knowledge},
year={2018},
volume={},
number={},
pages={3141-3146},
abstract={In the field of manufacturing industry, it is difficult to make full use of the research results for production optimization and/or management due to the low quality of real workshop data. Typical quality problems of the real workshop data include: data conflict, missing recessive data, and false error identification. The conventional data analysis methods cannot handle most such issues because they fail to consider professional insights into and domain knowledge about the data. The real production data from an actual semiconductor manufacturing workshop are adopted as the objective data in this paper. A series of data process techniques with domain knowledge are proposed to solve those data quality problems according to specific flaws of the data respectively. The work in this paper has the potential to be further extended and applied to other big data applications beyond the manufacturing industry.},
keywords={Conferences;Big Data;Manufacturing processes;Manufacturing industries;Data analysis;Data integrity},
doi={10.1109/SMC.2018.00532},
ISSN={2577-1655},
month={Oct},}
@INPROCEEDINGS{8937930,
author={Togneri, Rodrigo and Camponogara, Glauber and Soininen, Juha-Pekka and Kamienski, Carlos},
booktitle={2019 IEEE Latin-American Conference on Communications (LATINCOM)}, title={Foundations of Data Quality Assurance for IoT-based Smart Applications},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Most current scientific and industrial efforts in IoT are geared towards building integrated platforms to finally realize its potential in commercial scale applications. The IoT and Big Data contemporary context brings a number of challenges, such as providing quality assurance (defined by availability and veracity) for sensor data. Traditional signal processing approaches are no longer sufficient, requiring combined approaches in both architectural and analytical layers. This paper proposes a discussion on the adequate foundations of a new general approach aimed at increasing robustness and antifragility of IoT-based smart applications. In addition, it shows results of preliminary experiments with real data in the context of precision irrigation using multivariate methods to identify relevant situations, such as sensor failures and the mismatch of contextual sensor information due to different spatial granularities capture. Our results provide initial indications of the adequacy of the proposed framework.},
keywords={Irrigation;Quality assurance;Data integrity;Signal processing;Feature extraction;Robustness;Object recognition;Data quality;internet of things;smart applications;precision irrigation},
doi={10.1109/LATINCOM48065.2019.8937930},
ISSN={2330-989X},
month={Nov},}
@ARTICLE{8820081,
author={Gong, Xiaowen and Shroff, Ness B.},
journal={IEEE/ACM Transactions on Networking}, title={Truthful Mobile Crowdsensing for Strategic Users With Private Data Quality},
year={2019},
volume={27},
number={5},
pages={1959-1972},
abstract={Mobile crowdsensing has found a variety of applications (e.g., spectrum sensing, environmental monitoring) by leveraging the “wisdom” of a potentially large crowd of mobile users. An important metric of a crowdsensing task is data accuracy, which relies on the data quality of the participating users' data (e.g., users' received SNRs for measuring a transmitter's transmit signal strength). However, the quality of a user can be its private information (which, e.g., may depend on the user's location) that it can manipulate to its own advantage, which can mislead the crowdsensing requester about the knowledge of the data's accuracy. This issue is exacerbated by the fact that the user can also manipulate its effort made in the crowdsensing task, which is a hidden action that could result in the requester having incorrect knowledge of the data's accuracy. In this paper, we devise truthful crowdsensing mechanisms for Quality and Effort Elicitation (QEE), which incentivize strategic users to truthfully reveal their private quality and truthfully make efforts as desired by the requester. The QEE mechanisms achieve the truthful design by overcoming the intricate dependency of a user's data on its private quality and hidden effort. Under the QEE mechanisms, we show that the crowdsensing requester's optimal (RO) effort assignment assigns effort only to the best user that has the smallest “virtual valuation”, which depends on the user's quality and the quality's distribution. We also show that, as the number of users increases, the performance gap between the RO effort assignment and the socially optimal effort assignment decreases, and converges to 0 asymptotically. We further discuss some extensions of the QEE mechanisms. Simulation results demonstrate the truthfulness of the QEE mechanisms and the system efficiency of the RO effort assignment.},
keywords={Task analysis;Transmitters;Signal to noise ratio;Data integrity;Sensors;IEEE transactions;Big Data;Crowdsensing;truthful incentive mechanism;data quality},
doi={10.1109/TNET.2019.2934026},
ISSN={1558-2566},
month={Oct},}
@INPROCEEDINGS{7498306,
author={Cao, Wei and Wu, Zhengwei and Wang, Dong and Li, Jian and Wu, Haishan},
booktitle={2016 IEEE 32nd International Conference on Data Engineering (ICDE)}, title={Automatic user identification method across heterogeneous mobility data sources},
year={2016},
volume={},
number={},
pages={978-989},
abstract={With the ubiquity of location based services and applications, large volume of mobility data has been generated routinely, usually from heterogeneous data sources, such as different GPS-embedded devices, mobile apps or location based service providers. In this paper, we investigate efficient ways of identifying users across such heterogeneous data sources. We present a MapReduce-based framework called Automatic User Identification (AUI) which is easy to deploy and can scale to very large data set. Our framework is based on a novel similarity measure called the signal based similarity (SIG) which measures the similarity of users' trajectories gathered from different data sources, typically with very different sampling rates and noise patterns. We conduct extensive experimental evaluations, which show that our framework outperforms the existing methods significantly. Our study on one hand provides an effective approach for the mobility data integration problem on large scale data sets, i.e., combining the mobility data sets from different sources in order to enhance the data quality. On the other hand, our study provides an in-depth investigation for the widely studied human mobility uniqueness problem under heterogeneous data sources.},
keywords={Trajectory;Urban areas;Buildings;Noise measurement;Mobile communication;Navigation;Education},
doi={10.1109/ICDE.2016.7498306},
ISSN={},
month={May},}
@INPROCEEDINGS{9245416,
author={Georgieva, P. and Nikolova, E. and Orozova, D.},
booktitle={2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)}, title={Data Cleaning Techniques in Detecting Tendencies in Software Engineering},
year={2020},
volume={},
number={},
pages={1028-1033},
abstract={The world of software engineering is dynamically changing over the last decade. Providing adequate university education is one of the key goals of the academic community for ensuring advanced and up-to-date students' training. One direction in achieving this goal is to constantly monitor the trends in the Information Technology (IT) sector. A reliable source of information is the data from the annual survey on technology and programming languages, as well as on preferred learning methods and ways to enhance competencies, conducted amongst Stack Overflow users since 2011. In processing the data from the survey, the authors have faced several problems that have provoked interest in a more general data problem - data quality and data cleaning.This paper looks into data quality, tools for data cleaning and the characteristics of high-quality data. A classification of data problems is proposed in the context of analyzing the information about software developers. Additionally, the proposed process of data cleaning in illustrated with data for 2018 and 2019.},
keywords={Training;Data integrity;Tools;Strategic planning;Cleaning;Monitoring;Software engineering;Big Data Analytics;Data quality;Data cleaning;Software engineering},
doi={10.23919/MIPRO48935.2020.9245416},
ISSN={2623-8764},
month={Sep.},}
@ARTICLE{7900340,
author={Luo, Xin and Zhou, MengChu and Li, Shuai and Xia, YunNi and You, Zhu-Hong and Zhu, QingSheng and Leung, Hareton},
journal={IEEE Transactions on Cybernetics}, title={Incorporation of Efficient Second-Order Solvers Into Latent Factor Models for Accurate Prediction of Missing QoS Data},
year={2018},
volume={48},
number={4},
pages={1216-1228},
abstract={Generating highly accurate predictions for missing quality-of-service (QoS) data is an important issue. Latent factor (LF)-based QoS-predictors have proven to be effective in dealing with it. However, they are based on first-order solvers that cannot well address their target problem that is inherently bilinear and nonconvex, thereby leaving a significant opportunity for accuracy improvement. This paper proposes to incorporate an efficient second-order solver into them to raise their accuracy. To do so, we adopt the principle of Hessian-free optimization and successfully avoid the direct manipulation of a Hessian matrix, by employing the efficiently obtainable product between its Gauss-Newton approximation and an arbitrary vector. Thus, the second-order information is innovatively integrated into them. Experimental results on two industrial QoS datasets indicate that compared with the state-of-the-art predictors, the newly proposed one achieves significantly higher prediction accuracy at the expense of affordable computational burden. Hence, it is especially suitable for industrial applications requiring high prediction accuracy of unknown QoS data.},
keywords={Quality of service;Predictive models;Optimization;Computational modeling;Mathematical model;Data models;Web services;Big data;latent factor model;missing data prediction;quality-of-service (QoS);second-order solver;service computing sparse matrices;Web service},
doi={10.1109/TCYB.2017.2685521},
ISSN={2168-2275},
month={April},}
@INPROCEEDINGS{9377801,
author={Cai, Xumin and Aydin, Berkay and Maydeo, Saurabh and Ji, Anli and Angryk, Rafal},
booktitle={2020 IEEE International Conference on Big Data (Big Data)}, title={Local Outlier Detection for Multi-type Spatio-temporal Trajectories},
year={2020},
volume={},
number={},
pages={4509-4518},
abstract={Outlier detection has become one of the core tasks in spatio-temporal data mining. It plays an essential role in data quality improvement for the machine learning models and recognizing the anomalous patterns, which may remarkably deviate from expected patterns among the trajectory datasets. In this work, we propose a clustering-based technique to detect local outliers in trajectory datasets by utilizing spatial and temporal attributes of moving objects. This local outlier detection involves three phases. In the first phase, we apply a temporal partition procedure to divide the raw trajectory into multiple trajectory segments and extract trajectory features from spatial and temporal attributes for each trajectory segment. Then, we generate template features of trajectory segments by applying a clustering schema in the second phase. Finally, we use the abnormal score - a novel dissimilarity measure, which quantifies the disparity among the query and template trajectory segments in terms of trajectory features and hence determines the local outliers based on the distribution of abnormal score. To demonstrate the effectiveness of our method, we conduct three case studies on the real-life spatio-temporal trajectory datasets from the solar astroinformatics domain (i.e., solar active regions, coronal mass ejections, polarity inversion lines (PIL)). Our experimental results show that our local outlier detection approach can effectively discover the erroneous reports from the reporting module and abnormal phenomenon in various spatio-temporal trajectory datasets.},
keywords={Machine learning;Big Data;Feature extraction;Spatial databases;Trajectory;Task analysis;Anomaly detection},
doi={10.1109/BigData50022.2020.9377801},
ISSN={},
month={Dec},}
@ARTICLE{7523405,
author={Wang, Jianmin and Song, Shaoxu and Zhu, Xiaochen and Lin, Xuemin and Sun, Jiaguang},
journal={IEEE Transactions on Knowledge and Data Engineering}, title={Efficient Recovery of Missing Events},
year={2016},
volume={28},
number={11},
pages={2943-2957},
abstract={For various entering and transmission issues raised by human or system, missing events often occur in event data, which record execution logs of business processes. Without recovering the missing events, applications such as provenance analysis or complex event processing built upon event data are not reliable. Following the minimum change discipline in improving data quality, it is also rational to find a recovery that minimally differs from the original data. Existing recovery approaches fall short of efficiency owing to enumerating and searching over all of the possible sequences of events. In this paper, we study the efficient techniques for recovering missing events. According to our theoretical results, the recovery problem appears to be NP-hard. Nevertheless, advanced indexing, pruning techniques are developed to further improve the recovery efficiency. The experimental results demonstrate that our minimum recovery approach achieves high accuracy, and significantly outperforms the state-of-the-art technique for up to five orders of magnitudes improvement in time performance.},
keywords={Business;Petri nets;Engineering drawings;Indexes;Routing;Sun;Data mining;Data repairing;event data processing;petri net},
doi={10.1109/TKDE.2016.2594785},
ISSN={1558-2191},
month={Nov},}
@INPROCEEDINGS{9064032,
author={Gao, Jian and Zhen, Yan and Bai, Huifeng and Huo, Chao and Wang, Dongshan and Zhang, Ganghong},
booktitle={2019 IEEE 5th International Conference on Computer and Communications (ICCC)}, title={Research and Analysis Validation of Data Fusion Technology Based on Edge Computing},
year={2019},
volume={},
number={},
pages={97-101},
abstract={Based on the smart grid as the research background, this paper responded to the massive multi-source data processing requirements of the smart grid, and combined with distributed computing to provide the edge of the solution, aiming at the existing data of electric power equipment state monitoring data in noise and redundant data problems. A distributed kalman filter algorithm based on edge of computing was put forward. In this algorithm, event decision strategy was added to the data processing and transmission process of edge computing terminal to control the communication times between nodes and terminals in an event-driven way. Meanwhile, redundant data and data interfered by noise were reduced through the processing of the algorithm, so as to ensure the data quality and improve the fusion efficiency. Finally, the effectiveness of the method was verified by the analysis of compression efficiency and data fusion time.},
keywords={Data integration;Power grids;Kalman filters;Monitoring;Big Data;Data models;Distributed databases;smart grid;multi-source data;edge computing;filtering algorithm},
doi={10.1109/ICCC47050.2019.9064032},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8258225,
author={Tayeb, Shahab and Pirouz, Matin and Cozzens, Brittany and Huang, Richard and Jay, Maxwell and Khembunjong, Kyle and Paliskara, Sahan and Zhan, Felix and Zhang, Mark and Zhan, Justin and Latifi, Shahram},
booktitle={2017 IEEE International Conference on Big Data (Big Data)}, title={Toward data quality analytics in signature verification using a convolutional neural network},
year={2017},
volume={},
number={},
pages={2644-2651},
abstract={Many studies have been conducted on Handwritten Signature Verification. Researchers have taken many different approaches to accurately identify valid signatures from skilled forgeries, which closely resemble the real signature. The purpose of this paper is to suggest a method for validating written signatures on bank checks. This model uses a convolutional neural network (CNN) to analyze pixels from a signature image to recognize abnormalities. We believe the feature extraction capabilities of a CNN can optimize processing time and feature analysis of signature verification. Unique characteristics from signatures can be accurately and rapidly analyzed with multiple layers of receptive fields and hidden layers. Our method was able to correctly detect the validity of the inputted signature approximately 83 percent of the time. We tested our method using the SIGCOMP 2011 dataset. The main contribution of this method is to detect and decrease fraud committed, especially in the banking industry. Future uses of signature verification could include legal documents and the justice system.},
keywords={Neural networks;Image recognition;Feature extraction;Forgery;Machine learning;convolutional neural network;handwriting;deep learning;signature authentication;signature verification;machine learning;image classifier},
doi={10.1109/BigData.2017.8258225},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9361005,
author={Lu, Xin and Wang, Yu and Yuan, Jiao and Wang, Xun and Fu, Kun and Yang, Ke},
booktitle={2020 2nd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)}, title={A Parallel Adaptive DBSCAN Algorithm Based on k-Dimensional Tree Partition},
year={2020},
volume={},
number={},
pages={249-256},
abstract={The existing parallel DBSCAN (density based spatial clustering of applications with noise) algorithm needs to determine the parameter settings manually, and the datasets will be repeatedly accessed in the process of data partitioning and data merging, which reduces the efficiency of the algorithm excuting. Therefore, this paper proposes a parallel adaptive DBSCAN algorithm based on k-dimensional tree partition. It divides the dataset into several balanced data partitions by using k-dimensional tree, and carries out parallel computing in spark distributed computing framework, thus increasing the concurrent processing ability of the algorithm program and improving the I/O access speed. In addition, the improved adaptive DBSCAN parameter method is applied to each data partition for clustering analysis to obtain local clusters, which solves the random problem of manual setting parameters in the clustering process, and ensures the data quality of clustering mining. At the same time of creating local clusters, this algorithm also puts the mapping relationship between data points and adjacent points into the HashMap data structure of the master node, and uses it to merge local clusters into whole clusters, which can reduce the time cost of data merging. The experimental results show that the proposed algorithm can save about 18% running time compared with RDD-DBSCAN algorithm without reducing the clustering quality. With the increase of the number of cluster nodes, the running efficiency of the algorithm can be further improved, so it is suitable for processing massive data clustering analysis.},
keywords={Machine learning algorithms;Merging;Clustering algorithms;Data structures;Partitioning algorithms;Sparks;Data mining;clustering analysis;data partition;k-dimensional tree;adaptive computing;Spark framework},
doi={10.1109/MLBDBI51377.2020.00053},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8791314,
author={Qiao, Lin and Chen, Shuo and Bo, Jue and Liu, Sai and Ma, Guiwei and Wang, Haixin and Yang, Junyou},
booktitle={2019 IEEE International Conference on Energy Internet (ICEI)}, title={Wind power generation forecasting and data quality improvement based on big data with multiple temporal-spatual scale},
year={2019},
volume={},
number={},
pages={554-559},
abstract={Wind energy is one of the renewable energy sources with a large number of installations in the world. The accuracy of power generation prediction using wind speed data severely challenges the regulation and safe operation of power system. Since there are many time points in the dispatching strategy of power system, which is related to the area condition. It is of great significance for power grid dispatching to be able to timely and accurately predict the generation capacity of wind turbines in a certain period. Due to the randomness and intermittency of wind speed, the accuracy of data quality will be influenced greatly. In this paper, a neural network algorithm based on combination of back propagation (BP) and Newton interpolation mathematical function method is proposed to effectively process wind speed data, so as to predict power generation. BP neural network is a kind of multi-layer feedforward neural network including a hidden layer, which can solve the learning problem of hidden layer connection weight in a multi-layer network. From the perspective of space scale, this paper studies different wind speed data at different heights in the same area. Research results show: compared with the traditional support vector machine method, the accuracy with the proposed method is improved by 3.1%.},
keywords={Wind speed;Support vector machines;Biological neural networks;Indexes;Wind turbines;Mathematical model},
doi={10.1109/ICEI.2019.00104},
ISSN={},
month={May},}
@ARTICLE{8746175,
author={Saberi, Morteza and Hussain, Omar Khadeer and Chang, Elizabeth},
journal={IEEE Access}, title={Quality Management of Workers in an In-House Crowdsourcing-Based Framework for Deduplication of Organizations’ Databases},
year={2019},
volume={7},
number={},
pages={90715-90730},
abstract={While organizations in the current era of big data are generating massive volumes of data, they also need to ensure that its quality is maintained for it to be useful in decision-making purposes. The problem of dirty data plagues every organization. One aspect of dirty data is the presence of duplicate data records that negatively impact the organization's operations in many ways. Many existing approaches attempt to address this problem by using traditional data cleansing methods. In this paper, we address this problem by using an in-house crowdsourcing-based framework, namely, DedupCrowd. One of the main obstacles of crowdsourcing-based approaches is to monitor the performance of the crowd, by which the integrity of the whole process is maintained. In this paper, a statistical quality control-based technique is proposed to regulate the performance of the crowd. We apply our proposed framework in the context of a contact center, where the Customer Service Representatives are used as the crowd to assist in the process of deduplicating detection. By using comprehensive working examples, we show how the different modules of the DedupCrowd work not only to monitor the performance of the crowd but also to assist in duplicate detection.},
keywords={Crowdsourcing;Databases;Task analysis;Object recognition;Monitoring;Error analysis;Big Data;Quality management;quality control;data quality;duplicate detection;in-house crowdsourcing},
doi={10.1109/ACCESS.2019.2924979},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8279590,
author={Marone, Reine Marie and Camara, Fodé and Ndiaye, Samba},
booktitle={2017 IEEE 4th International Conference on Soft Computing Machine Intelligence (ISCMI)}, title={A large-scale filter method for feature selection based on spark},
year={2017},
volume={},
number={},
pages={16-20},
abstract={Recently, enormous volumes of data are generated in information systems. That's why data mining area is facing new challenges of transforming this “big data” into useful knowledge. In fact, “big data” relies low density of information (low data quality) and data redundancy, which negatively affect the data mining process. Therefore, when the number of variables describing the data is high, features selection methods are crucial for selecting relevant data. Features selection is the process of identifying the most relevant variables and removing those are redundant and irrelevant. In this paper, we propose a parallel, scalable feature selection algorithm based on mRMR (Max-Relevance and Min-Redundancy) in Spark, an in-memory parallel computing framework specialized in computation for large distributed datasets. Our experiments using real-world data of high dimensionality demonstrated that our proposition scale well and efficiently with large datasets.},
keywords={Feature extraction;Mutual information;Sparks;Algorithm design and analysis;Redundancy;Big Data;Classification algorithms;feature selection;filter method;parallel computing;apache spark;mRMR;SVM},
doi={10.1109/ISCMI.2017.8279590},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7214177,
author={Zhu, Hong and Bayley, Ian and Younas, M. and Lightfoot, David and Yousef, Basel and Liu, Dongmei},
booktitle={2015 IEEE 8th International Conference on Cloud Computing}, title={Big SaaS: The Next Step beyond Big Data},
year={2015},
volume={},
number={},
pages={1131-1140},
abstract={Software-as-a-Service (SaaS) is a model of cloud computing in which software functions are delivered to the users as services. The past few years have witnessed its global flourishing. In the foreseeable future, SaaS applications will integrate with the Internet of Things, Mobile Computing, Big Data, Wireless Sensor Networks, and many other computing and communication technologies to deliver customizable intelligent services to a vast population. This will give rise to an era of what we call Big SaaS systems of unprecedented complexity and scale. They will have huge numbers of tenants/users interrelated in complex ways. The code will be complex too and require Big Data but provide great value to the customer. With these benefits come great societal risks, however, and there are other drawbacks and challenges. For example, it is difficult to ensure the quality of data and metadata obtained from crowd sourcing and to maintain the integrity of conceptual model. Big SaaS applications will also need to evolve continuously. This paper will discuss how to address these challenges at all stages of the software lifecycle.},
keywords={Software as a service;Checkpointing;Fault tolerance;Fault tolerant systems;Ontologies;Computer architecture},
doi={10.1109/CLOUD.2015.167},
ISSN={2159-6190},
month={June},}
@INPROCEEDINGS{6406445,
author={Loh, Ji Meng and Dasu, Tamraparni},
booktitle={2012 IEEE 12th International Conference on Data Mining Workshops}, title={Effect of Data Repair on Mining Network Streams},
year={2012},
volume={},
number={},
pages={226-233},
abstract={Data quality issues have special implications in network data. Data glitches are propagated rapidly along pathways dictated by the hierarchy and topology of the network. In this paper, we use temporal data from a vast data network to study data glitches and their effect on network monitoring tasks such as anomaly detection. We demonstrate the consequences of cleaning the data, and develop targeted and customized cleaning strategies by exploiting the network hierarchy.},
keywords={Maintenance engineering;Data mining;Cleaning;Measurement;Time series analysis;Context;Information management;Data glitches;Big Data;missing values;outliers;network analysis;Earth Mover Distance},
doi={10.1109/ICDMW.2012.125},
ISSN={2375-9259},
month={Dec},}
@INPROCEEDINGS{9401161,
author={Shioiri, Satoshi and Sato, Yoshiyuki and Horaguchi, Yuta and Muraoka, Hiroaki and Nihei, Mariko},
booktitle={2021 IEEE International Symposium on Circuits and Systems (ISCAS)}, title={Quali-Informatics in the Society with Yotta Scale Data},
year={2021},
volume={},
number={},
pages={1-4},
abstract={Accumulation of information is essential for human knowledge production, and information technology has accelerated the speed of data accumulation. The increase in quantity of information with high speed does not promise high-quality knowledge production and possibly does cause problems. One big problem is lack of storage for such big data. Another critical problem in information usage is information overload, that is, deterioration of productivity by too much information. Decision accuracy decrease with amount of information beyond a certain point while it increases at the beginning. We introduce an approach for solution of these problems with an example of research along the approach.},
keywords={Productivity;Circuits and systems;Big Data;Acceleration;Information technology;data quality;human judgments},
doi={10.1109/ISCAS51556.2021.9401161},
ISSN={2158-1525},
month={May},}
@INPROCEEDINGS{9251151,
author={Lu, Jian and Li, Wei and Wang, Qingren and Zhang, Yiwen},
booktitle={2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)}, title={Research on Data Quality Control of Crowdsourcing Annotation: A Survey},
year={2020},
volume={},
number={},
pages={201-208},
abstract={It is well known that many intelligent and computer-hard tasks cannot be effectively addressed by existing machine-based approaches, so that it is nature to think of utilizing the intelligence of human being. With the popularization of crowdsourcing concepts as well as the development of crowdsourcing platforms, as a new way of human intelligence to participate in machine computing, crowdsourcing annotation helps more and more supervised-learning-based approaches easily obtain enormous labeled data with relatively low cost. However, because of the diversity of the crowd employed by crowdsourcing platforms, how to control qualities of labels coming from the crowd plays a key role in crowdsourcing annotation. In this survey, we first present basic concepts and definitions of crowdsourcing annotation. Then, we review existing ground truth inference algorithms and learning models. After that, the advantages and distinctions among these algorithms and learning models as well as the levels of study progresses will be reported. And finally, we summarize realworld datasets widely utilized in the field of crowdsourcing annotation as well as available open source tools.},
keywords={Crowdsourcing;Training;Annotations;Big Data;Tools;Inference algorithms;Task analysis;label;truth inference;learning model;crowdsourcing},
doi={10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00044},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9005484,
author={Wang, Qiyao and Wang, Haiyan and Gupta, Chetan and Serita, Susumu},
booktitle={2019 IEEE International Conference on Big Data (Big Data)}, title={Regularized Operating Envelope with Interpretability and Implementability Constraints},
year={2019},
volume={},
number={},
pages={1506-1516},
abstract={Operating envelope is an important concept in industrial operations. Accurate identification for operating envelope can be extremely beneficial to stakeholders as it provides a set of operational parameters that optimizes some key performance indicators (KPI) such as product quality, operational safety, equipment efficiency, environmental impact, etc. Given the importance, data-driven approaches for computing the operating envelope are gaining popularity. These approaches typically use classifiers such as support vector machines, to set the operating envelope by learning the boundary in the operational parameter spaces between the manually assigned `large KPI' and `small KPI' groups. One challenge to these approaches is that the assignment to these groups is often ad-hoc and hence arbitrary. However, a bigger challenge with these approaches is that they don't take into account two key features that are needed to operationalize operating envelopes: (i) interpretability of the envelope by the operator and (ii) implementability of the envelope from a practical standpoint. In this work, we propose a new definition for operating envelope which directly targets the expected magnitude of KPI (i.e., no need to arbitrarily bin the data instances into groups) and accounts for the interpretability and the implementability. We then propose a regularized `GA +penalty' algorithm that outputs an envelope where the user can tradeoff between bias and variance. The validity of our proposed algorithm is demonstrated by two sets of simulation studies and an application to a real-world challenge in the mining processes of a flotation plant.},
keywords={Genetic algorithms;Machine learning;Oils;Optimization;Search problems;Big Data;Quality assessment;Operating envelope;Genetic algorithm;Penalty approach;Generalization},
doi={10.1109/BigData47090.2019.9005484},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8547038,
author={Brahim, Mohamed Ben and Menouar, Hamid},
booktitle={2017 6th IEEE International Conference on Advanced Logistics and Transport (ICALT)}, title={Optimizing V2X Data Collection and Storage for a Better Cost and Quality Trade-off},
year={2017},
volume={},
number={},
pages={7-12},
abstract={Future vehicles will be equipped with advanced communication capabilities and a multitude of sensing devices. Vehicle-to-vehicle and to Infrastructure (V2X) is one of these future technologies. V2X-technology-enabled vehicles are expected to become a great source of big data. This data, if gathered in the right time and processed in the right way, can enable an interesting number of existing and new applications. This can be a challenging task, taken into account the considerable size of the data that will be gathered. One of the challenges is to find a good balance between the number of data to filter out and the quality of the end data. This contribution tackles this specific challenge, by studying data storage cost reduction and evaluating its impact on the data quality. The proposed solution compares three approaches of treating the collected data at the road-side unit after taking out unnecessary information details. This solution has been tested and validated through simulations that show promising results.},
keywords={Sensors;Containers;Roads;Data integrity;Vehicle dynamics;Data mining;Memory;V2X wireless communication;vehicular edge computing;data storage;data quality;dimension reduction;data sampling},
doi={10.1109/ICAdLT.2017.8547038},
ISSN={},
month={July},}
@INPROCEEDINGS{8572515,
author={Zhang, Lichen},
booktitle={2018 17th International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES)}, title={Specifying and Modeling Cloud Cyber Physical Systems Based on AADL},
year={2018},
volume={},
number={},
pages={26-29},
abstract={In cyber physical systems(CPS), the physical world and the information world are merged to form a new structure that combines both hardware and software, and become the core technology system that supports and leads the transformation of a new generation of industries. With the rapid development of network technology, the data generated has also increased rapidly, which means that today's information society has entered the era of big data, and the technology of adapting to the cloud platform has gradually matured. The cloud computing platform provides flexible and relatively inexpensive storage space and computing resources for the development of big data technology. This also provides basic support for the development of big data driven CPS based on the cloud platform. In this paper, we specify and model cloud cyber physical systems based on AADL, which can specify, model, and analyze cloud cyber physical systems, finally implement cyber physical systems on cloud platforms, provide availability analysis, reliability analysis. data quality analysis, real-time performance analysis, security analysis and resource consumption analysis.},
keywords={Cloud computing;Computational modeling;Unified modeling language;Analytical models;Computer architecture;Software;Object oriented modeling;cloud;CPS;big data;AADL;specification},
doi={10.1109/DCABES.2018.00017},
ISSN={2473-3636},
month={Oct},}
@INPROCEEDINGS{8024681,
author={Guang Wei and Hailong Yang and Zhongzhi Luan and Depei Qian},
booktitle={2017 IEEE Symposium on Computers and Communications (ISCC)}, title={iDPL: A scalable and flexible inter-continental testbed for data placement research and experiment},
year={2017},
volume={},
number={},
pages={1158-1163},
abstract={In this paper, we propose the China-US international data placement laboratory (iDPL) based on an inter-continental testbed for data placement research. iDPL is able to support various data placement research due to its scalability and flexibility in deploying the experiments in the real network environment. The core design of iDPL leverages reliable workflow management and lightweight I/O protocol to allow complex experiment setup and on-the-fly experiment deployment. It is also extensible to plugin different network profiling tools such as iperf. We expect the powerful measurement capability of iDPL promotes research study on the intelligent data placement policies which adapt to the uncertainty of the wide-area network and guarantee the quality of service (QoS) of the big data applications. As a case study, we setup a set of data placement experiments to measure the end-to-end network performance constantly among several sites between China and US using different data placement tools. The experiments have been running for more than one year, and its measurement data is public available (http://mickey.buaa.edu.cn:8080/). We believe the measurement data is valuable for both network and big data researchers to understand the performance disparity between the raw network and the actual data placement, which provides useful insights to design big data applications with performance awareness. We encourage more researchers to deploy their own data placement experiments on iDPL, expediting the research direction of intelligent data placement with real network environment.},
keywords={Tools;Synchronization;Data visualization;Protocols;Distributed databases;Computers;Software},
doi={10.1109/ISCC.2017.8024681},
ISSN={},
month={July},}
@INPROCEEDINGS{8441160,
author={Soe, Thin Thin and Min, Myat Myat},
booktitle={2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)}, title={Speeding up Incomplete Data Analysis using Matrix-Represented Approximations},
year={2018},
volume={},
number={},
pages={206-211},
abstract={The veracity related with data quality such as incomplete, imprecise and inconsistent data creates a major challenge to data mining and data analysis. Rough set theory provides a special tool for handling the imprecise and incomplete data in information systems. However, the existing rough set based incomplete data analysis methods may not be able to handle large amount of data within the acceptable time. This paper focuses on speeding up the incomplete data analysis. The computation of the lower and upper approximations is a vital step for improving the performance of rough set based data analysis process. In this paper, the lower and upper approximations are characterized as matrix-represented approximations. The resulting approximations are exploited as inputs for data analysis method LERS (Learning from Examples based on Rough Set) used with LEM2 (Learning from Examples Module, Version2) rule induction algorithm. Then, this paper provides a set of experiments on missing datasets with different missing percent. The experimental results on incomplete or missing datasets from UCI Machine Learning Repository show that the proposed system effectively reduces the computational time in comparison with the existing system.},
keywords={Rough sets;Data analysis;Approximation algorithms;Big Data;Data integrity;Tools;rough set;incomplete data;missing values;approximations;matrix},
doi={10.1109/SNPD.2018.8441160},
ISSN={},
month={June},}
@INPROCEEDINGS{7951874,
author={Fan Xiaojiang and Zheng Liwei and Liu Jianbin},
booktitle={2017 IEEE 2nd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)}, title={Measurement for social network data currency and trustworthiness},
year={2017},
volume={},
number={},
pages={1-5},
abstract={Along with the explosive growth of the information in Social Network Service, the research of the quality of data has become a new hot point in related research field. High quality social data can more effectively support data mining, knowledge discovery, and can provide reliable and efficient data for users. Based on the measure problems of data quality, this paper discussed the measurement of two important dimensions of data quality: currency and trustworthiness. Computing models for currency measurement of data with or without time stamp are given. And based on the currency values, a trustworthiness measurement method is also given.},
keywords={Facsimile;social network service;data quality;currency;trustworthiness},
doi={10.1109/ICCCBDA.2017.7951874},
ISSN={},
month={April},}
@INPROCEEDINGS{7004389,
author={Fähnrich, Cindy and Schapranow, Matthieu-P. and Plattner, Hasso},
booktitle={2014 IEEE International Conference on Big Data (Big Data)}, title={Towards integrating the detection of genetic variants into an in-memory database},
year={2014},
volume={},
number={},
pages={27-32},
abstract={Next-generation sequencing enables whole genome sequencing within a few hours at a minimum of cost, entailing advanced medical applications such as personalized treatments. However, this recent technology imposes new challenges to alignment and variant calling as subsequent analysis steps. Compared to former sequencing, both must deal with an increasing amount of data to process at a significantly lower data quality - and are currently not capable of that. In this work, we focus on addressing these challenges for identifying Single Nucleotide Polymorphisms, i.e. SNP calling, in genome data as one subtask of variant calling. We propose the application of a column-store in-memory database for efficient data processing and apply the statistical model that is provided by the Genome Analysis Toolkit's UnifiedGenotyper. Comparisons with the UnifiedGenotyper show that our approach can exploit all computational resources available and accelerates SNP calling up to a factor of 22x.},
keywords={Genomics;Bioinformatics;Databases;Sequential analysis;Runtime;Biological cells;Instruction sets;Genome Data Analysis;Variant Calling;Single Nucleotide Polymorphism;In-Memory Database Technology;Next-Generation Sequencing},
doi={10.1109/BigData.2014.7004389},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8574573,
author={Lv, Yirong and Sun, Bin and Luo, Qingyi and Wang, Jing and Yu, Zhibin and Qian, Xuehai},
booktitle={2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, title={CounterMiner: Mining Big Performance Data from Hardware Counters},
year={2018},
volume={},
number={},
pages={613-626},
abstract={Modern processors typically provide a small number of hardware performance counters to capture a large number of microarchitecture events. These counters can easily generate a huge amount (e.g., GB or TB per day) of data, which we call big performance data in cloud computing platforms with more than thousands of servers and millions of complex workloads running in a "24/7/365" manner. The big performance data provides a precious foundation for root cause analysis of performance bottlenecks, architecture and compiler optimization, and many more. However, it is challenging to extract value from the big performance data due to: 1) the many unperceivable errors (e.g., outliers and missing values); and 2) the difficulty of obtaining insights, e.g., relating events to performance. In this paper, we propose CounterMiner, a rigorous methodology that enables the measurement and understanding of big performance data by using data mining and machine learning techniques. It includes three novel components: 1) using data cleaning to improve data quality by replacing outliers and filling in missing values; 2) iteratively quantifying, ranking, and pruning events based on their importance with respect to performance; 3) quantifying interaction intensity between two events by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from the Spark version of HiBench) to evaluate CounterMiner. The experimental results show that CounterMiner reduces the average error from 28.3% to 7.7% when multiplexing 10 events on 4 hardware counters. We also conduct a real-world case study, showing that identifying important configuration parameters of Spark programs by event importance is much faster than directly ranking the importance of these parameters.},
keywords={Hardware;Time series analysis;Program processors;Cloud computing;Microarchitecture;Data mining;Benchmark testing;Performance;big data;computer architecture;performance counters;data mining},
doi={10.1109/MICRO.2018.00056},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9156127,
author={Kumar, Abhishek and Braud, Tristan and Tarkoma, Sasu and Hui, Pan},
booktitle={2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)}, title={Trustworthy AI in the Age of Pervasive Computing and Big Data},
year={2020},
volume={},
number={},
pages={1-6},
abstract={The era of pervasive computing has resulted in countless devices that continuously monitor users and their environment, generating an abundance of user behavioural data. Such data may support improving the quality of service, but may also lead to adverse usages such as surveillance and advertisement. In parallel, Artificial Intelligence (AI) systems are being applied to sensitive fields such as healthcare, justice, or human resources, raising multiple concerns on the trustworthiness of such systems. Trust in AI systems is thus intrinsically linked to ethics, including the ethics of algorithms, the ethics of data, or the ethics of practice. In this paper, we formalise the requirements of trustworthy AI systems through an ethics perspective. We specifically focus on the aspects that can be integrated into the design and development of AI systems. After discussing the state of research and the remaining challenges, we show how a concrete use-case in smart cities can benefit from these methods.},
keywords={Artificial intelligence;Ethics;Data privacy;Biological system modeling;Training;Pervasive computing;Robustness;Artificial Intelligence;Pervasive Computing;Ethics;Data Fusion;Transparency;Privacy;Fairness;Accountability;Federated Learning},
doi={10.1109/PerComWorkshops48775.2020.9156127},
ISSN={},
month={March},}
@INPROCEEDINGS{8258016,
author={Kraus, Naama and Carmel, David and Keidar, Idit},
booktitle={2017 IEEE International Conference on Big Data (Big Data)}, title={Fishing in the stream: Similarity search over endless data},
year={2017},
volume={},
number={},
pages={964-969},
abstract={Similarity search is the task of retrieving data items that are similar to a given query. In this paper, we introduce the time-sensitive notion of similarity search over endless data-streams (SSDS), which takes into account data quality and temporal characteristics in addition to similarity. SSDS is challenging as it needs to process unbounded data, while computation resources are bounded. We propose Stream-LSH, a randomized SSDS algorithm that bounds the index size by retaining items according to their freshness, quality, and dynamic popularity attributes. We show that Stream-LSH increases recall when searching for similar items compared to alternative approaches using the same space capacity.},
keywords={Redundancy;Heuristic algorithms;Indexing;Approximation algorithms;Measurement;Runtime;Similarity search;Stream search;Retention policy;Locality sensitive hashing;Dynamic popularity},
doi={10.1109/BigData.2017.8258016},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8653702,
author={Suresh, T. and Murugan, A.},
booktitle={2018 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 2018 2nd International Conference on}, title={Strategy for Data Center Optimization : Improve Data Center capability to meet business opportunities},
year={2018},
volume={},
number={},
pages={184-189},
abstract={Considering current evolving technology and the way data are growing, IT consulting and outsourcing industry expected to be strategic partner for technology innovation in addition to support on-going business with reduced operational cost. Data Center is backbone for digital economy, big data, cloud, artificial intelligence, IoT or wearable technology. Data growth and on-demand data access changed the focus of data center as storage and disaster recovery to access data instantly from cloud without compromising security controls and data quality. These technology transformations create demand for latency. Every organization like Facebook, Equinix, Amazon, and Google are having their own data centers and expanding their business on cloud services. Data Center plays major critical on success of digital business. It is important to find possible options to optimize infrastructure and improve efficiency and productivity of Data Center. At the same time, we need to make sure that environment is up and running without compromising quality and security of data. This paper gives few solutions to get more from Data Center, reduce operational cost and optimize infrastructure utilization.},
keywords={Data centers;Cloud computing;Optimization;Organizations;Servers;Maintenance engineering;Data Center;energy saving;green computing;server;network devices;cloud storage},
doi={10.1109/I-SMAC.2018.8653702},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8002473,
author={Haneem, Faizura and Ali, Rosmah and Kama, Nazri and Basri, Sufyan},
booktitle={2017 International Conference on Research and Innovation in Information Systems (ICRIIS)}, title={Descriptive analysis and text analysis in Systematic Literature Review: A review of Master Data Management},
year={2017},
volume={},
number={},
pages={1-6},
abstract={Systematic Literature Review (SLR) is a structured way of conducting a review of existing research works produced by the earlier researchers. The application of right data analysis technique during the SLR evaluation stage would give an insight to the researcher in achieving the SLR objective. This paper presents how descriptive analysis and text analysis can be applied to achieve one of the common SLR objectives which is to study the progress of specific research domain. These techniques have been demonstrated to synthesis the progress of Master Data Management research domain. Using descriptive analysis technique, this study has identified a trend of related literary works distribution by years, sources, and publication types. Meanwhile, text analysis shows the common terms and interest topics in the Master Data Management research which are 1) master data, 2) data quality, 3) business intelligence, 4) business process, 5) data integration, 6) big data, 7) data governance, 8) information governance, 9) data management and 10) product data. It is hoped that other researchers would be able to replicate these analysis techniques in performing SLR for other research domains.},
keywords={Text analysis;Databases;Technological innovation;Frequency-domain analysis;Text mining;Quality assessment;Systematic Literature Review;Descriptive Analysis;Text Analysis;Master Data Management},
doi={10.1109/ICRIIS.2017.8002473},
ISSN={2324-8157},
month={July},}
@INPROCEEDINGS{9226358,
author={Al-Sabbagh, Khaled Walid and Staron, Miroslaw and Hebig, Regina and Meding, Wilhelm},
booktitle={2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)}, title={Improving Data Quality for Regression Test Selection by Reducing Annotation Noise},
year={2020},
volume={},
number={},
pages={191-194},
abstract={Big data and machine learning models have been increasingly used to support software engineering processes and practices. One example is the use of machine learning models to improve test case selection in continuous integration. However, one of the challenges in building such models is the identification and reduction of noise that often comes in large data. In this paper, we present a noise reduction approach that deals with the problem of contradictory training entries. We empirically evaluate the effectiveness of the approach in the context of selective regression testing. For this purpose, we use a curated training set as input to a tree-based machine learning ensemble and compare the classification precision, recall, and f-score against a non-curated set. Our study shows that using the noise reduction approach on the training instances gives better results in prediction with an improvement of 37% on precision, 70% on recall, and 59% on f-score.},
keywords={Training;Testing;Annotations;Predictive models;Noise reduction;Feature extraction;Dictionaries;Annotation Noise;Regression Testing;Machine Learning Models},
doi={10.1109/SEAA51224.2020.00042},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8258313,
author={Yang, Dazhi and Zhang, Allan N. and Yan, Wenjing},
booktitle={2017 IEEE International Conference on Big Data (Big Data)}, title={Performing literature review using text mining, Part I: Retrieving technology infrastructure using Google Scholar and APIs},
year={2017},
volume={},
number={},
pages={3290-3296},
abstract={Technology infrastructure (TechInfra) refers to metadata describing an academic field, such as journals & conferences, authors, publications and organizations. Understanding the TechInfra is often the first step in performing a literature review on a particular topic. In this paper, a study is conducted to retrieve TechInfra for a topic in supply chain management, namely, last mile logistics. Google Scholar is used as the primary tool for data collection. The first 1,000 results returned by Google Scholar are downloaded as HTML files. Subsequently, various application programming interfaces (APIs) - e.g., ScienceDirect, IEEE, CrossRef APIs - are used to enhance the data quality. Some plots are used to provide visualization of TechInfra of last mile logistics.},
keywords={Google;Metadata;Logistics;Uniform resource locators;Transportation;Databases;Text mining;technology infrastructure;text mining;last mile logistics;Google Scholar},
doi={10.1109/BigData.2017.8258313},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9604414,
author={Lee, Jay},
booktitle={2021 IEEE World Congress on Services (SERVICES)}, title={Transformation: Case Studies and Lessons Learned : Keynote 2},
year={2021},
volume={},
number={},
pages={xxiii-xxiii},
abstract={Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Industrial AI, Big Data Analytics, Machine Learning, and Cyber Physical Systems are changing the way we design product, manufacturing, and service systems. It is clear that as more sensors and smart analytics software are integrated in the networked industrial products and manufacturing systems, predictive technologies can further learn and autonomously optimize service productivity and performance. This presentation will address the trends of Industrial AI for smart service realization. First, Industrial AI systematic approach will be introduced. Case studies on advanced predictive analytics technologies for different maintenance and service operations will be demonstrated. In addition, issues on data quality for high performance and real-time data analytics in future digital service will be discussed.},
keywords={},
doi={10.1109/SERVICES51467.2021.00055},
ISSN={2642-939X},
month={Sep.},}
@ARTICLE{9464670,
author={Zhang, Yang and Guo, Hanqi and Shang, Lanyu and Wang, Dong and Peterka, Tom},
journal={IEEE Transactions on Big Data}, title={A Multi-branch Decoder Network Approach toAdaptive Temporal Data Selection andReconstruction for Big Scientific Simulation Data},
year={2021},
volume={},
number={},
pages={1-1},
abstract={A key challenge in scientific simulation is that the simulation outputs often require intensive I/O and storage space to store the results for effective post hoc analysis. This paper focuses on a quality-aware adaptive temporal data selection and reconstruction problem where the goal is to adaptively select simulation data samples at certain key timesteps in situ and reconstruct the discarded samples with quality assurance during post hoc analysis. This problem is motivated by the limitation of current solutions that a significant amount of simulation data samples are either discarded or aggregated during the sampling process, leading to inaccuratemodeling of the simulated phenomena. Two unique challenges exist: 1) the sampling decisions have to be made in situ and adapted tothe dynamics of the complex scientific simulation data; 2) the reconstruction error must be strictly bounded to meet the application requirement. To address the above challenges, we developDeepSample, an error-controlled convolutional neural network framework, that jointly integrates a set of coherent multi-branch deep decoders to effectively reconstruct the simulation data with rigorous quality assurance. The results on two real-world scientific simulation applications show that DeepSample significantly outperforms other state-of-the-art methods on both sampling efficiency and reconstructed simulation data quality.},
keywords={Data models;Adaptation models;Computational modeling;Analytical models;Image reconstruction;Data integrity;Adaptive systems;Big Scientific Simulation Data;Adaptive Temporal Data Selection and Reconstruction;Multi-branch Decoder Network},
doi={10.1109/TBDATA.2021.3092174},
ISSN={2332-7790},
month={},}
@INPROCEEDINGS{6943406,
author={Zhi, Yanling and Liu, Gang and Wang, Huimin},
booktitle={2014 11th International Conference on Service Systems and Service Management (ICSSSM)}, title={Research on data gray correction model based on grey interval number — A case study of Chinese ecological civilization evaluation},
year={2014},
volume={},
number={},
pages={1-5},
abstract={In the age of Big Data, we must do best to economically extract value from very large volumes of a wide variety of statistics. However, because of subjective and objective reasons, it is becoming increasing clear that much data is of poor quality, which has serious effects on the research results. With the analysis on the cause and process of the low quality data, this paper introduces the concept of gray system and proposes a data-gray-correction model, which could change the original data into gray interval number and reduce the influence from the former. Assessing the quality of data with classical econometric model and correcting the data by error correction model, then find the reasonable range of the real data and instead the crisp data by interval number, which contain much more information. An example is provided to illustrate the ecological civilization evaluation process under gray interval number.},
keywords={Biological system modeling;Economic indicators;Data models;Educational institutions;Accuracy;Mathematical model;Analytical models;data quality;gray interval number;ecological civilization evaluation},
doi={10.1109/ICSSSM.2014.6943406},
ISSN={2161-1904},
month={June},}
@INPROCEEDINGS{9510797,
author={Yan, Xiaowen and Zhou, Yu and Huang, Fuxing and Wang, Xiaofen and Yuan, Peisen},
booktitle={2021 IEEE 4th International Electrical and Energy Conference (CIEEC)}, title={Privacy protection method of power metering data in clustering based on differential privacy},
year={2021},
volume={},
number={},
pages={1-6},
abstract={Power companies can use the power grid big data platform to cluster analysis of power metering data, which can improve the personalized service quality of power grid companies for different users and discover the power stealing behavior of users to protect the interests of power grid companies. However, in the cluster analysis of power measurement data, the privacy information of power users may also be disclosed. To defend the privacy information of power users, the article applies differential privacy technology to cluster analysis of power metering data to avoid power users’ privacy leakage. First, the article presents the attack model that exists in the cluster analysis of power metering data. Then, the article add Laplacian noise to the power metering data to defend against attacks in the cluster analysis of attackers. Next, to enhance the data availability of noise-added power measurement data in cluster analysis, the article limits noise distance based on the results of the cluster analysis. Experiments show that method proposed in article can guarantee the privacy information of power data during the cluster analysis of power metering data, and ensure the data quality of the power metering data after privacy protection.},
keywords={Differential privacy;Analytical models;Power measurement;Laplace equations;Data integrity;Conferences;Companies;power metering data;cluster analysis;differential privacy;Laplacian noise},
doi={10.1109/CIEEC50170.2021.9510797},
ISSN={},
month={May},}
@INPROCEEDINGS{6930153,
author={Yang, Longzhi and Neagu, Daniel},
booktitle={2014 14th UK Workshop on Computational Intelligence (UKCI)}, title={Integration strategies for toxicity data from an empirical perspective},
year={2014},
volume={},
number={},
pages={1-8},
abstract={The recent development of information techniques, especially the state-of-the-art “big data” solutions, enables the extracting, gathering, and processing large amount of toxicity information from multiple sources. Facilitated by this technology advance, a framework named integrated testing strategies (ITS) has been proposed in the predictive toxicology domain, in an effort to intelligently jointly use multiple heterogeneous toxicity data records (through data fusion, grouping, interpolation/extrapolation etc.) for toxicity assessment. This will ultimately contribute to accelerating the development cycle of chemical products, reducing animal use, and decreasing development costs. Most of the current study in ITS is based on a group of consensus processes, termed weight of evidence (WoE), which quantitatively integrate all the relevant data instances towards the same endpoint into an integrated decision supported by data quality. Several WoE implementations for the particular case of toxicity data fusion have been presented in the literature, which are collectively studied in this paper. Noting that these uncertainty handling methodologies are usually not simply developed from conventional probability theory due to the unavailability of big datasets, this paper first investigates the mathematical foundations of these approaches. Then, the investigated data integration models are applied to a representative case in the predictive toxicology domain, with the experimental results compared and analysed.},
keywords={Reliability;Mathematical model;Data integration;Bayes methods;Equations;Uncertainty},
doi={10.1109/UKCI.2014.6930153},
ISSN={2162-7657},
month={Sep.},}
@INPROCEEDINGS{9615868,
author={Li, Ling and Li, Weibang and Zhu, Lidong and Li, Chengjie and Zhang, Zhen},
booktitle={2021 International Symposium on Networks, Computers and Communications (ISNCC)}, title={Automatic Data Repairs with Statistical Relational Learning},
year={2021},
volume={},
number={},
pages={1-6},
abstract={Dirty data is ubiquitous in real-world, and data cleaning is a long-standing problem. The importance of data cleaning is growing in the era of big data. In this paper we propose a novel data repairing approach by leveraging statistical relational learning (SRL). We learn Bayesian networks of attributes from the dirty data, then transform the dependency relationships among attributes into first-order logic formulas. We calculate the weight of each formula based on the mutual information of the attributes involved in the formula and obtain Markov logic network (often abbreviated as MLN) by assigning weight to each first-order logic formula. Then we transform Markov logic networks into inference rules and conduct these inference rules on DeepDive. The inference results are utilized to repair dirty data at last. Experiments on real-world datasets demonstrate that our approach has higher accuracy in terms of different situations and is universal for different kinds of datasets.},
keywords={Data integrity;Transforms;Markov processes;Maintenance engineering;Probabilistic logic;Cleaning;Inference algorithms;Data repairing;data cleaning;data quality;statistical relational learning;DeepDive;factor graph},
doi={10.1109/ISNCC52172.2021.9615868},
ISSN={},
month={Oct},}
@ARTICLE{8962328,
author={Konanahalli, Ashwini and Marinelli, Marina and Oyedele, Lukumon},
journal={IEEE Transactions on Engineering Management}, title={Drivers and Challenges Associated With the Implementation of Big Data Within U.K. Facilities Management Sector: An Exploratory Factor Analysis Approach},
year={2020},
volume={},
number={},
pages={1-14},
abstract={The recent advances in Internet of Things (IoT), computational analytics, processing power, and assimilation of Big Data (BD) are playing an important role in revolutionizing maintenance and operations regimes within the wider facilities management (FM) sector. The BD offers the potential for the FM to obtain valuable insights from a large amount of heterogeneous data collected through various sources and IoT allows for the integration of sensors. The aim of this article is to extend the exploratory studies conducted on Big Data analytics (BDA) implementation and empirically test and categorize the associated drivers and challenges. Using exploratory factor analysis (EFA), the researchers aim to bridge the current knowledge gap and highlight the principal factors affecting the BDA implementation. Questionnaires detailing 26 variables are sent to the FM organization in the U.K. who are in the process or have already implemented BDA initiatives within their FM operations. Fifty-two valid responses are analyzed by conducting EFA. The findings suggest that driven by market competition and ambitious sustainability goals, the industry is moving to holistically integrate analytics into its decision making. However, data quality, technological barriers, inadequate preparedness, data management, and governance issues and skill gaps are posing to be significant barriers to the fulfillment of expected opportunities. The findings of this study have important implications for FM businesses that are evaluating the potential of the BDA and IoT applications for their operations. Most importantly, it addresses the role of the BD maturity in FM organizations and its implications for perception of drivers.},
keywords={Frequency modulation;Organizations;Big Data;Maintenance engineering;Data mining;Internet of Things;Analytics;Big Data (BD);facilities management (FM);technology implementation},
doi={10.1109/TEM.2019.2959914},
ISSN={1558-0040},
month={},}
@INPROCEEDINGS{6352418,
author={Fomferra, Norman and Böttcher, Martin and Zühlke, Marco and Brockmann, Carsten and Kwiatkowska, Ewa},
booktitle={2012 IEEE International Geoscience and Remote Sensing Symposium}, title={Calvalus: Full-mission EO cal/val, processing and exploitation services},
year={2012},
volume={},
number={},
pages={5278-5281},
abstract={ESA's Earth Observation (EO) missions provide a unique dataset of observational data of our environment. Calibration, algorithm development and validation of the derived products are indispensable tasks for an efficient exploitation of EO data and form the basis for reliable scientific conclusions. In spite of its importance, the cal/val and algorithm development work is often hindered by insufficient means to access data, time consuming work used to identify suitable in-situ data matching the EO data, incompatible software and limited possibilities for a rapid prototyping and testing of ideas. In view of the amount of data produced by the future ESAs series of Sentinel satellites, a very efficient technological backbone is required to maintain the ability of ensuring data quality and algorithm performance. Brockmann Consult has developed such a backbone based on leading edge technologies within an ESA R&D study. Calvalus is a new processing system that utilises the map-reduce programming model with a distributed file system.},
keywords={File systems;Algorithm design and analysis;Calibration;Reliability;Servers;Programming;Data processing;Algorithm prototyping;data processing;big data handling;map-reduce;calibration/validation},
doi={10.1109/IGARSS.2012.6352418},
ISSN={2153-7003},
month={July},}
@INPROCEEDINGS{7406326,
author={Ahmadov, Ahmad and Thiele, Maik and Eberius, Julian and Lehner, Wolfgang and Wrembel, Robert},
booktitle={2015 IEEE/ACM 2nd International Symposium on Big Data Computing (BDC)}, title={Towards a Hybrid Imputation Approach Using Web Tables},
year={2015},
volume={},
number={},
pages={21-30},
abstract={Data completeness is one of the most important data quality dimensions and an essential premise in data analytics. With new emerging Big Data trends such as the data lake concept, which provides a low cost data preparation repository instead of moving curated data into a data warehouse, the problem of data completeness is additionally reinforced. While traditionally the process of filling in missing values is addressed by the data imputation community using statistical techniques, we complement these approaches by using external data sources from the data lake or even the Web to lookup missing values. In this paper we propose a novel hybrid data imputation strategy that, takes into account the characteristics of an incomplete dataset and based on that chooses the best imputation approach, i.e. either a statistical approach such as regression analysis or a Web-based lookup or a combination of both. We formalize and implement both imputation approaches, including a Web table retrieval and matching system and evaluate them extensively using a corpus with 125M Web tables. We show that applying statistical techniques in conjunction with external data sources will lead to a imputation system which is robust, accurate, and has high coverage at the same time.},
keywords={Indexes;Lakes;Companies;Big data;Data mining;Industries;Data analysis;Web mining;Data preprocessing;Machine learning},
doi={10.1109/BDC.2015.38},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9155928,
author={Tavakoli, Mohammadreza and Elias, Mirette and Kismihók, Gábor and Auer, Sören},
booktitle={2020 IEEE 20th International Conference on Advanced Learning Technologies (ICALT)}, title={Quality Prediction of Open Educational Resources A Metadata-based Approach},
year={2020},
volume={},
number={},
pages={29-31},
abstract={In the recent decade, online learning environments have accumulated millions of Open Educational Resources (OERs). However, for learners, finding relevant and high quality OERs is a complicated and time-consuming activity. Furthermore, metadata play a key role in offering high quality services such as recommendation and search. Metadata can also be used for automatic OER quality control as, in the light of the continuously increasing number of OERs, manual quality control is getting more and more difficult. In this work, we collected the metadata of 8,887 OERs to perform an exploratory data analysis to observe the effect of quality control on metadata quality. Subsequently, we propose an OER metadata scoring model, and build a metadata-based prediction model to anticipate the quality of OERs. Based on our data and model, we were able to detect high-quality OERs with the F1 score of 94.6%.},
keywords={Metadata;Quality control;Predictive models;Open Educational Resources;Measurement;Data analysis;OER;open educational resources;metadata quality;OER quality;Big data;data analysis;quality prediction},
doi={10.1109/ICALT49669.2020.00007},
ISSN={2161-377X},
month={July},}
@INPROCEEDINGS{7515750,
author={Abdellaoui, Sabrina and Bellatreche, Ladjel and Nader, Fahima},
booktitle={2016 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)}, title={A Quality-Driven Approach for Building Heterogeneous Distributed Databases: The Case of Data Warehouses},
year={2016},
volume={},
number={},
pages={631-638},
abstract={Data Warehouse (DW) is a collection of data, consolidated from several heterogeneous sources, used to perform data analysis and support decision making in an organization. Extract-Transform-Load (ETL) phase plays a crucial role in designing DW. To overcome the complexity of the ETL phase, different studies have recently proposed the use of ontologies. Ontology-based ETL approaches have been used to reduce heterogeneity between data sources and ensure automation of the ETL process. Existing studies in semantic ETL have largely focused on fulfilling functional requirements. However, the ETL process quality dimension has not been sufficiently considered by these studies. As the amount of data has exploded with the advent of big data era, dealing with quality challenges in the early stages of designing the process become more important than ever. To address this issue, we propose to keep data quality requirements at the center of the ETL phase design. We present in this paper an approach, defining the ETL process at the ontological level. We define a set of quality indicators and quantitative measures that can anticipate data quality problems and identify causes of deficiencies. Our approach checks the quality of data before loading them into the target data warehouse to avoid the propagation of corrupted data. Finally, our proposal is validated through a case study, using Oracle Semantic DataBase sources (SDBs), where each source references the Lehigh University BenchMark ontology (LUBM).},
keywords={Ontologies;Semantics;Measurement;Unified modeling language;Standards;Proposals;Data warehouse;ETL design;Ontologies;Data quality;Semantic Database Sources},
doi={10.1109/CCGrid.2016.79},
ISSN={},
month={May},}
@INPROCEEDINGS{8545080,
author={Hou, Jiaxin and Chen, Jing and Liao, Shijie and Wen, Junhao and Xiong, Qingyu},
booktitle={2018 24th International Conference on Pattern Recognition (ICPR)}, title={Predicting Traffic Flow via Ensemble Deep Convolutional Neural Networks with Spatio-temporal Joint Relations},
year={2018},
volume={},
number={},
pages={1487-1492},
abstract={Traffic flow prediction is a crucial task for the intelligent traffic management and control. Various machine learning based methods have been applied in this field. Most of these methods encounter three fundamental issues: feature representation of traffic patterns, learning from single location or network, and data quality. In order to address these issues, in this work we present a deep architecture for traffic flow prediction that learns deep hierarchical feature representation with spatio-temporal relations over the traffic network. Furthermore, we design an ensemble learning strategy via random subspace learning to make the model be able to tolerate incomplete data. The experimental results corroborate the effectiveness of the proposed approach compared with the state of the art methods.},
keywords={Predictive models;Data models;Optimization;Task analysis;Kernel;Convolutional neural networks},
doi={10.1109/ICPR.2018.8545080},
ISSN={1051-4651},
month={Aug},}
@INPROCEEDINGS{7785346,
author={Drakopoulos, Georgios and Megalooikonomou, Vasileios},
booktitle={2016 7th International Conference on Information, Intelligence, Systems Applications (IISA)}, title={Regularizing large biosignals with finite differences},
year={2016},
volume={},
number={},
pages={1-6},
abstract={In the biomedical analytics pipeline data preprocessing is the first and crucial step as subsequent results and visualization depend heavily on original data quality. However, the latter often contain a large number of outliers or missing values. Moreover, they may be corrupted by noise of unknown characteristics. This is in many cases aggravated by lack of sufficient information to construct a data cleaning mechanism. Regularization techniques remove erroneous values and complete missing ones while requiring little or no information regarding either data or noise dynamics. This paper examines the theory and practice of a regularization class based on finite differences and implemented through the conjugate gradient method. Moreover, it explores the connection of finite differences to the discrete Laplace operator. The results obtained from applying the proposed regularization techniques to heart rate time series from the MIT-BIH dataset are discussed.},
keywords={Time series analysis;Signal processing algorithms;Laplace equations;Cost function;Electrocardiography;Finite difference methods;Big data;Finite difference matrix;Regularization;Biosignal processing;Big data analytics;Conjugate gradient;Discrete Laplace operator;Electrocardiogram;Heartbeat rate},
doi={10.1109/IISA.2016.7785346},
ISSN={},
month={July},}
@INPROCEEDINGS{9760331,
author={Wang, Shiyang},
booktitle={2022 7th International Conference on Big Data Analytics (ICBDA)}, title={The Prediction Method of KPIs by Using LS-TSVR},
year={2022},
volume={},
number={},
pages={177-180},
abstract={Closely monitoring service performance and making predictions of Key Performance Indicators (KPIs) are critical for Internet-based services. However, fast yet accurate prediction of these seasonal KPIs with various patterns and data quality has been a great challenge. This paper tackles this challenge through a novel approach based on auto-regressive Least Square Twin Support Vector Regression (LS-TSVR). As an improved version of SVR, LS-TSVR can handle big data without any external optimization, and meanwhile, the prediction accuracy is better than that of SVR. For seasonal KPI data in a production dataset, our methods satisfy or approximate a mean average error (MAE) of around 0.013, which is significantly lower than the baseline method.},
keywords={Support vector machines;Data integrity;Key performance indicator;Time series analysis;Production;Prediction methods;Big Data;time series prediction;support vector regression;least square approximation},
doi={10.1109/ICBDA55095.2022.9760331},
ISSN={},
month={March},}
@INPROCEEDINGS{9307781,
author={López-Acosta, Araceli and García-Hernández, Alejandra and Vázquez-Reyes, Sodel and Mauricio-González, Alejandro},
booktitle={2020 8th International Conference in Software Engineering Research and Innovation (CONISOFT)}, title={A Metadata Application Profile to Structure a Scientific Database for Social Network Analysis (SNA)},
year={2020},
volume={},
number={},
pages={208-215},
abstract={There are a number of challenges associated to metadata in its different applications including data quality, data acquisition, computing resources, interoperability, and discoverability. This work presents an approach to structure metadata of scientific information for social network analysis based on an academic case study from scientific articles published by universities, to evaluate the area of risk assessment. Studying metadata for scientists' social networks helps identify authors' relevance based on their position within the network. By using Elasticsearch (ES) and Python technologies, this work addresses big data analysis issues related to data structure and volume, given ES full-text search engine capabilities for indexing and searching data, and Python's processing support. The data is obtained from the ArnetMiner (Aminer) open scientific database providing a fresh overview of scientific records up to January 2019. From a sample of 64,070 publications, a total of 45, 000 relations are graphed in a co-authorship network. Through the computation of network centrality measures, this work identifies central-positioned authors, clusters of research, and their affiliations. The results show that degree centrality is an important measure to identify prominent scientists in this co-authorship network, and closeness and betweenness centralities together are dominant measures to pinpoint the key players in the flow of information within the network. We conclude that the application of this approach allows rapid full-text search, visualizing dense co-authorship networks, and identifying central authors through centrality metrics. The results presented in this work can help researchers or research groups identify key research collaborators, multi-disciplinary areas, and international stakeholders.},
keywords={Metadata;Social networking (online);Databases;Search engines;Interoperability;Internet;Libraries;Scientific Data;Metadata;Elasticsearch;Social Network Analysis},
doi={10.1109/CONISOFT50191.2020.00038},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7364059,
author={Wu, Chieh-Han and Song, Yang},
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, title={Robust and distributed web-scale near-dup document conflation in microsoft academic service},
year={2015},
volume={},
number={},
pages={2606-2611},
abstract={In modern web-scale applications that collect data from different sources, entity conflation is a challenging task due to various data quality issues. In this paper, we propose a robust and distributed framework to perform conflation on noisy data in the Microsoft Academic Service dataset. Our framework contains two major components. In the offline component, we train a GBDT model to determine whether two papers from different sources should be conflated to the same paper entity. In the online component, we propose a scalable shingling algorithm that can apply our offline model to over 100 million instances. The result shows that our algorithm can conflate noisy data robustly and efficiently.},
keywords={Algorithm design and analysis;Noise measurement;Resource management;Data models;Robustness;Computational modeling;Proteins;Near-duplicate detection;shingling algorithm;n-gram;entity conflation},
doi={10.1109/BigData.2015.7364059},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9194651,
author={Müftüoğlu, Zümrüt and Kizrak, M. Ayyüce and Yildlnm, Tülay},
booktitle={2020 International Conference on INnovations in Intelligent SysTems and Applications (INISTA)}, title={Differential Privacy Practice on Diagnosis of COVID-19 Radiology Imaging Using EfficientNet},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Medical sciences are an important application area of artificial intelligence. Healthcare requires meticulousness in the whole process from collecting data to processing. It should also be handled in terms of data quality, data size, and data privacy. Various data are used within the scope of the COVID-19 outbreak struggle. Medical and location data collected from mobile phones and wearable devices are used to prevent the spread of the epidemic. In addition to this, artificial intelligence approaches are presented by using medical images in order to identify COVID-19 infected people. However, studies should be carried out by taking care not to endanger the security of the data, people, and countries needed for these useful applications. Therefore, differential privacy (DP) application, which was an interesting research subject, has been included in this study. CXR images have been collected from COVID-19 infected 139 and a total of 373 public data sources were used for a diagnostic concept. It has been trained with EfficientNet- B0, a recent and robust deep learning model, and proposal the possibility of infected with an accuracy of 94.7%. Other evaluation parameters were also discussed in detail. Despite the data constraint, this performance showed that it can be improved by augmenting the dataset. The most important aspect of the study was the proposal of differential privacy practice for such applications to be reliable in real-life use cases. With this view, experiments were repeated with DP applied images and the results obtained were presented. Here, Private Aggregation of Teacher Ensembles (PATE) approach was used to ensure privacy assurance.},
keywords={Machine learning;Privacy;Computed tomography;Medical diagnostic imaging;COVID-19;deep learning;EfficientNet;X-Ray;radiology imaging;PATE;differential privacy},
doi={10.1109/INISTA49547.2020.9194651},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9060131,
author={Wang, Liang and Yang, Congying and Yu, Zhiwen and Liu, Yimeng and Wang, Zhu and Guo, Bin},
booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence Computing, Advanced Trusted Computing, Scalable Computing Communications, Cloud Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)}, title={CrackSense: A CrowdSourcing Based Urban Road Crack Detection System},
year={2019},
volume={},
number={},
pages={944-951},
abstract={As a common road surface distress, cracks pose a serious threat to road infrastructure and traffic safety in cities today. Consequently, road crack detection is considered as an essential step for effective road maintenance and road structure sustainability. However, due to the high cost incurred by dedicated devices and professional operators, it is impossible for existing systems to achieve universal spatiotemporal coverage across citywide road networks. To fill this gap, in this paper, we present the CrackSense, a mobile crowdsourcing based system to detect urban road crack and estimate its damage degree. Specifically, for the heterogeneous crack data, we put forward a crowdsourcing data quality evaluation and selection mechanism. And then, by utilizing the multi-source sensing data aggregation, we propose tow algorithms, namely RCTR and RCDE, to recognize road crack types, i.e., horizontal crack, vertical crack, and net crack, and estimate the crack damage degree, respectively. We implement the system and develop a smartphone APP for mobile users. By conducting intensive experiments and field study, the results demonstrate the accuracy and effectiveness of our proposed approaches.},
keywords={Roads;Crowdsourcing;Sensors;Estimation;Data models;Data integrity;Three-dimensional displays;Mobile crowdsourcing;road crack detection;image processing;sensors},
doi={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00188},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7724270,
author={Gopal, R. Chandangole and Bharat, A. Tidke},
booktitle={2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)}, title={A generic tool to process mongodb or Cassandra dataset using Hadoop streaming},
year={2016},
volume={},
number={},
pages={273-276},
abstract={Now a days Bulk of data generating on the system. This data is very important for user and today's user accessing, searching and sorting the data from database is very difficult. To overcome this problem, data is distributed in different node using Hadoop technology. A system is proposed in which the collected data is to be distributed using map reduce technique for sorting the data is very easily on Hadoop environment. In this case used Cassandra and mongodb tools to storing large amount of data on Hadoop Framework. NoSQL data is stores in unstructured data format which is a key focus area for “Big Data” research. The quantity and quality of unstructured data growing high. The Hadoop Framework used to large amount of data on a different nodes in a cluster data. NoSQL databases using different structure and unstructured data of high scalability for getting high performance of system. To present the approaches solving Problem of NoSQL data to stores with MapReduce process to under in non-Java application. A Cassandra is to provide the platform for the fast and efficient data queries. In this paper presents the tools of the Cassandra and the mongodb using NoSQL database for connecting different node with the Hadoop MapReduce engine.},
keywords={Decision support systems;Handheld computers;Conferences;Hadoop Streaming;Cassandra;Mongodb;MapReduce},
doi={},
ISSN={},
month={March},}
@INPROCEEDINGS{6729628,
author={Xie, Sihong and Kong, Xiangnan and Gao, Jing and Fan, Wei and Yu, Philip S.},
booktitle={2013 IEEE 13th International Conference on Data Mining}, title={Multilabel Consensus Classification},
year={2013},
volume={},
number={},
pages={1241-1246},
abstract={In the era of big data, a large amount of noisy and incomplete data can be collected from multiple sources for prediction tasks. Combining multiple models or data sources helps to counteract the effects of low data quality and the bias of any single model or data source, and thus can improve the robustness and the performance of predictive models. Out of privacy, storage and bandwidth considerations, in certain circumstances one has to combine the predictions from multiple models or data sources without accessing the raw data. Consensus-based prediction combination algorithms are effective for such situations. However, current research on prediction combination focuses on the single label setting, where an instance can have one and only one label. Nonetheless, data nowadays are usually multilabeled, such that more than one label have to be predicted at the same time. Direct applications of existing prediction combination methods to multilabel settings can lead to degenerated performance. In this paper, we address the challenges of combining predictions from multiple multilabel classifiers and propose two novel algorithms, MLCM-r (MultiLabel Consensus Maximization for ranking) and MLCM-a (MLCM for microAUC). These algorithms can capture label correlations that are common in multilabel classifications, and optimize corresponding performance metrics. Experimental results on popular multilabel classification tasks verify the theoretical analysis and effectiveness of the proposed methods.},
keywords={Predictive models;Correlation;Measurement;Prediction algorithms;Data models;Bipartite graph;Algorithm design and analysis;multilabel classification;ensemble},
doi={10.1109/ICDM.2013.97},
ISSN={2374-8486},
month={Dec},}
@INPROCEEDINGS{7509814,
author={Fang, Dianjun and Zhang, Yin and Spicher, Klaus},
booktitle={2016 IEEE International Conference on Big Data Analysis (ICBDA)}, title={Forecasting accuracy analysis based on two new heuristic methods and Holt-Winters-Method},
year={2016},
volume={},
number={},
pages={1-6},
abstract={Since 1970s, many academic researchers and business practitioners have started to develop different forecasting methods and models. Most of them are still used in the IT-Systems nowadays. However, they don't perform well enough in practice. People pay much attention to data collection but ignore the data quality, which could lead to low forecasting accuracy. In this paper, we will introduce two new heuristic business forecasting techniques (Revinda and Metrix). Both methods utilize inherent structures of time series. The error analysis is based on B2C and B2B aggregated commercial data. In addition, these two methods will be compared with HOLT-WiNTERS-Methods (HWM) by using error measures MAPE, percentage better and THEIL's U2.},
keywords={Forecasting;Time series analysis;Market research;Mathematical model;Measurement;Predictive models;time series;forecasting method;data quality;error measurement;B2C/B2B forecasting},
doi={10.1109/ICBDA.2016.7509814},
ISSN={},
month={March},}
@ARTICLE{9693431,
author={Kardash, Adam and Morin, Suzanne},
journal={IEEE Security Privacy}, title={The Practices and Challenges of Generating Nonidentifiable Data},
year={2022},
volume={20},
number={1},
pages={113-118},
abstract={This article summarizes the key findings of a Canadian Anonymization Network study of several large data custodians who utilize deidentification and similar privacy-enhancing processes prior to engaging in analytics, secondary uses, and disclosure of personal information.},
keywords={Data privacy;Big data;Computer security;Data quality},
doi={10.1109/MSEC.2021.3126185},
ISSN={1558-4046},
month={Jan},}
@INPROCEEDINGS{6727311,
author={Martelli, Cristina and Bellini, Emanuele},
booktitle={2013 International Conference on Signal-Image Technology Internet-Based Systems}, title={Using Value Network Analysis to Support Data Driven Decision Making in Urban Planning},
year={2013},
volume={},
number={},
pages={998-1003},
abstract={This article provides a methodology of assessing the (Big)/(Open) Data quality in Data Driven Decision Making with the Value Network Analysis approach discovering the value creation failure point(s) in the network and evaluating the impact of loss of vale of data in DDDM process.},
keywords={Cities and towns;Decision making;IEEE 802.11 Standards;Time series analysis;Context;Urban planning;Data Driven Decision Making;Value Network Analysis;urban planning},
doi={10.1109/SITIS.2013.161},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7724956,
author={Bhargava, Deepshikha and Poonia, Ramesh C. and Arora, Upma},
booktitle={2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)}, title={Design and development of an intelligent agent based framework for predictive analytics},
year={2016},
volume={},
number={},
pages={3715-3718},
abstract={The arrival of World Wide Web has led to the explosive growth of information on the web. There is a sudden boom in quality of raw data/information, which is commonly referred as Big Data. Very often, this raw information contains very useful insights which are ignored most of the time and are difficult to analyze due to the enormous size of these datasets. The feasibility for human to extract this information from the vast web and build useful application on the top of it, is very low. Hence to create predictive models, there is a huge need for intelligent and autonomous software agents which can procure useful information from the large datasets of raw information. Predictive analytics models can be created from these datasets which can be further used for various applications in security, future prediction etc. This research paper gives an overview of how these software agents will become the most important tools in coming days for building predictive models.},
keywords={Intelligent agents;Predictive models;Analytical models;Market research;Conferences;Data mining;Software Agents;Intelligent Agents;Predictive Analytics},
doi={},
ISSN={},
month={March},}
@INPROCEEDINGS{7982314,
author={Makoondlall, Y.K. and Khaddaj, S. and Makoond, B. and Kethan, K.},
booktitle={2016 IEEE Intl Conference on Computational Science and Engineering (CSE) and IEEE Intl Conference on Embedded and Ubiquitous Computing (EUC) and 15th Intl Symposium on Distributed Computing and Applications for Business Engineering (DCABES)}, title={ZDLC : Layered Lineage Report across Technologies},
year={2016},
volume={},
number={},
pages={638-641},
abstract={As technology moves from being an enabler to become the lifeblood of businesses in the digital era, so does the data used to support the implementation of these technological shifts. In order for the data to be an asset and not a liability, it is primordial to ensure that the data captured and maintained over time is accurate, traceable, reliable and current. However, very often, in order to complete a business function the data from one system may be exported to another system or to a third party tool, which also changes the data. In order to keep track of the data and maximize the use and analysis of data, the Zero Deviation Life Cycle (ZDLC) framework proposes a series of tools to which can trace the data lineage, across several database technologies.},
keywords={Databases;Tools;Software;Big Data;Data mining;Companies;Zero Deviation Life Cycle (ZDLC);ZDLC;layered lineage;lineage report;data lineage;data quality},
doi={10.1109/CSE-EUC-DCABES.2016.252},
ISSN={},
month={Aug},}
@ARTICLE{7762161,
author={Wang, Hongbing and Wang, Lei and Yu, Qi and Zheng, Zibin},
journal={IEEE Transactions on Services Computing}, title={Learning the Evolution Regularities for BigService-Oriented Online Reliability Prediction},
year={2019},
volume={12},
number={3},
pages={398-411},
abstract={Service computing is an emerging technology in System of Systems Engineering (SoS Engineering or SoSE), which regards a System as a Service, and aims at constructing a robust and value-added complex system by outsourcing external component systems through service composition. The burgeoning Big Service computing just covers the significant challenges in constructing and maintaining a stable service-oriented SoS. A service-oriented SoS runs under a volatile and uncertain environment. As a step toward big service, service fault tolerance (FT) can guarantee the run-time quality of a service-oriented SoS. To successfully deploy FT in an SoS, online reliability time series prediction, which aims at predicting the reliability in near future for a service-oriented SoS arises as a grand challenge in SoS research. In particular, we need to tackle a number of big data related issues given the large and fast increasing size of the historical data that will be used for prediction purpose. The decision-making of prediction solution space be more complex. To provide highly accurate prediction results, we tackle the prediction challenges by identifying the evolution regularities of component systems' running states via different machine learning models. We present in this paper the motifs-based Dynamic Bayesian Networks (or m_DBNs) to perform one-step-ahead online reliability time series prediction. We also propose a multi-steps trajectory DBNs (or multi_DBNs) to further improve the accuracy of future reliability prediction. Finally, a Convolutional Neural Networks (CNN)-based prediction approach is developed to deal with the big data challenges. Extensive experiments conducted on real-world Web services demonstrate that our models outperform other well-known approaches consistently.},
keywords={Reliability;Time series analysis;Web services;Computer network reliability;Meteorology;Big data;Quality of service;Temporal evolution regularities;online reliability prediction;big service;convolutional neural networks},
doi={10.1109/TSC.2016.2633264},
ISSN={1939-1374},
month={May},}
@INPROCEEDINGS{8622435,
author={Matsubara, Masaki and Kobayashi, Masaki and Morishima, Atsuyuki},
booktitle={2018 IEEE International Conference on Big Data (Big Data)}, title={A Learning Effect by Presenting Machine Prediction as a Reference Answer in Self-correction},
year={2018},
volume={},
number={},
pages={3522-3528},
abstract={Can people learn from machines behavior in microtask based crowdsourcing? Can we train the machines as our mentor even without domain expertise? In this paper, we investigate how the task results improve concerning quality during and after presenting machine prediction as a reference answer in self-correction. Four reference types were examined in the experiment; Correct, Random, Machine prediction trained by correct answers, and that trained by human answers. Learning effects were observed only in presenting machine prediction, although those accuracy rates were far from correct (100%). Moreover, there were no learning effects in "Correct" and "Random". This suggests the following hypothesis: Since machine learners make some "models" for the problem, it is easier for humans to interpret the outputs of machine learners than the results without via them; it is more difficult to interpret not only random answers but also the correct answers in a case where the perfect interpretation of the problem is difficult. Furthermore, some workers answered with higher accuracy rate than machines in the post-test. Therefore, this strategy can be expected to be useful for bootstrapping solutions in the situation where unknown problems occur without expertise or at a low cost.},
keywords={Task analysis;Painting;Training data;Quality assurance;Training;Machine learning;Crowdsourcing;Quality Assurance;Self Correction;Machine Teaching},
doi={10.1109/BigData.2018.8622435},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8877003,
author={Palacio, Ana León and López, Óscar Pastor},
booktitle={2019 13th International Conference on Research Challenges in Information Science (RCIS)}, title={Infoxication in the Genomic Data Era and Implications in the Development of Information Systems},
year={2019},
volume={},
number={},
pages={1-9},
abstract={We live in an age where data acquisition is no longer a problem and the real challenge is how to determine which information is the right one to take important and sometimes difficult decisions. Infoxication (also known as Infobesity or Information Overload) is a term used to describe the difficulty of adapting to new situations and effectively making decisions when there is too much information to manage. With the advent of the Big Data, infoxication is affecting critical domains such as Health Sciences, where tough decisions for patient's health is being taken every day based on heterogeneous, unconnected and sometimes conflicting information. In order to understand the magnitude of the challenge, based on the information publicly available about the genetic causes of the disease and using data quality assessment techniques, we performed an exhaustive analysis of the DNA variations that have been associated to the risk of suffering migraine headache. The same analysis has been repeated 8 months after, and the results have allowed us to exemplify i) how fragile is the information in this domain, ii) the difficulty of finding repositories of contrasted and reliable data, and iii) the need to have information systems that, far from integrating and storing huge volumes of data, are able to support the decision-making process by providing mechanisms agile and flexible enough to be able to adapt to the changing user needs.},
keywords={Bioinformatics;Genomics;Diseases;DNA;Task analysis;Databases;Infoxication;Genomics;Information Systems;SILE method},
doi={10.1109/RCIS.2019.8877003},
ISSN={2151-1357},
month={May},}
@INPROCEEDINGS{8400228,
author={Pełech-Pilichowski, T.},
booktitle={2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)}, title={On adaptive prediction of nonstationary and inconsistent large time series data},
year={2018},
volume={},
number={},
pages={1260-1265},
abstract={The use of time series prediction results in benefits for an organization. Forecasting efficiency relies on applied prediction formula and quality of data received from technical devices and manually inputted. They are often of low quality, with inconsistencies. However, high data quality is crucial for efficient forecasting/prediction purposes (also event detection from time series and pattern recognition), in particular during large data sets processing (often heterogeneous, including data obtained from IoT devices). Such processing should cover inconsistency analysis, interpolation of missing/lacking data, as well as the use of data pre-transformations. The paper presents problems of inconsistent, nonstationary data prediction on the example of stock level daily forecasting. Selected methods of time series interpolation are outlined. Results of implementation of algorithms for short-term time series prediction are illustrated and discussed. Prediction quality measured based on errors values calculated both in total and in a moving window is discussed. A concept of an adaptive algorithm based on a change in the prognostic formula depending on short-term characteristics of time series is outlined.},
keywords={Time series analysis;Interpolation;Predictive models;Forecasting;Prediction algorithms;Extrapolation;Adaptation models;time series analysis;prediction;forecasting;interpolation;adaptive prediction algorithms;Big Data;IoT},
doi={10.23919/MIPRO.2018.8400228},
ISSN={},
month={May},}
@INPROCEEDINGS{8712843,
author={Feng, Zikun and Yang, Haojie and Li, Xinyi and Li, Yan and Liu, Zhao and Liu, Ryan Wen},
booktitle={2019 IEEE 4th International Conference on Big Data Analytics (ICBDA)}, title={Real-Time Vessel Trajectory Data-Based Collison Risk Assessment in Crowded Inland Waterways},
year={2019},
volume={},
number={},
pages={128-134},
abstract={With the rapid development of maritime industries, the vessel traffic density has been gradually increased leading to increasing the potential risk of ship collision accidents in crowded inland waterways. It will bring negative effects on human life safety and enterprise economy. Therefore, it is of vital significance to study the risk of ship collision in practical applications. This paper proposes to quantitatively estimate the ship collision risk based on ship domain modeling and real-time vessel trajectory data. In particular, the trajectory data quality is improved using the cubic spline interpolation method. We assume that the ship collision risk is highly related to the cross areas of ship domains between different ships, which are computed via the Monte Carlo probabilistic algorithm. For the sake of better understanding, the kernel density estimation method is adopted to visually generate the ship collision risk in maps. Experimental results have illustrated the effectiveness of the proposed method in crowded inland waterways.},
keywords={Marine vehicles;Artificial intelligence;Interpolation;Trajectory;Navigation;Rivers;Accidents;Ship domain;trajectory data;ship collision risk;automatic identification system;Monte Carlo method},
doi={10.1109/ICBDA.2019.8712843},
ISSN={},
month={March},}
@ARTICLE{9261414,
author={Duan, Gui-Jiang and Yan, Xin},
journal={IEEE Access}, title={A Real-Time Quality Control System Based on Manufacturing Process Data},
year={2020},
volume={8},
number={},
pages={208506-208517},
abstract={Quality prediction is one of the key links of quality control. Benefitting from the development of digital manufacturing, manufacturing process data have grown rapidly, which allows product quality predictions to be made based on a real-time manufacturing process. A real-time quality control system (RTQCS) based on manufacturing process data is presented in this paper. In this study, the relationship between the product real-time quality status and processing task process was established by analyzing the relationship between the product manufacturing resources and the quality status. The key quality characteristics of the product were identified by analyzing the similarity of the product quality characteristic variations in the manufacturing process based on the big data technology, and a quality-resource matrix was constructed. Based on the quality-resource matrix, the RTQCS was established by introducing an association-rule incremental-update algorithm. Finally, the RTQCS was applied in actual production, and the performance of RTQCS was verified by experiments. The experiments showed that the RTQCS can effectively guarantee the quality of product manufacturing and improve the manufacturing efficiency during production.},
keywords={Manufacturing processes;Production;Quality control;Product design;Real-time systems;Manufacturing;Quality assessment;Quality management;production control;prediction methods},
doi={10.1109/ACCESS.2020.3038394},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8410313,
author={Wang, Sheng and Fu, Lieyong and Yao, Jianmin and Li, Yun},
booktitle={2018 International Conference on Robots Intelligent System (ICRIS)}, title={The Application of Deep Learning in Biomedical Informatics},
year={2018},
volume={},
number={},
pages={391-394},
abstract={The expansion of big data in biomedical and health field has driven the need of new effective analysis technology. Deep learning is a powerful machine learning method. With the contribution of rapid computational power improvement, it is becoming a promising technique to generate new knowledge, interpretation and gain insights from high-throughout, heterogeneous and complex biomedical data from different sources, such as medical imaging, clinical genomics, and electronic health records. This paper presents an overview of the application of deep learning approach in the biomedical informatics. First we introduce the development of artificial neural network and deep learning, then mainly focus on the researches applying deep learning in biomedical informatics field. We also discuss the challenges for future improvement, such as data quality and interpretability.},
keywords={Conferences;Robots;Intelligent systems;Deep Learning;Healthcare;Biomedical informatics},
doi={10.1109/ICRIS.2018.00104},
ISSN={},
month={May},}
@INPROCEEDINGS{7997260,
author={Li, Peng and Luo, Hong and Wu, Tin-Yu and Obaidat, Mohammad S.},
booktitle={2017 IEEE International Conference on Communications (ICC)}, title={QoS prediction method for data supply chain based on context},
year={2017},
volume={},
number={},
pages={1-7},
abstract={Due to the execution paradigm may be different at different invocation time, users obtain different QoS when interacting with the same Data Supply Chain (DSC). However, existing QoS prediction methods seldom took this observation into consideration, which shall decrease the prediction accuracy. In this paper, we propose a context-based QoS prediction method for data supply chain. First, a QoS mathematical model is developed for considering the mass data transmission across elementary sub-chains. Then, two execution paradigms of data supply chain are discussed. Besides, we explored several special context factors of data supply chain (such as invocation time, data source update period and execution paradigm) which influence QoS. By processing such context information, we can obtain the part of data supply chain which is need to execute when the user query occurs and leverage them to predict QoS. Experimental results indicate that our approach improves the prediction accuracy and efficiency of QoS when compared to previous methods.},
keywords={Quality of service;Supply chains;Web services;Mathematical model;Prediction methods;Time factors;Corporate acquisitions;Data supply chain;QoS model;processing context;QoS prediction},
doi={10.1109/ICC.2017.7997260},
ISSN={1938-1883},
month={May},}
@INPROCEEDINGS{9136335,
author={Zhichun, Yang and Yu, Shen and Fan, Yang and Yang, LEI and Lei, Su and Fangbin, Yan},
booktitle={2020 5th Asia Conference on Power and Electrical Engineering (ACPEE)}, title={Topology identification method of low voltage distribution network based on data association analysis},
year={2020},
volume={},
number={},
pages={2226-2230},
abstract={This paper introduces a topology identification method of low-voltage distribution network based on data association analysis. The low-voltage distribution network to be identified is divided into the single distribution transformer power off station areas, multiple distribution transformer station areas caused by 10kV distribution line power outage and the distribution transformer areas without power interruption based on low-voltage distribution network blackout event, restoration power on event and geographic location information. In each type of station area, Tanimoto similarity coefficient is used to calculate the correlation and non-correlation between distribution transformer, branch box, meter box and smart meter in each group, so as to achieve the topology identification of the low-voltage distribution network. And then the identified topology can be verified by combining the topology verification rules of the same distribution transformer station area has the same of outage and live state, outage duration, geographical location, power supply radius and so on. Through the actual case, it is proved that the method proposed in this paper can solve the problems of large amount of calculation, inaccuracy of calculation results, and inability to verify based on the existing big data mining methods. It realizes the efficient and accurate identification of distribution transformer substation topology, and improves the information level and data quality of distribution network.},
keywords={Meters;Low voltage;Substations;Network topology;Power supplies;Distribution networks;Transformers;Low voltage distribution network;Internet of things;topology identification;association analysis;topology verification},
doi={10.1109/ACPEE48638.2020.9136335},
ISSN={},
month={June},}
@INPROCEEDINGS{9149070,
author={Yang, Wanting and Chi, Xuefen and Zhao, Linlin},
booktitle={ICC 2020 - 2020 IEEE International Conference on Communications (ICC)}, title={Proactive VoD delivery pattern reconfiguration based on temporal-spatial channel prediction},
year={2020},
volume={},
number={},
pages={1-7},
abstract={With the help of big data analytics, predictive resource allocation (PRA) techniques for video on demand (VoD) have been recognized as promising methods to save time-frequency resources, for a number of VoD packets can be transmitted in good channels in advance to avoid the predicted transmissions in bad channel conditions. With the increasing demands on a fantastic user quality of experience, a smooth playback and a low start-up delay are of equal importance to the emerging VoD with high fidelity, which inevitably leads to a critical delay requirement of VoD packets. However, the issue of resource estimation with quality of service (QoS) requirements is still an unsolved puzzle in PRA. In this paper, we propose a martingales-based physical resource block (PRB) abstraction method, where the random characteristics of the service process are embedded in the minimum PRB consumption. Based on the method, a proactive QoS-guaranteed reconfiguration algorithm is developed to optimize the multi-user delivery pattern applied in the prediction window, aiming to maximize spectrum efficiency. In this algorithm, since the delay sensitivity of VoD content transmitted in advance is dulled compared with the original VoD stream, we divide the original VoD slice into two sub-slices and derive a three-dimensional delivery pattern. The gain of resource saving and the capability of QoS guarantee brought by the reconfiguration have been demonstrated by the simulation results.},
keywords={Delays;Quality of service;Prediction algorithms;Resource management;Estimation;Probability;Big Data;channel state prediction;reconfiguration;delivery pattern;martingales;VoD;spectrum efficiency;delay-QoS},
doi={10.1109/ICC40277.2020.9149070},
ISSN={1938-1883},
month={June},}
@INPROCEEDINGS{9351123,
author={Ouyang, Jianna and Liang, Shuo and Chen, Shaonan and Li, Shan and Zhou, Yangjun and Liwen, QIN},
booktitle={2020 IEEE Sustainable Power and Energy Conference (iSPEC)}, title={Design and Realization of Data Application Architecture Oriented to the Requirements of Distribution Network},
year={2020},
volume={},
number={},
pages={2354-2359},
abstract={In recent years, the rapid growth of all kinds of data and information in power grid has brought great challenges to the safe and stable operation and data analysis of the system. This paper constructs the data application architecture oriented to the requirements of distribution network based on the data requirements of reliability and economy evaluation, operation state evaluation and weak link identification, asset operation efficiency evaluation and lean management. It can realize the functions of data automatic classification storage, data processing, data quality monitoring and evaluation, multi-source heterogeneous data fusion and hierarchical classification database construction, etc. It supports the lean management of production business in distribution network comprehensively.},
keywords={Data integration;Systems architecture;Distribution networks;Production;Big Data applications;Reliability engineering;Business;distribution network;application requirements;data application architecture;design and realization},
doi={10.1109/iSPEC50848.2020.9351123},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9197774,
author={Ahlawat, Deepak and Kaur, Amandeep and Gupta, Deepali},
booktitle={2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)}, title={Enhancement of the Accuracy and QoS in Clustering of Data},
year={2020},
volume={},
number={},
pages={849-853},
abstract={Clustering is an important data mining and tool for examining data. The paper compares the two techniques of clustering, in the first technique only Cosine similarity is used and in the second technique Improved Rank Similarity (Cosine Similarity + Gaussian Similarity) is used. The results are compared with various parameters constituting the Accuracy in NetBeans and QoS parameters using AODV routing protocol. The simulation is done on MATLAB, a network is created and communication from source node to target node is noted.},
keywords={Throughput;Clustering algorithms;Big Data;Quality of service;Data mining;Tools;Matlab;Clustering;Cosine Similarity;Gaussian Similarity;Hybrid Similarity;AODV},
doi={10.1109/ICRITO48877.2020.9197774},
ISSN={},
month={June},}
@INPROCEEDINGS{7378670,
author={Yan Zhou and Haitian Xie},
booktitle={2015 23rd International Conference on Geoinformatics}, title={The integration technology of sensor network based on web crawler},
year={2015},
volume={},
number={},
pages={1-7},
abstract={Along with the development of the sensor system and sensor network, the wide applications of sensor networks have arisen at the historic moment. In reality, all kinds of sensors monitor every aspect of our life, which provides various services and brings the challenge: how to effectively integrate those distributed sensor resources and then can be used to find more advanced information or implement the sharing of resources are the big problems to be solved. Based on the framework of Sensor Web Enablement(SWE) which was proposed by Open GIS Consortium (OGC)and combined with the function of web crawler, we study and find Sensor Observation Service (SOS) service which is the core components of the SWE then we design a system based on the web crawler technology and the Istituto Scienze della Terra Sensor Observation Service (Istsos) architecture. The design of sensor network technology integration architecture includes three parts. The layer of data access which is the lowest layer encapsulates the access to the database or other source of resources. The layer of business logic it provides the core operation of component which was named Request Operator, this layer is used for processing various requests from the lowest layer in order to return the classes of listening. The layer of web and the client is connected, which can provide some thin client of SOS. The published server includes the ability of new services creation, addition of new sensors and relative metadata, visualization, and manipulation of stored observations, registration of new measures and setting of system properties like observable properties and data quality codes. In order to get sensor data, web crawler technology is used in our research, which can make us get sensor data from the target website, and the standardized sensor data is gotten by filtering the original data and then the data is uploaded to the database of Istsos with the standardized format. At last, the implementation of SOS architecture has been configured. The test's results show that the integrated architecture of services can effectively obtain the required sensor data and display them graphically.},
keywords={Service-oriented architecture;web crawler;sensor network;Sensor Observation Service (SOS);Tomact;Istsos},
doi={10.1109/GEOINFORMATICS.2015.7378670},
ISSN={2161-024X},
month={June},}
@INPROCEEDINGS{9041478,
author={Ding, Jian and Ma, Chunlei and Fu, Bin and Liu, Bing},
booktitle={8th Renewable Power Generation Conference (RPG 2019)}, title={Active distribution network state estimation algorithm based on decision tree of self-identification},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Under the background of active distribution network, this paper proposes a state estimation algorithm of managing analysis data to solve the problem of big data, data missing and complex analysis. This paper proposes an active distribution network state estimation algorithm based on decision tree self-identification. Setting appropriate quality weight of big data based on the check rules different from traditional single-phase currents. Data in the input state estimation model is better compatible by estimating the pre-processed data, classifying and correcting the data including voltage and current. Moreover, on the premise of lacking of distributed energy measurement devices, this paper establishes a state estimation model for distributed power, which is used to correct the default data of distributed energy and improve the quality of input data in wind power and photovoltaic. The method can be verified in the actual example. Compared with the traditional state estimation, the active distribution network state estimation algorithm based on decision tree self-identification has better estimation effect and faster iteration speed. Therefore, the proposed algorithm can be effectively applied to the current state estimation of large-scale distributed energy access.},
keywords={DATA CHECK;DATA QUALITY IDENTIFICATION;DECISION TREE;STATE ESTIMATION;BIG DATA},
doi={10.1049/cp.2019.0490},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6354737,
author={Curé, Olivier and Kerdjoudj, Fadhela and Faye, David and Le Duc, Chan and Lamolle, Myriam},
booktitle={2012 Third International Conference on Emerging Intelligent Data and Web Technologies}, title={On the Potential Integration of an Ontology-Based Data Access Approach in NoSQL Stores},
year={2012},
volume={},
number={},
pages={166-173},
abstract={No SQL stores are emerging as an efficient alternative to relational database management systems in the context of big data. Many actors in this domain consider that to gain a wider adoption, several extensions have to be integrated. Some of them focus on the ways of proposing more schema, supporting adapted declarative query languages and providing integrity constraints in order to control data consistency and enhance data quality. We consider that these issues can be dealt with in the context of Ontology Based Data Access (OBDA). OBDA is a new data management paradigm that exploits the semantic knowledge represented in ontologies when querying data stored in a database. We provide a proof of concept of OBDA's ability to tackle these three issues in a social application related to the medical domain.},
keywords={Ontologies;Diseases;Indexes;Drugs;Semantics;Context;Ontology Based Data Access (OBDA);NoSQL;Document store;SPARQL;Social Application},
doi={10.1109/EIDWT.2012.27},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7588816,
author={Wang, Fei and Wang, Hongbo},
booktitle={2016 IEEE 14th Intl Conf on Dependable, Autonomic and Secure Computing, 14th Intl Conf on Pervasive Intelligence and Computing, 2nd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)}, title={Record Linkage Using the Combination of Twice Iterative SVM Training and Controllable Manual Review},
year={2016},
volume={},
number={},
pages={31-38},
abstract={Record linkage is widely used in many fields, which is a crucial step to increase data quality before data analyzing and data mining. The task of record linkage is to identify records that correspond to the same entities from multi-sources data. In this paper, we describe detailed process of record linkage through an application of internet video, with the purpose of guiding the practice. A method of combination of twice Iterative SVM (Support Vector Machine) training and controllable manual review has been presented. The experiment based on abundant actual data achieves over 98% in F-score.},
keywords={Couplings;Support vector machines;Training;Indexing;Manuals;Motion pictures;record linkage;internet video;support vector machine;manual review},
doi={10.1109/DASC-PICom-DataCom-CyberSciTec.2016.21},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8455917,
author={Qiang, Li and Zhengwei, Jiang and Zeming, Yang and Baoxu, Liu and Xin, Wang and Yunan, Zhang},
booktitle={2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)}, title={A Quality Evaluation Method of Cyber Threat Intelligence in User Perspective},
year={2018},
volume={},
number={},
pages={269-276},
abstract={With the widely use of cyber threat intelligence, the influence of security threats and cyber attacks have been relieved and controlled in a degree. More and more users have accepted the conception of threat intelligence and are trying to use threat intelligence in routine security protection. Then, how to choose appropriate threat intelligence vendors and services has become a crucial issue. The present research of threat intelligence evaluation mainly focused on one-sided threat intelligence contents and approaches, which was lack of comprehensiveness and effectiveness. Aiming at this situation, we propose the comprehensive evaluation architecture of threat intelligence in user perspective to evaluate threat intelligence services in several dimensions with quantitative index system. We also carried out typical experiments for threat intelligence data feeds and comprehensive situation to verify the feasibility of proposed method. The results show that the proposed evaluation method has a clear advantage in coverage and partition degree.},
keywords={Indexes;Testing;Quantization (signal);Feeds;Blacklisting;Business;Threat Intelligence, Quality Evaluation, User Perspective, Vendor},
doi={10.1109/TrustCom/BigDataSE.2018.00049},
ISSN={2324-9013},
month={Aug},}
@INPROCEEDINGS{8713242,
author={Li, Xinyi and Feng, Zikun and Li, Yan and Liu, Zhao and Liu, Ryan Wen},
booktitle={2019 IEEE 4th International Conference on Big Data Analytics (ICBDA)}, title={Spatio-Temporal Vessel Trajectory Smoothing Using Empirical Mode Decomposition and Wavelet Transform},
year={2019},
volume={},
number={},
pages={106-111},
abstract={The Automatic Identification System (AIS) has attracted increasing attention in recent years for its superior properties in ocean engineering and maritime management. The spatio-temporal vessel trajectory data is highly related to the received AIS data. However, the AIS raw data often suffer from undesirable noise during signal acquisition and analog-to-digital conversion. To improve AIS-based vessel trajectory data quality, we propose to develop a vessel trajectory smoothing method by combining empirical mode decomposition (EMD) with wavelet transform. In particular, EMD is introduced to decompose the original vessel trajectory data into several sub-trajectories. The EMD decomposition is able to assist in enhancing the robustness of trajectory smoothing. Wavelet transform is directly adopted to smooth the decomposed sub-trajectories. The final high-quality trajectory is obtained by combining the smoothed sub-trajectories in this work. The proposed method has the capacity of removing the unwanted noise while preserving the important trajectory features. Numerous experiments have illustrated the superior smoothing performance of the proposed combined method.},
keywords={Trajectory;Wavelet transforms;Noise reduction;Artificial intelligence;Marine vehicles;Navigation;Empirical mode decomposition;wavelet transform;data denoising;automatic identification system;trajectory data},
doi={10.1109/ICBDA.2019.8713242},
ISSN={},
month={March},}
@INPROCEEDINGS{8258096,
author={Appiktala, Nirupama and Chen, Miao and Natkovich, Michael and Walters, Joshua},
booktitle={2017 IEEE International Conference on Big Data (Big Data)}, title={Demystifying dark matter for online experimentation},
year={2017},
volume={},
number={},
pages={1620-1626},
abstract={The rise of online controlled experimentation, a.k.a. A/B testing began around the turn of the millennium with the emergence of internet giants like Amazon, Bing, Facebook, Google, LinkedIn, and Yahoo. A step towards good experimental design includes the planning for sample size, confidence level, metrics to be measured and test duration. Generally, these factors impact the quality and validity of an experiment. In practice, additional factors may also impact the validity of an experiment. One such critical factor is the discrepancy between the planned bucket size and the actual bucket size. We call this hidden gap “Experimentation Dark Matter”. Experimentation dark matter is invisible to A/A or A/B validation of experimental analysis but can impact the validity of an experiment. In this paper, we have demonstrated in detail, this gap that may cause the loss of statistical power as well as the loss of representativeness and generalizability of an experiment. We have proposed a framework to monitor experimentation dark matter that may go unnoticed in a balanced AB test. We have further discussed the remediation of a recent dark matter issue using our framework. This scalable, low-latency framework is effective and applicable to similar online controlled experimentation systems.},
keywords={Sociology;Statistics;Servers;Testing;Finance;Google;Measurement;online experimentation;quality assurance;bucket size gap;dark matter;loss of traffic;data quality},
doi={10.1109/BigData.2017.8258096},
ISSN={},
month={Dec},}
@ARTICLE{9093050,
author={Azmy, Sherif B. and Zorba, Nizar and Hassanein, Hossam S.},
journal={IEEE Internet of Things Journal}, title={Quality Estimation for Scarce Scenarios Within Mobile Crowdsensing Systems},
year={2020},
volume={7},
number={11},
pages={10955-10968},
abstract={Mobile crowdsensing (MCS) is a paradigm that exploits the presence of a crowd of moving human participants to acquire, or generate, data from their environment. As a part of the Internet-of-Things (IoT) paradigm, MCS serves the quest for a more efficient operation of a smart city. Big data techniques employed on this data produce inferences about the participants' environment, the smart city. However, sufficient amounts of data are not always available. Sometimes, the available data are scarce as it is obtained at different times, locations, and from different MCS participants who may not be present. As a consequence, the scale of data acquired may be small and susceptible to errors. In such scenarios, the MCS system requires techniques that acquire reliable inferences from such limited data sets. To that end, we resort to small data (SD) techniques that are relevant for scarce and erroneous scenarios. In this article, we discuss SD and propose schemes to tackle the problems associated with such limited data sets, in the context of the smart city. We propose two novel quality metrics: 1) MAD quality metric (MAD-Q) and 2) MAD bootstrap quality metric (MADBS-Q), to deal with SD, focusing on evaluating the quality of a data set within MCS. We also propose an MCS-specific coverage metric that combines the spatial dimension with MAD-Q and MADBS-Q. We show the performance of all the presented techniques through closed-form mathematical expressions, with which simulation results were found to be consistent.},
keywords={Measurement;Internet of Things;Standards;Smart cities;Task analysis;Intelligent sensors;Data quality;Internet of Things (IoT);IoT architectures;IoT-based services;mobile crowdsensing (MCS);small data (SD)},
doi={10.1109/JIOT.2020.2994556},
ISSN={2327-4662},
month={Nov},}
@INPROCEEDINGS{7336420,
author={Zhao, Liang and Chen, Zhikui and Yang, Zhennan and Hu, Yueming},
booktitle={2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems}, title={A Hybrid Method for Incomplete Data Imputation},
year={2015},
volume={},
number={},
pages={1725-1730},
abstract={With the explosive increase of data volume, the research of data quality and data usability draws extensive attention. In this work, we focus on one aspect of data usability -- incomplete data imputation, and present a novel missing value imputation method using stacked auto-encoder and incremental clustering (SAICI). Specifically, SAICI's functionality rests on four pillars: (i) a distinctive value assigned to impute missing values initially, (ii) the stacked auto-encoder(SAE) applied to locate principal features, (iii) a new incremental clustering utilized to partition incomplete data set, and (iv) the top nearest neighbors' weighted values designed to refill the missing values. Most importantly, stages (ii)~(iv) iterate until convergence condition is satisfied. Experimental results demonstrate that the proposed scheme not only imputes the missing data values effectively, but also has better time performance. Moreover, this work is suitable for distributed data processing framework, which can be applied to the imputation of incomplete big data.},
keywords={Clustering algorithms;Partitioning algorithms;Algorithm design and analysis;Accuracy;Feature extraction;Integrated circuits;Time complexity;missing values;data imputation;stacked auto-encoder;incremental clustering},
doi={10.1109/HPCC-CSS-ICESS.2015.103},
ISSN={},
month={Aug},}
@ARTICLE{9143114,
author={Liu, Yangxiaoyue and Yang, Yaping and Jing, Wenlong},
journal={IEEE Access}, title={Potential Applicability of SMAP in ECV Soil Moisture Gap-Filling: A Case Study in Europe},
year={2020},
volume={8},
number={},
pages={133114-133127},
abstract={The Essential Climate Variable (ECV) soil moisture (SM) datasets, originated from the European Space Agency, have revealed great potential for application in hydrology and agriculture. Hence, it is essential to continuously enhance the data quality and spatial completeness to satisfy the increasing scientific research requirements. In this study, we explore the potential possibility of Soil Moisture Active Passive (SMAP) datasets in filling the gaps of ECV SM. The comprehensive assessment results show that: (1) The data missing percent of gap-filled ECV decreases 20% on average, which can be one step closer to generate a seamlessly covered global land surface SM product with favorable quality. (2) Compared to the original ECV, the gap-filled ECV products express similar good response to the in-situ measurements, suggesting that the SMAP SM products could be taken to efficiently fill the gaps and consistently maintain favorable accuracy at the same time. (3) Compared to the in-situ measurements, the original ECV SM products demonstrate extremely high probability density peak percentages. Fortunately, this eminent high value could be effectively rectified through gap-filling progress using SMAP. Overall, this study conducts objective and detailed evaluation on the performance of applying SMAP to fill the gaps of ECV, and is expected to act as a valuable reference in ECV SM gap-filling method.},
keywords={Microwave radiometry;Meteorology;Satellite broadcasting;Sensors;Soil moisture;Synthetic aperture radar;Soil measurements;Gap-filling;satellite retrieved soil moisture;the essential climate variable soil moisture;the soil moisture active passive soil moisture},
doi={10.1109/ACCESS.2020.3009977},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8904039,
author={Wallis, Kevin and Schillinger, Fabian and Reich, Christoph and Schindelhauer, Christian},
booktitle={2019 Third World Conference on Smart Trends in Systems Security and Sustainablity (WorldS4)}, title={Safeguarding Data Integrity by Cluster-Based Data Validation Network},
year={2019},
volume={},
number={},
pages={78-86},
abstract={Ensuring data quality is central to the digital transformation in industry. Business processes such as predictive maintenance or condition monitoring can be implemented or improved based on the available data. In order to guarantee high data quality, a single data validation system are usually used to validate the production data for further use. However, using a single system allows an attacker only to perform one successful attack to corrupt the whole system. We present a new approach in which a data validation system using multiple different validators minimizes the probability of success for the attacker. The validators are arranged in clusters based on their properties. For a validation process, a challenge is given that specifies which validators should perform the current validation. Validation results from other validators are dropped. This ensures that even for more than half of the validators being corrupted anomalies can be detected during the validation process.},
keywords={Logic gates;Data integrity;Task analysis;Maintenance engineering;Blockchain;Industrial Internet of Things;Internet of Things;Data Validation;Cluster-Based Data Validation;Big Data},
doi={10.1109/WorldS4.2019.8904039},
ISSN={},
month={July},}
@INPROCEEDINGS{9060201,
author={Chang, Huijuan and Yu, Zhiyong and Yu, Zhiwen and Guo, Bin},
booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence Computing, Advanced Trusted Computing, Scalable Computing Communications, Cloud Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)}, title={Selecting Sensing Location Leveraging Spatial and Cross-Domain Correlations},
year={2019},
volume={},
number={},
pages={661-666},
abstract={In environmental monitoring applications, selecting appropriate locations to sense is important relating to data quality and Sensing cost. This paper addresses the challenge by collecting data from a subset of locations, then leveraging the spatial and cross-domain correlations to deduce data of other locations, thus can obtain acceptable data quality with lower sensing cost. Referring to active learning, the proposed framework is constructed by two types modules (i.e., estimators and selectors) and a cyclic process of estimating and selecting. Estimators based on kriging interpolation and regression tree are implemented, and their corresponding selectors are designed. We evaluate the effectiveness of the framework by taking air quality sensing as an example. Results show that to reach data quality of about 25% MAPE, the framework only needs 15% locations, while random selector needs 25% locations.},
keywords={Sensors;Correlation;Air quality;Data models;Estimation;Data integrity;Task analysis;Active learning;location selection;kriging interpolation;regression tree},
doi={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00149},
ISSN={},
month={Aug},}
@BOOK{8187386,
author={Ilyas, Ihab F. and Chu, Xu},
booktitle={Trends in Cleaning Relational Data: Consistency and Deduplication},
year={2015},
volume={},
number={},
pages={},
abstract={Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and wrong business decisions. According to a report by InsightSquared in 2012, poor data across businesses and the government cost the United States economy 3.1 trillion dollars a year. To detect data errors, data quality rules or integrity constraints (ICs) have been proposed as a declarative way to describe legal or correct data instances. Any subset of data that does not conform to the defined rules is considered erroneous, which is also referred to as a violation. Various kinds of data repairing techniques with different objectives have been introduced where algorithms are used to detect subsets of the data that violate the declared integrity constraints, and even to suggest updates to the database such that the new database instance conforms with these constraints. While some of these algorithms aim to minimally change the database, others involve human experts or knowledge bases to verify the repairs suggested by the automatic repeating algorithms. Trends in Cleaning Relational Data: Consistency and Deduplication discusses the main facets and directions in designing error detection and repairing techniques. It proposes a taxonomy of current anomaly detection techniques, including error types, the automation of the detection process, and error propagation. It also sets out a taxonomy of current data repairing techniques, including the repair target, the automation of the repair process, and the update model. It concludes by highlighting current trends in "big data" cleaning.},
keywords={Data Cleaning and Information Extraction;Data Integration and Exchange},
doi={10.1561/1900000045},
ISSN={},
publisher={now},
isbn={9781680830231},
url={https://ieeexplore.ieee.org/document/8187386},}
@INPROCEEDINGS{7815071,
author={Tas, Yucel and Baeth, Mohamed Jehad and Aktas, Mehmet S.},
booktitle={2016 12th International Conference on Semantics, Knowledge and Grids (SKG)}, title={An Approach to Standalone Provenance Systems for Big Social Provenance Data},
year={2016},
volume={},
number={},
pages={9-16},
abstract={Provenance about data derivations in social networks is usually called social data provenance, which helps in the assessment of data quality, resource tracking, and understanding the dissemination of information in social networks. The collection and processing of social data provenance leads to some challenges such as scalability, data quality, and privacy awareness. This study introduces a test suite to evaluate the current state-of-the-art standalone and centralized provenance systems. We conduct performance (responsiveness) and scalability experiments and investigate whether the standalone provenance systems are capable of handling large-size social provenance data. We also propose a software architecture for a decentralized and scalable provenance management system for big social provenance data.},
keywords={Social network services;Scalability;Data privacy;Distributed databases;Web services;provenance systems;social provenance data;big provenance data;provenance storage systems;decentralized provenance systems},
doi={10.1109/SKG.2016.010},
ISSN={},
month={Aug},}
@ARTICLE{8649758,
author={Li, Zhi and Guo, Hanyang and Wang, Wai Ming and Guan, Yijiang and Barenji, Ali Vatankhah and Huang, George Q. and McFall, Kevin S. and Chen, Xin},
journal={IEEE Transactions on Industrial Informatics}, title={A Blockchain and AutoML Approach for Open and Automated Customer Service},
year={2019},
volume={15},
number={6},
pages={3642-3651},
abstract={Customer service is transforming from traditional manual service toward automated service, which utilizes different computational informatics to achieve a higher efficient and quality services. Automated customer service requires big data and expertise in data analysis as prerequisites. However, many companies, especially small and medium enterprises, do not have sufficient data and experience due to their limited scale and resources. They need to rely on third parties, and this reliance results in the lack of development of core customer service competency. In order to overcome these challenges, an open and automated customer service platform based on Internet of things (IoT), blockchain, and automated machine learning (AutoML) is proposed. The data are gathered with the use of IoT devices during the customer service. An open but secured environment to achieve data trading is ensured by using blockchain. AutoML is adopted to automate the data analysis processes for reducing the reliance of costly experts. The proposed platform is analyzed through use case evaluation. A prototype system has also been developed and evaluated. The simulation results show that our platform is scalable and efficient.},
keywords={Customer services;Blockchain;Companies;Machine learning;Personnel;Data models;Smart contracts;Automated customer service;automated machine learning (AutoML);blockchain;open customer service},
doi={10.1109/TII.2019.2900987},
ISSN={1941-0050},
month={June},}
@INPROCEEDINGS{6903506,
author={Chakravorty, Antorweep and Wlodarczyk, Tomasz Wiktor and Rong, Chunming},
booktitle={2014 IEEE International Conference on Cloud Engineering}, title={A Scalable K-Anonymization Solution for Preserving Privacy in an Aging-in-Place Welfare Intercloud},
year={2014},
volume={},
number={},
pages={424-431},
abstract={Aging-in-Place solutions are becoming increasingly prevalent in our society. New age big data technologies can harness upon enormous amount of data generated from sensors in smart homes to provide enabling services. Added care and preventive services can be furnished through interoperability and bidirectional dataflow across the value chain. However the nature of the problem domain which although allows establishing better care through sharing of information also risks disclosing complete living behavior of individuals. In this paper, we introduce and evaluate a novel scalable k-anonymization solution based upon the distributed map-reduce paradigm for preserving privacy of the shared data in a welfare intercloud. Our evaluation benchmarks both information loss and data quality metrics and demonstrates better scalability/performance than any other available solutions.},
keywords={Data privacy;Partitioning algorithms;Cryptography;Smart homes;Scalability;Dictionaries;Privacy;privacy;k-anonymization;hadoop;intercloud;aging in place},
doi={10.1109/IC2E.2014.43},
ISSN={},
month={March},}
@ARTICLE{7933943,
author={Islam, MD. Mofijul and Razzaque, MD. Abdur and Hassan, Mohammad Mehedi and Ismail, Walaa Nagy and Song, Biao},
journal={IEEE Access}, title={Mobile Cloud-Based Big Healthcare Data Processing in Smart Cities},
year={2017},
volume={5},
number={},
pages={11887-11899},
abstract={In recent years, the Smart City concept has become popular for its promise to improve the quality of life of urban citizens. The concept involves multiple disciplines, such as Smart health care, Smart transportation, and Smart community. Most services in Smart Cities, especially in the Smart healthcare domain, require the real-time sharing, processing, and analyzing of Big Healthcare Data for intelligent decision making. Therefore, a strong wireless and mobile communication infrastructure is necessary to connect and access Smart healthcare services, people, and sensors seamlessly, anywhere at any time. In this scenario, mobile cloud computing (MCC) can play a vital role by offloading Big Healthcare Data related tasks, such as sharing, processing, and analysis, from mobile applications to cloud resources, ensuring quality of service demands of end users. Such resource migration, which is also termed virtual machine (VM) migration, is effective in the Smart healthcare scenario in Smart Cities. In this paper, we propose an ant colony optimization-based joint VM migration model for a heterogeneous, MCC-based Smart Healthcare system in Smart City environment. In this model, the user’s mobility and provisioned VM resources in the cloud address the VM migration problem. We also present a thorough performance evaluation to investigate the effectiveness of our proposed model compared with the state-of-the-art approaches.},
keywords={Cloud computing;Medical services;Mobile communication;Smart cities;Servers;Real-time systems;Quality of service;Smart health care;smart city;big data;quality of service (QoS);virtual machine migration;ant colony optimization},
doi={10.1109/ACCESS.2017.2707439},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9183812,
author={Tshikomba, Salome C. and Estrice, Milton and Ojo, Evans and Davidson, Innocent E},
booktitle={2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)}, title={Curbing Electricity Theft Using Wireless Technique with Communication Constraints},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Utility services are experiencing a common problem of power losses, which impose a significant impact on their annual budget. Practically, power losses consist of technical losses and non-technical losses. Technical losses are due to operations and aging of infrastructure, while nontechnical losses (NTL) are due to non-metered energy. The focus is on managing non-technical losses using an automation wireless method. The wireless ZigBee technique is proposed and further investigated for communication failure over long distances while solving the problem of stealing electricity. Advance-metering infrastructure (AMI) technique and smart meters are feasible for system integration; that is why they are chosen to be part of this study. The success of the study depends on quality data of the Utility, meaning the more accurate the data, the easier the analysis of outliers. The operation and planning of revenue protection contain a large amount of data that needs to be worked on, so data mining assists in that regard. Then the load profiling method assists in illustrating the variation in demand/electricalload over a specific time. This is a preliminary investigation using a wireless communication technique as a viable solution in curbing electricity theft. The uniqueness of the proposed ZigBee system is that it recognizes the everyday act of stealing electricity through tempering with the meter box and tapping of the supply.},
keywords={ZigBee;Meters;Wireless communication;Automation;Smart meters;Monitoring;Sensors;Losses;smart meters;AMI;wireless technique;ZigBee technology;NTL},
doi={10.1109/icABCD49160.2020.9183812},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8471712,
author={Darari, Fariz and Nutt, Werner and Razniewski, Simon},
booktitle={2018 International Workshop on Big Data and Information Security (IWBIS)}, title={Comparing Index Structures for Completeness Reasoning},
year={2018},
volume={},
number={},
pages={49-56},
abstract={Data quality is a major issue in the devel- opment of knowledge graphs. Data completeness is a key factor in data quality pertaining to how broad and deep is information contained in knowledge graphs. As for large- scale knowledge graphs (e.g., DBpedia, Wikidata), it is conceivable that given the vast amount of information contained in there, they may be complete for a wide range of topics, such as children of Joko Widodo, cantons of Switzerland, and presidents of Indonesia. Previous research has shown how one can augment knowledge graphs with statements about their completeness, stating which parts of data are complete. Such meta-information can be leveraged to check query completeness, that is, whether the answer returned by a query is complete. Yet, it is still unclear how such a check can be done in practice, especially when many completeness statements are involved. We devise implementation techniques to make completeness reasoning in the presence of large sets of completeness statements feasible, and experimentally evaluate their effectiveness in realistic settings based on the characteristics of real-world knowledge graphs.},
keywords={Cognition;Resource description framework;Metadata;Data integrity;Tools;Iris;Complexity theory},
doi={10.1109/IWBIS.2018.8471712},
ISSN={},
month={May},}
@ARTICLE{7953577,
author={Shuai, Hong-Han and Yang, De-Nian and Shen, Chih-Ya and Yu, Philip S. and Chen, Ming-Syan},
journal={IEEE Transactions on Big Data}, title={QMSampler: Joint Sampling of Multiple Networks with Quality Guarantee},
year={2018},
volume={4},
number={1},
pages={90-104},
abstract={Because Online Social Networks (OSNs) have become increasingly important in the last decade, they have motivated a great deal of research on Social Network Analysis (SNA). Currently, SNA algorithms are evaluated on real datasets obtained from large-scale OSNs, which are usually sampled by Breadth-First-Search (BFS), Random Walk (RW), or some variations of the latter. However, none of the released datasets provides any statistical guarantees on the difference between the sampled datasets and the ground truth. Moreover, all existing sampling algorithms only focus on sampling a single OSN, but each OSN is actually a sampling of a complete social network. Hence, even if the whole dataset from a single OSN is sampled, the results may still be skewed and may not fully reflect the properties of the complete social network. To address the above issues, we have made the first attempt to explore the joint sampling of multiple OSNs and propose an approach called Quality-guaranteed Multi-network Sampler (QMSampler) that can jointly sample multiple OSNs. QMSampler provides a statistical guarantee on the difference between the sampled real dataset and the ground truth (the perfect integration of all OSNs). Our experimental results demonstrate that the proposed approach generates a much smaller bias than any existing method. QMSampler has also been released as a free download.},
keywords={Facebook;Roads;Big Data;Electronic mail;LinkedIn;Measurement;Social network;graph sampler;data quality analysis;optimization},
doi={10.1109/TBDATA.2017.2715847},
ISSN={2332-7790},
month={March},}
@INPROCEEDINGS{7724532,
author={Manjula, K. R. and Gangothri, R.},
booktitle={2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)}, title={Hybrid model based uncertainty analysis for geospatial metadata supporting decision making for spatial exploration},
year={2016},
volume={},
number={},
pages={1575-1579},
abstract={The proliferation of Geospatial data analytics has greatly increased the usage of GIS in all smartphones and gadgets in today's Big Data environment. The footprints of GIS are found in all fields from government to business analytics. Therefore the error propagation in such a substantial need may lead to misunderstood decisions and confusions during emergency. In this paper we take up a chance to document the data quality assurance using the geospatial metadata based uncertainty analysis approach. The paper takes up an initial attempt to state that chances occur for existence of uncertainty in metadata. And it proposes a hybrid model combing ontology, standard deviation and probability density function for detecting the occurrence of uncertainties in geospatial metadata.},
keywords={Metadata;Uncertainty;Geospatial analysis;Standards;Spatial databases;Analytical models;Geospatial Metadata;GIS;ontology;standard deviation;probability density function},
doi={},
ISSN={},
month={March},}
@ARTICLE{8993839,
author={Qadri, Yazdan Ahmad and Nauman, Ali and Zikria, Yousaf Bin and Vasilakos, Athanasios V. and Kim, Sung Won},
journal={IEEE Communications Surveys Tutorials}, title={The Future of Healthcare Internet of Things: A Survey of Emerging Technologies},
year={2020},
volume={22},
number={2},
pages={1121-1167},
abstract={The impact of the Internet of Things (IoT) on the advancement of the healthcare industry is immense. The ushering of the Medicine 4.0 has resulted in an increased effort to develop platforms, both at the hardware level as well as the underlying software level. This vision has led to the development of Healthcare IoT (H-IoT) systems. The basic enabling technologies include the communication systems between the sensing nodes and the processors; and the processing algorithms for generating an output from the data collected by the sensors. However, at present, these enabling technologies are also supported by several new technologies. The use of Artificial Intelligence (AI) has transformed the H-IoT systems at almost every level. The fog/edge paradigm is bringing the computing power close to the deployed network and hence mitigating many challenges in the process. While the big data allows handling an enormous amount of data. Additionally, the Software Defined Networks (SDNs) bring flexibility to the system while the blockchains are finding the most novel use cases in H-IoT systems. The Internet of Nano Things (IoNT) and Tactile Internet (TI) are driving the innovation in the H-IoT applications. This paper delves into the ways these technologies are transforming the H-IoT systems and also identifies the future course for improving the Quality of Service (QoS) using these new technologies.},
keywords={Internet of Things;Medical services;Edge computing;Sensors;Blockchain;Quality of service;Big Data;H-IoT;WBAN;machine learning;fog computing;edge computing;blockchain;software defined networks},
doi={10.1109/COMST.2020.2973314},
ISSN={1553-877X},
month={Secondquarter},}
@INPROCEEDINGS{9332846,
author={Fei, Chen},
booktitle={2020 IEEE International Conference on Industrial Application of Artificial Intelligence (IAAI)}, title={Research on Lidar Data Error Correction Method Based on Bayesian Network},
year={2020},
volume={},
number={},
pages={486-491},
abstract={Data quality analysis is the first and key step of remote sensing mechanism/application research (especially quantitative remote sensing), which directly affects the accuracy of remote sensing inversion and the effect of remote sensing applications. Lidar (light detection and ranging, referred to as lidar) is a new active remote sensing technology. The research on hardware system development and data post-processing algorithm needs to be further strengthened, especially for the quality analysis of lidar data. After initializing the Bayesian network and checking the error, a correction mathematical model is established. Experiments have proved that after correction, the angle error of the radar is significantly improved, which verifies the feasibility and reliability of the precision orbit star calibration method.},
keywords={Laser radar;Orbits (stellar);Bayes methods;Error correction;Calibration;Reliability;Remote sensing;Bayesian network;lidar;data error;correction methoIntroduction},
doi={10.1109/IAAI51705.2020.9332846},
ISSN={},
month={Dec},}
@ARTICLE{9739736,
author={Baoyu, Li and Guoxing, Li and Guiyu, Wang and Guofeng, Zhang and Man, Yang},
journal={IEEE Access}, title={Research on CART Model of Mass Concrete Temperature Prediction Based on Big Data Processing Technology},
year={2022},
volume={10},
number={},
pages={32845-32854},
abstract={Due to the influence of temperature changes or temperature gradients in the construction process of mass concrete, temperature cracks will occur in the concrete. In order to achieve a reasonable prediction of the temperature change of the mass concrete during the construction process and accurately obtain the temperature change trend, this paper attempts to construct a CART prediction model based on the big data processing technology based on the characteristics of the temperature change of the mass concrete. This paper introduces in detail how to use data processing methods such as outlier identification, missing value filling and random error elimination to improve data quality, as well as the method for constructing the CART prediction model, and combines engineering examples to demonstrate the feasibility of the model method. The results show that the model and method can better predict the temperature change of mass concrete. It has high prediction accuracy and can provide necessary guidance for practical engineering.},
keywords={Temperature sensors;Temperature distribution;Temperature measurement;Temperature control;Predictive models;Data models;Cooling;Mass concrete temperature;big data processing technology;CART prediction model},
doi={10.1109/ACCESS.2022.3161556},
ISSN={2169-3536},
month={},}
@ARTICLE{9431193,
author={Yustiawan, Yoga and Ramadhan, Hani and Kwon, Joonho},
journal={IEEE Access}, title={A Stacked Denoising Autoencoder and Long Short-Term Memory Approach With Rule-Based Refinement to Extract Valid Semantic Trajectories},
year={2021},
volume={9},
number={},
pages={73152-73168},
abstract={Indoor location-based services have been widely investigated to take advantage of semantic trajectories for providing user oriented services in indoor environments. Although indoor semantic trajectories can provide seamless understanding to users regarding the provided location-based services, studies on the application of deep learning approaches for robust and valid semantic indoor localization are lacking. In this study, we combined a stacked denoising autoencoder and long short term memory technique with a rule-based refinement method applying a rule-based hidden Markov model (HMM) to perform robust and valid semantic trajectory extraction. In particular, our rule-based HMM approach incorporates a direct set of rules into HMM to resolve invalid movements of the extracted semantic trajectories and is extensible to various deep learning techniques. We compared the performance of our proposed approach with that of other cutting-edge deep learning approaches on two different real-world data sets. The experimental results demonstrate the feasibility of our proposed approach to produce more robust and valid semantic trajectories.},
keywords={Semantics;Trajectory;Location awareness;Hidden Markov models;Deep learning;Noise measurement;Indoor environment;Deep learning;indoor localization;the Internet of Things;rule-based refinement;semantic trajectories},
doi={10.1109/ACCESS.2021.3080288},
ISSN={2169-3536},
month={},}
@ARTICLE{8553660,
author={Shen, Jian and Zhou, Tianqi and Wang, Kun and Peng, Xin and Pan, Li},
journal={IEEE Network}, title={Artificial Intelligence Inspired Multi-Dimensional Traffic Control for Heterogeneous Networks},
year={2018},
volume={32},
number={6},
pages={84-91},
abstract={The heterogeneous network is the foundation of next-generation networks. It aims to explore the existing network resources effectively, and providing better QoS for every kind of traffic flow as far as possible. However, the diversity and dynamic nature of heterogeneous networks will bring a huge burden and big data to the network traffic control. Therefore, how to achieve efficient and intelligent network traffic control becomes the key problem of heterogeneous networks. In this article, an AI-inspired traffic control scheme is proposed. In order to realize fine-grained traffic control in heterogeneous networks, multi-dimensional (i.e., inter-layer, intra-layer, and caching and pushing) network traffic control is introduced. It is worth noting that backpropagation in deep recurrent neural networks is applied in the intra-layer such that an intelligent traffic control scheme can be derived efficiently when facing the huge traffic load in heterogeneous networks. Moreover, DBSCAN is adopted in the inter-layer, which supports efficient classification in the inter-layer. In addition, caching and pushing is adopted to make full use of network resources and provide better QoS. Simulation results demonstrate the effectiveness and practicability of the proposed scheme.},
keywords={Heterogeneous networks;Backpropagation;Telecommunication traffic;Big Data;Neural networks;Traffic control;Intelligent networks;Big Data;Quality of service;Networked control systems;Recurrent neural networks},
doi={10.1109/MNET.2018.1800120},
ISSN={1558-156X},
month={November},}
@ARTICLE{9352014,
author={Jiang, Haoyu and Chen, Kai and Ge, Quanbo and Wang, Yun and Xu, Jinqiang and Li, Chunxi},
journal={IEEE Internet of Things Journal}, title={Fault Diagnosis of Power IoT System Based on Improved Q-KPCA-RF Using Message Data},
year={2021},
volume={8},
number={11},
pages={9450-9459},
abstract={As the power system develops from informatization to intelligence. Research on data services based on the Internet of Things (IoT) focuses more on application functions, but the research on the data quality of the IoT itself is insufficient. Long-term continuous operation of the big data IoT system has the risk of performance degradation or even partial fault, which leads to a decrease in the availability of collected data for intelligent analysis. In this article, based on the power IoT message data, the characteristics are established through a variety of improved detection methods, and then the abnormal data type is obtained through Q learning and fusion of the random forest (RF) identification features. Finally, the topology of the specific power user IoT system is combined with kernel principal component analysis (KPCA) + improved RF algorithm getting the abnormal location of the IoT. The results show that the research method has a significantly higher positioning accuracy (from 61% to 97%) than the traditional RF method, and the combination method has more advantages in parameter adjustment and classification accuracy than directly using a multilayer perceptron (MLP).},
keywords={Monitoring;Internet of Things;Random forests;Logic gates;Feature extraction;Clustering algorithms;Neural networks;Communication message;power Internet of Things (IoT) system;Q learning;random forest (RF)},
doi={10.1109/JIOT.2021.3058563},
ISSN={2327-4662},
month={June},}
@INPROCEEDINGS{6691642,
author={Chen, Chien-Chih and Chang, Yu-Jung and Chung, Wei-Chun and Lee, Der-Tsai and Ho, Jan-Ming},
booktitle={2013 IEEE International Conference on Big Data}, title={CloudRS: An error correction algorithm of high-throughput sequencing data based on scalable framework},
year={2013},
volume={},
number={},
pages={717-722},
abstract={Next-generation sequencing (NGS) technologies produce huge amounts of data. These sequencing data unavoidably are accompanied by the occurrence of sequencing errors which constitutes one of the major problems of further analyses. Error correction is indeed one of the critical steps to the success of NGS applications such as de novo genome assembly and DNA resequencing as illustrated in literature. However, it requires computing time and memory space heavily. To design an algorithm to improve data quality by efficiently utilizing on-demand computing resources in the cloud is a challenge for biologists and computer scientists. In this study, we present an error-correction algorithm, called the CloudRS algorithm, for correcting errors in NGS data. The CloudRS algorithm aims at emulating the notion of error correction algorithm of ALLPATHS-LG on the Hadoop/ MapReduce framework. It is conservative in correcting sequencing errors to avoid introducing false decisions, e.g., when dealing with reads from repetitive regions. We also illustrate several probabilistic measures we introduce into CloudRS to make the algorithm more efficient without sacrificing its effectiveness. Running time of using up to 80 instances each with 8 computing units shows satisfactory speedup. Experiments of comparing with other error correction programs show that CloudRS algorithm performs lower false positive rate for most evaluation benchmarks and higher sensitivity on genome S. cerevisiae. We demonstrate that CloudRS algorithm provides significant improvements in the quality of the resulting contigs on benchmarks of NGS de novo assembly.},
keywords={Error correction;Algorithm design and analysis;Sequential analysis;Assembly;Bioinformatics;Genomics;Benchmark testing;error correction;mapreduce;genome assembly;next-generation sequencing},
doi={10.1109/BigData.2013.6691642},
ISSN={},
month={Oct},}
@ARTICLE{9058606,
author={Lin, Mengting and Zhao, Youping},
journal={China Communications}, title={Artificial intelligence-empowered resource management for future wireless communications: A survey},
year={2020},
volume={17},
number={3},
pages={58-77},
abstract={How to explore and exploit the full potential of artificial intelligence (AI) technologies in future wireless communications such as beyond 5G (B5G) and 6G is an extremely hot inter-disciplinary research topic around the world. On the one hand, AI empowers intelligent resource management for wireless communications through powerful learning and automatic adaptation capabilities. On the other hand, embracing AI in wireless communication resource management calls for new network architecture and system models as well as standardized interfaces/protocols/data formats to facilitate the large-scale deployment of AI in future B5G/6G networks. This paper reviews the state-of-art AI-empowered resource management from the framework perspective down to the methodology perspective, not only considering the radio resource (e.g., spectrum) management but also other types of resources such as computing and caching. We also discuss the challenges and opportunities for AI-based resource management to widely deploy AI in future wireless communication networks.},
keywords={Resource management;Artificial intelligence;5G mobile communication;Wireless communication;Big Data;Quality of service;Network slicing;5G;beyond 5G (B5G);6G;artificial intelligence (AI);machine learning (ML);network slicing;resource management},
doi={10.23919/JCC.2020.03.006},
ISSN={1673-5447},
month={March},}
@ARTICLE{9057697,
author={Lattuada, Marco and Barbierato, Enrico and Gianniti, Eugenio and Ardagna, Danilo},
journal={IEEE Transactions on Cloud Computing}, title={Optimal Resource Allocation of Cloud-Based Spark Applications},
year={2020},
volume={},
number={},
pages={1-1},
abstract={Nowadays, the big data paradigm is consolidating its central position in the industry, as well as in society at large. As big data applications gain more and more importance over time and given the dynamic nature of cloud resources, it is fundamental to develop an intelligent resource management system to provide Quality of Service guarantees to end-users. This paper presents a set of run-time optimization-based resource management policies for advanced big data analytics. Users submit Spark applications characterized by a priority and by a hard or soft deadline. Optimization policies address two scenarios: i) identification of the minimum capacity to run a Spark application within the deadline; ii) re-balance of the cloud resources in case of heavy load, minimising the weighted soft deadline application tardiness. The results obtained in the first scenario demonstrate that the percentage error of the prediction of the optimal resource usage with respect to system measurement and exhaustive search is the range 4%-29% while literature-based techniques present an average error in the range 6%-63%. Moreover, in the second scenario, the proposed algorithms can address complex scenarios with an error of 8% on average while literature-based approaches obtain an average error of about 57%.},
keywords={Cloud computing;Sparks;Big Data;Task analysis;Resource management;Computational modeling;Optimization;Big Data;Quality of Service;Elastic resource provisioning;Cluster management},
doi={10.1109/TCC.2020.2985682},
ISSN={2168-7161},
month={},}
@INPROCEEDINGS{7877045,
author={Yaseen, Muhammad Usman and Anjum, Ashiq and Antonopoulos, Nick},
booktitle={2016 IEEE/ACM 3rd International Conference on Big Data Computing Applications and Technologies (BDCAT)}, title={Spatial Frequency Based Video Stream Analysis for Object Classification and Recognition in Clouds},
year={2016},
volume={},
number={},
pages={18-26},
abstract={The recent rise in multimedia technology has made it easier to perform a number of tasks. One of these tasks is monitoring where cheap cameras are producing large amount of video data. This video data is then processed for object classification to extract useful information. However, the videodata obtained by these cheap cameras is often of low qualityand results in blur video content. Moreover, various illuminationeffects caused by lightning conditions also degradethe video quality. These effects present severe challenges forobject classification. We present a cloud-based blur and illumination invariant approach for object classification fromimages and video data. The bi-dimensional empirical modedecomposition (BEMD) has been adopted to decompose avideo frame into intrinsic mode functions (IMFs). TheseIMFs further undergo to first order Reisz transform to generatemonogenic video frames. The analysis of each IMF hasbeen carried out by observing its local properties (amplitude, phase and orientation) generated from each monogenic videoframe. We propose a stack based hierarchy of local patternfeatures generated from the amplitudes of each IMF whichresults in blur and illumination invariant object classification. The extensive experimentation on video streams aswell as publically available image datasets reveals that oursystem achieves high accuracy from 0.97 to 0.91 for increasingGaussian blur ranging from 0.5 to 5 and outperformsstate of the art techniques under uncontrolled conditions. The system also proved to be scalable with high throughputwhen tested on a number of video streams using cloud infrastructure.},
keywords={Streaming media;Cameras;Cloud computing;Lighting;Image color analysis;Empirical mode decomposition;Feature extraction;Empirical Mode Decomposition;Local Ternary Patterns;Riesz Transform;Amplitude Spectrum;Cloud Computing;Big Data Analytics;Object Classification},
doi={},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6863306,
author={McMorran, A. W. and Rudd, S. E. and Shand, C. M. and Simmins, J. J. and McCollough, N. and Stewart, E. M.},
booktitle={2014 IEEE PES T D Conference and Exposition}, title={Data integration challenges for standards-compliant mobile applications},
year={2014},
volume={},
number={},
pages={1-5},
abstract={Modern mobile devices are capable of running sophisticated, network-enabled applications exploiting a variety of sensors on a single low-cost piece of hardware. The electrical industry can benefit from these new platforms to automate existing processes and provide engineers and field crew with access to large amounts of complex data in real-time, anywhere in the world. The development of a standards-based application decouples the mobile client application from a single vendor or existing enterprise system, but requires a complex data integration architecture to support the use and exploitation of large amounts of data spread across multiple existing systems. The integration with a mobile application introduces new challenges when dealing with remote devices where data network communications cannot be relied on, especially under storm conditions, and the devices themselves are at risk of being lost or stolen. Addressing these challenges offers the potential to improve data quality, enable access to accurate, up-to-date information in the field and ultimately save a utility time and money.},
keywords={Computer integrated manufacturing;Mobile communication;IEC standards;Logic gates;Servers;Data models;Asset management;Application virtualization;Virtual reality;Visualization;Standards;Data handling;Data visualization;CIM;Data integration;Big Data},
doi={10.1109/TDC.2014.6863306},
ISSN={2160-8563},
month={April},}
@ARTICLE{9046025,
author={Pan, Yongsheng and Liu, Mingxia and Lian, Chunfeng and Xia, Yong and Shen, Dinggang},
journal={IEEE Transactions on Medical Imaging}, title={Spatially-Constrained Fisher Representation for Brain Disease Identification With Incomplete Multi-Modal Neuroimages},
year={2020},
volume={39},
number={9},
pages={2965-2975},
abstract={Multi-modal neuroimages, such as magnetic resonance imaging (MRI) and positron emission tomography (PET), can provide complementary structural and functional information of the brain, thus facilitating automated brain disease identification. Incomplete data problem is unavoidable in multi-modal neuroimage studies due to patient dropouts and/or poor data quality. Conventional methods usually discard data-missing subjects, thus significantly reducing the number of training samples. Even though several deep learning methods have been proposed, they usually rely on pre-defined regions-of-interest in neuroimages, requiring disease-specific expert knowledge. To this end, we propose a spatially-constrained Fisher representation framework for brain disease diagnosis with incomplete multi-modal neuroimages. We first impute missing PET images based on their corresponding MRI scans using a hybrid generative adversarial network. With the complete (after imputation) MRI and PET data, we then develop a spatially-constrained Fisher representation network to extract statistical descriptors of neuroimages for disease diagnosis, assuming that these descriptors follow a Gaussian mixture model with a strong spatial constraint (i.e., images from different subjects have similar anatomical structures). Experimental results on three databases suggest that our method can synthesize reasonable neuroimages and achieve promising results in brain disease identification, compared with several state-of-the-art methods.},
keywords={Magnetic resonance imaging;Feature extraction;Diseases;Positron emission tomography;Medical diagnosis;Brain modeling;Deep learning;Multi-modal neuroimage;incomplete data;generative adversarial network;fisher vector;brain disease diagnosis;MRI;PET},
doi={10.1109/TMI.2020.2983085},
ISSN={1558-254X},
month={Sep.},}
@INPROCEEDINGS{7011533,
author={Song, Guanli and Wang, Yinghui and Zhang, Runshun and Liu, Baoyan and Zhou, Xuezhong and Song, Guanbo and Xie, Liang and Huang, Xinghuan},
booktitle={2014 IEEE Symposium on Computational Intelligence in Big Data (CIBD)}, title={Methods and technologies of traditional Chinese medicine clinical information datamation in real world},
year={2014},
volume={},
number={},
pages={1-5},
abstract={Under the guidance of clinical research paradigm of traditional Chinese medicine (TCM) in real world, the research group developed the clinical research information sharing system, in which structured electronic medical record system of traditional Chinese medicine is the technology platform of datamation of clinical diagnosis and treatment information. The clinical diagnosis and treatment information can be activated and used effectively only after datamation and truly become the treasures of knowledge of TCM. This paper discusses the implementation process and technologies and methods of TCM clinical information datamation, and take admission records as an example to demonstrate the contents and realization way of datamation, and a brief introduction of the effect of implementation and application of datamation. By making full use of technologies and methods of datamation, strengthening data quality control in the datamation process, greatly improving the quality of TCM clinical research data, to lay a good foundation for establishment of knowledge base through further statistical analysis or data mining of TCM clinical data.},
keywords={Medical diagnostic imaging;Clinical diagnosis;Electronic medical records;History;Maintenance engineering;Discharges (electric);Hospitals;traditional Chinese Medicine (TCM);clinical research paradigm;clinical research information sharing system;datamation;structured electronic medical record},
doi={10.1109/CIBD.2014.7011533},
ISSN={},
month={Dec},}
@ARTICLE{9126214,
author={Ren, Lei and Meng, Zihao and Wang, Xiaokang and Lu, Renquan and Yang, Laurence T.},
journal={IEEE Transactions on Neural Networks and Learning Systems}, title={A Wide-Deep-Sequence Model-Based Quality Prediction Method in Industrial Process Analysis},
year={2020},
volume={31},
number={9},
pages={3721-3731},
abstract={Product quality prediction, as an important issue of industrial intelligence, is a typical task of industrial process analysis, in which product quality will be evaluated and improved as feedback for industrial process adjustment. Data-driven methods, with predictive model to analyze various industrial data, have been received considerable attention in recent years. However, to get an accurate prediction, it is an essential issue to extract quality features from industrial data, including several variables generated from supply chain and time-variant machining process. In this article, a data-driven method based on wide-deep-sequence (WDS) model is proposed to provide a reliable quality prediction for industrial process with different types of industrial data. To process industrial data of high redundancy, in this article, data reduction is first conducted on different variables by different techniques. Also, an improved wide-deep (WD) model is proposed to extract quality features from key time-invariant variables. Meanwhile, an long short-term memory (LSTM)-based sequence model is presented for exploring quality information from time-domain features. Under the joint training strategy, these models will be combined and optimized by a designed penalty mechanism for unreliable predictions, especially on reduction of defective products. Finally, experiments on a real-world manufacturing process data set are carried out to present the effectiveness of the proposed method in product quality prediction.},
keywords={Feature extraction;Predictive models;Data models;Quality assessment;Product design;Data mining;Analytical models;Industrial artificial intelligence (AI);industrial big data;Industrial Internet of Things;product quality prediction;wide-deep-sequence (WDS) model},
doi={10.1109/TNNLS.2020.3001602},
ISSN={2162-2388},
month={Sep.},}
@INPROCEEDINGS{7777890,
author={Finkelstein, Joseph and Jeong, In Cheol},
booktitle={2016 IEEE 7th Annual Ubiquitous Computing, Electronics Mobile Communication Conference (UEMCON)}, title={Using CART for advanced prediction of asthma attacks based on telemonitoring data},
year={2016},
volume={},
number={},
pages={1-5},
abstract={Advanced prediction of asthma exacerbations may significantly improve patient quality of life and reduce costs of urgent care delivery. Majority of current algorithms predict who is likely to experience asthma exacerbation rather than when it is about to occur. We used data from asthma home-based telemonitoring for advanced prediction of asthma exacerbation. The goal of this project was to develop an algorithm that predicts asthma exacerbation one day in advance based on previous 7-day window. CART was used for predictive modeling. Resulting algorithm had specificity 0.971, sensitivity of 0.647, and accuracy of 0.809. We concluded that machine learning has great potential for advanced prediction of chronic disease exacerbations based on home telemonitoring data.},
keywords={Medical treatment;Diseases;Prediction algorithms;Predictive models;Pediatrics;Monitoring;Big data analytics;artificial intelligence;asthma;telemonitoring;exacerbation prediction},
doi={10.1109/UEMCON.2016.7777890},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9402999,
author={Lu, Tielin and Fan, Zitian and Lei, Yue and Shang, Yujia and Wang, Chunxi},
booktitle={2021 IEEE 6th International Conference on Big Data Analytics (ICBDA)}, title={The Edge Computing Cloud Architecture Based on 5G Network for Industrial Vision Detection},
year={2021},
volume={},
number={},
pages={328-332},
abstract={The emergence of a large number of real-time data putforward higher requirements on network transmission technology. The new edge computing cloud technology based on 5G network has become an important research direction of vision detection. However, for the industrial users, they still confuse the architecture of the non-public 5G network (NPN) and misunderstand the data quality of service (QOS). In order to overcome the unstable network structure of 5G for vision detection in industry in a limited bandwidth, achieve high-quality transmission of detection image, and obtain intelligent optimal results, has become an urgent problem to be solved. This paper proposes the network configuration and mode, also design a intelligent edge computing cloud based on 5G scheme. In the ends, an vision detection architecture case has been developed on the 5G communication structure and verified visual detection application scene design its feasibility purpose in the wireless network.},
keywords={Industries;Cloud computing;Visualization;5G mobile communication;Wireless networks;Data integrity;Computer architecture;5G network;vision detection;communication structure;edge computing;information systems},
doi={10.1109/ICBDA51983.2021.9402999},
ISSN={},
month={March},}
@ARTICLE{9761878,
author={Yu, Han and Hanafy, Sherif M. and Liu, Lulu},
journal={IEEE Transactions on Geoscience and Remote Sensing}, title={A Weighted Closure-Phase Statics Correction Method: Synthetic and Field Data Examples},
year={2022},
volume={},
number={},
pages={1-1},
abstract={Recorded seismograms are usually distorted by statics owing to complex geological conditions, such as lateral variations in sediment thickness or complex topographies. These distorted and discontinuous signals usually exist in either arrival times or amplitudes of waves, and they are mostly likely to be smeared as velocity perturbations along their associated raypaths. Therefore, statics may blur images of the target bodies, or even worse, introduce unexpected and false anomalies into subsurface structures. To partly resolve this problem, we develop a weighted statics correction method to estimate unwanted temporal shifts of traces using the closure-phase technique which is utilized in astronomical imaging. In the proposed method, the source and the receiver statics are regarded as independent quantities contributing to the waveform shifts based on their acquisition geometries. Numerical tests on both the synthetic and field cases show noticeable, although gradual, improvements of data quality compared to the conventional Plus-Minus method. In general, this method provides a straightforward strategy to reedit the traveltimes in seismic profiles without inverting for a near-surface velocity model. Moreover, it can be extended to any interferometrical methods in seismic data processing that satisfy the closure-phase conditions.},
keywords={Receivers;Mathematical models;Surface treatment;Sea surface;Indexes;Earth;Computational modeling;Statics;closure phase;first arrivals;interferometry},
doi={10.1109/TGRS.2022.3169519},
ISSN={1558-0644},
month={},}
@INPROCEEDINGS{8510969,
author={Zhang, Yang and Wang, Dong},
booktitle={2018 14th International Conference on Distributed Computing in Sensor Systems (DCOSS)}, title={Poster: On Cost-Sensitive Task Allocation in Social Sensing: An Online Learning Approach},
year={2018},
volume={},
number={},
pages={115-116},
abstract={Social sensing has emerged as a new sensing paradigm where human sensors collectively report measurements about the physical world. This paper focuses on the cost-sensitive task allocation problem in social sensing where the goal is to effectively allocate sensing tasks to the human sensors to meet the desirable data quality requirement of the applications while minimizing the sensing cost. While recent progress has been made to tackle the cost-sensitive task allocation problem, an important challenge has not been well addressed, namely "real time task allocation", the task allocation schemes need to respond quickly to the potential large dynamics of the measured variables in social sensing. To address this challenge, this paper presents a Cost-Sensitive Task Allocation (CSTA) scheme inspired by techniques from online learning. The preliminary results show that our new scheme significantly outperforms the-state-of-the-art baselines.},
keywords={Sensors;Task analysis;Resource management;Real-time systems;Big Data;Dynamic scheduling;Air quality;Social Sensing;Task Allocation;Online Learning},
doi={10.1109/DCOSS.2018.00024},
ISSN={2325-2944},
month={June},}
@ARTICLE{8988265,
author={Zhang, Yuhui and Li, Ming and Yang, Dejun and Tang, Jian and Xue, Guoliang and Xu, Jia},
journal={IEEE Internet of Things Journal}, title={Tradeoff Between Location Quality and Privacy in Crowdsensing: An Optimization Perspective},
year={2020},
volume={7},
number={4},
pages={3535-3544},
abstract={Crowdsensing enables a wide range of data collection, where the data are usually tagged with private locations. Protecting users' location privacy has been a central issue. The study of various location perturbation techniques, e.g., k-anonymity, for location privacy has received widespread attention. Despite the huge promise and considerable attention, provable good algorithms considering the tradeoff between location privacy and location information quality from the optimization perspective in crowdsensing are lacking in the literature. In this article, we study two related optimization problems from two different perspectives. The first problem is to minimize the location quality degradation caused by the protection of users' location privacy. We present an efficient optimal algorithm OLoQ for this problem. The second problem is to maximize the number of protected users, subject to a location quality degradation constraint. To satisfy the different requirements of the platform, we consider two cases for this problem: 1) overlapping and 2) nonoverlapping perturbations. For the former case, we give an efficient optimal algorithm OPUMO. For the latter case, we first prove its NP-hardness. We then design a (1-E)-approximation algorithm NPUMN and a fast and effective heuristic algorithm HPUMN. Extensive simulations demonstrate that OLoQ, OPUMO, and HPUMN significantly outperform an existing algorithm.},
keywords={Privacy;Crowdsensing;Degradation;Optimization;Perturbation methods;Sensors;Approximation algorithms;Crowdsensing;location data quality;location privacy;k-anonymity},
doi={10.1109/JIOT.2020.2972555},
ISSN={2327-4662},
month={April},}
@INPROCEEDINGS{7152629,
author={Yan, Cairong and Song, Yalong and Wang, Jian and Guo, Wenjing},
booktitle={2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing}, title={Eliminating the Redundancy in MapReduce-Based Entity Resolution},
year={2015},
volume={},
number={},
pages={1233-1236},
abstract={Entity resolution is the basic operation of data quality management, and the key step to find the value of data. The parallel data processing framework based on MapReduce can deal with the challenge brought by big data. However, there exist two important issues, avoiding redundant pairs led by the multi-pass blocking method and optimizing candidate pairs based on the transitive relations of similarity. In this paper, we propose a multi-signature based parallel entity resolution method, called multi-sig-er, which supports unstructured data and structured data. Two redundancy elimination strategies are adopted to prune the candidate pairs and reduce the number of similarity computation without affecting the resolution accuracy. Experimental results on real-world datasets show that our method tends to handle large datasets and it is more suitable for complex similarity computation than simple object matching.},
keywords={Redundancy;Computational modeling;Accuracy;Big data;Parallel processing;Time complexity;Conferences;entity resolution;MapReduce;blocking;redundancy elimination},
doi={10.1109/CCGrid.2015.24},
ISSN={},
month={May},}
@ARTICLE{8660441,
author={Guo, Wenzhong and Zhu, Weiping and Yu, Zhiyong and Wang, Jiangtao and Guo, Bin},
journal={IEEE Access}, title={A Survey of Task Allocation: Contrastive Perspectives From Wireless Sensor Networks and Mobile Crowdsensing},
year={2019},
volume={7},
number={},
pages={78406-78420},
abstract={Wireless sensor networks (WSNs) and mobile crowdsensing (MCS) are two important paradigms in urban dynamic sensing. In both sensing paradigms, task allocation is a significant problem, which may affect the completion quality of sensing tasks. In this paper, we give a survey of task allocation in WSNs and MCS from the contrastive perspectives in terms of data quality and sensing cost, which help to better understand related objectives and strategies. We first analyze the different characteristics of two sensing paradigms, which may lead to difference in task allocation issues or strategies. Then, we present some common issues in task allocation with objectives in data quality and sensing cost. Furthermore, we provide reviews of unique task allocation issues in MCS according to its new characteristics. Finally, we identify some potential opportunities for the future research.},
keywords={Sensors;Task analysis;Wireless sensor networks;Resource management;Data integrity;Wireless communication;Mobile handsets;Mobile crowdsensing (MCS);task allocation;wireless sensor networks (WSNs)},
doi={10.1109/ACCESS.2019.2896226},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8530818,
author={Francis, Akindipe Olusegun and Emmanuel, Bugingo and Zhang, Defu and Zheng, Wei and Qin, Yingsheng and Zhang, Dongzhan},
booktitle={2018 Sixth International Conference on Advanced Cloud and Big Data (CBD)}, title={Exploration of Secured Workflow Scheduling Models in Cloud Environment: A Survey},
year={2018},
volume={},
number={},
pages={71-76},
abstract={Cloud computing (CC) is a useful tool for executing complex applications. As a result of this, it has become so popular and used in diverse domains such as science, engineering, medicine. etc. CC structure is composed of a number of virtual machines(VMs) provisioned on demand and charged on a "Pay-as-you-go" basis, it is deployed in different form of access levels. Complex applications needed to be executed on clouds are represented as workflows. Workflow scheduling (WS) is one of the most important concepts in cloud computing. WS model contributes to minimizing cost, makespan and energy as well as maximize the quality of service(QoS) of applications in clouds. Despite the security constraints set by each provider, CC has become so critical due to the considerations of applications with sensitive intermediate data, this thereby requires a security level known as Secured workflow Scheduling(SWS). This security is on the level of executing workflows. It indicates that applications with sensitive interdependent data have to be protected during their execution across different cloud VMs. The addition of security in workflow execution generates time overhead, making it complex to meet up with the QoS required by the users. Some research works have proposed algorithms for providing the QoS requirements and security at the same time. In this work, we survey some existing works, by defining the factors needed in securing workflows during execution, clarifying the domains for security, sources of security threats and their solutions as well as cloud computing services that needs security and lastly classify the proposed algorithm depending cloud computing components.},
keywords={Cloud computing;Security;Task analysis;Processor scheduling;Quality of service;Scheduling;Computational modeling;cloud computing;workflow scheduling;security;survey},
doi={10.1109/CBD.2018.00022},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9644693,
author={Wu, Junhang and Hu, Ruimin and Li, Dengshi and Xiao, Yilin and Ren, Lingfei and Hu, Wenyi},
booktitle={2021 IEEE Intl Conf on Parallel Distributed Processing with Applications, Big Data Cloud Computing, Sustainable Computing Communications, Social Computing Networking (ISPA/BDCloud/SocialCom/SustainCom)}, title={Multi-network Embedding for Missing Point-of-Interest Identification},
year={2021},
volume={},
number={},
pages={1386-1393},
abstract={The large volume of data flowing throughout location-based social networks (LBSNs) provides an opportunity for human mobility behavior understanding and prediction. However, data quality issues (e.g., historical check-in POI missing, data sparsity) limit the effectiveness of existing LBSN-oriented studies, e.g., Point-of-Interest (POI) recommendation or prediction. Contrary to previous efforts in next POI recommendation or prediction, we focus on identifying the missing POI which the user has visited at a past specific time and proposed a multi-network Embedding (MNE) method. Specifically, the model jointly captures temporal cyclic effect, user preference and sequence transition influence in a unified way by embedding five relational information graphs into a shared dimensional space from both POI- and category-instance levels. The proposed model also incorporates region-level spatial proximity to explore geographical influence, and derives the ranking score list of candidates for missing POI identification. We conduct extensive experiments to evaluate the performance of our model on two real large-scale datasets, and the experimental results show its superiority over other competitors. Significantly, it also proves that the proposed model can be naturally transferred to general next POI recommendation and prediction tasks with competitive performances.},
keywords={Social networking (online);Data integrity;Predictive models;Task analysis;Tuning;Context modeling;location-based social networks (LBSNs);missing POI identification;multi-network embedding},
doi={10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00189},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8890371,
author={Liang, Zilu and Chapa Martell, Mario Alberto},
booktitle={2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)}, title={Achieving Accurate Ubiquitous Sleep Sensing with Consumer Wearable Activity Wristbands Using Multi-class Imbalanced Classification},
year={2019},
volume={},
number={},
pages={768-775},
abstract={Consumer activity wristbands such as Fitbit provide an affordable method for ubiquitous sleep sensing in daily settings. These devices are also increasingly used in scientific studies as measurement tools of sleep outcomes. Nevertheless, the accuracy of Fitbit has raised wide concern. In this paper, we explore the feasibility of applying machine learning to improve the quality of Fitbit sleep data. The problem of interest was formulated into a multiclass imbalanced classification problem. We examined the performance of different combinations of seven machine learning algorithms and three resampling techniques. The preliminary results showed that the accuracy in detecting wakefulness and light sleep was improved by up to 43% and 44% respectively compared to the proprietary algorithm of Fitbit. Our future work will focus on improving the overall accuracy of the classification models in detecting all sleep stages.},
keywords={Sleep;Training;Machine learning algorithms;Classification algorithms;Standards;Vegetation;Heart rate;wearable;data quality;sleep;machine learning;Fitbit},
doi={10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00143},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7229729,
author={Gowsalya, M and Krushitha, K and Valliyammai, C},
booktitle={2014 Sixth International Conference on Advanced Computing (ICoAC)}, title={Predicting the risk of readmission of diabetic patients using MapReduce},
year={2014},
volume={},
number={},
pages={297-301},
abstract={From the banking to retail, many sectors have already embraced big data regardless of whether the information comes from public or private sources. In the clinical sphere, the amount of patient data has grown exponentially because of computer based information systems. E-Health monitoring applications have some particularities concerning the importance on data quality. This paper presents a novel solution using Hadoop Mapreduce to analyze large datasets and extract useful insights from the dataset which helps doctors to effectively allocate resources. The successful healthcare delivery and planning strongly rely on data (e.g. sensed data, diagnosis, administration information); the higher quality of the data, the better will be the patient assistance. The applications are also particularly exposed to a contextual environment (i.e., patient's mobility, communication technologies, performance, information heterogeneity, etc.) that has an important impact on information management and application achievement. The main objective of our system is to predict the risk of diabetic patients for readmission in the next 30 days by measuring the probability using MapReduce. This risk score helps the physicians in recommending appropriate care for the patients.},
keywords={Atmospheric measurements;Particle measurements;Diabetes;Sociology;Statistics;Artificial neural networks;Big data;Healthcare;Predictive analytics;Diabetes},
doi={10.1109/ICoAC.2014.7229729},
ISSN={2377-6927},
month={Dec},}
@INPROCEEDINGS{9708977,
author={Hu, Pan and Gu, Hailin and Qi, Jun and Gao, Qiang and Xia, Yu and Qu, Ruiting},
booktitle={2021 2nd International Conference on Big Data Economy and Information Management (BDEIM)}, title={Design of two-stage federal learning incentive mechanism under specific indicators},
year={2021},
volume={},
number={},
pages={475-478},
abstract={As a privacy-focused distributed machine learning, federated learning can not only train models effectively but also prevent data sets from being leaked easily. However, like crowdsensing perception and other technologies, participants often lack the motivation to learn and the quality of participation is not high. Therefore, this paper mainly designs a two-stage federal learning incentive mechanism based on the Stackelberg game model under a specific model accuracy index. Firstly, we combine data quality and data quantity to construct a federal learning incentive mechanism model under specific indicators. Then, we conduct a two-stage Stackelberg game analysis on the incentive mechanism model based on the utility function construction of the server platform and data island. In the first stage, the platform server is the leader and the data island is the follower. The second stage is a Nash equilibrium game between data islands. Finally, we construct the objective function of the server platform and data island, namely utility maximization, deduce the optimal equilibrium solution of the two-stage game, and determine the optimal strategy of the platform server and data island.},
keywords={Analytical models;Biological system modeling;Games;Machine learning;Nash equilibrium;Linear programming;Collaborative work;Federal learning;Incentive mechanism;Stackelberg game;Nash equilibrium;Specific indicators},
doi={10.1109/BDEIM55082.2021.00103},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9047270,
author={Huang, Chao and Lin, Mingwei and Chen, Riqing},
booktitle={2019 IEEE Intl Conf on Parallel Distributed Processing with Applications, Big Data Cloud Computing, Sustainable Computing Communications, Social Computing Networking (ISPA/BDCloud/SocialCom/SustainCom)}, title={Probabilistic Linguistic VIKOR Method Based on TODIM for Reliable Participant Selection Problem in Mobile Crowdsensing},
year={2019},
volume={},
number={},
pages={712-717},
abstract={In the mobile crowdsensing systems, the participants of great variety and diversity voluntarily submit their sensing data. Evaluating the participants and ranking them is a critical problem that should be solved to ensure the data quality. In this paper, we introduce the concept of probabilistic linguistic term sets (PLTSs) to model the group preference information during the process of ranking candidate participants and then propose novel VIKOR methods based on TODIM for solving the process of ranking reliable participants and selecting the best one in the mobile crowdsensing system. This proposed methods combine the advantages from the VIKOR method and TODIM method. To show the implementation process of evaluating participants and selecting the best one under the PLTS information context, a practical case is given to verify the feasibility of the proposed methods. Compared with the existing decision making methods, the proposed methods show their effectiveness.},
keywords={Linguistics;Sensors;Crowdsensing;Task analysis;Decision making;Probabilistic logic;Cloud computing;Mobile crowdsensing, Probabilistic linguistic term set, Participants ranking, TODIM, VIKOR},
doi={10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00108},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8965303,
author={Caihong, Zhou and Zengyuan, Wu and Chang, Liu},
booktitle={2019 IEEE International Conference on Smart Manufacturing, Industrial Logistics Engineering (SMILE)}, title={A Study on Quality Prediction for Smart Manufacturing Based on the Optimized BP-AdaBoost Model},
year={2019},
volume={},
number={},
pages={1-3},
abstract={To accurately predict the product quality in smart manufacturing, this paper designs the BP-AdsysBoost model on the basis of BP neural network and AdaBoost algorithm. The BP-AdsysBoost model considers both the data characteristics and the technology advantages, which pays more attentions to the unqualified products wrongly predicted. To further examine the model, the 110560 data of smart manufacturing from German BOSCH company is used for this research. The proposed BP-AdsysBoost model is compared with the BP neural network and the unmodified BP-AdaBoost model according to prediction performance. The results show that the BP-AdsysBoost model has significant advantages in prediction accuracy and FDR, which proves its satisfied prediction ability for product quality in smart manufacturing.},
keywords={smart manufacturing;big data;quality prediction;BP neural network;AdaBoost algorithm;BP-AdsysBoost model},
doi={10.1109/SMILE45626.2019.8965303},
ISSN={},
month={April},}
@INPROCEEDINGS{7336422,
author={Zhang, Jinkui and Zhu, Yongxin and Shi, Weiwei and Sheng, Gehao and Chen, Yufeng},
booktitle={2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems}, title={An Improved Machine Learning Scheme for Data-Driven Fault Diagnosis of Power Grid Equipment},
year={2015},
volume={},
number={},
pages={1737-1742},
abstract={In recent power grid systems, data-driven approach has been taken to grid condition evaluation and classification after successful adoption of big data techniques in internet applications. However, the raw training data from single monitoring system, e.g. dissolved gas analysis (DGA), are rarely sufficient for training in the form of valid instances and the data quality can rarely meet the requirement of precise data analytics since raw data set usually contains samples with noisy data. This paper proposes a machine learning scheme (PCA_IR) to improve the accuracy of fault diagnose, which combines dimension-increment procedure based on association analysis, dimension-reduction procedure based on principal component analysis and back propagation neural network (BPNN). First, the dimension of training data is increased by adding selected data which originates from different source such as production management system (PMS) to the original data obtained by DGA. The added data would also inevitably result in more noise. Thus, we then take advantage of the PCA method to reduce the noise in the training data as well as retaining significant information for classification. Finally, the new training data yielded after PCA procedure is inputted into BPNN for classification. We test the PCA_IR scheme on fault diagnosis of power transformers in power grid system. The experimental results show that the classifiers based on our scheme achieve higher accuracy than traditional ones. Therefore, the scheme PCA_IR would be successfully deployed for fault diagnosis in power grid system.},
keywords={Fault diagnosis;Principal component analysis;Accuracy;Power grids;Correlation;Power transformers;Correlation coefficient;transformer fault diagnosis;PCC;PCA;BPNN},
doi={10.1109/HPCC-CSS-ICESS.2015.236},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8428903,
author={Dai, Minghui and Su, Zhou and Wang, Yuntao and Xu, Qichao},
booktitle={2018 International Conference on Selected Topics in Mobile and Wireless Networking (MoWNeT)}, title={Contract Theory Based Incentive Scheme for Mobile Crowd Sensing Networks},
year={2018},
volume={},
number={},
pages={1-5},
abstract={Mobile crowd sensing networks (MCSNs) have emerged as a promising paradigm to provide various sensing services. With the increasing number of mobile users, how to develop an effective scheme to provide the high-quality and secure sensing data becomes a new challenge. In this paper, we propose a contract theory based scheme to provide sensing service in MCSNs. At first, with the analysis of the interaction experience between the crowd sensing platform and mobile user, a trust scheme is introduced to guarantee the quality of sensing data by considering the direct trust and indirect trust. Next, according to the transaction between crowd sensing platform and mobile user, an optimal contract based on incentive scheme is designed to stimulate mobile users to participate in crowd sensing network, where the contract item can not only maximize the platform utility, but also satisfy individual rationality and incentive compatibility. Finally, the numerical results show that the proposal outperforms the conventional schemes.},
keywords={Sensors;Contracts;Task analysis;Conferences;Big Data;Edge computing;Smart cities;Mobile crowd sensing;trust scheme;optimal contract},
doi={10.1109/MoWNet.2018.8428903},
ISSN={},
month={June},}
@INPROCEEDINGS{8419368,
author={Shi, Jingyi and Zheng, Mingna and Yao, Lixia and Ge, Yaorong},
booktitle={2018 IEEE International Conference on Healthcare Informatics (ICHI)}, title={A Publication-Based Popularity Index (PPI) for Healthcare Dataset Ranking},
year={2018},
volume={},
number={},
pages={247-254},
abstract={Data are critical in this age of big data and machine learning. Due to their inherent complexity, health-related data are unique in that the datasets are usually acquired for specific purposes and with special designs. As more and more healthcare datasets become available, of which many are public, choosing a quality dataset that is suitable for specific research inquiries is becoming a challenging question for health informatics researchers, especially the learners of this field. On the other hand, from the data provider's perspective, it is important to identify features of datasets that make some datasets more valuable than others so as to improve the design and acquisition of future datasets. To address these questions, we need to develop formal mechanisms to measure the goodness of datasets according to certain criteria. In this study, we propose one way of measuring the value of healthcare datasets that is based on how often the datasets are used and reported by researchers, which we call the Publication-based Popularity Index (PPI). In this article, we describe the design of the PPI and discuss its properties. We demonstrate the utility of the PPI by ranking 14 representative healthcare datasets. We believe that the PPI can enable an overall ranking of all healthcare datasets and thus provide an important dimension to sort search results for dataset integration systems as well as a starting point for identifying and examining the design of the most valuable healthcare datasets so that features of these datasets can inform future designs.},
keywords={Medical services;Market research;Indexes;Data integrity;Informatics;Histograms;Software;popularity index;healthcare dataset;data quality;quantified measurement;regression},
doi={10.1109/ICHI.2018.00035},
ISSN={2575-2634},
month={June},}
@INPROCEEDINGS{6832210,
author={Zheng, Liwei},
booktitle={2013 IEEE 10th International Conference on High Performance Computing and Communications 2013 IEEE International Conference on Embedded and Ubiquitous Computing}, title={AMD Based Service Agent Collaboration and Specification},
year={2013},
volume={},
number={},
pages={2277-2284},
abstract={With the emergence of Big Data in Internet, composing existing web services for satisfying new requirements, such as data quality enhancing, effective data choosing, knowledge discovering etc, has gained daily expanding attentions and interests. Many efforts have been pursued for supporting the essential activities in service composition. However, the existing techniques only focus on passive services which are waiting there for being discovered and invoked. We argue that it might be more attractive when Web services become active entities (Service Agent) distributed in Internet which can recognize the newly emergent requirements and compete with others for realize (part of) the requirements. Retreating or refinement of Big data will hardly be accomplished by one or two data handling center, Service Agent collaboration would be a competitive method for the big data handling problem. Mostly more than one service agents have to collaborate to satisfy requirements in current internet environment especially with social networks. That could be called as the requirement driven agent collaboration. Research on such collaboration might be useful for the previous problem. We have given a preliminary model for the requirement driven agent collaboration based on a function ontology and the automated mechanism design in the earlier work. This paper extended the Function Ontology, and enhanced the AMD model. That makes the interactions in MAS generated by agent collaboration can be described. A negotiation frame for the evaluation and choice of collaboration solutions is also given in this paper. It helps the requester evaluate the possible MAS systems, and helps the service agents make decisions to choose a good enough solution by negotiation. According to the dependencies provided in Function Ontology, a specification is given to describe the execution process of the chosen MAS. And also a method is given to translate the specification to BPEL which is more standard, acceptable, and easier to understood.},
keywords={Collaboration;Ontologies;Dynamic scheduling;Vectors;Web services;Multi-Agent;Collaboration;Mechanism design},
doi={10.1109/HPCC.and.EUC.2013.327},
ISSN={},
month={Nov},}
@ARTICLE{8528409,
author={Xiong, Jinbo and Chen, Xiuhua and Tian, Youliang and Ma, Rong and Chen, Lei and Yao, Zhiqiang},
journal={IEEE Access}, title={MAIM: A Novel Incentive Mechanism Based on Multi-Attribute User Selection in Mobile Crowdsensing},
year={2018},
volume={6},
number={},
pages={65384-65396},
abstract={In the user selection phase of mobile crowdsensing, most existing incentive mechanisms focus on either single-attribute selection or random selection, which possibly lead to serious consequences such as low user enthusiasm, decreased task completion rate, and increased cost of platform consumption. To tackle these issues, in this paper, we propose a novel incentive mechanism MAIM, which is based on multi-attribute user selection and participation intention analysis function in mobile crowdsensing. In this mechanism, the sensing platform employs the analytic hierarchy process to determine the weights of three attributes: participation threshold, cost, and reputation. The weight calculation results of each sensing user with respect to each attribute are then integrated to obtain the sorted weight of each user, with which the sensing platform will then obtain the optimal user set. From the users' perspective, they can autonomously decide whether to accept task processing requests, as enabled by the participation intention analysis function, thereby voiding the absolute authority and control of the sensing platform over users and achieving a two-way selection between the sensing platform and the sensing users. Furthermore, the sensing platform establishes a score-based reputation reward to inspire active performers and utilizes a punishment mechanism to overawe malicious vandals, which substantially helps activize enthusiasm of user participation and improve sensing data quality. Simulation results indicate that the proposed MAIM has significantly improved the sensing task completion ratio and the budget surplus ratio compared with the existing incentive mechanisms in mobile crowdsensing.},
keywords={Sensors;Task analysis;Analytic hierarchy process;Training;Technological innovation;Data integrity;Simulation;Mobile crowdsensing;incentive mechanism;analytic hierarchy process;multi-attribute user selection;participation intention analysis},
doi={10.1109/ACCESS.2018.2878761},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9752191,
author={Zhao, Lixia and Jin, Wei},
booktitle={2022 International Conference on Electronics and Renewable Systems (ICEARS)}, title={Analysis on the Design and Implementation of the Metadata Management Model in the Cloud Computing Business Intelligence Platform},
year={2022},
volume={},
number={},
pages={1656-1659},
abstract={Through the method of metadata management development, it can give full play to its advantages and make up for its disadvantages. In order to fully grasp the composition, conversion, analysis and processing process of data in the platform, from metadata sources, metadata scope, metadata classification, metadata users, metadata integration project development methods, metadata models and metadata standards, metadata management the implementation of the system and other aspects expounded the metadata management strategy in the business intelligence system. Effective metadata management has increased the usability of the platform by 5.6% and the data quality of the platform by 7.8%.},
keywords={Analytical models;Renewable energy sources;Codes;Databases;Computational modeling;Data integrity;Metadata;Metadata Management Model;Cloud Computing;Physical Business Intelligence Platform;Big Data},
doi={10.1109/ICEARS53579.2022.9752191},
ISSN={},
month={March},}
@INPROCEEDINGS{7396216,
author={Wiktorski, Tomasz and Hacker, Thomas and Hansen, Raymond A. and Rodgers, Gregory},
booktitle={2015 IEEE 7th International Conference on Cloud Computing Technology and Science (CloudCom)}, title={Experience with Problem-Based Learning in a Hybrid Classroom},
year={2015},
volume={},
number={},
pages={575-581},
abstract={Constructive alignment has been shown to elicit higher levels of learning among students. Problem-based Learning is one of the forms of constructive alignment often used in medicine and engineering education. We have applied a PBL-based approach to a graduate (master) course in Data Intensive Systems taught simultaneously through a video link at two universities in Europe and USA. Application of standardized measuring methodology shows inconclusive impact of PBL approach on students' learning. We present survey results and analyze major factors that could have lead to inconclusive result, including: low data quality, general applicability of constructivism in computer science, and issues with hybrid classroom and alternative laboratory environments. Finally, we discuss reversed classroom method as a potential solution to the issues encountered.},
keywords={Cloud computing;Computer science;Big data;Computers;Education;Virtual machining;problem-based learning;constructive alignment;data intensive systems;data science},
doi={10.1109/CloudCom.2015.70},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9060228,
author={Zhang, Xuejun and Chen, Qian and Peng, Xiaohui and Jiang, Xinlong},
booktitle={2019 IEEE SmartWorld, Ubiquitous Intelligence Computing, Advanced Trusted Computing, Scalable Computing Communications, Cloud Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)}, title={Differential Privacy-Based Indoor Localization Privacy Protection in Edge Computing},
year={2019},
volume={},
number={},
pages={491-496},
abstract={With the popularity of smart devices and the widespread use of the Wi-Fi-based indoor localization, edge computing is becoming the mainstream paradigm of processing massive sensing data to acquire indoor localization service. However, these data which were conveyed to train the localization model unintentionally contain some sensitive information of users/devices, and were released without any protection may cause serious privacy leakage. To solve this issue, we propose a lightweight differential privacy-preserving mechanism for the edge computing environment. We extend ε-differential privacy theory to a mature machine learning localization technology to achieve privacy protection while training the localization model. Experimental results on multiple real-world datasets show that, compared with the original localization technology without privacy-preserving, our proposed scheme can achieve high accuracy of indoor localization while providing differential privacy guarantee. Through regulating the value of ε, the data quality loss of our method can be controlled up to 8.9% and the time consumption can be almost negligible. Therefore, our scheme can be efficiently applied in the edge networks and provides some guidance on indoor localization privacy protection in the edge computing.},
keywords={Privacy;Training;Fingerprint recognition;Edge computing;Cloud computing;Indoor localization, Differential privacy, Privacy preserving, Edge computing.},
doi={10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00125},
ISSN={},
month={Aug},}
@INPROCEEDINGS{9760371,
author={Han, Shengqiang and Qu, Jianhua and Song, Jinyi and Liu, Zijing},
booktitle={2022 7th International Conference on Big Data Analytics (ICBDA)}, title={Second-hand Car Price Prediction Based on a Mixed-Weighted Regression Model},
year={2022},
volume={},
number={},
pages={90-95},
abstract={With the development of motor vehicles, the circulation demand of motor vehicles in the form of "second-hand cars" in circulation links is increasing. As a special "e-commerce commodity", second-hand cars are more complicated than ordinary e-commerce commodities. As a result, it is difficult to estimate the price of second-hand cars, which is not only influenced by the basic configuration of the car, but also by the car conditions. At present, the state has not issued a standard to judge the value of second-hand car. To solve this problem, in this paper, first making feature engineering, which includes data preprocessing and feature screening. Data preprocessing includes data cleaning and data transformation, data cleaning includes removing outliers and filling missing values, and data transformation is used to unify data format to improve data quality. The feature screening includes correlation analysis and feature extraction based on LightMBG, and the screened features provide the basis for model building, training and prediction. Then, five regression models are constructed by using the feature attributes obtained by the feature engineering for training, and evaluated. Then, Random Forest and XGBoost are weighted and mixed to got a novel regression model, and the effect of the model is better than that of the five regression models. Finally, the novel regression model is used to predict the price of second-hand cars.},
keywords={Training;Analytical models;Data preprocessing;Linear regression;Predictive models;Feature extraction;Cleaning;second-hand car price prediction;weighted regression model;LightGBM;XGBoost;random forest},
doi={10.1109/ICBDA55095.2022.9760371},
ISSN={},
month={March},}
@INPROCEEDINGS{8847069,
author={Zhao, Peng and Quan, Dekui and Yu, Wei and Yang, Xinyu and Fu, Xinwen},
booktitle={2019 28th International Conference on Computer Communication and Networks (ICCCN)}, title={Towards Deep Learning-Based Detection Scheme with Raw ECG Signal for Wearable Telehealth Systems},
year={2019},
volume={},
number={},
pages={1-9},
abstract={The electrocardiogram (ECG) signal, as one of the most important vital signs, can provide indications of many heart-related diseases. Nonetheless, in the case of telehealth context, the automated analysis and accurate detection of ECG signals remain unsolved issues, because the poor data quality collected by the wearable devices and unprofessional users further increases the complexity of hand-crafted feature extraction, ultimately affecting the efficiency of feature extraction and the detection accuracy. To address this issue and improve the detection accuracy, in this paper we present a novel detection scheme with the raw ECG signal in wearable telehealth system. Our system benefits from the concept of big data, sensing and pervasive computing and the emerging deep learning technology. In particular, a Deep Heartbeat Classification (DHC) scheme is proposed to analyze the ECG signal for arrhythmia detection. Distinct from existing solutions, the detection model in DHC can be trained directly on the raw ECG signal without hand-crafted feature extraction. A cloud-based prototypical system is also designed and implemented with the functions of data acquisition, wireless transmission, back-end data management, and ECG detection. The experimental results demonstrate that our prototypical system is feasible and effective in real-world practice, and extensive experimentation based on the MIT-BIH database demonstrates that the proposed DHC scheme outperforms baseline schemes.},
keywords={Electrocardiography;Feature extraction;Biomedical monitoring;Sensors;Cloud computing;Computational modeling;Data analysis},
doi={10.1109/ICCCN.2019.8847069},
ISSN={2637-9430},
month={July},}
@INPROCEEDINGS{7425923,
author={Hailong Liu and Zhanhuai Li and Cheqing Jin and Qun Chen},
booktitle={2016 International Conference on Big Data and Smart Computing (BigComp)}, title={Web-based techniques for automatically detecting and correcting information errors in a database},
year={2016},
volume={},
number={},
pages={261-264},
abstract={It is critical to detect and correct information errors effectively to achieve higher data quality in many applications. Most existing techniques only use the intrinsic information to detect and correct a database, provided that data is adequate and well-structured. These techniques will not work properly if there is no sufficient data available. Integrating the information from external sources, like the World Wide Web (WWW), can help us overcome the shortcomings of existing techniques to a great extent. In this paper, we introduce our on-going work that is capable of detecting and correcting data errors in a database by integrating external information from the WWW. The goal of our research is to pursuit another effective way to enhance information quality.},
keywords={World Wide Web;Databases;Knowledge based systems;Computational fluid dynamics;Data mining;Web sites;Strain},
doi={10.1109/BIGCOMP.2016.7425923},
ISSN={2375-9356},
month={Jan},}
@ARTICLE{8892516,
author={Liang, Tingting and Chen, Yishan and Gao, Wei and Chen, Ming and Zheng, Meilian and Wu, Jian},
journal={IEEE Access}, title={Exploiting User Tagging for Web Service Co-Clustering},
year={2019},
volume={7},
number={},
pages={168981-168993},
abstract={We propose a novel Web services clustering framework by considering the word distribution of WSDL documents and tags. Typically, tags are annotated to Web services by users for organization. In this paper, four strategies are proposed to integrate the tagging data and WSDL documents in the process of service clustering. Tagging data is inherently uncontrolled, ambiguous, and overly personalized. Two tag recommendation approaches are proposed to improve the tagging data quality and service clustering performance. Comprehensive experiments demonstrate the effectiveness of the proposed framework using a real-world dataset.},
keywords={Web services;Tagging;Search engines;Clustering algorithms;Task analysis;Feature extraction;Web service;WSDL documents clustering;co-clustering;tag recommendation},
doi={10.1109/ACCESS.2019.2950355},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{7543784,
author={Moysen, Jessica and Giupponi, Lorenza and Mangues-Bafalluy, Josep},
booktitle={2016 IEEE Symposium on Computers and Communication (ISCC)}, title={On the potential of ensemble regression techniques for future mobile network planning},
year={2016},
volume={},
number={},
pages={477-483},
abstract={Planning of current and future mobile networks is becoming increasingly complex due to the heterogeneity of deployments, which feature not only macrocells, but also an underlying layer of small cells whose deployment is not fully under the control of the operator. In this paper, we focus on selecting the most appropriate Quality of Service (QoS) prediction techniques for assisting network operators in planning future dense deployments. We propose to use machine learning as a tool to extract the relevant information from the huge amount of data generated in current 4G and future 5G networks during normal operation, which is then used to appropriately plan networks. In particular, we focus on radio measurements to develop correlative statistical models with the purpose of improving QoS-based network planning. In this direction, we combine multiple learners by building ensemble methods and use them to do regression in a reduced space rather than in the original one. We then compare the QoS prediction accuracy of various approaches that take as input the 3GPP Minimization of Drive Tests (MDT) measurements collected throughout a heterogeneous network and analyse their trade-offs. We also explain how the collected data is processed and used to predict QoS expressed in terms of Physical Resource Block (PRB)/ Megabit (MB) transmitted. This metric was selected because of the interest it may have for operators in planning, since it relates lower layer resources with their impact in terms of QoS up in the protocol stack, hence closer to the end-user.},
keywords={Quality of service;Planning;Training;Computers;Measurement;Principal component analysis;3GPP;Machine Learning;Big Data;Quality of Service;Prediction;Network planning;Minimization of Drive Tests},
doi={10.1109/ISCC.2016.7543784},
ISSN={},
month={June},}
@INPROCEEDINGS{9730364,
author={Wu, Hao and Liu, Qi and Liu, Xiaodong and Zhang, Yonghong and Xu, Xiaolong and Bilal, Muhammad},
booktitle={2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)}, title={A FCN Approach to Blockage Correction in Radars},
year={2021},
volume={},
number={},
pages={482-487},
abstract={Doppler weather radar is the most widely used convection detector with the highest resolution in the ground. Echo reflectance data from the weather radar is the key reference for the meteorological department to carry out severe convective weather forecast and early warning, quantitative precipitation estimation(QPE) and quantitative precipitation forecast(QPF). However, in the process of radar detection, it is inevitable to be affected by obstacles, ground object echo interference, radar echo attenuation and other phenomena, resulting in poor data quality of detection results. Therefore, it is very important to correct the missing or disturbed data. On the other hand, with the rapid development of artificial intelligence technology in recent years, more and more meteorological researchers begin to introduce deep learning and other machine learning methods into the research of meteorological field such as weather radar data processing. In this paper, a deep convolutional encoder-decoder network is proposed to correct the beam blocking of weather radar. In this study, the correction of radar beam blockage is regarded as an image inpainting problem. It's the first trying to use deep learning to realize the correction of radar beam blockage. Experiment shows that the method proposed in this paper is significantly better than the traditional method in accuracy, error rate, false alarm rate and other aspects. The method can directly identify and correct the blocking area, and the operation procedure is simple compared traditional methods.},
keywords={Deep learning;Reflectivity;Meteorological radar;Semantics;Neural networks;Radar detection;Weather forecasting;Deep learning;convolutional neural networks;encoder-decoder network;weather radar;image inpainting;blockage correction},
doi={10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00086},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6860977,
author={Fresco Zamora, Jane Louie and Sawada, Naoya and Sahara, Takemi and Kashihara, Shigeru and Taenaka, Yuzo and Yamaguchi, Suguru},
booktitle={2014 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) Proceedings}, title={Surface weather observation via distributed devices},
year={2014},
volume={},
number={},
pages={1405-1410},
abstract={With rain-related hazards, it is difficult to forecast and prepare for it due to the decline in the availability and reliability of global daily weather reports. Therefore, we need to make use of available unconventional commercial weather instruments to provide supplementary information to existing systems. A more accurate and reliable weather forecast can then be made due to the prompt availability of information from ubiquitous devices. As smartphones become a widely used device, we propose a conceptual design of a multi-device ground weather observation network using smartphones and other sensors. In this paper, we first investigate the differences between smartphone-based sensors and other sensors to determine issues to address for big data on global weather information. In our experiments, we found that the data quality differs among devices in a very small area of 100 m grid.},
keywords={Temperature measurement;Temperature sensors;Rain;Smart phones;Humidity;Distributed devices;Measurement;Localized Rain},
doi={10.1109/I2MTC.2014.6860977},
ISSN={1091-5281},
month={May},}
@INPROCEEDINGS{9744889,
author={Yang, Bikai and Bing, Han and Zhao, Haiyang and Gu, Tianshu and Cai, Tianxiao},
booktitle={2022 IEEE International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)}, title={Multi dimensional disease intelligent detection device for underwater pier column structure through machine learning},
year={2022},
volume={},
number={},
pages={544-549},
abstract={The total number of railway bridges in China has exceeded twenty thousand. As an important part of the railway line, the existing railway bridge bears great live load, excessive concentrated force, obvious dynamic effect and high requirements for foundation stability. However, because the underwater structure of Railway Pier is constantly affected by many factors such as water scouring and ship collision during its service, it leads to a variety of diseases such as concrete peeling and exposed reinforcement. These diseases are often hidden in the water, which poses a serious threat to the structural safety of the bridge and the safety of people&#x0027;s lives and property. However, the traditional detection method of underwater structure of Railway Pier is affected by water flow, water quality, water depth and other environment, which has the pain points of high detection risk, low efficiency, poor detection data quality and missing disease location, so it is difficult to effectively detect underwater structure diseases; In this context, a multi-dimensional disease detection device is designed and developed for Wuhan Yangtze River Bridge (both highway and railway). The device includes a fixed module on water and an underwater detection module, which can realize safe and efficient detection under the condition of rapids and deep water, solve the problems that it is difficult for personnel and existing equipment to reach the structure to be tested, and the detection effect obtained by existing detection means is not ideal The detection information is not comprehensive, which is difficult to be used for key problems such as follow-up structural technical state evaluation.},
keywords={Underwater structures;Bridges;Road transportation;Employee welfare;Structural panels;Water quality;Rail transportation;Railway bridges;Pier underwater structure;Non destructive testing;Disease location;Machine learning},
doi={10.1109/EEBDA53927.2022.9744889},
ISSN={},
month={Feb},}
@ARTICLE{9729745,
author={Kazemi, Arefeh and Mozafari, Jamshid and Nematbakhsh, Mohammad Ali},
journal={IEEE Access}, title={PersianQuAD: The Native Question Answering Dataset for the Persian Language},
year={2022},
volume={10},
number={},
pages={26045-26057},
abstract={Developing Question Answering systems (QA) is one of the main goals in Artificial Intelligence. With the advent of Deep Learning (DL) techniques, QA systems have witnessed significant advances. Although DL performs very well on QA, it requires a considerable amount of annotated data for training. Many annotated datasets have been built for the QA task; most of them are exclusively in English. In order to address the need for a high-quality QA dataset in the Persian language, we present PersianQuAD, the native QA dataset for the Persian language. We create PersianQuAD in four steps: 1) Wikipedia article selection, 2) question-answer collection, 3) three-candidates test set preparation, and 4) Data Quality Monitoring. PersianQuAD consists of approximately 20,000 questions and answers made by native annotators on a set of Persian Wikipedia articles. The answer to each question is a segment of the corresponding article text. To better understand PersianQuAD and ensure its representativeness, we analyze PersianQuAD and show it contains questions of varying types and difficulties. We also present three versions of a deep learning-based QA system trained with PersianQuAD. Our best system achieves an F1 score of 82.97% which is comparable to that of QA systems on English SQuAD, made by the Stanford University. This shows that PersianQuAD performs well for training deep-learning-based QA systems. Human performance on PersianQuAD is significantly better (96.49%), demonstrating that PersianQuAD is challenging enough and there is still plenty of room for future improvement. PersianQuAD and all QA models implemented in this paper are freely available.},
keywords={Internet;Online services;Encyclopedias;Training;Task analysis;Machine translation;Buildings;Dataset;deep learning;natural language processing;Persian;question answering;machine reading comprehension},
doi={10.1109/ACCESS.2022.3157289},
ISSN={2169-3536},
month={},}
@ARTICLE{9382202,
author={},
journal={IEEE Std 3652.1-2020}, title={IEEE Guide for Architectural Framework and Application of Federated Machine Learning},
year={2021},
volume={},
number={},
pages={1-69},
abstract={Federated machine learning defines a machine learning framework that allows a collective model to be constructed from data that is distributed across repositories owned by different organizations or devices. A blueprint for data usage and model building across organizations and devices while meeting applicable privacy, security and regulatory requirements is provided in this guide. It defines the architectural framework and application guidelines for federated machine learning, including description and definition of federated machine learning; the categories federated machine learning and the application scenarios to which each category applies; performance evaluation of federated machine learning; and associated regulatory requirements.},
keywords={IEEE Standards;Machine learning;Privacy;Modeling;Economics;Collaborative work;Metasearch;computation efficiency;economic viability;federated machine learning (FML);IEEE 3652.1™;incentive mechanism;machine learning;model performance;privacy;privacy regulations;security},
doi={10.1109/IEEESTD.2021.9382202},
ISSN={},
month={March},}
@ARTICLE{8704713,
author={Ahmad, Arshad and Feng, Chong and Li, Kan and Asim, Syed Mohammad and Sun, Tingting},
journal={IEEE Access}, title={Toward Empirically Investigating Non-Functional Requirements of iOS Developers on Stack Overflow},
year={2019},
volume={7},
number={},
pages={61145-61169},
abstract={Context: Mobile application developers are getting more concerned due to the importance of quality requirements or non-functional requirements (NFR) in software quality. Developers around the globe are actively asking a question(s) and sharing solutions to the problems related to software development on Stack Overflow (SO). The knowledge shared by developers on SO contains useful information related to software development such as feature requests (functional/non-functional), code snippets, reporting bugs or sentiments. Extracting the NFRs shared by iOS developers on programming Q&A website SO has become a challenge and a less researched area. Objective: To identify and understand the real problems, needs, trends, and the critical NFRs or quality requirements discussed on Stack Overflow related to iOS mobile application development. Method: We extracted and used only the iOS posts data of SO. We applied the well-known statistical topical model Latent Dirichlet Allocation (LDA) to identify the main topics in iOS posts on SO. Then, we labeled the extracted topics with quality requirements or NFRs by using the wordlists to assess the trend, evolution, hot and unresolved NFRs in all iOS discussions. Results: Our findings revealed that the highly frequent topics the iOS developers discussed are related to usability, reliability, and functionality followed by efficiency. Interestingly, the most problematic areas unresolved are also usability, reliability, and functionality though followed by portability. Besides, the evolution trend of each of the six different quality requirements or NFRs over time is depicted through comprehensive visualization. Conclusion: Our first empirical investigation on approximately 1.5 million iOS posts and comments of SO gives insight on comprehending the NFRs in iOS application development through the lens of real-world practitioners.},
keywords={Software;Market research;Data mining;Application programming interfaces;Mobile communication;Mobile applications;Technological innovation;Non-functional requirements (NFRs);quality requirements;iOS;Latent Dirichlet allocation (LDA);Stack Overflow},
doi={10.1109/ACCESS.2019.2914429},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9677226,
author={Qasim, Amer and El Refae, Ghaleb A. and Issa, Hussein and Eletter, Shorouq},
booktitle={2021 22nd International Arab Conference on Information Technology (ACIT)}, title={The Impact of Drone Technology on The Accounting Profession: The Case of Revenue Recognition in Long-Term Construction Contracts},
year={2021},
volume={},
number={},
pages={1-4},
abstract={The accounting profession has gone through radical changes due to recent technological advancements in AI, blockchain technologies, big data, etc. More recently, the accounting literature discussed the possibility of using the drone innovative technology in conducting inventory observation as well as internal and external auditing. This study is a visionary paper which investigates the applicability of a remotely auditing process using drone technology in real-estate accounting. The drone will be used to conduct site inspection to assess and monitor the construction progress through applying the percentage of completion method to recognize revenues from long-term contracts. This innovative technology has the ability to collect better data quality, saving cost and saving time with improved site safety which will help improve the auditor tasks.},
keywords={Costs;Data integrity;Inspection;Safety;Blockchains;Task analysis;Information technology;Drones;Artificial Intelligence;Real-Estate Accounting;ERP systems;Revenue Recognition},
doi={10.1109/ACIT53391.2021.9677226},
ISSN={},
month={Dec},}
@ARTICLE{9715073,
author={Rudnitckaia, Julia and Venkatachalam, Hari Santhosh and Essmann, Roland and Hruška, Tomáš and Colombo, Armando Walter},
journal={IEEE Access}, title={Screening Process Mining and Value Stream Techniques on Industrial Manufacturing Processes: Process Modelling and Bottleneck Analysis},
year={2022},
volume={10},
number={},
pages={24203-24214},
abstract={One major result of the Industrial Digitalization is the access to a large set of digitalized data and information, i.e. Big Data. The market of analytic tools offers a huge variety of algorithms and software to exploit big datasets. Implementing their advantages into one approach brings better results and empower possibilities for process analysis. Its application in the manufacturing industry requires a high level of effort and remains to be challenging due to product complexity, human-centric processes, and data quality. In this manuscript, the authors combine process mining and value streams methods for analyzing the data from the information management system, applying the approach to the data delivered by one specific manufacturing system. The manufacturing process to be examined is the process of assembling gas meters in the manufacture. This specific and important part of the whole supply-chain process was taken as suitable for the study due to almost full-automated line with data about each process activity of the value-stream in the information system. The paper applies process mining algorithms in discovering a descriptive process model that plays the main role as a basis for further analysis. At the same time, modern techniques of the bottleneck analysis are described, and two new comprehensible methods of bottlenecks detection (TimeLag and Confidence intervals methods), as well as their advantages, will be discussed. Achieved results can be subsequently used for other sources of big data and industrial-compliant Information Management Systems.},
keywords={Data mining;Manufacturing;Information management;Production;Manufacturing processes;Companies;Analytical models;Bottleneck analysis;manufacturing process;process mining;process modelling;information management system;value stream},
doi={10.1109/ACCESS.2022.3152211},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8278765,
author={Ruester, Christian and Haussel, Fabian and Huehn, Thomas and El Sayed, Nadim},
booktitle={International ETG Congress 2017}, title={VEREDELE-FACDS Field Trial: Wide Area Power Quality Assessment With IOT Sensors and Cloud-Based Analytics},
year={2017},
volume={},
number={},
pages={1-5},
abstract={With the increasing share of renewable generation in low voltage distribution, the edge of the power grid is slowly replacing classical, large-scale power stations, taking up roles and responsibilities that were unimaginable only a few years ago. However, while feed-in on the 400V level is now commonplace, grid state and power quality monitoring still lag far behind because of obvious cost and complexity reasons. In fact, only very few parts of Germanys approx. 1,1 million km long 400V distribution grid are actively monitored today and even basic grid quality parameters such as the voltage level at the end of the line are typically unknown. Reducing the cost per measurement point and the complexity of data analysis is thus of paramount importance for enabling wide-area monitoring of the LV power grid. The authors explore the feasibility of this goal by examining the performance of a nonconventional measurement system. It consists of a network of Internet-of-Things (IOT)-based power quality sensors, connected to a cloud-based big data analysis platform. Specifically, measurement nodes comprise of voltage sensors attached to consumer-grade smartphones and WIFI access points. Sensor data is automatically uploaded to the cloud system with the MQTT protocol. First results from a field trial in a rural area in Germany indicate good data quality and show excellent promise for detailed assessments of the edge of the power grid.},
keywords={},
doi={},
ISSN={},
month={Nov},}
@ARTICLE{9267091,
author={Yuan, Yanhua O and Bozdağ, Ebru and Ciardelli, Caio and Gao, Fuchun and Simons, F J},
journal={Geophysical Journal International}, title={The exponentiated phase measurement, and objective-function hybridization for adjoint waveform tomography},
year={2019},
volume={221},
number={1},
pages={1145-1164},
abstract={Seismic tomography has arrived at the threshold of the era of big data. However, how to extract information optimally from every available time-series remains a challenge; one that is directly related to the objective function chosen as a distance metric between observed and synthetic data. Time-domain cross-correlation and frequency-dependent multitaper traveltime measurements are generally tied to window selection algorithms in order to balance the amplitude differences between seismic phases. Even then, such measurements naturally favour the dominant signals within the chosen windows. Hence, it is difficult to select all usable portions of seismograms with any sort of optimality. As a consequence, information ends up being lost, in particular from scattered waves. In contrast, measurements based on instantaneous phase allow extracting information uniformly over the seismic records without requiring their segmentation. And yet, measuring instantaneous phase, like any other phase measurement, is impeded by phase wrapping. In this paper, we address this limitation by using a complex-valued phase representation that we call ‘exponentiated phase’. We demonstrate that the exponentiated phase is a good substitute for instantaneous-phase measurements. To assimilate as much information as possible from every seismogram while tackling the non-linearity of inversion problems, we discuss a flexible hybrid approach to combine various objective functions in adjoint seismic tomography. We focus on those based on the exponentiated phase, to take into account relatively small-magnitude scattered waves; on multitaper measurements of selected surface waves; and on cross-correlation measurements on specific windows to select distinct body-wave arrivals. Guided by synthetic experiments, we discuss how exponentiated-phase, multitaper and cross-correlation measurements, and their hybridization, affect tomographic results. Despite their use of multiple measurements, the computational cost to evaluate gradient kernels for the objective functions is scarcely affected, allowing for issues with data quality and measurement challenges to be simultaneously addressed efficiently.},
keywords={Inverse theory;Time-series analysis;Seismic tomography},
doi={10.1093/gji/ggaa063},
ISSN={1365-246X},
month={Dec},}
@INPROCEEDINGS{9713501,
author={Liu, Jiaxin and Wang, Shuai and Lu, Xuchen and Li, Tong},
booktitle={2021 IEEE 5th Conference on Energy Internet and Energy System Integration (EI2)}, title={Research on Online Status Evaluation Technology for Main Equipment of Power Transmission and Transformation Based on Digital Twin},
year={2021},
volume={},
number={},
pages={3368-3373},
abstract={Traditional status evaluation for main equipment of power transmission and transformation has some shortages, such as low timeliness, low data quality and difficulty for evaluation model construction. Based on digital twin technology system, this paper presents technology of equipment status, and constructs digital twin for power transmission and transformation equipment. According to operation characteristics of power transmission and transformation equipment, fusion and cleansing of perception data is realized. Relying on big data analysis and data mining, status evaluation differentiation, accurate fault diagnosis and status prediction for power transmission and transformation equipment is realized. Further more, this paper analyzes the application of digital twin technology in on-line status evaluation for transformer equipment, expounds specific application of digital twin technology including data governance and model building, and summarizes application prospect of digital twin technology in on-line status evaluation for main equipment of power transmission and transformation.},
keywords={Digital twin;Power transmission;System integration;Maintenance engineering;Reliability engineering;Transformers;Real-time systems;digital twin;status evaluation;online evaluation;fault diagnosis;status prediction;transformer},
doi={10.1109/EI252483.2021.9713501},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7577612,
author={Xue, Chunlu and Guo, Lin and Hu, Hualang and Pei, Zhiyuan},
booktitle={2016 Fifth International Conference on Agro-Geoinformatics (Agro-Geoinformatics)}, title={Management and spatial evolution of rural land circulation: Taking Zhengjia Town as an example},
year={2016},
volume={},
number={},
pages={1-6},
abstract={The scale operation in various forms of rural land circulation in China is the way for the development of modern agriculture, and is also the basic direction of agricultural reform. The right to rural land contractual management registration makes the position of cadastral land and its interrelated information, as contractor and area, clear by Geographic Information Systems (GIS). That supplied a data base for land circulation spatial management. In this paper, we discussed the correlation between the management and the spatial position of transferred rural land, in order to support the policy and decision for agriculture. Taking Zhengjia Town as an example, which located in the west of Dongchangfu District, Liaocheng, Shandong, we made a survey and got the information about the land circulation, the operators who engaged in the transferred land, and the farmers who had transferred their farmlands from 2011 to 2015. Based on the outcome data of right to rural land contractual management registration, the circulation parcels and the cadastral parcels were linked by their only parcel code, and then formed the land circulation spatial information. Using ArcGIS 10.1 for spatial analysis and correlation analysis, we analyzed the correlation between the management and the spatial evolution of the transferred land on some indicators including acquirement intention, operation style, land use, circulation variability, distribution of circulation management right, and compared the income of the operators and the farmers. According to the survey, the scope of the transferred land was gradually increased. The area was 16.09ha in 2011 and 147.27ha in 2015, 19.22% of the contracted land. The results show that the main form of land circulation was to rent, and scale operation in northern and fragmented operation in southern. The transferred lands which were acquired by family contract have the largest area. However, in the south, the farmers seemed like to transfer their lands which were acquired by bidding. The operators aged in 40-49 managed the most transferred land, accounting for 77.00% of the total circulation area. Followed by 39 years old under, and the over 50 years old was the least. Before transferred, all farmland was used to grow grain. But 93% farmlands were changed the crops after land circulation. As the investment of time, capital, farmland and implied labor increased, there was a certain increase in income of the operators and the farmers, however, and a few operators had no profit because of the business model. There was no spatial correlation between the farmlands position and the farmers' income at the fragmented operation region. The results can provide an idea for spatial information management of land circulation based on GIS, and defend the farmers' contractual rights when the boundaries are destroyed. With the spatial informatization of farmland and the improvement of data quality and quantity, big data will further predict land circulation spatial arrangement and business model.},
keywords={Correlation;Agriculture;Vegetation;Geographic information systems;Economics;Information management;GIS;land circulation;right registration;spatial evolution},
doi={10.1109/Agro-Geoinformatics.2016.7577612},
ISSN={},
month={July},}
@INPROCEEDINGS{7944912,
author={},
booktitle={2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)}, title={Big Data Quality Assurance Workshop Chairs and Committee},
year={2017},
volume={},
number={},
pages={xvii-xvii},
abstract={Provides a listing of current committee members and society officers.},
keywords={},
doi={10.1109/BigDataService.2017.57},
ISSN={},
month={April},}
@ARTICLE{8913513,
author={Garg, S. and Guizani, M. and Guo, S. and Verikoukis, C.},
journal={IEEE Transactions on Industrial Informatics}, title={Guest Editorial Special Section on AI-Driven Developments in 5G-Envisioned Industrial Automation: Big Data Perspective},
year={2020},
volume={16},
number={2},
pages={1291-1295},
abstract={The papers in this special section examine artificial intelligence (AI)-driven developments in 5G mobile communications for industrial automation applications from a Big Data perspective. With the recent advances in information and communication technologies, industrial automation is expanding at a rapid pace. This transition is characterized by “Industry 4.0”, the fourth revolution in the field of manufacturing. Industry 4.0, also called as “Industrial Internet of Things (IIoT)” or “Smart Factories”, is a reflection of new industrial revolution that is not only interconnected, but also communicate, analyze, and use the information to create a more holistic and better connected ecosystem for the industries.},
keywords={Special issues and sections;Artificial intelligence;Big Data;Quality of service;Automation;5G mobile communication},
doi={10.1109/TII.2019.2955963},
ISSN={1941-0050},
month={Feb},}
@INPROCEEDINGS{7207185,
author={},
booktitle={2015 IEEE International Congress on Big Data}, title={[Title page i]},
year={2015},
volume={},
number={},
pages={i-i},
abstract={The following topics are dealt with: data mining; social network; data privacy; learning; query processing; big data processing; big data quality; big data platform; big data semantics; health care; network management; distributed processing; social media; and image processing.},
keywords={},
doi={10.1109/BigDataCongress.2015.1},
ISSN={2379-7703},
month={June},}
@INPROCEEDINGS{7133983,
author={Nicklas, Daniela},
booktitle={2015 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops)}, title={Keynote: Context, big data, and digital prejudices},
year={2015},
volume={},
number={},
pages={1-1},
abstract={In pervasive computing research and literature, context has mostly been seen as an information source for applications that adapt their behavior according to the current situation of their user or their (often physical) environment. This adaptation could be the change of the user interface, the performance of actions (like sending messages or triggering actuators), or the change of used resources (like network bandwidth or processing power). To determine relevant situations, many heterogeneous data sources could be used, ranging from sensor data over mined patterns in files to explicit user input. Since most sensors are not perfect, context quality has to be considered. And since many context-aware applications are mobile, the set of data sources may change during runtime. According to the widely used definition by Anind Dey, context can be “any information that can be used to characterize the situation of an entity”. In the past years, we have seen a significant increase in the so-called “big data” domain, in research, technology, and industrial usage. The desire to analyze, gain knowledge and use more and more data it in new ways is rising in a way that resemble a gold rush. Data is the new oil. Beside applications like predictive maintenance of machines or optimization of industrial processes, a main target for big data analyses are humans - in their roles as travelers, current or potential clients, or application users. We could say that big data is “any information that can be used to characterize the situation of a user”, and relate these approaches to what have been done in context modelling and reasoning. This gets even clearer when these analyses leave the virtual world (e.g., client behavior in web shops) and enter the real world (e.g., client behavior in retail). In addition to the ambiguities of the analysis itself that only leads to predictions with a limited probability, sensor data quality becomes an issue: the sensor data might be inaccurate, outdated or conflicting with other observations or physical laws; in addition, sensor data processing algorithms like object classification or tracking might lead to ambiguous results. In this talk, we will shortly review these two domains and derive what could be learned for context-aware applications. A special focus will be given on quality of context on all semantic levels, and how the improper consideration of quality issues can lead to dangerous digital prejudices.},
keywords={context;big data},
doi={10.1109/PERCOMW.2015.7133983},
ISSN={},
month={March},}
@INPROCEEDINGS{7944901,
author={},
booktitle={2017 IEEE Third International Conference on Big Data Computing Service and Applications (BigDataService)}, title={[Front cover]},
year={2017},
volume={},
number={},
pages={c1-c1},
abstract={The following topics are dealt with: intelligent data mining; frameworks for Big Data processing; Big Data processing and mining; Big Data analysis; smart city Big Data; data analytics and visualization; Big Data applications; Big Data framework, technology and solutions; security services for smart cities; Big Data for security; Big Data quality assurance and validation; and quality assurance and validation for Big Data-based applications.},
keywords={},
doi={10.1109/BigDataService.2017.59},
ISSN={},
month={April},}
@INPROCEEDINGS{7858181,
author={},
booktitle={2016 International Conference on Information Technology Systems and Innovation (ICITSI)}, title={[Title page]},
year={2016},
volume={},
number={},
pages={1-1},
abstract={The following topics are dealt with: SDLC SPASI v. 4.0. business process; information extraction; statistics indicator tables; rule generalizations; ontology; conventional learning system; ICT-based learning; job training system; time-series data; RAID; software-based accelerator; virtualization environment; enterprise architecture government organization; TOGAF ADM; SONA; e- library; modified quantitative models for performance measurement system method; business process improvement; district government innovation service case study; government organization; m-government implementation evaluation; trusted Big Data; official statistics study case; data profiling; data quality improvement; secure internet access; copyright protection; color images; transform domain; luminance component; information network architecture; local government; software as a service; expert system; meningitis disease; certainty factor method; digital asset management system; broadcasting organizations; e-portofolio definition; system security requirement identification; electronic payment system; Internet-based long distance education; operational model data governance; requirement engineering; open government information network development; process capability assessment; information security management; information security governance; national cyber physical systems; e-learning readiness; remote control system; serial communications mobile; microcontroller; knowledge sharing; indonesia higher educational institutions; cultural heritage metadata; geo linked open data; NUSANTARA: knowledge management system; adaptive personalized learning system; interactive learning media; RPP ICT; government human capital management; knowledge management tools utilization; knowledge management readiness; analytic hierarchy process; government institutions; usability testing; scrum methodology; assistant information system; automatic arowana raiser; pSPEA2; strength Pareto evolutionary algorithm 2; early diagnosis expert system deficiency; digital forensic; user acceptance; human resource information system; automated plasmodium detection; malaria diagnosis; thin blood smear image; 3D virtual game; MOODLE; SLOODLE; open simulator case study; color-based segmentation; feature detection; ball post; goal post; mobile soccer robot game field; smart farming; real time q-log-based feature normalization; distant speech recognition; Monte Carlo localization; robot operating system; finite element method; 3D DC resistivity modeling; multi GPU; breast cancer lesions; adaptive thresholding; morphological operation; gamification framework; online training; collaborative working system; classification breast cancer ultrasound images; posterior features; three-wheeled omnidirectional robot controller; public services satisfaction; sentiment analysis; color blind test quantification; RGB primary color cluster; ERP modules requirement; micro, small and medium enterprise fashion industry; small culinary enterprises; business system requirement; small craft companies ; power analysis attack; DES and IT value model.},
keywords={},
doi={10.1109/ICITSI.2016.7858181},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7407050,
author={Xiaofang Zhou},
booktitle={2015 Conference on Technologies and Applications of Artificial Intelligence (TAAI)}, title={Keynote speech V deriving values from spatial trajectories},
year={2015},
volume={},
number={},
pages={29-29},
abstract={Spatial trajectory data is widely available today. Over a sustained period of time, trajectory data has been collected from numerous GPS devices, smartphones, sensors and social media applications. How do we manage them? What values can a business derive from them, and how? Due to their very large volumes, the nature of streaming itself, highly variable levels of data quality, as well as many possible links with other types of data, making sense of spatial trajectory data is one of the crucial areas for big data analytics. In this talk, we will introduce this increasingly important research area in the context of new applications, new problems and new opportunities. We will discuss recent advances in trajectory data management and trajectory mining, from their foundations to high performance processing with modern computing infrastructures.},
keywords={},
doi={10.1109/TAAI.2015.7407050},
ISSN={2376-6824},
month={Nov},}