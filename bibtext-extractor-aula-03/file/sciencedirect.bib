@article{SKRIPCAK2014303,
title = {Creating a data exchange strategy for radiotherapy research: Towards federated databases and anonymised public datasets},
journal = {Radiotherapy and Oncology},
volume = {113},
number = {3},
pages = {303-309},
year = {2014},
issn = {0167-8140},
doi = {https://doi.org/10.1016/j.radonc.2014.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167814014004071},
author = {Tomas Skripcak and Claus Belka and Walter Bosch and Carsten Brink and Thomas Brunner and Volker Budach and Daniel Büttner and Jürgen Debus and Andre Dekker and Cai Grau and Sarah Gulliford and Coen Hurkmans and Uwe Just and Mechthild Krause and Philippe Lambin and Johannes A. Langendijk and Rolf Lewensohn and Armin Lühr and Philippe Maingon and Michele Masucci and Maximilian Niyazi and Philip Poortmans and Monique Simon and Heinz Schmidberger and Emiliano Spezi and Martin Stuschke and Vincenzo Valentini and Marcel Verheij and Gillian Whitfield and Björn Zackrisson and Daniel Zips and Michael Baumann},
keywords = {Data pooling, Interoperability, Data exchange, Large scale studies, Public data, Radiotherapy},
abstract = {Disconnected cancer research data management and lack of information exchange about planned and ongoing research are complicating the utilisation of internationally collected medical information for improving cancer patient care. Rapidly collecting/pooling data can accelerate translational research in radiation therapy and oncology. The exchange of study data is one of the fundamental principles behind data aggregation and data mining. The possibilities of reproducing the original study results, performing further analyses on existing research data to generate new hypotheses or developing computational models to support medical decisions (e.g. risk/benefit analysis of treatment options) represent just a fraction of the potential benefits of medical data-pooling. Distributed machine learning and knowledge exchange from federated databases can be considered as one beyond other attractive approaches for knowledge generation within “Big Data”. Data interoperability between research institutions should be the major concern behind a wider collaboration. Information captured in electronic patient records (EPRs) and study case report forms (eCRFs), linked together with medical imaging and treatment planning data, are deemed to be fundamental elements for large multi-centre studies in the field of radiation therapy and oncology. To fully utilise the captured medical information, the study data have to be more than just an electronic version of a traditional (un-modifiable) paper CRF. Challenges that have to be addressed are data interoperability, utilisation of standards, data quality and privacy concerns, data ownership, rights to publish, data pooling architecture and storage. This paper discusses a framework for conceptual packages of ideas focused on a strategic development for international research data exchange in the field of radiation therapy and oncology.}
}
@article{SEMLALI2021107257,
title = {SAT-CEP-monitor: An air quality monitoring software architecture combining complex event processing with satellite remote sensing},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107257},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107257},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002421},
author = {Badr-Eddine Boudriki Semlali and Chaker El Amrani and Guadalupe Ortiz and Juan Boubeta-Puig and Alfonso Garcia-de-Prado},
keywords = {Remote sensing, Satellite sensors, Air quality, Complex event processing, Big data, Decision-making},
abstract = {Air pollution is a major problem today that causes serious damage to human health. Urban areas are the most affected by the degradation of air quality caused by anthropogenic gas emissions. Although there are multiple proposals for air quality monitoring, in most cases, two limitations are imposed: the impossibility of processing data in Near Real-Time (NRT) for remote sensing approaches and the impossibility of reaching areas of limited accessibility or low network coverage for ground data approaches. We propose a software architecture that efficiently combines complex event processing with remote sensing data from various satellite sensors to monitor air quality in NRT, giving support to decision-makers. We illustrate the proposed solution by calculating the air quality levels for several areas of Morocco and Spain, extracting and processing satellite information in NRT. This study also validates the air quality measured by ground stations and satellite sensor data.}
}
@article{PAGGI2021106907,
title = {Towards the definition of an information quality metric for information fusion models},
journal = {Computers & Electrical Engineering},
volume = {89},
pages = {106907},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106907},
url = {https://www.sciencedirect.com/science/article/pii/S004579062030759X},
author = {Horacio Paggi and Javier Soriano and Juan A. Lara and Ernesto Damiani},
keywords = {Adaptive Peer-to-Peer systems, Information fusion, Uncertain information handling, Information quality metric},
abstract = {Managing information quality has become important in cyber-physical systems dealing with big data. In this regard, different models have been proposed, mainly in flat peer-to-peer networks, in which exchanging information efficiently is a key aspect due to scarce resources. However, little research has been conducted on information quality metrics for cyber-physical scenarios. In this paper, we propose an information quality metric and show its application to an information fusion model. It is a “model-oriented quality metric” since it allows non-predefined variants on its configuration depending on the application domain. The model was tested on several simulations using open datasets. The results obtained in the performance of the model confirm the validity of the information quality metric, proposed in this paper, on which the model is based. The model may have a wide variety of applications such as mobile recommendation or decision making in critical environments (emergencies, war, and so on).}
}
@article{WATT2016423,
title = {Privacy Matters – Issues within Mechatronics},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {21},
pages = {423-430},
year = {2016},
note = {7th IFAC Symposium on Mechatronic Systems MECHATRONICS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.10.641},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316322534},
author = {Steve Watt and Chris Milne and David Bradley and David Russell and Peter Hehenberger and Jorge Azorin-Lopez},
keywords = {Privacy, Users, Big Data, Security, Mechatronics, Cyber-Physical Systems, Internet of Things},
abstract = {Abstract:
As mechatronic devices and components become increasingly integrated with and within wider systems concepts such as Cyber-Physical Systems and the Internet of Things, designer engineers are faced with new sets of challenges in areas such as privacy. The paper looks at the current, and potential future, of privacy legislation, regulations and standards and considers how these are likely to impact on the way in which mechatronics is perceived and viewed. The emphasis is not therefore on technical issues, though these are brought into consideration where relevant, but on the soft, or human centred, issues associated with achieving user privacy.}
}
@incollection{HOVENGA2020355,
title = {Chapter 11 - Measuring health service quality},
editor = {Evelyn J.S. Hovenga and Cherrie Lowe},
booktitle = {Measuring Capacity to Care Using Nursing Data},
publisher = {Academic Press},
pages = {355-388},
year = {2020},
isbn = {978-0-12-816977-3},
doi = {https://doi.org/10.1016/B978-0-12-816977-3.00011-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128169773000113},
author = {Evelyn J.S. Hovenga and Cherrie Lowe},
keywords = {Performance measurement, Nursing ecosystem, Productivity, Nursing practice environments, Collegial culture, Accountability, Data quality, Data governance, Accreditation, Standards},
abstract = {Quality can be defined in multiple ways and is impacted by multiple factors. It applies to any operational process within health care and has a strong relationship with the performance of individual staff members as well as overall organizational performance outcomes. The characteristics of any nursing practice environment influence the quality of service provided. The ability to measure the quality of services provided is largely dependent upon the availability and type of data that can be accessed and processed. Meaningful measurement, trend analysis and monitoring to enable continuous improvements to be made, are only possible when governed data standards are used. This chapter has a strong focus on health and nursing, including acuity and clinical data use and provides global recommendations on health data, data standards and governance. Reference is made to other types of related standards, including accreditation standards and standards governance. The chapter concludes with an examination of various international and national outcomes research organizations, their comparative studies, and use of performance indicator data sets, clinical standards and guidelines, big data and secondary data use. Caring has been well defined yet doesn't appear to be routinely measured even though this is a major component directly impacting patient satisfaction.}
}
@article{KALANTARI20182,
title = {Computational intelligence approaches for classification of medical data: State-of-the-art, future challenges and research directions},
journal = {Neurocomputing},
volume = {276},
pages = {2-22},
year = {2018},
note = {Machine Learning and Data Mining Techniques for Medical Complex Data Analysis},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.01.126},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217315436},
author = {Ali Kalantari and Amirrudin Kamsin and Shahaboddin Shamshirband and Abdullah Gani and Hamid Alinejad-Rokny and Anthony T. Chronopoulos},
keywords = {Computational intelligence, Medical application, Big data, Detection, Ensemble algorithm},
abstract = {The explosive growth of data in volume, velocity and diversity that are produced by medical applications has contributed to abundance of big data. Current solutions for efficient data storage and management cannot fulfill the needs of heterogeneous data. Therefore, by applying computational intelligence (CI) approaches in medical data helps get better management, faster performance and higher level of accuracy in detection. This paper aims to investigate the state-of-the-art of computational intelligence approaches in medical data and to categorize the existing CI techniques, used in medical fields, as single and hybrid. In addition, the techniques and methodologies, their limitations and performances are presented in this study. The limitations are addressed as challenges to obtain a set of requirements for Computational Intelligence Medical Data (CIMD) in establishing an efficient CIMD architectural design. The results show that on the one hand Support Vector Machine (SVM) and Artificial Immune Recognition System (AIRS) as a single based computational intelligence approach were the best methods in medical applications. On the other hand, the hybridization of SVM with other methods such as SVM-Genetic Algorithm (SVM-GA), SVM-Artificial Immune System (SVM-AIS), SVM-AIRS and fuzzy support vector machine (FSVM) had great performances achieving better results in terms of accuracy, sensitivity and specificity.}
}
@incollection{BERANGER2016167,
title = {3 - Management and Governance of Personal Health Data},
editor = {Jérôme Béranger},
booktitle = {Big Data and Ethics},
publisher = {Elsevier},
pages = {167-236},
year = {2016},
isbn = {978-1-78548-025-6},
doi = {https://doi.org/10.1016/B978-1-78548-025-6.50003-8},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480256500038},
author = {Jérôme Béranger},
keywords = {Controlled regulation, Data lifecycle, Environmental digital ecosystem, Governance, Management, Quality control, Regulatory and organizational aspects, Relational and cultural aspects, Strategic and methodological aspects, Structural and technological aspects},
abstract = {Abstract:
Every company has its own culture, its organization, its governance mode and its project management models. Nevertheless, a number of significant and universal principles concerning governance can be identified, both at the approach level as well as that of actors and of responsibilities. Data governance is one of the key factors to success in the protection of information. It is one of the components that defines the rules, guides and charters of good practice, establishes references and policies (management, classification, storage, and conservation of personal data), and further describes the responsibilities and controls their application. Therefore, it becomes paramount to understand: how can the complexity around personal data management be apprehended and in particular in health fields? In addition, what are the possible mechanisms to process these data pools to turn them into consistent and relevant information? To this end, it is essential to have a detailed and an accurate understanding of algorithmic governance, of the environmental numerical ecosystem, of safety and protection, and of the lifecycle of Big Data.}
}
@article{YANG2019277,
title = {Ontology: Footstone for Strong Artificial Intelligence},
journal = {Chinese Medical Sciences Journal},
volume = {34},
number = {4},
pages = {277-280},
year = {2019},
issn = {1001-9294},
doi = {https://doi.org/10.24920/003701},
url = {https://www.sciencedirect.com/science/article/pii/S1001929420300080},
author = {Xiaolin Yang and Zhe Wang and Hongjie Pan and Yan Zhu},
keywords = {ontology, artificial intelligence, biomedicine, big data},
abstract = {Abstract
In the past ten years, the application of artificial intelligence (AI) in biomedicine has increased rapidly, which roots in the rapid growth of biomedicine data, the improvement of computing performance, and the development of deep learning methods. At present, there are great difficulties in front of AI for solving complex and comprehensive medical problems. Ontology can play an important role in how to make machines have stronger intelligence and has wider applications in the medical field. By using ontologies, (meta) data can be standardized so that data quality is improved and more data analysis methods can be introduced, data integration can be supported by the semantics relationships which are specified in ontologies, and effective logic expression in nature language can be better understood by machine. This can be a pathway to stronger AI. Under this circumstance, the Chinese Conference on Biomedical Ontology and Terminology was held in Beijing in autumn 2019, with the theme “Making Machine Understand Data”. The success of this conference further improves the development of ontology in the field of biomedical information in China, and will promote the integration of Chinese ontology research and application with the international standards and the findability, accessibility, interoperability, and reusability(FAIR) Data Principle.}
}
@article{WOODALL201972,
title = {Potential Problem Data Tagging: Augmenting information systems with the capability to deal with inaccuracies},
journal = {Decision Support Systems},
volume = {121},
pages = {72-83},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619300740},
author = {Philip Woodall and Vaggelis Giannikas and Wenrong Lu and Duncan McFarlane},
keywords = {Data quality, Information quality, Accuracy, Metadata, Data analytics, Data tags},
abstract = {Data quality tags are a means of informing decision makers about the quality of the data they use from information systems. Unfortunately, data quality tags have not been successfully adopted despite their potential to assist decision makers. One reason for the non-adoption is that maintaining the tags is expensive and time-consuming: having a tag that represents accuracy, for example, would be massively time-consuming to measure because it requires some physical observation of reality to check the true value. We argue that a useful surrogate tag for accuracy can be created—without having to physically measure it—by counting the number of times the data has been exposed to an event that could cause it to become inaccurate. Experimental results show that the tags can help to avoid problems caused by inaccuracies, and also to help find the inaccuracies themselves.}
}
@article{KHOURY2022,
title = {A Framework for Augmented Intelligence in Allergy and Immunology Practice and Research—A Work Group Report of the AAAAI Health Informatics, Technology and Education Committee},
journal = {The Journal of Allergy and Clinical Immunology: In Practice},
year = {2022},
issn = {2213-2198},
doi = {https://doi.org/10.1016/j.jaip.2022.01.047},
url = {https://www.sciencedirect.com/science/article/pii/S221321982200143X},
author = {Paneez Khoury and Renganathan Srinivasan and Sujani Kakumanu and Sebastian Ochoa and Anjeni Keswani and Rachel Sparks and Nicholas L. Rider},
keywords = {Artificial intelligence, Asthma, Primary immunodeficiency, Atopic dermatitis, Augmented intelligence, Clinical decision support, Electronic health records, Equity, Machine learning, Natural language processing, Medical education},
abstract = {Artificial and augmented intelligence (AI) and machine learning (ML) methods are expanding into the health care space. Big data are increasingly used in patient care applications, diagnostics, and treatment decisions in allergy and immunology. How these technologies will be evaluated, approved, and assessed for their impact is an important consideration for researchers and practitioners alike. With the potential of ML, deep learning, natural language processing, and other assistive methods to redefine health care usage, a scaffold for the impact of AI technology on research and patient care in allergy and immunology is needed. An American Academy of Asthma Allergy and Immunology Health Information Technology and Education subcommittee workgroup was convened to perform a scoping review of AI within health care as well as the specialty of allergy and immunology to address impacts on allergy and immunology practice and research as well as potential challenges including education, AI governance, ethical and equity considerations, and potential opportunities for the specialty. There are numerous potential clinical applications of AI in allergy and immunology that range from disease diagnosis to multidimensional data reduction in electronic health records or immunologic datasets. For appropriate application and interpretation of AI, specialists should be involved in the design, validation, and implementation of AI in allergy and immunology. Challenges include incorporation of data science and bioinformatics into training of future allergists-immunologists.}
}
@article{GAI2018262,
title = {A survey on FinTech},
journal = {Journal of Network and Computer Applications},
volume = {103},
pages = {262-273},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2017.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S1084804517303247},
author = {Keke Gai and Meikang Qiu and Xiaotong Sun},
keywords = {FinTech, Cloud computing, Cyber security, Big data, Financial computing, Data-driven framework},
abstract = {As a new term in the financial industry, FinTech has become a popular term that describes novel technologies adopted by the financial service institutions. This term covers a large scope of techniques, from data security to financial service deliveries. An accurate and up-to-date awareness of FinTech has an urgent demand for both academics and professionals. This work aims to produce a survey of FinTech by collecting and reviewing contemporary achievements, by which a theoretical data-driven FinTech framework is proposed. Five technical aspects are summarized and involved, which include security and privacy, data techniques, hardware and infrastructure, applications and management, and service models. The main findings of this work are fundamentals of forming active FinTech solutions.}
}
@article{HEZEL20181,
title = {What we know about elemental bulk chondrule and matrix compositions: Presenting the ChondriteDB Database},
journal = {Geochemistry},
volume = {78},
number = {1},
pages = {1-14},
year = {2018},
issn = {0009-2819},
doi = {https://doi.org/10.1016/j.chemer.2017.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0009281916302896},
author = {Dominik C. Hezel and Markus Harak and Guy Libourel},
keywords = {Chondrules, Matrix, Elemental composition, ChondritedDB, Database},
abstract = {Chondrules and matrix are the major components of chondritic meteorites and represent a significant evolutionary step in planet formation. The formation and evolution of chondrules and matrix and, in particular, the mechanics of chondrule formation remain the biggest unsolved challenge in meteoritics. A large number of studies of these major components not only helped to understand these in ever greater detail, but also produced a remarkably large body of data. Studying all available data has become known as ‹big data› analyses and promises deep insights – in this case – to chondrule and matrix formation and relationships. Looking at all data may also allow one to better understand the mechanism of chondrule formation or, equally important, what information we might be missing to identify this process. A database of all available chondrule and matrix data further provides an overview and quick visualisation, which will not only help to solve actual problems, but also enable students and future researchers to quickly access and understand all we know about these components. We collected all available data on elemental bulk chondrule and matrix compositions in a database that we call ChondriteDB. The database also contains petrographic and petrologic information on chondrules. Currently, ChondriteDB contains about 2388 chondrule and 1064 matrix data from 70 different publications and 161 different chondrites. Future iterations of ChondriteDB will include isotope data and information on other chondrite components. Data quality is of critical importance. However, as we discuss, quality is not an objective category, but a subjective judgement. Quantifiable data acquisition categories are required that allow selecting the appropriate data from a database in the context of a given research problem. We provide a comprehensive overview on the contents of ChondriteDB. The database is available as an Excel file upon request from the senior author of this paper, or can be accessed through MetBase.}
}
@incollection{ZHOU2018423,
title = {5.11 Smart Energy Management},
editor = {Ibrahim Dincer},
booktitle = {Comprehensive Energy Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {423-456},
year = {2018},
isbn = {978-0-12-814925-6},
doi = {https://doi.org/10.1016/B978-0-12-809597-3.00525-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095973005253},
author = {Kaile Zhou and Shanlin Yang},
keywords = {Demand side management, Energy big data, Energy consumption behavior, Energy informatics, Energy social informatics, Smart energy management, Smart energy system},
abstract = {Smart energy management is the path to achieve the management and operational objectives of smart energy systems (SESs). First, some related concepts of smart energy management are introduced. The evolution of energy systems in four stages and the three dimensions of smart energy management are also proposed. Then the overall structure and key technologies of SESs are provided, followed by the introduction of the composition of energy big data and its application in demand side management (DSM). Furthermore, the Ubiquitous Energy Internet in China, the smart energy management in smart buildings, smart manufacturing, and smart transportation are discussed as case studies of smart energy management. Finally, the research paradigms of smart energy management are presented and future directions are pointed out.}
}
@article{YE2021102513,
title = {A feasible framework to downscale NPP-VIIRS nighttime light imagery using multi-source spatial variables and geographically weighted regression},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {104},
pages = {102513},
year = {2021},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2021.102513},
url = {https://www.sciencedirect.com/science/article/pii/S0303243421002208},
author = {Yang Ye and Linyan Huang and Qiming Zheng and Chenxin Liang and Baiyu Dong and Jinsong Deng and Xiuzhen Han},
keywords = {Nighttime light (NTL), Downscaling, Geographically weighted regression (GWR), Impervious surface detection},
abstract = {The cloud-free monthly composite of global nighttime light (NTL) data of the Suomi National Polar-orbiting Partnership with the Visible Infrared Imaging Radiometer Suite (NPP-VIIRS) day/night band (DNB) provides indispensable indications of human activities and settlements. However, the coarse spatial resolution (15 arc sec) of NTL imagery greatly restricts its application potential. This study proposes a feasible framework to downscale NPP-VIIRS NTL using muti-source spatial variables and geographically weighted regression (GWR) method. High-resolution auxiliary variables were acquired from the Landsat 8 OLI/ TIRS and social media platforms. GWR-based downscaling procedures were consequently implemented to obtain NTL at a 100-m resolution. The downscaled NTL data were validated against Loujia1-01 imagery based on the coefficient of determination (R2) and root-mean-square error (RMSE). The results suggest that the data quality was suitably improved after downscaling, yielding higher R2 (0.604 vs. 0.568) and lower RMSE (8.828 vs. 9.870 nW/cm2/sr) values than those of the original NTL data. Finally, the NTL was extendedly applied to detect impervious surfaces, and the downscaled NTL had higher accuracy than the original NTL. Therefore, this study facilitates data quality improvement of NPP-VIIRS NTL imagery by downscaling, thus enabling more accurate applications.}
}
@article{TAYLOR2022107492,
title = {An interdisciplinary research perspective on the future of multi-vector energy networks},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {135},
pages = {107492},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107492},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521007316},
author = {P.C. Taylor and M. Abeysekera and Y. Bian and D. Ćetenović and M. Deakin and A. Ehsan and V. Levi and F. Li and R. Oduro and R. Preece and P.G. Taylor and V. Terzija and S.L. Walker and J. Wu},
keywords = {Energy markets, Information and communication technologies, Modelling, Multi-vector energy networks, Policy, Risk},
abstract = {Understanding the future of multi-vector energy networks in the context of the transition to net zero and the energy trilemma (energy security, environmental impact and social cost) requires novel interdisciplinary approaches. A variety of challenges regarding systems, plant, physical infrastructure, sources and nature of uncertainties, technological in general and more specifically Information and Communication Technologies requirements, cyber security, big data analytics, innovative business models and markets, policy and societal changes, are critically important to ensure enhanced flexibility and higher resilience, as well as reduced costs of an integrated energy system. Integration of individual energy networks into multi-vector entities opens a number of opportunities, but also presents a number of challenges requiring interdisciplinary perspectives and solutions. Considering drivers like societal evolution, climate change and technology advances, this paper describes the most important aspects which have to be taken into account when designing, planning and operating future multi-vector energy networks. For this purpose, the issues addressing future architecture, infrastructure, interdependencies and interactions of energy network infrastructures are elaborated through a novel interdisciplinary perspective. Aspects related to optimal operation of multi-vector energy networks, implementation of novel technologies, jointly with new concepts and algorithms, are extensively discussed. The role of policy, markets and regulation in facilitating multi-vector energy networks is also reported. Last but not least, the aspects of risks and uncertainties, relevant for secure and optimal operation of future multi-vector energy networks are discussed.}
}
@article{JUNG202115,
title = {The potential of remote sensing and artificial intelligence as tools to improve the resilience of agriculture production systems},
journal = {Current Opinion in Biotechnology},
volume = {70},
pages = {15-22},
year = {2021},
note = {Food Biotechnology ● Plant Biotechnology},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2020.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0958166920301257},
author = {Jinha Jung and Murilo Maeda and Anjin Chang and Mahendra Bhandari and Akash Ashapure and Juan Landivar-Bowles},
abstract = {Modern agriculture and food production systems are facing increasing pressures from climate change, land and water availability, and, more recently, a pandemic. These factors are threatening the environmental and economic sustainability of current and future food supply systems. Scientific and technological innovations are needed more than ever to secure enough food for a fast-growing global population. Scientific advances have led to a better understanding of how various components of the agricultural system interact, from the cell to the field level. Despite incredible advances in genetic tools over the past few decades, our ability to accurately assess crop status in the field, at scale, has been severely lacking until recently. Thanks to recent advances in remote sensing and Artificial Intelligence (AI), we can now quantify field scale phenotypic information accurately and integrate the big data into predictive and prescriptive management tools. This review focuses on the use of recent technological advances in remote sensing and AI to improve the resilience of agricultural systems, and we will present a unique opportunity for the development of prescriptive tools needed to address the next decade’s agricultural and human nutrition challenges.}
}
@article{DELRIOCASTRO2021122204,
title = {Unleashing the convergence amid digitalization and sustainability towards pursuing the Sustainable Development Goals (SDGs): A holistic review},
journal = {Journal of Cleaner Production},
volume = {280},
pages = {122204},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.122204},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620322514},
author = {Gema {Del Río Castro} and María Camino {González Fernández} and Ángel {Uruburu Colsa}},
keywords = {Sustainability, Sustainable development goals (SDGs), Digitalization, ICT, Big data, Artificial intelligence},
abstract = {The Sustainable Development Goals (SDGs) within the United Nations 2030 Agenda emerged in 2015, becoming an unprecedented global compass for navigating extant sustainability challenges. Nevertheless, it still represents a nascent field enduring uncertainties and complexities. In this regard, the interplay between digitalization and sustainability unfolds bright opportunities for shaping a greener economy and society, paving the way towards the SDGs. However, little evidence exists so far, about a genuine contribution of digital paradigms to sustainability. Besides, their role to tackle the SDGs research gaps remains unexplored. Thus, a holistic characterization of the aforementioned topics has not been fully explored in the emerging literature, deserving further research. The article endeavors a twofold purpose: (1) categorizing the main SDGs research gaps; (2) coupled with a critical exploration of the potential contribution of digital paradigms, particularly Big Data and Artificial Intelligence, towards overcoming the aforesaid caveats and pursuing the 2030 Agenda. Ultimately, the study seeks to bridge literature gaps by providing a first-of-its-kind overview on the SDGs and their nexus with digitalization, while unraveling policy implications and future research directions. The methodology has consisted of a systematic holistic review and in-depth qualitative analysis of the literature on the realms of the SDGs and digitalization. Our findings evidence that the SDGs present several research gaps, namely: flawed understanding of complexities and interlinkages; design shortcomings and imbalances; implementation and governance hurdles; unsuitable indicators and assessment methodologies; truncated adoption and off-target progress; unclear responsibilities and lacking coordination; untapped role of technological innovation and knowledge management. Moreover, our results show growing expectations about the added value brought by digitalization for pursuing the SDGs, through novel data sources, enhanced analytical capacities and collaborative digital ecosystems. However, current research and practice remains in early-stage, pointing to ethical, social and environmental controversies, along with policy caveats, which merit additional research. In light of the findings, the authors suggest a first-approach exploration of research and policy implications. Results suggest that further multidisciplinary research, dialogue and concerted efforts for transformation are required. Reframing the Agenda, while aligning the sustainable development and digitalization policies, seems advisable to ensure a holistic sustainability. The findings aim at guiding and stimulating further research and science-policy dialogue on the promising nexus amid the SDGs and digitalization.}
}
@article{PASICHNYI2019486,
title = {Energy performance certificates — New opportunities for data-enabled urban energy policy instruments?},
journal = {Energy Policy},
volume = {127},
pages = {486-499},
year = {2019},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2018.11.051},
url = {https://www.sciencedirect.com/science/article/pii/S0301421518307894},
author = {Oleksii Pasichnyi and Jörgen Wallin and Fabian Levihn and Hossein Shahrokni and Olga Kordas},
keywords = {Energy performance certificate (EPC), Building energy efficiency, Data applications, Data quality, Sweden},
abstract = {Energy performance certificates (EPC) were introduced in European Union to support reaching energy efficiency targets by informing actors in the building sector about energy efficiency in buildings. While EPC have become a core source of information about building energy, the domains of its applications have not been studied systematically. This partly explains the limitation of conventional EPC data quality studies that fail to expose the essential problems and secure effective use of the data. This study reviews existing applications of EPC data and proposes a new method for assessing the quality of EPCs using data analytics. Thirteen application domains were identified from systematic mapping of 79 papers, revealing increases in the number and complexity of studies and advances in applied data analysis techniques. The proposed data quality assurance method based on six validation levels was tested using four samples of EPC dataset for the case of Sweden. The analysis showed that EPC data can be improved through adding or revising the EPC features and assuring interoperability of EPC datasets. In conclusion, EPC data have wider applications than initially intended by the EPC policy instrument, placing stronger requirements on the quality and content of the data.}
}
@article{SCHOCK2021636,
title = {Data Acquisition and Preparation – Enabling Data Analytics Projects within Production},
journal = {Procedia CIRP},
volume = {104},
pages = {636-640},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.107},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121010052},
author = {Christoph Schock and Jonas Dumler and Frank Doepper},
keywords = {Data Analytics, CRISP-DM, Data Acquisition, Data Preparation, Feature Engineering, Process Monitoring, Condition Monitoring},
abstract = {The increasing amount of available data in production systems is associated with great potential for process optimization. Due to lack of a data analytics methodology and low data quality within production these potentials often remain unused. Therefore, in this paper we present a model for data acquisition and data preparation including feature engineering for characteristic sensor signals of production machines. The model allows the extraction of relevant process information from the signal, which can be used for monitoring, KPI tracking, trend analysis and anomaly detection. The approach is evaluated on an industrial turning process.}
}
@article{LIU2020263,
title = {Super Resolution Perception for Smart Meter Data},
journal = {Information Sciences},
volume = {526},
pages = {263-273},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.03.088},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520302681},
author = {Guolong Liu and Jinjin Gu and Junhua Zhao and Fushuan Wen and Gaoqi Liang},
keywords = {Super resolution perception, Smart meter data, High-frequency data, Big data analysis},
abstract = {In this paper, we present the problem formulation and methodology framework of Super Resolution Perception (SRP) on smart meter data. With the widespread use of smart meters, a massive amount of electricity consumption data can be obtained. Smart meter data is the basis of automated billing and pricing, appliance identification, demand response, etc. However, the provision of high-quality data may be expensive in many cases. In this paper, we propose a novel problem - the SRP problem as reconstructing high-quality data from unsatisfactory data in smart grids. Advanced generative models are then proposed to solve the problem. This technology makes it possible for empowering existing facilities without upgrading existing meters or deploying additional meters. We first mathematically formulate the SRP problem under the Maximum a Posteriori (MAP) estimation framework. The dataset namely Super Resolution Perception Dataset (SRPD) is designed for this problem and released. A case study is then presented, which performs SRP on smart meter data. A network namely Super Resolution Perception Convolutional Neural Network (SRPCNN) is proposed to generate high-frequency load data from low-frequency data. Experiments demonstrate that our SRP models can reconstruct high-frequency data effectively. Moreover, the reconstructed high-frequency data can lead to better appliance identification results.}
}
@article{KONGBOON2022130711,
title = {Greenhouse gas emissions inventory data acquisition and analytics for low carbon cities},
journal = {Journal of Cleaner Production},
volume = {343},
pages = {130711},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.130711},
url = {https://www.sciencedirect.com/science/article/pii/S095965262200350X},
author = {Ratchayuda Kongboon and Shabbir H. Gheewala and Sate Sampattagul},
keywords = {Sustainable city, Low carbon city, Greenhouse gas inventory, Greenhouse gas emissions, Municipalities},
abstract = {This paper studied greenhouse gas inventory data acquisition and analytics for municipalities in Thailand. A complete and transparent GHG inventory of eight municipalities was developed to document the current situation, and to help decision-makers to clarify their priorities for reducing greenhouse gas emissions. The Global Protocol for Community-Scale Greenhouse Gas Emissions Inventories guidelines was used to investigate and calculate the greenhouse gas emissions and assess data accuracy. The results indicated that the data source, data format, and data collection of each municipality are relatively similar. Moreover, the activity data needed to be obtained from several authorities. The results showed that Nonthaburi Municipality had the highest greenhouse gas emissions at 2,286,838 tCO2e/yr and Buriram Municipality, the lowest at 239,795 tCO2e/yr. On a per-capita basis, Lamphun Municipality was the highest with 10.1 tCO2e/capita and Buriram Municipality the lowest with 3.8 tCO2e/capita. The results suggest that the municipalities should continually develop a GHG database by creating a routine procedure. An information management system should be produced in the shape of big data which can lead to state policies, plans, and actions for city development to ensure the reduction of greenhouse gas emissions. This in turn will lead to a low carbon city.}
}
@article{LNENICKA2019129,
title = {Big and open linked data analytics ecosystem: Theoretical background and essential elements},
journal = {Government Information Quarterly},
volume = {36},
number = {1},
pages = {129-144},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18302545},
author = {Martin Lnenicka and Jitka Komarkova},
keywords = {Big and open linked data, Ecosystem approach, Dimensions, Data analytics lifecycle, Stakeholders, Conceptual framework},
abstract = {Big and open linked data are often mentioned together because storing, processing, and publishing large amounts of these data play an increasingly important role in today's society. However, although this topic is described from the political, economic, and social points of view, a technical dimension, which is represented by big data analytics, is insufficient. The aim of this review article was to provide a theoretical background of big and open linked data analytics ecosystem and its essential elements. First, the key terms were introduced including related dimensions. Then, the key lifecycle phases were defined and involved stakeholders were identified. Finally, a conceptual framework was proposed. In contrast to previous research, the new ecosystem is formed by interactions of stakeholders in the following dimensions and their sub-dimensions: transparency, engagement, legal, technical, social, and economic. These relationships are characterized by the most important requisites and public policy choices affecting the data analytics ecosystem together with the key phases and activities of the data analytics lifecycle. The findings should contribute to relevant initiatives, strategies, and policies and their effective implementation.}
}
@article{RODRIGUES2022101625,
title = {Species misidentification affects biodiversity metrics: Dealing with this issue using the new R package naturaList},
journal = {Ecological Informatics},
volume = {69},
pages = {101625},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101625},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122000747},
author = {Arthur Vinicius Rodrigues and Gabriel Nakamura and Vanessa Graziele Staggemeier and Leandro Duarte},
keywords = {Occurrence records, Biodiversity, Species misidentification, Taxonomy, Ecological patterns},
abstract = {Biodiversity databases are increasingly available and have fostered accelerated advances in many disciplines within ecology and evolution. However, the quality of the evidence generated depends critically on the quality of the input data, and species misidentifications are present in virtually any occurrence dataset. Yet, the lack of automatized tools makes the assessment of the quality of species identification in big datasets time-consuming, which often induces researchers to assume that all species are reliably identified. In this study, we address this issue by evaluating how species misidentification can impact our ability to capture ecological patterns, and by presenting an R package, called naturaList, designed to classify species occurrence data according to identification reliability. naturaList allows the classification of species occurrences up to six confidence levels, in which the highest level is assigned to records identified by specialists. We obtained a list of specialists by using the species occurrence dataset itself, based on the identifier names within it, and by entering an independent list, obtained by contacting experts. Further, we evaluate the effects of filtering out occurrence records not identified by specialists on the estimations of species niche and diversity patterns. We used the tribe Myrteae (Myrtaceae) as a study model, which is a species-rich group in Central and South America and with challenging taxonomy. We found a significant change in species niche in 13% of species when using only occurrences identified by specialists. We found changes in patterns of alpha diversity in four genera and changes in beta diversity in all genera analyzed. We show how the uncertainty in species identification in occurrence datasets affects conclusions on macroecological patterns by generating bias or noise in different aspects of macroecological patterns (niche, alpha, and beta diversity). Therefore, to guarantee reliability in species identification in big data sets we recommend the use of automated tools such as the naturaList package, especially when analyzing variation in species composition. This study also represents a step forward to increasing the quality of large-scale studies that rely on species occurrence data.}
}
@article{JIN2020112412,
title = {Artificial intelligence biosensors: Challenges and prospects},
journal = {Biosensors and Bioelectronics},
volume = {165},
pages = {112412},
year = {2020},
issn = {0956-5663},
doi = {https://doi.org/10.1016/j.bios.2020.112412},
url = {https://www.sciencedirect.com/science/article/pii/S0956566320304061},
author = {Xiaofeng Jin and Conghui Liu and Tailin Xu and Lei Su and Xueji Zhang},
keywords = {Wearable biosensor, Artificial intelligence, Biomarker, Wireless communication, Machine learning, Healthcare},
abstract = {Artificial intelligence (AI) and wearable sensors are two essential fields to realize the goal of tailoring the best precision medicine treatment for individual patients. Integration of these two fields enables better acquisition of patient data and improved design of wearable sensors for monitoring the wearers' health, fitness and their surroundings. Currently, as the Internet of Things (IoT), big data and big health move from concept to implementation, AI-biosensors with appropriate technical characteristics are facing new opportunities and challenges. In this paper, the most advanced progress made in the key phases for future wearable and implantable technology from biosensing, wearable biosensing to AI-biosensing is summarized. Without a doubt, material innovation, biorecognition element, signal acquisition and transportation, data processing and intelligence decision system are the most important parts, which are the main focus of the discussion. The challenges and opportunities of AI-biosensors moving forward toward future medicine devices are also discussed.}
}
@incollection{ONIK2019197,
title = {Chapter 8 - Blockchain in Healthcare: Challenges and Solutions},
editor = {Nilanjan Dey and Himansu Das and Bighnaraj Naik and Himansu Sekhar Behera},
booktitle = {Big Data Analytics for Intelligent Healthcare Management},
publisher = {Academic Press},
pages = {197-226},
year = {2019},
series = {Advances in ubiquitous sensing applications for healthcare},
isbn = {978-0-12-818146-1},
doi = {https://doi.org/10.1016/B978-0-12-818146-1.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128181461000088},
author = {Md. Mehedi Hassan Onik and Satyabrata Aich and Jinhong Yang and Chul-Soo Kim and Hee-Cheol Kim},
keywords = {Blockchain, Big data, Healthcare, Data privacy, EHR security},
abstract = {The main challenge in distributing electronic health records (EHRs) for patient-centered research, market analysis, medicine investigation, healthcare data mining etc., is data privacy. Handling the large-scale data and preserving the privacy of patients has been a challenge to researchers for a long period of time. On the contrary, blockchain technology has alleviated some of the problems by providing a protected and distributed platform. Sadly, existing electronic health record (EHR) management systems suffer from data manipulation, delayed communication, and trustless cooperation in data collection, storage, and distribution. This chapter discusses the current issues of healthcare data privacy and existing and upcoming regulations on this sector. This chapter also includes an overview of the architecture, existing issues, and future scope of blockchain technology for successfully handling privacy and management of current and future medical records. This chapter also presents few blockchain solutions that advocate the future research scopes in healthcare, big data, and blockchain.}
}
@article{FAN2018296,
title = {Unsupervised data analytics in mining big building operational data for energy efficiency enhancement: A review},
journal = {Energy and Buildings},
volume = {159},
pages = {296-308},
year = {2018},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0378778817326671},
author = {Cheng Fan and Fu Xiao and Zhengdao Li and Jiayuan Wang},
keywords = {Unsupervised data mining, Big data, Building operational performance, Building energy management, Building energy efficiency},
abstract = {Building operations account for the largest proportion of energy use throughout the building life cycle. The energy saving potential is considerable taking into account the existence of a wide variety of building operation deficiencies. The advancement in information technologies has made modern buildings to be not only energy-intensive, but also information-intensive. Massive amounts of building operational data, which are in essence the reflection of actual building operating conditions, are available for knowledge discovery. It is very promising to extract potentially useful insights from big building operational data, based on which actionable measures for energy efficiency enhancement are devised. Data mining is an advanced technology for analyzing big data. It consists of two main types of data analytics, i.e., supervised and unsupervised analytics. Despite of the power of supervised analytics in predictive modeling, unsupervised analytics are more practical and promising in discovering novel knowledge given limited prior knowledge. This paper provides a comprehensive review on the current utilization of unsupervised data analytics in mining massive building operational data. The commonly used unsupervised analytics are summarized according to their knowledge representations and applications. The challenges and opportunities are elaborated as guidance for future research in this multi-disciplinary field.}
}
@article{LEPRINCE2021111195,
title = {Data mining cubes for buildings, a generic framework for multidimensional analytics of building performance data},
journal = {Energy and Buildings},
volume = {248},
pages = {111195},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111195},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821004795},
author = {Julien Leprince and Clayton Miller and Wim Zeiler},
keywords = {Data mining, Data cube, Generic method, Multidimensional analytics, Machine learning, Building data},
abstract = {Over the last decade, collecting massive volumes of data has been made all the more accessible, pushing the building sector to embrace data mining as a powerful tool for harvesting the potential of big data analytics. However repetitive challenges still persist emerging from the need for a common analytical frame, effective application- and insight-driven targeted data selection, as well as benchmarked-supported claims. This study addresses these concerns by putting forward a generic stepwise multidimensional data mining framework tailored to building data, leveraging the dimensional-structures of data cubes. Using the open Building Data Genome Project 2 set, composed of 3053 energy meters from 1636 buildings, we provide an online, open access, implementation illustration of our method applied to automated pattern identification. We define a 3-dimensional building cube echoing typical analytical frames of interest, namely, bottom-up, top-down and temporal drill-in approaches. Our results highlight the importance of application and insight driven mining for effective dimensional-frame targeting. Impactful visualizations were developed allowing practical human inspection, paving the path towards more interpretable analytics.}
}
@article{NASHAAT2019131,
title = {M-Lean: An end-to-end development framework for predictive models in B2B scenarios},
journal = {Information and Software Technology},
volume = {113},
pages = {131-145},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919301247},
author = {Mona Nashaat and Aindrila Ghosh and James Miller and Shaikh Quader and Chad Marston},
keywords = {Big data, Machine learning, Business-to-business, User trust, Case study},
abstract = {Context
The need for business intelligence has led to advances in machine learning in the business domain, especially with the rise of big data analytics. However, the resulting predictive systems often fail to maintain a satisfactory level of performance in production. Besides, for predictive systems used in business-to-business scenarios, user trust is subject to the model performance. Therefore, the processes of creating, evaluating, and deploying machine learning systems in the business domain need innovative solutions to solve the critical challenges of assuring the quality of the resulting systems.
Objective
Applying machine learning in business-to-business situations imposes specific requirements. This paper aims at providing an integrated solution to businesses to help them transform their data into actions.
Method
The paper presents MLean, an end-to-end framework, that aims at guiding businesses in designing, developing, evaluating, and deploying business-to-business predictive systems. The framework employs the Lean Startup methodology and aims at maximizing the business value while eliminating wasteful development practices.
Results
To evaluate the proposed framework, with the help of our industrial partner, we applied the framework to a case study to build a predictive product. The case study resulted in a predictive system to predict the risks of software license cancellations. The system was iteratively developed and evaluated while adopting the management and end-user perspectives.
Conclusion
It is concluded that, in industry, it is important to be aware of the businesses requirements before considering the application of machine learning. The framework accommodates business perspective from the beginning to produce a holistic product. From the results of the case study, we think that this framework can help businesses define the right opportunities for applying machine learning, developing solutions, evaluating the effectiveness of these solutions, and maintaining their performance in production.}
}
@article{AMENGUALGUAL201931,
title = {Status epilepticus prevention, ambulatory monitoring, early seizure detection and prediction in at-risk patients},
journal = {Seizure},
volume = {68},
pages = {31-37},
year = {2019},
note = {Pediatric Convulsive Status Epilepticus},
issn = {1059-1311},
doi = {https://doi.org/10.1016/j.seizure.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S1059131118304059},
author = {Marta Amengual-Gual and Adriana Ulate-Campos and Tobias Loddenkemper},
keywords = {Epilepsy, Status epilepticus, Closed-loop systems, Machine learning, Seizure detection sensors, Automated seizure detection},
abstract = {Purpose
Status epilepticus is an often apparently randomly occurring, life-threatening medical emergency which affects the quality of life in patients with epilepsy and their families. The purpose of this review is to summarize information on ambulatory seizure detection, seizure prediction, and status epilepticus prevention.
Method
Narrative review.
Results
Seizure detection devices are currently under investigation with regards to utility and feasibility in the detection of isolated seizures, mainly in adult patients with generalized tonic-clonic seizures, in long-term epilepsy monitoring units, and occasionally in the outpatient setting. Detection modalities include accelerometry, electrocardiogram, electrodermal activity, electroencephalogram, mattress sensors, surface electromyography, video detection systems, gyroscope, peripheral temperature, photoplethysmography, and respiratory sensors, among others. Initial detection results are promising, and improve even further, when several modalities are combined. Some portable devices have already been U.S. FDA approved to detect specific seizures. Improved seizure prediction may be attainable in the future given that epileptic seizure occurrence follows complex patient-specific non-random patterns. The combination of multimodal monitoring devices, big data sets, and machine learning may enhance patient-specific detection and predictive algorithms. The integration of these technological advances and novel approaches into closed-loop warning and treatment systems in the ambulatory setting may help detect seizures sooner, and tentatively prevent status epilepticus in the future.
Conclusions
Ambulatory monitoring systems are being developed to improve seizure detection and the quality of life in patients with epilepsy and their families.}
}
@article{LI2021692,
title = {Data science skills and domain knowledge requirements in the manufacturing industry: A gap analysis},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {692-706},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001448},
author = {Guoyan Li and Chenxi Yuan and Sagar Kamarthi and Mohsen Moghaddam and Xiaoning Jin},
keywords = {Industry 4.0, Labor market analysis, Skills gap, Data science},
abstract = {Manufacturing has adopted technologies such as automation, robotics, industrial Internet of Things (IoT), and big data analytics to improve productivity, efficiency, and capabilities in the production environment. Modern manufacturing workers not only need to be adept at the traditional manufacturing technologies but also ought to be trained in the advanced data-rich computer-automated technologies. This study analyzes the data science and analytics (DSA) skills gap in today's manufacturing workforce to identify the critical technical skills and domain knowledge required for data science and intelligent manufacturing-related jobs that are highly in-demand in today's manufacturing industry. The gap analysis conducted in this paper on Emsi job posting and profile data provides insights into the trends in manufacturing jobs that leverage data science, automation, cyber, and sensor technologies. These insights will be helpful for educators and industry to train the next generation manufacturing workforce. The main contribution of this paper includes (1) presenting the overall trend in manufacturing job postings in the U.S., (2) summarizing the critical skills and domain knowledge in demand in the manufacturing sector, (3) summarizing skills and domain knowledge reported by manufacturing job seekers, (4) identifying the gaps between demand and supply of skills and domain knowledge, and (5) recognize opportunities for training and upskilling workforce to address the widening skills and knowledge gap.}
}
@article{KOTSIOPOULOS2021100341,
title = {Machine Learning and Deep Learning in smart manufacturing: The Smart Grid paradigm},
journal = {Computer Science Review},
volume = {40},
pages = {100341},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2020.100341},
url = {https://www.sciencedirect.com/science/article/pii/S157401372030441X},
author = {Thanasis Kotsiopoulos and Panagiotis Sarigiannidis and Dimosthenis Ioannidis and Dimitrios Tzovaras},
keywords = {Industry 4.0, Machine Learning, Deep Learning, Industrial AI, Smart Grid},
abstract = {Industry 4.0 is the new industrial revolution. By connecting every machine and activity through network sensors to the Internet, a huge amount of data is generated. Machine Learning (ML) and Deep Learning (DL) are two subsets of Artificial Intelligence (AI), which are used to evaluate the generated data and produce valuable information about the manufacturing enterprise, while introducing in parallel the Industrial AI (IAI). In this paper, the principles of the Industry 4.0 are highlighted, by giving emphasis to the features, requirements, and challenges behind Industry 4.0. In addition, a new architecture for AIA is presented. Furthermore, the most important ML and DL algorithms used in Industry 4.0 are presented and compiled in detail. Each algorithm is discussed and evaluated in terms of its features, its applications, and its efficiency. Then, we focus on one of the most important Industry 4.0 fields, namely the smart grid, where ML and DL models are presented and analyzed in terms of efficiency and effectiveness in smart grid applications. Lastly, trends and challenges in the field of data analysis in the context of the new Industrial era are highlighted and discussed such as scalability, cybersecurity, and big data.}
}
@article{SUGDEN2020100014,
title = {Patterns of Reliability: Assessing the Reproducibility and Integrity of DNA Methylation Measurement},
journal = {Patterns},
volume = {1},
number = {2},
pages = {100014},
year = {2020},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2020.100014},
url = {https://www.sciencedirect.com/science/article/pii/S2666389920300143},
author = {Karen Sugden and Eilis J. Hannon and Louise Arseneault and Daniel W. Belsky and David L. Corcoran and Helen L. Fisher and Renate M. Houts and Radhika Kandaswamy and Terrie E. Moffitt and Richie Poulton and Joseph A. Prinz and Line J.H. Rasmussen and Benjamin S. Williams and Chloe C.Y. Wong and Jonathan Mill and Avshalom Caspi},
keywords = {DSML 3:  Data science output has been rolled out/validated across multiple domains/problems},
abstract = {Summary
DNA methylation plays an important role in both normal human development and risk of disease. The most utilized method of assessing DNA methylation uses BeadChips, generating an epigenome-wide “snapshot” of >450,000 observations (probe measurements) per assay. However, the reliability of each of these measurements is not equal, and little consideration is paid to consequences for research. We correlated repeat measurements of the same DNA samples using the Illumina HumanMethylation450K and the Infinium MethylationEPIC BeadChips in 350 blood DNA samples. Probes that were reliably measured were more heritable and showed consistent associations with environmental exposures, gene expression, and greater cross-tissue concordance. Unreliable probes were less replicable and generated an unknown volume of false negatives. This serves as a lesson for working with DNA methylation data, but the lessons are equally applicable to working with other data: as we advance toward generating increasingly greater volumes of data, failure to document reliability risks harming reproducibility.}
}
@article{MATE2016131,
title = {A hybrid integrated architecture for energy consumption prediction},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {131-147},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16300644},
author = {Alejandro Maté and Jesús Peral and Antonio Ferrández and David Gil and Juan Trujillo},
keywords = {Data mining, Energy consumption, Information Extraction, Big data, Decision trees, Social networks},
abstract = {Irresponsible and negligent use of natural resources in the last five decades has made it an important priority to adopt more intelligent ways of managing existing resources, especially the ones related to energy. The main objective of this paper is to explore the opportunities of integrating internal data already stored in Data Warehouses together with external Big Data to improve energy consumption predictions. This paper presents a study in which we propose an architecture that makes use of already stored energy data and external unstructured information to improve knowledge acquisition and allow managers to make better decisions. This external knowledge is represented by a torrent of information that, in many cases, is hidden across heterogeneous and unstructured data sources, which are recuperated by an Information Extraction system. Alternatively, it is present in social networks expressed as user opinions. Furthermore, our approach applies data mining techniques to exploit the already integrated data. Our approach has been applied to a real case study and shows promising results. The experiments carried out in this work are twofold: (i) using and comparing diverse Artificial Intelligence methods, and (ii) validating our approach with data sources integration.}
}
@article{RISLING201789,
title = {Educating the nurses of 2025: Technology trends of the next decade},
journal = {Nurse Education in Practice},
volume = {22},
pages = {89-92},
year = {2017},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2016.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1471595316302748},
author = {Tracie Risling},
keywords = {Informatics, Curriculum development, Technology},
abstract = {The pace of technological evolution in healthcare is advancing. In this article key technology trends are identified that are likely to influence nursing practice and education over the next decade. The complexity of curricular revision can create challenges in the face of rapid practice change. Nurse educators are encouraged to consider the role of electronic health records (EHRs), wearable technologies, big data and data analytics, and increased patient engagement as key areas for curriculum development. Student nurses, and those already in practice, should be offered ongoing educational opportunities to enhance a wide spectrum of professional informatics skills. The nurses of 2025 will most certainly inhabit a very different practice environment than what exists today and technology will be key in this transformation. Nurse educators must prepare now to lead these practitioners into the future.}
}
@article{NAM2019411,
title = {Business analytics adoption process: An innovation diffusion perspective},
journal = {International Journal of Information Management},
volume = {49},
pages = {411-423},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218311435},
author = {Dalwoo Nam and Junyeong Lee and Heeseok Lee},
keywords = {Business analytics, Innovation diffusion, Adoption process, Data infrastructure, Data quality management, Analytics centralization},
abstract = {Although business analytics (BA) have been increasingly adopted into businesses, there is limited empirical research examining the drivers of each stage of BA adoption in organizations. Drawing upon technological-organizational-environmental framework and innovation diffusion process, we developed an integrative model to examine BA adoption processes and tested with 170 Korean firms. The analysis shows data-related technological characteristics derive all stages of BA adoption: initiation, adoption and assimilation. While organizational characteristics are associated with adoption and assimilation stage, only competition intensity in environmental characteristics is associated with initiation stage. Our findings help practitioners and researchers to understand what factors can enable companies to adopt BA in each stage.}
}
@incollection{DZIUBANY2019239,
title = {Chapter 11 - Machine learning-based artificial nose on a low-cost IoT-hardware},
editor = {Guido Dartmann and Houbing Song and Anke Schmeink},
booktitle = {Big Data Analytics for Cyber-Physical Systems},
publisher = {Elsevier},
pages = {239-257},
year = {2019},
isbn = {978-0-12-816637-6},
doi = {https://doi.org/10.1016/B978-0-12-816637-6.00011-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166376000117},
author = {Matthias Dziubany and Marcel Garling and Anke Schmeink and Guido Burger and Guido Dartmann and Stefan Naumann and Klaus-Uwe Gollmer},
keywords = {Artificial Nose, PCA, SVM, Feature selection, low cost},
abstract = {In order to make Internet of things applications easily available and cost-effective, we aim at using low-cost hardware for typical measurement tasks, and in return putting more effort into the signal processing and data analysis. By the example of beverage recognition with a low-cost temperature-modulated gas sensor, we demonstrate the benefits of processing techniques in big data such as feature selection and dimensionality reduction. Specifically, we determine a subset of temperatures that yields good support vector machine classification results and thereby shortens the measurement process.}
}
@article{CHUNG2020101837,
title = {Data science and analytics in aviation},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {134},
pages = {101837},
year = {2020},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2020.101837},
url = {https://www.sciencedirect.com/science/article/pii/S1366554520300077},
author = {Sai-Ho Chung and Hoi-Lam Ma and Mark Hansen and Tsan-Ming Choi},
keywords = {Data science, Aviation, Analytics, Flight, Air logistics},
abstract = {Data science and analytics are attracting more and more attention from researchers and practitioners in recent years. Due to the rapid development of advanced technologies nowadays, a massive amount of real time data regarding flight information, flight performance, airport conditions, air traffic conditions, weather, ticket prices, passengers comments, crew comments, etc., are all available from a diverse set of sources, including flight performance monitoring systems, operational systems of airlines and airports, and social media platforms. Development of data analytics in aviation and related applications is also growing rapidly. This paper concisely examines data science and analytics in aviation studies in several critical areas, namely big data analysis, air transport network management, forecasting, and machine learning. The papers featured in this special issue are also introduced and reviewed, and future directions for data science and analytics in aviation are discussed.}
}
@article{DING2021,
title = {An Internet of Things Based Scalable Framework for Disaster Data Management},
journal = {Journal of Safety Science and Resilience},
year = {2021},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2021.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S2666449621000542},
author = {Zhiming Ding and Shan Jiang and Xinrun Xu and Yanbo Han},
keywords = {Disaster Data Management, IoT, Disaster Detection, Big data, Artificial Intelligence},
abstract = {In recent years, undesirable disasters attacked the cities frequently, leaving heavy casualties and serious economic losses. Meanwhile, disaster detection based on the Internet of Things(IoT) has become a hot spot benefited by the established development of smart city construction. And the IoT is visibly sensitive to the management and monitor of disasters, but massive amounts of monitoring data have brought huge challenges to data storage and data analysis. This article develops a new and much more general framework for disaster emergency management under the IoT environment. The framework is a bottom-up integration of highly scalable Raw Data Storages(RD-Stores) technology, hybrid indexing and queries technology, and machine learning technology for emergency disasters. Experimental results show that hybrid index and query technology have better performance under the condition of supporting multi-modal retrieval, and providing a better solution to offer real-time retrieval for the massive sensor sampling data in the IoT. In addition, further works to evaluate the top-level sub-application system in this framework were performed based on the GPS trajectory data of 35,000 Beijing taxis and the volumetric ground truth data of 7,500 images. The results show that the framework has desirable scalability and higher utility.}
}
@article{NILASHI2021102630,
title = {Big social data and customer decision making in vegetarian restaurants: A combined machine learning method},
journal = {Journal of Retailing and Consumer Services},
volume = {62},
pages = {102630},
year = {2021},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2021.102630},
url = {https://www.sciencedirect.com/science/article/pii/S096969892100196X},
author = {Mehrbakhsh Nilashi and Hossein Ahmadi and Goli Arji and Khalaf Okab Alsalem and Sarminah Samad and Fahad Ghabban and Ahmed Omar Alzahrani and Ali Ahani and Ala Abdulsalam Alarood},
keywords = {Online reviews, Food quality, Vegetarian friendly restaurants, Text mining, Segmentation},
abstract = {Customers increasingly use various social media to share their opinion about restaurants service quality. Big data collected from social media provides a data platform to improve the service quality of restaurants through customers' online reviews, where online reviews are a trustworthy and reliable source that helps consumers to evaluate food quality. Developing methods for effective evaluation of customer-generated reviews of restaurant services is important. This study develops a new method through effective learning techniques for customer segmentation and their preferences prediction in vegetarian friendly restaurants. The method is developed through text mining (Latent Dirichlet Allocation), cluster analysis (Self Organizing Map) and predictive learning technique (Classification and Regression Trees) to reveal the customer’ satisfaction levels from the service quality in vegetarian friendly restaurants. Based on the obtained results of our experiments on the data vegetarian friendly restaurants in Bangkok, the models constructed by Classification and Regression Trees were able to give an accurate prediction of customers' preferences on the basis of restaurants' quality factors. The results showed that customers’ online reviews analysis can be an effective way for customers segmentation to predict their preferences and help the restaurant managers to set priority instructions for service quality improvements.}
}
@incollection{SEBASTIANCOLEMAN2022131,
title = {Chapter 7 - The People Challenge: Building Data Literacy},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {131-164},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00007-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000079},
author = {Laura Sebastian-Coleman},
keywords = {Data literacy, data visualization, analytics, metadata management, critical thinking, scientific thinking, data management},
abstract = {This chapter addresses the skills, knowledge, and experience people require to create, use, and interpret data. Data literacy is the ability to read, understand, interpret, and learn from data in different contexts and to communicate about data with other people. The people challenge is both a skills challenge and a knowledge challenge. No single individual can know everything about an organization’s data. But, together, people can solve more problems in better ways if they understand data as a construct, recognize the risks associated with data production and use, cultivate a level of skepticism about data, and develop skill in visualizing and interpreting data. They will solve even more problems if the organization supports these efforts through disciplined metadata management and data quality management.}
}
@article{YUAN201786,
title = {Exploring inter-country connection in mass media: A case study of China},
journal = {Computers, Environment and Urban Systems},
volume = {62},
pages = {86-96},
year = {2017},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2016.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0198971516303313},
author = {Yihong Yuan and Yu Liu and Guixing Wei},
keywords = {Time series, Inter-country relations, Spatio-temporal data mining, Mass media events, GDELT},
abstract = {The development of theories and techniques for big data analytics offers tremendous possibility for investigating large-scale events and patterns that emerge over space and time. In this research, we utilize a unique open dataset “The Global Data on Events, Location and Tone” (GDELT) to model the image of China in mass media, specifically, how China has related to the rest of the world and how this connection has evolved upon time. The results of this research contribute to both the methodological and the empirical perspectives: We examined the effectiveness of the dynamic time warping (DTW) distances in measuring the differences between long-term mass media data. We identified four types of connection strength patterns between China and its top 15 related countries. With that, the distance decay effect in mass media is also examined and compared with social media and public transportation data. While using multiple datasets and focusing on mass media, this study generates valuable input regarding the interpretation of the diplomatic and regional correlation for the nation of China. It also provides methodological references for investigating international relations in other countries and regions in the big data era.}
}
@incollection{LARKIN2022283,
title = {Chapter Five - Connecting marine data to society},
editor = {Giuseppe Manzella and Antonio Novellino},
booktitle = {Ocean Science Data},
publisher = {Elsevier},
pages = {283-317},
year = {2022},
isbn = {978-0-12-823427-3},
doi = {https://doi.org/10.1016/B978-0-12-823427-3.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234273000037},
author = {Kate E. Larkin and Andrée-Anne Marsan and Nathalie Tonné and Nathalie {Van Isacker} and Tim Collart and Conor Delaney and Mickaël Vasquez and Eleonora Manca and Helen Lillis and Jan-Bart Calewaert},
keywords = {Big data, Climate change, Data visualization, Digital ocean, Ecosystems, FAIR, Hackathon, Knowledge broker, Marine data, Marine map, Ocean, Ocean literacy, Open data, Science communication, Seabed habitats},
abstract = {This chapter looks at connecting marine data to society, with a focus on key developments in Europe, set in a global context. It presents the European Marine Observation and Data Network-EMODnet as an exemplar in marine domain. EMODnet has significantly advanced European capability for Findable, Accessible, Interoperable, and Reusable marine knowledge, offering access to standardized and harmonized in-situ marine data and added value data products across seven marine environmental themes. Open and free data, products and associated metadata, are available for discovery and access through a wide range of web/data services. These ensure that the wealth of existing ocean observations and marine data collected in Europe and beyond can be easily discovered and used by a growing, and diversifying, user community. Interoperability with key services is crucial toward a pan-European and global approach. Key partnerships include the Copernicus Marine Environment Monitoring Service and international initiatives, e.g., the International Oceanographic Data and Information Exchange. Looking at societal tools and applications, the chapter provides a case study of the European Atlas of the Seas, a web-mapping tool that communicates marine and other open-source data and information in an attractive and interactive way. The EU Atlas is a key tool for the European ocean literacy initiative EU4Ocean, contributing to engage citizens and drive the societal change that is required for Europe to meet the ambitious targets to be climate neutral by 2050. The paper introduces examples of emerging tools for data visualization and presents hackathons, a powerful method to cocreate and innovate applications for society. Finally, the chapter looks toward the digital era and addresses the emerging challenges and opportunities of marine data, e.g., big data and plans for a digital twin of the Ocean, as tools to enable a step-change in societal connection, understanding, and action regarding the ocean.}
}
@article{PIRI2020113339,
title = {Missing care: A framework to address the issue of frequent missing values;The case of a clinical decision support system for Parkinson's disease},
journal = {Decision Support Systems},
volume = {136},
pages = {113339},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113339},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620300944},
author = {Saeed Piri},
keywords = {Electronic health records, Data missing values, Clinical decision support systems, Predictive healthcare analytics, Imbalanced data learning, Parkinson's disease},
abstract = {In recent decades, the implementation of electronic health record (EHR) systems has been evolving worldwide, leading to the creation of immense data volume in healthcare. Moreover, there has been a call for research studies to enhance personalized medicine and develop clinical decision support systems (CDSS) by analyzing the available EHR data. In EHR data, usually, there are millions of patients records with hundreds of features collected over a long period of time. This enormity of EHR data poses significant challenges, one of which is dealing with many variables with very high degrees of missing values. In this study, the data quality issue of incompleteness in EHR data is discussed, and a framework called ‘Missing Care’ is introduced to address this issue. Using Missing Care, researchers will be able to select the most important variables at an acceptable missing values degree to develop predictive models with high predictive power. Moreover, Missing Care is applied to analyze a unique, large EHR data to develop a CDSS for detecting Parkinson's disease. Parkinson is a complex disease, and even a specialist's diagnosis is not without error. Besides, there is a lack of access to specialists in more remote areas, and as a result, about half of the patients with Parkinson's disease in the US remain undiagnosed. The developed CDSS can be integrated into EHR systems or utilized as an independent tool by healthcare practitioners who are not necessarily specialists; therefore, making up for the limited access to specialized care in remote areas.}
}
@article{MA2020109941,
title = {A bi-directional missing data imputation scheme based on LSTM and transfer learning for building energy data},
journal = {Energy and Buildings},
volume = {216},
pages = {109941},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.109941},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819333717},
author = {Jun Ma and Jack C.P. Cheng and Feifeng Jiang and Weiwei Chen and Mingzhu Wang and Chong Zhai},
keywords = {Bi-directional estimation, Building energy, Deep learning, Electric power, Missing data, Transfer learning},
abstract = {Improving the energy efficiency of the buildings is a worldwide hot topic nowadays. To assist comprehensive analysis and smart management, high-quality historical data records of the energy consumption is one of the key bases. However, the energy data records in the real world always contain different kinds of problems. The most common problem is missing data. It is also one of the most frequently reported data quality problems in big data/machine learning/deep learning related literature in energy management. However, limited studied have been conducted to comprehensively discuss different kinds of missing data situations, including random missing, continuous missing, and large proportionally missing. Also, the methods used in previous literature often rely on linear statistical methods or traditional machine learning methods. Limited study has explored the feasibility of advanced deep learning and transfer learning techniques in this problem. To this end, this study proposed a methodology, namely the hybrid Long Short Term Memory model with Bi-directional Imputation and Transfer Learning (LSTM-BIT). It integrates the powerful modeling ability of deep learning networks and flexible transferability of transfer learning. A case study on the electric consumption data of a campus lab building was utilized to test the method. Results show that LSTM-BIT outperforms other methods with 4.24% to 47.15% lower RMSE under different missing rates.}
}
@incollection{CAVALCANTE2019265,
title = {Chapter 4-2 - Novel Bioinformatics Methods for Toxicoepigenetics},
editor = {Shaun D. McCullough and Dana C. Dolinoy},
booktitle = {Toxicoepigenetics},
publisher = {Academic Press},
pages = {265-288},
year = {2019},
isbn = {978-0-12-812433-8},
doi = {https://doi.org/10.1016/B978-0-12-812433-8.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128124338000125},
author = {Raymond G. Cavalcante and Tingting Qin and Maureen A. Sartor},
keywords = {Epigenomic analysis, Toxicoepigenomics, Bisulfite sequencing, Chromatin accessibility, ChIP-seq, Integrative analysis, Chromosomal interactions},
abstract = {The use of high-throughput, genome-wide assays in toxicoepigenetics is rapidly developing and expanding. With recent advances in experimental technologies, a great amount of multiomics epigenomic data has been generated requiring the development of correspondingly advanced bioinformatics approaches to analyze and interpret such big data. This chapter discusses analysis methods for current epigenomic assays available for use in toxicoepigenetic and novel bioinformatics approaches to interpret, visualize, and integrate a variety of epigenomic data and data resources. The epigenomic features covered include DNA methylation, DNA hydroxymethylation, histone modification, chromatin accessibility, and chromatin interaction. For each type of assay used to interrogate those features, bioinformatics tools for data quality control, epigenetic mark detection, comparative analysis, data visualization, functional analysis, and integrative analysis are suggested. Looking forward, it is anticipated that researchers in toxicoepigenomics will adopt newer techniques such as single-cell assays and the bioinformatics methods will continue to evolve.}
}
@article{CHAKRABORTY2021662,
title = {The role of surrogate models in the development of digital twins of dynamic systems},
journal = {Applied Mathematical Modelling},
volume = {90},
pages = {662-681},
year = {2021},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2020.09.037},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X20305588},
author = {S. Chakraborty and S. Adhikari and R. Ganguli},
keywords = {Digital twin, Vibration, Response, Frequency, Surrogate},
abstract = {Digital twin technology has significant promise, relevance and potential of widespread applicability in various industrial sectors such as aerospace, infrastructure and automotive. However, the adoption of this technology has been slower due to the lack of clarity for specific applications. A discrete damped dynamic system is used in this paper to explore the concept of a digital twin. As digital twins are also expected to exploit data and computational methods, there is a compelling case for the use of surrogate models in this context. Motivated by this synergy, we have explored the possibility of using surrogate models within the digital twin technology. In particular, the use of Gaussian process (GP) emulator within the digital twin technology is explored. GP has the inherent capability of addressing noisy and sparse data and hence, makes a compelling case to be used within the digital twin framework. Cases involving stiffness variation and mass variation are considered, individually and jointly, along with different levels of noise and sparsity in data. Our numerical simulation results clearly demonstrate that surrogate models, such as GP emulators, have the potential to be an effective tool for the development of digital twins. Aspects related to data quality and sampling rate are analysed. Key concepts introduced in this paper are summarised and ideas for urgent future research needs are proposed.}
}
@article{ZHAO2020264,
title = {The Value of the Surgeon Informatician},
journal = {Journal of Surgical Research},
volume = {252},
pages = {264-271},
year = {2020},
issn = {0022-4804},
doi = {https://doi.org/10.1016/j.jss.2020.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0022480420302079},
author = {Jane Zhao and Raquel Forsythe and Alexander Langerman and Genevieve B. Melton and David F. Schneider and Gretchen Purcell Jackson},
keywords = {Clinical informatics, Surgery, Health information technology, Interoperability, Telemedicine, Clinical decision support},
abstract = {Clinical informatics is an interdisciplinary specialty that leverages big data, health information technologies, and the science of biomedical informatics within clinical environments to improve quality and outcomes in the increasingly complex and often siloed health care systems. Core competencies of clinical informatics primarily focus on clinical decision making and care process improvement, health information systems, and leadership and change management. Although the broad relevance of clinical informatics is apparent, this review focuses on its application and pertinence to the discipline of surgery, which is less well defined. In doing so, we hope to highlight the importance of the surgeon informatician. Topics covered include electronic health records, clinical decision support systems, computerized order entry, data analytics, clinical documentation, information architectures, implementation science, quality improvement, simulation, education, and telemedicine. The formal pathway for surgeons to become clinical informaticians is also discussed.}
}
@article{KHATRI2016673,
title = {Managerial work in the realm of the digital universe: The role of the data triad},
journal = {Business Horizons},
volume = {59},
number = {6},
pages = {673-688},
year = {2016},
note = {CYBERSECURITY IN 2016: PEOPLE, TECHNOLOGY, AND PROCESSES},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2016.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007681316300519},
author = {Vijay Khatri},
keywords = {Analytics, Big data, Managerial decision making, Managerial work, Digital universe},
abstract = {With the explosion of the digital universe, it is becoming increasingly important to understand how organizational decision making (i.e., the business-oriented perspective) is intertwined with an understanding of enterprise data assets (i.e., the data-oriented perspective). This article first compares the business- and data-oriented perspectives to describe how the two views mesh with each other. It then presents three elements in the data-oriented perspective that are collectively referred to as the data triad: (1) use, (2) design and storage, and (3) processes and people. In describing the data triad, this article highlights practices, architectural techniques, and example tools that are used to manage, access, analyze, and deliver data. By presenting different elements of the data-oriented perspective, this article broadly and concretely describes the data triad and how it can play a role in the redefined scope of work for data-driven business managers.}
}
@incollection{CINNAMON202057,
title = {Geographic Information Systems; Ethics},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {57-62},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10554-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105542},
author = {Jonathan Cinnamon},
keywords = {Access, Big data, Cartography, Digital divide, Geodemographics, Mapping, Morals, Privacy, Profiling, Representation, Responsibilities, Spatial data, Surveillance, Values},
abstract = {The development and use of geographic information system (GIS) within particular sociopolitical contexts means that ethical issues can arise both from how GIS is used and also due to the affordances and constraints of the software, hardware, and data. Ethics is a longstanding concern in the field of geographic information science (GIScience), arising amid the critical cartography and GIS and Society debates beginning in the late 1980s, in which human geographers and GIS scholars began to call for more attention to the implications of maps, GIS, and spatial data. Ethics in GIS draws on normative frameworks including deontological (duties and obligations) and teleological (consequences and outcomes) ethical perspectives, as well as nonnormative critical ethics to understand concerns that arise with GIS and map-based representation, uneven access to spatial data and technologies, and the use of GIS in geodemographic profiling, location analytics, and war. Attention to ethics in GIS has led to the development of ethics education, guidance, and codes of conduct for GIS users. Recent advancements in the availability of geolocated personal data, wider societal use of geospatial technologies, and data analytics have pulled GIS ethics to the forefront of the larger domain of information ethics, as location becomes increasingly central to wider ethical debates in the era of big data, automation, and artificial intelligence.}
}
@incollection{LINSTEDT20161,
title = {Chapter 1 - Introduction to Data Warehousing},
editor = {Daniel Linstedt and Michael Olschimke},
booktitle = {Building a Scalable Data Warehouse with Data Vault 2.0},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-15},
year = {2016},
isbn = {978-0-12-802510-9},
doi = {https://doi.org/10.1016/B978-0-12-802510-9.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128025109000015},
author = {Daniel Linstedt and Michael Olschimke},
keywords = {data, data warehouse, big data, decision support systems, scalability, business intelligence},
abstract = {This chapter introduces basic terminology of data warehousing, its applications, and the business context. It provides a brief description of its history and where it is heading. Basic data warehouse architectures that have been established in the industry are presented. Issues faced by data warehouse practitioners are explained, including topics such as big data, changing business requirements, performance issues, complexity, auditability, restart checkpoints, and fluctuation of team members.}
}
@article{LIM2018121,
title = {From data to value: A nine-factor framework for data-based value creation in information-intensive services},
journal = {International Journal of Information Management},
volume = {39},
pages = {121-135},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217300816},
author = {Chiehyeon Lim and Ki-Hun Kim and Min-Jun Kim and Jun-Yeon Heo and Kwang-Jae Kim and Paul P. Maglio},
keywords = {Big data, Data-based value creation, Information-intensive service, Factor, Data–Value Chain},
abstract = {Service is a key context for the application of IT, as IT digitizes information interactions in service and facilitates value creation, thereby contributing to service innovation. The recent proliferation of big data provides numerous opportunities for information-intensive services (IISs), in which information interactions exert the greatest effect on value creation. In the modern data-rich economy, understanding mechanisms and related factors of data-based value creation in IISs is essential for using IT to improve such services. This study identified nine key factors that characterize this data-based value creation: (1) data source, (2) data collection, (3) data, (4) data analysis, (5) information on the data source, (6) information delivery, (7) customer (information user), (8) value in information use, and (9) provider network. These factors were identified and defined through six action research projects with industry and government that used specific datasets to design new IISs and by analyzing data usage in 149 IIS cases. This paper demonstrates the usefulness of these factors for describing, analyzing, and designing the entire value creation chain, from data collection to value creation, in IISs. The main contribution of this study is to provide a simple yet comprehensive and empirically tested basis for the use and management of data to facilitate service value creation.}
}
@article{HE2022100140,
title = {GARD: Gender difference analysis and recognition based on machine learning},
journal = {Array},
volume = {14},
pages = {100140},
year = {2022},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2022.100140},
url = {https://www.sciencedirect.com/science/article/pii/S259000562200011X},
author = {Shiwen He and Jian Song and Yeyu Ou and Yuanhong Yuan and Xiaojie Zhang and Xiaohua Xu},
keywords = {Gender difference analysis, Gender recognition, Medical examination data, Machine learning},
abstract = {In recent years, intelligent diagnosis and intelligent medical treatment based on big data of medical examinations have become the main trend of medical development in the future. In this paper, we propose a method for analyzing the difference between males and females in medical examination items (medical attributes) and find that males and females of different ages have differences in medical attributes. Then, the cluster analysis method is used to further analyze the differences between male and female in medical examination items, such that some common important attributes (CIAs) that can be used for gender recognition are found within a specific age range. Following, we propose two gender recognition models (GRMs) by using the found CIAs to identify the gender. A large number of experimental results are provided to validate the effectiveness of the proposed GRMs. Experimental results show that the medical attributes with a large value of difference really contribute to gender recognition. Within a certain age range, such as 17 to 51 years old, the proposed GRM can reach 92.8% accuracy using only six medical attributes.}
}
@article{SQUITIERI2020231,
title = {Deriving Evidence from Secondary Data in Hand Surgery: Strengths, Limitations, and Future Directions},
journal = {Hand Clinics},
volume = {36},
number = {2},
pages = {231-243},
year = {2020},
note = {Health Policy and Advocacy in Hand Surgery},
issn = {0749-0712},
doi = {https://doi.org/10.1016/j.hcl.2020.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S0749071220300111},
author = {Lee Squitieri and Kevin C. Chung},
keywords = {Registry, Hand surgery, Administrative, Claims, Electronic health records, Big data, Secondary data analysis}
}
@article{LIU2020393,
title = {Wind speed forecasting using deep neural network with feature selection},
journal = {Neurocomputing},
volume = {397},
pages = {393-403},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.08.108},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220304148},
author = {Xiangjie Liu and Hao Zhang and Xiaobing Kong and Kwang Y. Lee},
keywords = {Wind speed forecasting, Deep neural network, Mutual information, Stacked auto-encoder, Denoising, Long short-term memory network},
abstract = {With the rapid growth of wind power penetration into modern power grids, wind speed forecasting (WSF) becomes an increasing important task in the planning and operation of electric power and energy systems. However, WSF is quite challengeable due to its highly varying and complex features. In this paper, a novel hybrid deep neural network forecasting method is constituted. A feature selection method based on mutual information is developed in the WSF problem. With the real-time big data from the wind farm running log, the deep neural network model for WSF is established using a stacked denoising auto-encoder and long short-term memory network. The effectiveness of the deep neural network is evaluated by 10-minutes-ahead WSF. Comparing with the traditional multi-layer perceptron network, conventional long short-term memory network and stacked auto-encoder, the resulting deep neural network significantly improves the forecasting accuracy.}
}
@article{ZHOU2020e667,
title = {Artificial intelligence in COVID-19 drug repurposing},
journal = {The Lancet Digital Health},
volume = {2},
number = {12},
pages = {e667-e676},
year = {2020},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(20)30192-8},
url = {https://www.sciencedirect.com/science/article/pii/S2589750020301928},
author = {Yadi Zhou and Fei Wang and Jian Tang and Ruth Nussinov and Feixiong Cheng},
abstract = {Summary
Drug repurposing or repositioning is a technique whereby existing drugs are used to treat emerging and challenging diseases, including COVID-19. Drug repurposing has become a promising approach because of the opportunity for reduced development timelines and overall costs. In the big data era, artificial intelligence (AI) and network medicine offer cutting-edge application of information science to defining disease, medicine, therapeutics, and identifying targets with the least error. In this Review, we introduce guidelines on how to use AI for accelerating drug repurposing or repositioning, for which AI approaches are not just formidable but are also necessary. We discuss how to use AI models in precision medicine, and as an example, how AI models can accelerate COVID-19 drug repurposing. Rapidly developing, powerful, and innovative AI and network medicine technologies can expedite therapeutic development. This Review provides a strong rationale for using AI-based assistive tools for drug repurposing medications for human disease, including during the COVID-19 pandemic.}
}
@incollection{MCGILVRAY202173,
title = {Chapter 4 - The Ten Steps Process},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {73-252},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150000062},
author = {Danette McGilvray},
keywords = {business needs, information environment, information life cycle, data quality dimensions, business impact techniques, root causes, improvement, correction, prevention, controls, monitor, communicate, ethics, change management},
abstract = {This chapter contains the step-by-step guide for creating, assessing, improving, sustaining, and managing information and data quality. Concrete instructions, sample output and templates, and practical advice for executing every step of the Ten Steps Process are provided. A step summary table gives an at-a-glance overview of objectives, purpose, inputs and outputs, techniques and tools, communication, and checkpoints for each step. The Ten Steps Process was designed to be flexible. Suggestions are given to help the reader select and adjust the Ten Steps to various situations, business needs, and data quality issues. The layout allows for quick reference with an easy-to-use format highlighting key concepts and definitions, important checkpoints, communication activities, best practices, and warnings. The experience of actual clients and users of the Ten Steps are highlighted in callout boxes called Ten Steps in Action.}
}
@article{LIU202131,
title = {Objects detection toward complicated high remote basketball sports by leveraging deep CNN architecture},
journal = {Future Generation Computer Systems},
volume = {119},
pages = {31-36},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21000303},
author = {Long Liu},
keywords = {Object detection, Sport action recognition, Image recognition, Basketball recognition},
abstract = {The analysis of high-difficulty action recognition technology in basketball is mainly to identify and analyze the physical behavior of basketball players in the video to complete the technical action. The purpose of video recognition is to provide an important guarantee for improving the level of basketball training. The current target recognition technology has achieved some results. It shows that the application of target detection technology in basketball sports scene is of great significance and can improve the effect of sports training. However, traditional sports target recognition is limited by technology and injury, and the analysis of difficult sports skills is limited by the scene, dynamic background and technology, and cannot achieve the desired effect. This is not conducive to the improvement of athletes’ skills. Therefore, this article aims to develop a big data motion target detection system based on deep convolutional neural network for sports difficult motion image recognition. More specifically, we use the high discriminative power of the convolutional neural network to extract images to perform computational preprocessing for the recognition of each human motion image in the video stream. Then, the skeleton recognition algorithm based on LSTM is used to detect the key points of the human body, which is of great significance for modeling different movements. Finally, we developed an object detection system to reconstruct each movement. By selecting five groups of highly difficult actions that are likely to cause sports injuries to conduct experimental research, the results prove the effectiveness of the target detection system we proposed.}
}
@article{JASEENA2020,
title = {Deterministic weather forecasting models based on intelligent predictors: A survey},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820304729},
author = {K.U. Jaseena and Binsu C. Kovoor},
keywords = {Weather forecasting, Artificial neural networks, Deep learning, Autoencoders, Recurrent neural networks},
abstract = {Weather forecasting is the practice of predicting the state of the atmosphere for a given location based on different weather parameters. Weather forecasts are made by gathering data about the current state of the atmosphere. Accurate weather forecasting has proven to be a challenging task for meteorologists and researchers. Weather information is essential in every facet of life like agriculture, tourism, airport system, mining industry, and power generation. Weather forecasting has now entered the era of Big Data due to the advancement of climate observing systems like satellite meteorological observation and also because of the fast boom in the volume of weather data. So, the traditional computational intelligence models are not adequate to predict the weather accurately. Hence, deep learning-based techniques are employed to process massive datasets that can learn and make predictions more effectively based on past data. The effective implementation of deep learning in various domains has motivated its use in weather forecasting and is a significant development for the weather industry. This paper provides a thorough review of different weather forecasting approaches, along with some publicly available datasets. This paper delivers a precise classification of weather forecasting models and discusses potential future research directions in this area.}
}
@article{MOHDFAIZAL2021106190,
title = {A review of risk prediction models in cardiovascular disease: conventional approach vs. artificial intelligent approach},
journal = {Computer Methods and Programs in Biomedicine},
volume = {207},
pages = {106190},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106190},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721002649},
author = {Aizatul Shafiqah {Mohd Faizal} and T. Malathi Thevarajah and Sook Mei Khor and Siow-Wee Chang},
keywords = {Cardiovascular diseases, Risk prediction, Artificial intelligence, Machine learning, Deep learning},
abstract = {Cardiovascular disease (CVD) is the leading cause of death worldwide and is a global health issue. Traditionally, statistical models are used commonly in the risk prediction and assessment of CVD. However, the adoption of artificial intelligent (AI) approach is rapidly taking hold in the current era of technology to evaluate patient risks and predict the outcome of CVD. In this review, we outline various conventional risk scores and prediction models and do a comparison with the AI approach. The strengths and limitations of both conventional and AI approaches are discussed. Besides that, biomarker discovery related to CVD are also elucidated as the biomarkers can be used in the risk stratification as well as early detection of the disease. Moreover, problems and challenges involved in current CVD studies are explored. Lastly, future prospects of CVD risk prediction and assessment in the multi-modality of big data integrative approaches are proposed.}
}
@article{MANTELERO2016238,
title = {Personal data for decisional purposes in the age of analytics: From an individual to a collective dimension of data protection},
journal = {Computer Law & Security Review},
volume = {32},
number = {2},
pages = {238-255},
year = {2016},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2016.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0267364916300280},
author = {Alessandro Mantelero},
keywords = {Big data, Right to privacy, Data protection, Group privacy, Collective interests, Data protection authorities, Risk assessment},
abstract = {In the big data era, new technologies and powerful analytics make it possible to collect and analyse large amounts of data in order to identify patterns in the behaviour of groups, communities and even entire countries. Existing case law and regulations are inadequate to address the potential risks and issues related to this change of paradigm in social investigation. This is due to the fact that both the right to privacy and the more recent right to data protection are protected as individual rights. The social dimension of these rights has been taken into account by courts and policymakers in various countries. Nevertheless, the rights holder has always been the data subject and the rights related to informational privacy have mainly been exercised by individuals. This atomistic approach shows its limits in the existing context of mass predictive analysis, where the larger scale of data processing and the deeper analysis of information make it necessary to consider another layer, which is different from individual rights. This new layer is represented by the collective dimension of data protection, which protects groups of persons from the potential harms of discriminatory and invasive forms of data processing. On the basis of the distinction between individual, group and collective dimensions of privacy and data protection, the author outlines the main elements that characterise the collective dimension of these rights and the representation of the underlying interests.}
}
@article{DUIN2020102544,
title = {The Current State of Analytics: Implications for Learning Management System (LMS) Use in Writing Pedagogy},
journal = {Computers and Composition},
volume = {55},
pages = {102544},
year = {2020},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2020.102544},
url = {https://www.sciencedirect.com/science/article/pii/S8755461520300050},
author = {Ann Hill Duin and Jason Tham},
keywords = {Learning management systems, Academic and learning analytics, Writing pedagogy, Student privacy, Access},
abstract = {Amid the burgeoning interest in and use of academic and learning analytics through learning management systems (LMS), the implications of big data and their uses should be central to computers and writing scholarship. In this case study we describe the UMN Canvas LMS experience in such as way so that writing instructors might become more familiar with levels of access to academic and learning analytics, more acquainted with the analytical capabilities in LMSs, and more mindful of implications of learning analytics stemming from LMS use in writing pedagogy. We provide a historical account on the development and infusion of LMS in writing pedagogy and demonstrate how these systems are affecting the way computers and composition scholars consider writing instruction and assessment. We then respond critically to the collection of data drawn from the authors’ use of these systems in on-campus and online teaching. We conclude with implications for writing pedagogy along with a matrix for addressing ethical concerns.}
}
@article{MONDEJAR2021148539,
title = {Digitalization to achieve sustainable development goals: Steps towards a Smart Green Planet},
journal = {Science of The Total Environment},
volume = {794},
pages = {148539},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.148539},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721036111},
author = {Maria E. Mondejar and Ram Avtar and Heyker Lellani Baños Diaz and Rama Kant Dubey and Jesús Esteban and Abigail Gómez-Morales and Brett Hallam and Nsilulu Tresor Mbungu and Chukwuebuka Christopher Okolo and Kumar Arun Prasad and Qianhong She and Sergi Garcia-Segura},
keywords = {Digitalization, Food-water-energy nexus, Internet of things, Geographic information system (GIS), Sustainable development},
abstract = {Digitalization provides access to an integrated network of unexploited big data with potential benefits for society and the environment. The development of smart systems connected to the internet of things can generate unique opportunities to strategically address challenges associated with the United Nations Sustainable Development Goals (SDGs) to ensure an equitable, environmentally sustainable, and healthy society. This perspective describes the opportunities that digitalization can provide towards building the sustainable society of the future. Smart technologies are envisioned as game-changing tools, whereby their integration will benefit the three essential elements of the food-water-energy nexus: (i) sustainable food production; (ii) access to clean and safe potable water; and (iii) green energy generation and usage. It then discusses the benefits of digitalization to catalyze the transition towards sustainable manufacturing practices and enhance citizens' health wellbeing by providing digital access to care, particularly for the underserved communities. Finally, the perspective englobes digitalization benefits by providing a holistic view on how it can contribute to address the serious challenges of endangered planet biodiversity and climate change.}
}
@article{LAMAAZI2022151,
title = {Smart-3DM: Data-driven decision making using smart edge computing in hetero-crowdsensing environment},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {151-165},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2200022X},
author = {Hanane Lamaazi and Rabeb Mizouni and Hadi Otrok and Shakti Singh and Ernesto Damiani},
keywords = {Smart edge computing, Crowdsensing, Distributed architecture, Data assessment, Data quality},
abstract = {Mobile Edge Computing (MEC) has recently emerged as a promising paradigm for Mobile Crowdsensing (MCS) environments. In a given Area of Interest (AoI), the sensing process is performed based on task requirements, which usually ask for a specific quality of the sensing outcome. In this work, a two-stage Data-Driven Decision-making Mechanism using smart edge computing (Smart-3DM) is proposed. It advocates the use of smart edge to better fulfill the data-related task requirements. Depending on the type of data to be collected, the minimum quality of the data required, and the heuristics to apply for each type of crowdsensing service, the smart edge orchestrates the selection of workers in MEC. Our approach relies on (a) smart-edge deployment: where a cluster-based distributed architecture using smart edge nodes is considered. Here, two entities are defined: the main edge node (MEN) and the local edge nodes (LENs); and (b) data management offloading where a two-layer re-selection strategy that considers data type and context-awareness is adopted, to reduce data computation complexity and to increase data quality while meeting the task target. The proposed Smart-3DM is evaluated using a real-life dataset and is compared to one-stage local and global approaches. The overall results show that by using two-stage re-selection strategies, better performance with lower processing power (CPU), less Storage(RAM), and improved execution time is achieved, when compared to the benchmarks.}
}
@article{SUN2022105034,
title = {A review of Earth Artificial Intelligence},
journal = {Computers & Geosciences},
volume = {159},
pages = {105034},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2022.105034},
url = {https://www.sciencedirect.com/science/article/pii/S0098300422000036},
author = {Ziheng Sun and Laura Sandoval and Robert Crystal-Ornelas and S. Mostafa Mousavi and Jinbo Wang and Cindy Lin and Nicoleta Cristea and Daniel Tong and Wendy Hawley Carande and Xiaogang Ma and Yuhan Rao and James A. Bednar and Amanda Tan and Jianwu Wang and Sanjay Purushotham and Thomas E. Gill and Julien Chastang and Daniel Howard and Benjamin Holt and Chandana Gangodagamage and Peisheng Zhao and Pablo Rivas and Zachary Chester and Javier Orduz and Aji John},
keywords = {Geosphere, Hydrology, Atmosphere, Artificial intelligence/machine learning, Big data, Cyberinfrastructure},
abstract = {In recent years, Earth system sciences are urgently calling for innovation on improving accuracy, enhancing model intelligence level, scaling up operation, and reducing costs in many subdomains amid the exponentially accumulated datasets and the promising artificial intelligence (AI) revolution in computer science. This paper presents work led by the NASA Earth Science Data Systems Working Groups and ESIP machine learning cluster to give a comprehensive overview of AI in Earth sciences. It holistically introduces the current status, technology, use cases, challenges, and opportunities, and provides all the levels of AI practitioners in geosciences with an overall big picture and to “blow away the fog to get a clearer vision” about the future development of Earth AI. The paper covers all the majorspheres in the Earth system and investigates representative AI research in each domain. Widely used AI algorithms and computing cyberinfrastructure are briefly introduced. The mandatory steps in a typical workflow of specializing AI to solve Earth scientific problems are decomposed and analyzed. Eventually, it concludes with the grand challenges and reveals the opportunities to give some guidance and pre-warnings on allocating resources wisely to achieve the ambitious Earth AI goals in the future.}
}
@article{ZHU202011283,
title = {Supervised Block-Aware Factorization Machine for Multi-Block Quality-Relevant Monitoring},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {11283-11288},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.370},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320306546},
author = {Qinqin Zhu},
keywords = {Quality-relevant monitoring, block-aware factorization machine, supervised learning, multi-block processes},
abstract = {Multi-block multivariate statistical methods have been developed to extract useful information from process and quality data in the era of big data, where process variables are partitioned into several meaningful blocks. However, most of these methods did not consider cross-correlations among divided blocks, which leads to inferior monitoring performance. In this article, a block-aware factorization machine (BAFM) algorithm is proposed to exploit information from process and quality data. In BAFM, quality data are first classified into normal and abnormal labels with principal component analysis based quality monitoring framework. Afterwards, a block number is attached to each process variable, and the interactions among different variables (both within and cross blocks) are learned through latent variables, which is supervised by the classified quality labels. Apart from the variable relation within the same block, BAFM also incorporates the block information; thus, both inner and cross correlations are constructed. The monitoring framework based on BAFM is developed, and its effectiveness and superiority are demonstrated through the Tennessee Eastman process.}
}
@incollection{PEZOULAS202067,
title = {Chapter 3 - Medical data sharing},
editor = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
booktitle = {Medical Data Sharing, Harmonization and Analytics},
publisher = {Academic Press},
pages = {67-104},
year = {2020},
isbn = {978-0-12-816507-2},
doi = {https://doi.org/10.1016/B978-0-12-816507-2.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165072000037},
author = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
keywords = {Data curation, Data quality management, Data sharing, Data sharing frameworks, Standardization},
abstract = {This chapter introduces the primary step toward the realization of a strategically federated platform, namely data sharing. The rationale behind medical data sharing is related to the interlinking of medical cohorts with respect to data protection regulations and dealing with the unmet needs in various clinical domains. Emphasis is given on data quality management and especially on the existence of a data curation mechanism toward an effective data quality assessment procedure. Data curation and standardization methods are presented along with the latest advances in data sharing assessment and case studies on real data. Existing data sharing frameworks and global initiatives are extensively discussed. Crucial barriers toward data sharing are finally stated along with solutions and guidelines against the misuse of shared data.}
}
@article{CHAN2022112017,
title = {Development and performance evaluation of a chiller plant predictive operational control strategy by artificial intelligence},
journal = {Energy and Buildings},
volume = {262},
pages = {112017},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112017},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822001888},
author = {K.C. Chan and Victor T.T. Wong and Anthony K.F. Yow and P.L. Yuen and Christopher Y.H. Chao},
keywords = {Chiller plant optimization, Artificial intelligence, Artificial neural network, Particle swarm optimization, VSD chiller, Building energy saving},
abstract = {Traditionally, chiller plants are controlled and monitored by a predetermined control strategy to ensure appropriate operation based on the designed system configuration. With the use of new technology of variable speed drive (VSD) for compressors, smart control strategies could be leveraged to enhance the system efficiency in lieu of traditional control strategies. For example, using orderly and straightforward switching procedures without considering various factors in switching the units, including the high-efficiency partial load range benefitted from the VSD, the actual performance of the units as a whole and the variable chilled water flow rate, result in the chiller plant not operating at maximum performance and efficiency. To address these issues, a hybrid predictive operational chiller plant control strategy is proposed to optimize the performance of the chiller plant. Artificial intelligence is employed as the data mining algorithm, with big data analysis based on the actual acquired voluminous operation data by fully considering the characteristics of chiller plants without additional installation of large-sized and high-priced equipment. Artificial neural network (ANN) was employed in the control strategy to predict the future outdoor temperature, building cooling load demand and the corresponding power consumption of the chiller plants. At the same time, particle swarm optimization (PSO) was applied to search for the optimized setpoints, e.g., chilled water supply temperature, operating sequence, chilled water flow rate, for the chiller plants. The developed control strategy has been launched in a chiller plant with a cooling capacity of 7,700 kW installed in a hospital in Hong Kong. The system coefficient of performance (COP) and overall energy consumption of the chiller plants were enhanced by about 8.6% and reduced by about 7.9%, respectively, compared with the traditional control strategy. This real-time, continuous, automatic optimization control strategy can determine the most efficient combination of operating parameters of a chiller plant with different control settings. This ensures that the chiller plant operates in its most efficient mode year-round under various operational conditions.}
}
@article{BENJELLOUN20211177,
title = {Improving outliers detection in data streams using LiCS and voting},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {33},
number = {10},
pages = {1177-1185},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819301454},
author = {Fatima-Zahra Benjelloun and Ahmed Oussous and Amine Bennani and Samir Belfkih and Ayoub {Ait Lahcen}},
keywords = {Data streams, Outlier detection, High-dimensional data, Big data mining, Intrusion detection},
abstract = {Detecting outliers in real-time is increasingly important for many real-world applications such as detecting abnormal heart activity, intrusions to systems, spams or abnormal credit card transactions. However, detecting outliers in data streams rises many challenges such as high-dimensionality, dynamic data distribution and unpredictable relationships. Our simulations demonstrate that some advanced solutions still show drawbacks. In this paper, first, we improve the capacity to detect outliers of both micro-clusters based algorithms (MCOD) and distance-based algorithms (Abstract-C and Exact-Storm) known for their performance. This is by adding a layer called LiCS that classifies online the K-nearest-neighbors (Knn) of each node based on their evolutionary status. This layer aggregates the results and uses a count threshold to better classify nodes. Experiments on SpamBase datasets confirmed that our technique enhances the accuracy and the precision of such algorithm and helps to reduce the unclassified nodes.Second, we propose a hybrid solution based on iterative majority voting and our LiCS. Experiments on real data proves that it outperforms discussed algorithms in terms of accuracy, precision and sensitivity in detecting outliers. It also minimizes the issue of unclassified instances and consolidate the different outputs of algorithms.}
}
@article{BAHLO2021106365,
title = {Livestock data – Is it there and is it FAIR? A systematic review of livestock farming datasets in Australia},
journal = {Computers and Electronics in Agriculture},
volume = {188},
pages = {106365},
year = {2021},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106365},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921003823},
author = {Christiane Bahlo and Peter Dahlhaus},
keywords = {Livestock data quality, Systematic data review, FAIR data, FAIR assessment, Precision livestock farming, Extensive livestock farming},
abstract = {The global adoption of the FAIR principles for scientific data: findable, accessible, interoperable and reusable, has been relatively slow in agriculture, compared to other disciplines. A recent review of the literature showed that the use of precision farming technologies and the development and adoption of open data standards was particularly low in extensive livestock farming. However, a plethora of public datasets exist that have the potential to be used to inform precision farming decision tools. Using extensive livestock farming in Australia as example, we investigate the quantity and quality of datasets available via a systematic dataset review. This systematic review of datasets begins with a search of open data catalogues and querying these to find datasets. Software scripts are developed and used to query the Application Programming Interfaces (APIs) of many of the large data catalogues in Australia, while catalogues without public APIs are queried manually via available web portals. Following the systematic search, a combined list of all datasets is collated and tested for FAIRness and other quality metrics. The contribution of this work is the resulting overview of the state of open datasets within the livestock farming domain on the one hand, but also the development of a systematic dataset search strategy, reusable methods and software scripts.}
}
@article{JIN2021202,
title = {Lidar sheds new light on plant phenomics for plant breeding and management: Recent advances and future prospects},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {171},
pages = {202-223},
year = {2021},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620303130},
author = {Shichao Jin and Xiliang Sun and Fangfang Wu and Yanjun Su and Yumei Li and Shiling Song and Kexin Xu and Qin Ma and Frédéric Baret and Dong Jiang and Yanfeng Ding and Qinghua Guo},
keywords = {Lidar, Traits, Phenomics, Breeding, Management, Multi-omics},
abstract = {Plant phenomics is a new avenue for linking plant genomics and environmental studies, thereby improving plant breeding and management. Remote sensing techniques have improved high-throughput plant phenotyping. However, the accuracy, efficiency, and applicability of three-dimensional (3D) phenotyping are still challenging, especially in field environments. Light detection and ranging (lidar) provides a powerful new tool for 3D phenotyping with the rapid development of facilities and algorithms. Numerous efforts have been devoted to studying static and dynamic changes of structural and functional phenotypes using lidar in agriculture. These progresses also improve 3D plant modeling across different spatial–temporal scales and disciplines, providing easier and less expensive association with genes and analysis of environmental practices and affords new insights into breeding and management. Beyond agriculture phenotyping, lidar shows great potential in forestry, horticultural, and grass phenotyping. Although lidar has resulted in remarkable improvements in plant phenotyping and modeling, the synthetization of lidar-based phenotyping for breeding and management has not been fully explored. We identify three main challenges in lidar-based phenotyping development: 1) developing low cost, high spatial–temporal, and hyperspectral lidar facilities, 2) moving into multi-dimensional phenotyping with an endeavor to generate new algorithms and models, and 3) embracing open source and big data.}
}
@article{PANG2021104454,
title = {Prediction of early childhood obesity with machine learning and electronic health record data},
journal = {International Journal of Medical Informatics},
volume = {150},
pages = {104454},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104454},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621000800},
author = {Xueqin Pang and Christopher B. Forrest and Félice Lê-Scherban and Aaron J. Masino},
keywords = {Data quality control, Early childhood obesity, Electronic health record, Machine learning, Prediction},
abstract = {Objective
This study compares seven machine learning models developed to predict childhood obesity from age > 2 to ≤ 7 years using Electronic Healthcare Record (EHR) data up to age 2 years.
Materials and methods
EHR data from of 860,510 patients with 11,194,579 healthcare encounters were obtained from the Children’s Hospital of Philadelphia. After applying stringent quality control to remove implausible growth values and including only individuals with all recommended wellness visits by age 7 years, 27,203 (50.78 % male) patients remained for model development. Seven machine learning models were developed to predict obesity incidence as defined by the Centers for Disease Control and Prevention (age/sex adjusted BMI>95th percentile). Model performance was evaluated by multiple standard classifier metrics and the differences among seven models were compared using the Cochran's Q test and post-hoc pairwise testing.
Results
XGBoost yielded 0.81 (0.001) AUC, which outperformed all other models. It also achieved statistically significant better performance than all other models on standard classifier metrics (sensitivity fixed at 80 %): precision 30.90 % (0.22 %), F1-socre 44.60 % (0.26 %), accuracy 66.14 % (0.41 %), and specificity 63.27 % (0.41 %).
Discussion and conclusion
Early childhood obesity prediction models were developed from the largest cohort reported to date. Relative to prior research, our models generalize to include males and females in a single model and extend the time frame for obesity incidence prediction to 7 years of age. The presented machine learning model development workflow can be adapted to various EHR-based studies and may be valuable for developing other clinical prediction models.}
}
@article{BETTS202035,
title = {Predicting postpartum psychiatric admission using a machine learning approach},
journal = {Journal of Psychiatric Research},
volume = {130},
pages = {35-40},
year = {2020},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2020.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0022395620308669},
author = {Kim S. Betts and Steve Kisely and Rosa Alati},
keywords = {Administrative data linkage, Postpartum psychiatric admissions, Predictive models, Machine learning},
abstract = {Aims
The accurate identification of mothers at risk of postpartum psychiatric admission would allow for preventive intervention or more timely admission. We developed a prediction model to identify women at risk of postpartum psychiatric admission.
Methods
Data included administrative health data of all inpatient live births in the Australian state of Queensland between January 2009 and October 2014. Analyses were restricted to mothers with one or more indicator of mental health problems during pregnancy (n = 75,054 births). The predictors included all maternal data up to and including the delivery, and neonatal data recorded at delivery. We used multiple machine learning methods to predict hospital admission in the 12 months following delivery in which the primary diagnosis was recorded as an ICD-10 psychotic, bipolar or depressive disorders.
Results
The boosted trees algorithm produced the best performing model, predicting postpartum psychiatric admission in the validation data with good discrimination [AUC = 0.80; 95% CI = (0.76, 0.83)] and achieving good calibration. This model outperformed benchmark logistic regression model and an elastic net model. In addition to indicators of maternal metal health history, maternal and neonatal anthropometric measures and social/lifestyle factors were strong predictors.
Conclusion
Our results indicate the potential of a big data approach when aiming to identify mothers at risk of postpartum psychiatric admission. Mothers at risk could be followed-up and supported after neonatal discharge to either remove the need for admission or facilitate more timely admission.}
}
@article{KRIEGER2021100511,
title = {Explaining the (non-) adoption of advanced data analytics in auditing: A process theory},
journal = {International Journal of Accounting Information Systems},
volume = {41},
pages = {100511},
year = {2021},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2021.100511},
url = {https://www.sciencedirect.com/science/article/pii/S1467089521000130},
author = {Felix Krieger and Paul Drews and Patrick Velte},
keywords = {Audit digitization, Audit data analytics, Big data, Machine learning, Advanced data analytics in auditing, Audit innovation},
abstract = {Audit firms are increasingly engaging with advanced data analytics to improve the efficiency and effectiveness of external audits through the automation of audit work and obtaining a better understanding of the client’s business risk and thus their own audit risk. This paper examines the process by which audit firms adopt advanced data analytics, which has been left unaddressed by previous research. We derive a process theory from expert interviews which describes the activities within the process and the organizational units involved. It further describes how the adoption process is affected by technological, organizational and environmental contextual factors. Our work contributes to the extent body of research on technology adoption in auditing by using a previously unused theoretical perspective, and contextualizing known factors of technology adoption. The findings presented in this paper emphasize the importance of technological capabilities of audit firms for the adoption of advanced data analytics; technological capabilities within audit teams can be leveraged to support both the ideation of possible use cases for advanced data analytics, as well as the diffusion of solutions into practice.}
}
@article{CHAUHAN2022121508,
title = {Linking circular economy and digitalisation technologies: A systematic literature review of past achievements and future promises},
journal = {Technological Forecasting and Social Change},
volume = {177},
pages = {121508},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.121508},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522000403},
author = {Chetna Chauhan and Vinit Parida and Amandeep Dhir},
keywords = {Circular economy, Sustainability, Product-service system (PSS), Circular business model, Artificial intelligence, Internet of things},
abstract = {The circular economy (CE) has the potential to capitalise upon emerging digital technologies, such as big data, artificial intelligence (AI), blockchain and the Internet of things (IoT), amongst others. These digital technologies combined with business model innovation are deemed to provide solutions to myriad problems in the world, including those related to circular economy transformation. Given the societal and practical importance of CE and digitalisation, last decade has witnessed a significant increase in academic publication on these topics. Therefore, this study aims to capture the essence of the scholarly work at the intersection of the CE and digital technologies. A detailed analysis of the literature based on emerging themes was conducted with a focus on illuminating the path of CE implementation. The results reveal that IoT and AI play a key role in the transition towards the CE. A multitude of studies focus on barriers to digitalisation-led CE transition and highlight policy-related issues, the lack of predictability, psychological issues and information vulnerability as some important barriers. In addition, product-service system (PSS) has been acknowledged as an important business model innovation for achieving the digitalisation enabled CE. Through a detailed assessment of the existing literature, a viable systems-based framework for digitalisation enabled CE has been developed which show the literature linkages amongst the emerging research streams and provide novel insights regarding the realisation of CE benefits.}
}
@article{GHASEMAGHAEI2018101,
title = {Data analytics competency for improving firm decision making performance},
journal = {The Journal of Strategic Information Systems},
volume = {27},
number = {1},
pages = {101-113},
year = {2018},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2017.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0963868717300768},
author = {Maryam Ghasemaghaei and Sepideh Ebrahimi and Khaled Hassanein},
keywords = {Data analytics competency, Data quality, Bigness of data, Analytical skills, Domain knowledge, Tools sophistication, Decision making performance},
abstract = {This study develops and validates the concept of Data Analytics Competency as a five multidimensional formative index (i.e., data quality, bigness of data, analytical skills, domain knowledge, and tools sophistication) and empirically examines its impact on firm decision making performance (i.e., decision quality and decision efficiency). The findings based on an empirical analysis of survey data from 151 Information Technology managers and data analysts demonstrate a large, significant, positive relationship between data analytics competency and firm decision making performance. The results reveal that all dimensions of data analytics competency significantly improve decision quality. Furthermore, interestingly, all dimensions, except bigness of data, significantly increase decision efficiency. This is the first known empirical study to conceptualize, operationalize and validate the concept of data analytics competency and to study its impact on decision making performance. The validity of the data analytics competency construct as conceived and operationalized, suggests the potential for future research evaluating its relationships with possible antecedents and consequences. For practitioners, the results provide important guidelines for increasing firm decision making performance through the use of data analytics.}
}
@article{RAHIMI2014768,
title = {Validating an ontology-based algorithm to identify patients with Type 2 Diabetes Mellitus in Electronic Health Records},
journal = {International Journal of Medical Informatics},
volume = {83},
number = {10},
pages = {768-778},
year = {2014},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2014.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1386505614001038},
author = {Alireza Rahimi and Siaw-Teng Liaw and Jane Taggart and Pradeep Ray and Hairong Yu},
keywords = {Ontology, SPARQL, Electronic Health Records, Diabetes Mellitus, Type 2, Validation studies},
abstract = {Background
Improving healthcare for people with chronic conditions requires clinical information systems that support integrated care and information exchange, emphasizing a semantic approach to support multiple and disparate Electronic Health Records (EHRs). Using a literature review, the Australian National Guidelines for Type 2 Diabetes Mellitus (T2DM), SNOMED-CT-AU and input from health professionals, we developed a Diabetes Mellitus Ontology (DMO) to diagnose and manage patients with diabetes. This paper describes the manual validation of the DMO-based approach using real world EHR data from a general practice (n=908 active patients) participating in the electronic Practice Based Research Network (ePBRN).
Method
The DMO-based algorithm to query, using Semantic Protocol and RDF Query Language (SPARQL), the structured fields in the ePBRN data repository were iteratively tested and refined. The accuracy of the final DMO-based algorithm was validated with a manual audit of the general practice EHR. Contingency tables were prepared and Sensitivity and Specificity (accuracy) of the algorithm to diagnose T2DM measured, using the T2DM cases found by manual EHR audit as the gold standard. Accuracy was determined with three attributes – reason for visit (RFV), medication (Rx) and pathology (path) – singly and in combination.
Results
The Sensitivity and Specificity of the algorithm were 100% and 99.88% with RFV; 96.55% and 98.97% with Rx; and 15.6% and 98.92% with Path. This suggests that Rx and Path data were not as complete or correct as the RFV for this general practice, which kept its RFV information complete and current for diabetes. However, the completeness is good enough for this purpose as confirmed by the very small relative deterioration of the accuracy (Sensitivity and Specificity of 97.67% and 99.18%) when calculated for the combination of RFV, Rx and Path. The manual EHR audit suggested that the accuracy of the algorithm was influenced by data quality such as incorrect data due to mistaken units of measurement and unavailable data due to non-documentation or documented in the wrong place or progress notes, problems with data extraction, encryption and data management errors.
Conclusion
This DMO-based algorithm is sufficiently accurate to support a semantic approach, using the RFV, Rx and Path to define patients with T2DM from EHR data. However, the accuracy can be compromised by incomplete or incorrect data. The extent of compromise requires further study, using ontology-based and other approaches.}
}
@article{BELLINI2014827,
title = {Km4City ontology building vs data harvesting and cleaning for smart-city services},
journal = {Journal of Visual Languages & Computing},
volume = {25},
number = {6},
pages = {827-839},
year = {2014},
note = {Distributed Multimedia Systems DMS2014 Part I},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2014.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X14001165},
author = {Pierfrancesco Bellini and Monica Benigni and Riccardo Billero and Paolo Nesi and Nadia Rauch},
keywords = {Smart city, Knowledge base construction, Reconciliation, Validation and verification of knowledge base, Smart city ontology, Linked open graph, Km4city},
abstract = {Presently, a very large number of public and private data sets are available from local governments. In most cases, they are not semantically interoperable and a huge human effort would be needed to create integrated ontologies and knowledge base for smart city. Smart City ontology is not yet standardized, and a lot of research work is needed to identify models that can easily support the data reconciliation, the management of the complexity, to allow the data reasoning. In this paper, a system for data ingestion and reconciliation of smart cities related aspects as road graph, services available on the roads, traffic sensors etc., is proposed. The system allows managing a big data volume of data coming from a variety of sources considering both static and dynamic data. These data are mapped to a smart-city ontology, called KM4City (Knowledge Model for City), and stored into an RDF-Store where they are available for applications via SPARQL queries to provide new services to the users via specific applications of public administration and enterprises. The paper presents the process adopted to produce the ontology and the big data architecture for the knowledge base feeding on the basis of open and private data, and the mechanisms adopted for the data verification, reconciliation and validation. Some examples about the possible usage of the coherent big data knowledge base produced are also offered and are accessible from the RDF-store and related services. The article also presented the work performed about reconciliation algorithms and their comparative assessment and selection.}
}
@article{BOOMGARDZAGRODNIK2022106580,
title = {Machine learning imputation of missing Mesonet temperature observations},
journal = {Computers and Electronics in Agriculture},
volume = {192},
pages = {106580},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2021.106580},
url = {https://www.sciencedirect.com/science/article/pii/S0168169921005974},
author = {Joseph P. Boomgard-Zagrodnik and David J. Brown},
keywords = {Machine learning, Big data, Surface weather observations, Degree day models, Missing data imputation},
abstract = {Uninterrupted and reliable weather data is a necessary foundation for agricultural decision making, required for models based on accumulated growing degree days (GDD), chill units, and evapotranspiration. When a weather station experiences a mechanical or communications failure, a replacement (imputed) value should be substituted for any missing data. This study introduces a machine learning, network-based approach to imputing missing 15-minute and daily maximum/minimum air temperature observations from 8.5 years of air temperature, relative humidity, wind, and solar radiation observations at 134 AgWeatherNet (AWN) stations in Washington State. A random forest imputation model trained on temperature and humidity observations from the full network predicted 15-minute, daily maximum, and daily minimum temperature values with mean absolute errors of 0.43 °C, 0.53 °C, and 0.70 °C, respectively. Sensitivity experiments determined that imputation skill was related a number of external factors including volume and type of training data, proximity of surrounding stations, and regional topography. In particular, nocturnal cold air flows in the upper Yakima Valley of south-central Washington caused temperature to be less correlated with surrounding stations in the overnight hours. In a separate experiment, the imputation model was used to predict base- 10 °C GDD on 2020 July 1 trained entirely on 15-minute station data from previous years. Even with the entire season of observations missing, the model predicted the GDD value within an average error 1.4% with 125 of 134 stations within 5% of observations. Since missing data can typically be resolved within a timeframe of a few days, the network-based imputation model is a sufficient substitute for short periods of missing observational weather data. Other potential applications of an imputation model are briefly discussed.}
}
@article{OLARU201854,
title = {Workshop Synthesis: Passive and sensor data - potential and application},
journal = {Transportation Research Procedia},
volume = {32},
pages = {54-61},
year = {2018},
note = {Transport Survey Methods in the era of big data:facing the challenges},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S2352146518301613},
author = {Doina Olaru and Alejandro Tudela},
keywords = {passive data, big data, sensor, GPS, Wi-Fi, smartcard, transport, travel surveys},
abstract = {The workshop on technology, tools and applications around passive and sensor travel data is summarized in this paper. Such data requires protocols for collection, storing/retrieving, sharing and processing; as well as the need for validation methods and multi-disciplinary work. Traditional surveys can complement passive and sensor data to aid a deeper understanding of travel behaviour. Passive data is particularly beneficial for planning long-distance travel and freight transport. While access to massive data collected by licensed operators should be guaranteed, maintaining data privacy and cultural sensitivity is a priority.}
}
@article{JIANG2020101505,
title = {Ignorance is bliss? An empirical analysis of the determinants of PSS usefulness in practice},
journal = {Computers, Environment and Urban Systems},
volume = {83},
pages = {101505},
year = {2020},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2020.101505},
url = {https://www.sciencedirect.com/science/article/pii/S0198971520302386},
author = {Huaxiong Jiang and Stan Geertman and Patrick Witte},
keywords = {Smart city, Implementation gap, Success and failure factors, Utility, Usability, Context},
abstract = {Planning support systems (PSS) enabled by smart city technologies (big data and information and communication technologies (ICTs)) are becoming more widespread in their availability, but have not yet been fully recognized as being useful in planning practice. Thus, a better understanding of the determinants of PSS usefulness in practice helps to improve the functional support of PSS for smart cities. This study is based on a recent international questionnaire (268 respondents) designed to evaluate the perceptions of scholars and practitioners in the smart city planning field. Based on the empirical evidence, this paper recommends that it is imperative for PSS developers and users to be more responsive to the fit for task-technology and user-technology (i.e., utility and usability, respectively) since they positively contribute to PSS usefulness in practice and to be more sensitive to the potential negative effects of contextual factors on PSS usefulness in smart cities. The empirical analyses further suggest that rather than merely striving for integrating smart city technologies into advancing PSS, the way that innovative PSS are integrated into the planning framework (i.e., how well PSS can satisfy the needs of planning tasks and users by considering context-specificities) is of great significance in promoting PSS's actual usefulness.}
}
@incollection{MALCOM2014325,
title = {Chapter 14 - Analysis of Deep Sequencing Data: Insights and Challenges},
editor = {Carolina Simó and Alejandro Cifuentes and Virginia García-Cañas},
series = {Comprehensive Analytical Chemistry},
publisher = {Elsevier},
volume = {63},
pages = {325-354},
year = {2014},
booktitle = {Fundamentals of Advanced Omics Technologies: From Genes to Metabolites},
issn = {0166-526X},
doi = {https://doi.org/10.1016/B978-0-444-62651-6.00015-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444626516000155},
author = {Jacob W. Malcom and John H. Malone},
keywords = {Deep sequencing, Seq, Big data, Statistical analysis},
abstract = {Modern biomedical research demands that investigators become familiar with deep sequencing data analysis, yet the vast nature of deep sequencing data creates a variety of roadblocks for biologists not familiar with the analysis of such large datasets. In this chapter, we provide an introduction to data analysis for biologists, review first principles, point out areas of concern, and suggest software tools that are becoming standards for analysis of deep sequencing data. Perhaps the biggest challenge in the analysis of deep sequencing data will be data management and storage and repeating complex, multitier computational analyses. The future of deep sequencing data analysis will be likely data-driven and rely on principles gleaned from “big data” analysis.}
}
@article{KIRTLEY2022243,
title = {Translating promise into practice: a review of machine learning in suicide research and prevention},
journal = {The Lancet Psychiatry},
volume = {9},
number = {3},
pages = {243-252},
year = {2022},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(21)00254-6},
url = {https://www.sciencedirect.com/science/article/pii/S2215036621002546},
author = {Olivia J Kirtley and Kasper {van Mens} and Mark Hoogendoorn and Navneet Kapur and Derek {de Beurs}},
abstract = {Summary
In ever more pressured health-care systems, technological solutions offering scalability of care and better resource targeting are appealing. Research on machine learning as a technique for identifying individuals at risk of suicidal ideation, suicide attempts, and death has grown rapidly. This research often places great emphasis on the promise of machine learning for preventing suicide, but overlooks the practical, clinical implementation issues that might preclude delivering on such a promise. In this Review, we synthesise the broad empirical and review literature on electronic health record-based machine learning in suicide research, and focus on matters of crucial importance for implementation of machine learning in clinical practice. The challenge of preventing statistically rare outcomes is well known; progress requires tackling data quality, transparency, and ethical issues. In the future, machine learning models might be explored as methods to enable targeting of interventions to specific individuals depending upon their level of need—ie, for precision medicine. Primarily, however, the promise of machine learning for suicide prevention is limited by the scarcity of high-quality scalable interventions available to individuals identified by machine learning as being at risk of suicide.}
}
@incollection{FILCHEV2020103,
title = {Chapter 6 - Surveys, Catalogues, Databases/Archives, and State-of-the-Art Methods for Geoscience Data Processing},
editor = {Petr Škoda and Fathalrahman Adam},
booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
publisher = {Elsevier},
pages = {103-136},
year = {2020},
isbn = {978-0-12-819154-5},
doi = {https://doi.org/10.1016/B978-0-12-819154-5.00016-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191545000163},
author = {Lachezar Filchev and Lyubka Pashova and Vasil Kolev and Stuart Frye},
keywords = {Geosensor networks, Remote sensing, Earth observations, Geoinformation, Big Data, Databases/archives, Satellite image processing, Geographic information systems, Web geoportals, ICT, Decision analysis and technologies, Spectral imaging, Fourier analysis, Principal component analysis, Karhunen–Loève transform, Continuous and discrete wavelet, Multiwavelet transforms, Hyperspectral images, Classification methods, Image denoising},
abstract = {Recent years are marked with rapid growth in sources and availability of geospatial data and information providing new opportunities and challenges for scientific knowledge and technology solutions on time. This chapter represents a general overview of modern ICT tools and methods for acquiring Earth observation (EO) data storage, processing, analysis, and interpretation for many research and applied purposes. The main contribution to Big Data developments in EO is the space activities of the space and governmental agencies, such as CNES, CSA, CSIRO, DLR, ESA, INPE, ISRO, JAXA, NASA, RADI, and Roscosmos. Special attention is devoted to the international archives, catalogues, and databases of satellite EO, which already become an indispensable and crucial source of information in support of many sectors of social-economic activities and resolving environmental issues. Main technological and information products, geoportals, and services to deal with Big EO datasets are shortly discussed. Some advanced contemporary approaches for processing big EO data, compressing, clustering, and denoising, and hyperspectral images in the geoinformation science are outlined.}
}
@article{ZHANG2021103691,
title = {Perspectives of big experimental database and artificial intelligence in tunnel fire research},
journal = {Tunnelling and Underground Space Technology},
volume = {108},
pages = {103691},
year = {2021},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2020.103691},
url = {https://www.sciencedirect.com/science/article/pii/S0886779820306453},
author = {Xiaoning Zhang and Xiqiang Wu and Younggi Park and Tianhang Zhang and Xinyan Huang and Fu Xiao and Asif Usmani},
keywords = {Big data, Empirical model, Deep learning, Critical event, Smart firefighting},
abstract = {Tunnel fire is one of the most severe global fire hazards and causes a significant amount of economic losses and casualties every year. Over the last 50 years, numerous full-scale and reduced-scale tunnel fire tests, as well as numerical simulations have been conducted to quantify the critical fire events and key parameters to guide the fire safety design of the tunnel. In light of the recent advances in big data and artificial intelligence, this paper aims to establish a database that contains all existing experimental data of tunnel fire, based on an extensive literature review on tunnel fire tests. This tunnel-fire database summarizes seven key parameters of flame, ventilation, and smoke in that is open access at a GitHub site: https://github.com/PolyUFire/Tunnel_Fire_Database. The test conditions, experimental phenomena, and data of each literature work were organized and categorized in a standard format that could be conveniently accessed and continuously updated. Based on this database, machine learning is applied to predict the critical ventilation velocity of a tunnel fire as a demonstration. The review of the current database not only reveals more valuable information and hidden problems in the conventional collection of test data, but also provides new directions in future tunnel fire research. The established database and methodology help promote the application of artificial intelligence and smart firefighting in tunnel fire safety.}
}
@article{MIGLANI202137,
title = {Blockchain management and machine learning adaptation for IoT environment in 5G and beyond networks: A systematic review},
journal = {Computer Communications},
volume = {178},
pages = {37-63},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002632},
author = {Arzoo Miglani and Neeraj Kumar},
keywords = {Blockchain, Machine learning, Federated learning, Internet of Things, Deep learning, 5G, 6G},
abstract = {Keeping in view of the constraints and challenges with respect to big data analytics along with security and privacy preservation for 5G and B5G applications, the integration of machine learning and blockchain, two of the most promising technologies of the modern era is inevitable. In comparison to the traditional centralized techniques for security and privacy preservation, blockchain uses decentralized consensus algorithms for verification and validation of different transactions which are supposed to become an integral part of blockchain network. Starting with the existing literature survey, we introduce the basic concepts of blockchain and machine learning in this article. Then, we presented a comprehensive taxonomy for integration of blockchain and machine learning in an IoT environment. We also explored federated learning, reinforcement learning, deep learning algorithms usage in blockchain based applications. Finally, we provide recommendations for future use cases of these emerging technologies in 5G and B5G technologies.}
}
@article{JI2020103459,
title = {Converting clinical document architecture documents to the common data model for incorporating health information exchange data in observational health studies: CDA to CDM},
journal = {Journal of Biomedical Informatics},
volume = {107},
pages = {103459},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103459},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420300873},
author = {Hyerim Ji and Seok Kim and Soyoung Yi and Hee Hwang and Jeong-Whun Kim and Sooyoung Yoo},
keywords = {Clinical document architecture, Common data model, Observational Medical Outcomes Partnership, Referral documents},
abstract = {Background
Utilization of standard health information exchange (HIE) data is growing due to the high adoption rate and interoperability of electronic health record (EHR) systems. However, integration of HIE data into an EHR system is not yet fully adopted in clinical research. In addition, data quality should be verified for the secondary use of these data. Thus, the aims of this study were to convert referral documents in a Health Level 7 (HL7) clinical document architecture (CDA) to the common data model (CDM) to facilitate HIE data availability for longitudinal data analysis, and to identify data quality levels for application in future clinical studies.
Methods
A total of 21,492 referral CDA documents accumulated for over 10 years in a tertiary general hospital in South Korea were analyzed. To convert CDA documents to the Observational Medical Outcomes Partnership (OMOP) CDM, processes such as CDA parsing, data cleaning, standard vocabulary mapping, CDA-to-CDM mapping, and CDM conversion were performed. The quality of CDM data was then evaluated using the Achilles Heel and visualized with the Achilles tool.
Results
Mapping five CDA elements (document header, problem, medication, laboratory, and procedure) into an OMOP CDM table resulted in population of 9 CDM tables (person, visit_occurrence, condition_occurrence, drug_exposure, measurement, observation, procedure_occurrence, care_site, and provider). Three CDM tables (drug_era, condition_era, and observation_period) were derived from the converted table. From vocabulary mapping codes in CDA documents according to domain, 98.6% of conditions, 68.8% of drugs, 35.7% of measurements, 100% of observation, and 56.4% of procedures were mapped as standard concepts. The conversion rates of the CDA to the OMOP CDM were 96.3% for conditions, 97.2% for drug exposure, 98.1% for procedure occurrence, 55.1% for measurements, and 100% for observation.
Conclusions
We examined the possibility of CDM conversion by defining mapping rules for CDA-to-CDM conversion using the referral CDA documents collected from clinics in actual medical practice. Although mapping standard vocabulary for CDM conversion requires further improvement, the conversion could facilitate further research on the usage patterns of medical resources and referral patterns.}
}
@article{PLAZAS2022101971,
title = {Sense, Transform & Send for the Internet of Things (STS4IoT): UML profile for data-centric IoT applications},
journal = {Data & Knowledge Engineering},
volume = {139},
pages = {101971},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101971},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000926},
author = {Julian Eduardo Plazas and Sandro Bimonte and Michel Schneider and Christophe {de Vaulx} and Pietro Battistoni and Monica Sebillo and Juan Carlos Corrales},
keywords = {Data-centric conceptual modelling, Model-driven architecture, Automatic code generation, Internet of Things},
abstract = {The Internet of Things is currently one of the most representative sources of Big Data. It can acquire real-time data from multiple spatially distributed points, allowing for the extraction of valuable insights. However, an appropriate integration, processing, and analysis of these data depends on several factors starting from the correct definition of the information systems. This paper introduces STS4IoT, a UML profile and automatic code-generation tool for model-driven IoT, to address this issue. STS4IoT allows designing and implementing an IoT application from the required data only, bridging the gaps between the IoT and database design worlds. The IoT data design includes both different in-network transformations and the join of streams from multiple sources. Besides, it follows the Model-Driven Architecture (MDA) guidelines to provide abstraction levels oriented to the different roles participating in the application design. The STS4IoT validation shows it has an excellent structure and is highly understandable. Its instance models are well-formed, highly abstract and readable. And the automatic implementation tool can generate complete code for complex real-world applications involving multiple IoT devices. Then, STS4IoT simplifies the definition and development of IoT applications and their integration into other information systems, such as stream data warehouses.}
}
@incollection{NETTLETON2014105,
title = {Chapter 7 - Data Sampling and Partitioning},
editor = {David Nettleton},
booktitle = {Commercial Data Mining},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {105-117},
year = {2014},
isbn = {978-0-12-416602-8},
doi = {https://doi.org/10.1016/B978-0-12-416602-8.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124166028000078},
author = {David Nettleton},
keywords = {sampling, data reduction, partitioning, business criteria, train, test, Big Data},
abstract = {This chapter discusses various types of sampling such as random sampling and sampling based on business criteria (age of customer, time as client, etc.). It also discusses extracting train and test datasets for specific business objectives and considers the issue of Big Data, given that it is currently a hot topic.}
}
@article{ALSHAER2019792,
title = {IBRIDIA: A hybrid solution for processing big logistics data},
journal = {Future Generation Computer Systems},
volume = {97},
pages = {792-804},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1830606X},
author = {Mohammed AlShaer and Yehia Taher and Rafiqul Haque and Mohand-Saïd Hacid and Mohamed Dbouk},
keywords = {Realtime processing, Clustering, Big data, Internet of Things, Logistics, Hierarchical clustering algorithm},
abstract = {Internet of Things (IoT) is leading to a paradigm shift within the logistics industry. Logistics services providers use sensor technologies such as GPS or telemetry to track and manage their shipment processes. Additionally, they use external data that contain critical information about events such as traffic, accidents, and natural disasters. Correlating data from different sensors and social media and performing analysis in real-time provide opportunities to predict events and prevent unexpected delivery delay at run-time. However, collecting and processing data from heterogeneous sources foster problems due to the variety and velocity of data. In addition, processing data in real-time is heavily challenging that it cannot be dealt with using conventional logistics information systems. In this paper, we present a hybrid framework for processing massive volume of data in batch style and real-time. Our framework is built upon Johnson’s hierarchical clustering (HCL) algorithm which produces a dendrogram that represents different clusters of data objects.}
}
@article{FRYE2021142,
title = {Production rescheduling through product quality prediction},
journal = {Procedia Manufacturing},
volume = {54},
pages = {142-147},
year = {2021},
note = {10th CIRP Sponsored Conference on Digital Enterprise Technologies (DET 2020) – Digital Technologies as Enablers of Industrial Competitiveness and Sustainability},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.07.022},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921001578},
author = {Maik Frye and Dávid Gyulai and Júlia Bergmann and Robert H. Schmitt},
keywords = {Machine Learning, Production Scheduling, Product Quality Prediction, Data Quality},
abstract = {In production management, efficient scheduling is key towards smooth and balanced production. Scheduling can be well-supported by real-time data acquisition systems, resulting in decisions that rely on actual or predicted status of production environment and jobs in progress. Utilizing advanced monitoring systems, prediction-based rescheduling method is proposed that can react on in-process scrap predictions, performed by machine learning algorithms. Based on predictions, overall production can be rescheduled with higher efficiency, compared to rescheduling after completion of the whole machining process with realization of scrap. Series of numerical experiments are presented to demonstrate potentials in prediction-based rescheduling, with early-stage scrap detection.}
}
@article{PEER20212162,
title = {Developing and evaluating a pediatric asthma severity computable phenotype derived from electronic health records},
journal = {Journal of Allergy and Clinical Immunology},
volume = {147},
number = {6},
pages = {2162-2170},
year = {2021},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2020.11.045},
url = {https://www.sciencedirect.com/science/article/pii/S0091674920324052},
author = {Komal Peer and William G. Adams and Aaron Legler and Megan Sandel and Jonathan I. Levy and Renée Boynton-Jarrett and Chanmin Kim and Jessica H. Leibler and M. Patricia Fabian},
keywords = {Asthma, electronic health records, big data, respiratory function tests, selection bias, health care disparities, delivery of health care, observer variation, National Heart, Lung, and Blood Institute (US), pediatrics},
abstract = {Background
Extensive data available in electronic health records (EHRs) have the potential to improve asthma care and understanding of factors influencing asthma outcomes. However, this work can be accomplished only when the EHR data allow for accurate measures of severity, which at present are complex and inconsistent.
Objective
Our aims were to create and evaluate a standardized pediatric asthma severity phenotype based in clinical asthma guidelines for use in EHR-based health initiatives and studies and also to examine the presence and absence of these data in relation to patient characteristics.
Methods
We developed an asthma severity computable phenotype and compared the concordance of different severity components contributing to the phenotype to trends in the literature. We used multivariable logistic regression to assess the presence of EHR data relevant to asthma severity.
Results
The asthma severity computable phenotype performs as expected in comparison with national statistics and the literature. Severity classification for a child is maximized when based on the long-term medication regimen component and minimized when based only on the symptom data component. Use of the severity phenotype results in better, clinically grounded classification. Children for whom severity could be ascertained from these EHR data were more likely to be seen for asthma in the outpatient setting and less likely to be older or Hispanic. Black children were less likely to have lung function testing data present.
Conclusion
We developed a pragmatic computable phenotype for pediatric asthma severity that is transportable to other EHRs.}
}
@article{MIETH2019868,
title = {Framework for the usage of data from real-time indoor localization systems to derive inputs for manufacturing simulation},
journal = {Procedia CIRP},
volume = {81},
pages = {868-873},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.216},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119305219},
author = {Carina Mieth and Anne Meyer and Michael Henke},
keywords = {real-time indoor localization system, input data management, cyber-physical system, manufacturing simulation, digital twin},
abstract = {Discrete event simulation is becoming increasingly important in the planning and operation of complex manufacturing systems. A major problem with today’s approach to manufacturing simulation studies is the collection and processing of data from heterogeneous sources, because the data is often of poor quality and does not contain all the necessary information for a simulation. This work introduces a framework that uses a real-time indoor localization systems (RTILS) as a central main data harmonizer, that is designed to feed production data into a manufacturing simulation from a single source of truth. It is shown, based on different data quality dimensions, how this contributes to a better overall data quality in manufacturing simulation. Furthermore, a detailed overview on which simulation inputs can be derived from the RTILS data is given.}
}
@incollection{MILLER2020205,
title = {Chapter 10 - AI, autonomous machines and human awareness: Towards shared machine-human contexts in medicine},
editor = {William F. Lawless and Ranjeev Mittu and Donald A. Sofge},
booktitle = {Human-Machine Shared Contexts},
publisher = {Academic Press},
pages = {205-220},
year = {2020},
isbn = {978-0-12-820543-3},
doi = {https://doi.org/10.1016/B978-0-12-820543-3.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205433000109},
author = {D. Douglas Miller and Elena A. Wood},
keywords = {Medicine, Health care, Artificial intelligence, Medical education, Applications, Challenges},
abstract = {Medical curricula trend to integrate clinical skills training and to create efficiencies in preclinical medical sciences, but the rapid emergence big data-intensive health care has led to initiating collaborations among data scientists, computer engineers, and medical educators that might generate novel educational high-technology platforms and innovative AI practice applications. The preprocessing of big data improves neural network feature recognition, improving the speed and accuracy of AI diagnostics and permitting chronic disease predictions. Applications of generative adversarial networks to create virtual patient phenotypes and image sets exposes medical learners to endless illness presentations, improving system-1 critical thinking for differential diagnosis development. AI offers great potential for education data managers working in support of medical educators and learners. These opportunities to build a shared context, in keeping with these themes of this book, include emerging data-driven AI applications for medical education and provider training include individual aptitude-based career advising, early identification of learners with academic difficulties, highly focused e-tutoring interventions, and natural language processing of standardized exam questions.}
}
@article{GLENNON2021100516,
title = {Challenges in modeling the emergence of novel pathogens},
journal = {Epidemics},
volume = {37},
pages = {100516},
year = {2021},
issn = {1755-4365},
doi = {https://doi.org/10.1016/j.epidem.2021.100516},
url = {https://www.sciencedirect.com/science/article/pii/S1755436521000621},
author = {Emma E. Glennon and Marjolein Bruijning and Justin Lessler and Ian F. Miller and Benjamin L. Rice and Robin N. Thompson and Konstans Wells and C. Jessica E. Metcalf},
keywords = {Immune landscape, Genotype to phenotype map, Big data, Data integration, Fundamental theory, Health system functioning},
abstract = {The emergence of infectious agents with pandemic potential present scientific challenges from detection to data interpretation to understanding determinants of risk and forecasts. Mathematical models could play an essential role in how we prepare for future emergent pathogens. Here, we describe core directions for expansion of the existing tools and knowledge base, including: using mathematical models to identify critical directions and paths for strengthening data collection to detect and respond to outbreaks of novel pathogens; expanding basic theory to identify infectious agents and contexts that present the greatest risks, over both the short and longer term; by strengthening estimation tools that make the most use of the likely range and uncertainties in existing data; and by ensuring modelling applications are carefully communicated and developed within diverse and equitable collaborations for increased public health benefit.}
}
@article{YUE2018316,
title = {Using Taiwan National Health Insurance Database to model cancer incidence and mortality rates},
journal = {Insurance: Mathematics and Economics},
volume = {78},
pages = {316-324},
year = {2018},
note = {Longevity risk and capital markets: The 2015–16 update},
issn = {0167-6687},
doi = {https://doi.org/10.1016/j.insmatheco.2017.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167668717304304},
author = {Jack C. Yue and Hsin-Chung Wang and Yin-Yee Leong and Wei-Ping Su},
keywords = {Cancer insurance, Longevity risk, Big data, Stochastic models, National Health Insurance},
abstract = {The increasing cancer incidence and decreasing mortality rates in Taiwan worsened the loss ratio of cancer insurance products and created a financial crisis for insurers. In general, the loss ratio of long-term health products seems to increase with the policy year. In the present study, we used the data from Taiwan National Health Insurance Research Database to evaluate the challenge of designing cancer products. We found that the Lee–Carter and APC models have the smallest estimation errors, and the CBD and Gompertz models are good alternatives to explore the trend of cancer incidence and mortality rates, especially for the elderly people. The loss ratio of Taiwan’s cancer products is to grow and this can be deemed as a form of longevity risk. The longevity risk of health products is necessary to face in the future, similar to the annuity products.}
}
@article{CANHOTO2021441,
title = {Leveraging machine learning in the global fight against money laundering and terrorism financing: An affordances perspective},
journal = {Journal of Business Research},
volume = {131},
pages = {441-452},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320306640},
author = {Ana Isabel Canhoto},
keywords = {Big data, Artificial intelligence, Machine learning, Algorithm, Customer profiling, Financial services, Anti-money laundering, United Nations, Sustainable development goals},
abstract = {Financial services organisations facilitate the movement of money worldwide, and keep records of their clients’ identity and financial behaviour. As such, they have been enlisted by governments worldwide to assist with the detection and prevention of money laundering, which is a key tool in the fight to reduce crime and create sustainable economic development, corresponding to Goal 16 of the United Nations Sustainable Development Goals. In this paper, we investigate how the technical and contextual affordances of machine learning algorithms may enable these organisations to accomplish that task. We find that, due to the unavailability of high-quality, large training datasets regarding money laundering methods, there is limited scope for using supervised machine learning. Conversely, it is possible to use reinforced machine learning and, to an extent, unsupervised learning, although only to model unusual financial behaviour, not actual money laundering.}
}
@article{WANG202183,
title = {The groundwater potential assessment system based on cloud computing: A case study in islands region},
journal = {Computer Communications},
volume = {178},
pages = {83-97},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002528},
author = {Daqing Wang and Haoli Xu and Yue Shi and Zhibin Ding and Zhengdong Deng and Zhixin Liu and Xingang Xu and Zhao Lu and Guangyuan Wang and Zijian Cheng and Xiaoning Zhao},
keywords = {Big data, Cloud computing, Remote sensing, Groundwater potential, Bedrock islands},
abstract = {Today’s intelligent system based on cloud computing platform can realize “unattended”, real-time monitoring observation and forecast by remote sensing. In order to import the development and efficiency of groundwater potential assessment(GPA) by remote sensing, the cloud computing platform was tried to use in the computing GPA. In this study, the Pearl River Estuary islands region(China) was selected as the study area. The slope, aspect, water-density(WD), land surface temperature(LST), NDVI and NDWI were used as the GPA indexes, which have been used before. Considering the similar geological and geomorphological conditions of the islands area, the analytic hierarchy process (AHP) method and these indexes can be used to assess GPA in the remote sensing cloud computing platform efficiently and conveniently. The results of the assessment were in good agreement with the actual hydrogeological map. Besides, the other intelligent algorithms can also be applied in this platform. Finally, this study realized the rapid “unattended” and “real-time monitoring” groundwater potential assessment, and carried out a multi-level GPA. It will be of certain reference significance to the exploitation of groundwater in the island area, which has realized convenient and efficient processing and analysis of data anytime and anywhere. At the same time, attention must be paid to the security of data and the maintenance of the system.}
}
@article{PRIYADARSHINI2015371,
title = {Semantic Retrieval of Relevant Sources for Large Scale Virtual Documents},
journal = {Procedia Computer Science},
volume = {54},
pages = {371-379},
year = {2015},
note = {Eleventh International Conference on Communication Networks, ICCN 2015, August 21-23, 2015, Bangalore, India Eleventh International Conference on Data Mining and Warehousing, ICDMW 2015, August 21-23, 2015, Bangalore, India Eleventh International Conference on Image and Signal Processing, ICISP 2015, August 21-23, 2015, Bangalore, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.06.043},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915013678},
author = {R. Priyadarshini and Latha Tamilselvan and T. Khuthbudin and S. Saravanan and S. Satish},
keywords = {Virtual documents (VD), Source document, Hadoop file System (HDFS), DW Ranking algorithm, Top  algorithm.},
abstract = {The term big data has come into use in recent years. It is used to refer to the ever-increasing amount of data that organizations are storing, processing and analyzing. An Interesting fact with bigdata is that it differ in Volume, Variety, Velocity characteristics which makes it difficult to process using the conventional Database Management System. Hence there is a need of schema less Management Systems even this will never be complete solution to bigdata analysis since the processing has no focus on the semantic information as they consider only the structural information. Content Management System like Wikipedia stores and links huge amount of documents and files. There is lack of semantic linking and analysis in such systems even though this kind of CMS uses clusters and distributed framework for storing big data. The retrieved references for a particular article are random and enormous. In order to reduce the number of references for a selected content there is a need for semantic matching. In this paper we propose framework which make use of the distributed parallel processing capability of Hadoop Distributed File System (HDFS) to perform semantic analysis over the volume of documents (bigdata) to find the best matched source document from the collection source documents for the same virtual document.}
}
@article{TIAN2020116335,
title = {Impact of water source mixture and population changes on the Al residue in megalopolitan drinking water},
journal = {Water Research},
volume = {186},
pages = {116335},
year = {2020},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2020.116335},
url = {https://www.sciencedirect.com/science/article/pii/S004313542030871X},
author = {Chenhao Tian and Chenghong Feng and Lei Chen and Qixuan Wang},
keywords = {Al residue, Mixed water sources, Big data analysis, Megalopolitan, Drinking water},
abstract = {This study establishes a new understanding of the contributions of Al residue in a megalopolitan drinking water supply system with mixed water sources. The different influences and contributions of foreign water source, resident migration and season changing to Al residue in drinking water were investigated. Especially, the role of Southern water transferred over 1200 km via the South-to-North Water Diversion Project in the Al residue of drinking water supply system of a northern megalopolitan were revealed for the first time. Comparisons of big data on Al residue in the water supply system with sole and mixed water sources showed that the introduction of Southern water enhanced the Al residue in drinking water by over 35%. The world's largest annual residents’ migration during Chinese Lunar New Year and the changes of season affect the water pipework hydrodynamics, which were embodied as the periodic changes of particulate aluminium and the relations with resident's temporal-spatial distribution in the megalopolitan. Because of the differences in water quality, Southern water promotes the release of historically deposited Al and facilitates the cleaning of old pipes.}
}
@article{JIANG2020161,
title = {Clicking position and user posting behavior in online review systems: A data-driven agent-based modeling approach},
journal = {Information Sciences},
volume = {512},
pages = {161-174},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.09.053},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519309089},
author = {Guoyin Jiang and Xiaodong Feng and Wenping Liu and Xingjun Liu},
keywords = {Agent-based modeling, Big data, Online review systems, Clicking position, Posting behavior},
abstract = {In online review systems, a participant's level of knowledge impacts his/her posting behaviors, and an increase in knowledge occurs when the participant reads the reviews posted on the systems. To capture the collective dynamics of posting reviews, we used real-world big data collected over 153 months to drive an agent-based model for replicating the operation process of online review systems. The model explains the effects of clicking position (e.g., on a review webpage's serial list) and the number of items per webpage on posting contributions. Reading reviews from the last webpage only, or from the first webpage and last webpage simultaneously, can promote a greater review volume than reading reviews in other positions. This illustrates that representing primacy (first items) and recency (recent items) within one page simultaneously, or displaying recent items in reverse chronological order, are relatively better strategies for the webpage display of online reviews. The number of items plays a nonlinear moderating role in bridging the clicking position and posting behavior, and we determine the optimal number of items. To effectively establish strategies for webpage design in online review systems, business managers must switch from reliance on experience to reliance on an agent-based model as a decision support system for the formalized webpage design of online review systems.}
}
@article{SHIROSAKIMARCALDESOUZA2021160,
title = {Evaluating and ranking secondary data sources to be used in the Brazilian LCA database – “SICV Brasil”},
journal = {Sustainable Production and Consumption},
volume = {26},
pages = {160-171},
year = {2021},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2020.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S2352550920303286},
author = {Luri {Shirosaki Marçal de Souza} and Andréa Oliveira Nunes and Gabriela Giusti and Yovana M.B. Saavedra and Thiago Oliveira Rodrigues and Tiago E. {Nunes Braga} and Diogo A. {Lopes Silva}},
keywords = {Qualidata guide, Data quality, Data format, Life Cycle Inventory, Weighting factors},
abstract = {The generation of reliable life cycle inventories is essential towards Life Cycle Assessment (LCA) development, and the use of literature inventories as data sources can serve as a driving force for emerging LCA databases. The aim of this paper was to propose a method to select and rank scientific publications to be used as possible data sources for supplying LCA databases with new datasets. A case study was designed to identify eligible datasets to compose the emergent Brazilian Life Cycle Inventory Database System – the “SICV Brasil” launched in 2016. The methodology used was based on an exploratory research composed of three steps: i) a bibliographic survey on the scientific productions of Life Cycle Inventories (LCI) in Brazil from 2000 to 2017; ii) a cross-check of LCI data and information based on the 40 selected requirements used in order to analyze the quality of LCI datasets in terms of mandatory, recommended and optional requirements; and iii) an analysis of the data quality requirements for those datasets with support of principles of Analytical Hierarchy Process (AHP) to elect possible datasets to be included in the SICV Brasil database. In total, 57 publications were analyzed and the results indicated that mandatory requirements had under 50% acceptance and only 10 requirements (less than 25%) were fully met. The best LCI dataset received 73 points (90%) with the scoring method, while 16 datasets were given less than 40 points (50%). Therefore, it is necessary to improve data quality of LCI datasets found in literature before using them to integrate LCA databases. In this regard, this study proposed a guide with short, medium, and long-term measures to mitigate this problem. The idea is to put an action plan into practice to gather more LCI datasets from literature which may be eligible for publication to SICV Brasil to improve this national database with more and relevant high-quality datasets.}
}
@article{CAUDAI20215762,
title = {AI applications in functional genomics},
journal = {Computational and Structural Biotechnology Journal},
volume = {19},
pages = {5762-5790},
year = {2021},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2021.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S2001037021004311},
author = {Claudia Caudai and Antonella Galizia and Filippo Geraci and Loredana {Le Pera} and Veronica Morea and Emanuele Salerno and Allegra Via and Teresa Colombo},
keywords = {Artificial intelligence, Functional genomics, Genomics, Proteomics, Epigenomics, Transcriptomics, Epitranscriptomics, Metabolomics, Machine learning, Deep learning},
abstract = {We review the current applications of artificial intelligence (AI) in functional genomics. The recent explosion of AI follows the remarkable achievements made possible by “deep learning”, along with a burst of “big data” that can meet its hunger. Biology is about to overthrow astronomy as the paradigmatic representative of big data producer. This has been made possible by huge advancements in the field of high throughput technologies, applied to determine how the individual components of a biological system work together to accomplish different processes. The disciplines contributing to this bulk of data are collectively known as functional genomics. They consist in studies of: i) the information contained in the DNA (genomics); ii) the modifications that DNA can reversibly undergo (epigenomics); iii) the RNA transcripts originated by a genome (transcriptomics); iv) the ensemble of chemical modifications decorating different types of RNA transcripts (epitranscriptomics); v) the products of protein-coding transcripts (proteomics); and vi) the small molecules produced from cell metabolism (metabolomics) present in an organism or system at a given time, in physiological or pathological conditions. After reviewing main applications of AI in functional genomics, we discuss important accompanying issues, including ethical, legal and economic issues and the importance of explainability.}
}
@incollection{KRISHNAN202099,
title = {5 - Pharmacy industry applications and usage},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {99-111},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000053},
author = {Krish Krishnan},
keywords = {Clinical Trials, Research, Multi-Teams, Non-Intrusive, Repeatable tests},
abstract = {One of the applications of big data applications and infrastructure is in the pharmaceutical industry. The complexity of the queries that are executed in these applications and the results they generate, make us feel the statement of torture the data and it will confess to anything. The relationships between the data in the different subject areas, the clinical trials and results, the communities in social media, the research labs and their outcomes, the clinical labs and patient results, and the financial outcomes of the pharmaceutical enterprise. Wow, think of all kinds of insights, add to this the markets, the competition, and the global industry, and we have phenomenal data to work with.}
}
@article{MOON2018304,
title = {Evaluating fidelity of lossy compression on spatiotemporal data from an IoT enabled smart farm},
journal = {Computers and Electronics in Agriculture},
volume = {154},
pages = {304-313},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.08.045},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918303697},
author = {Aekyeung Moon and Jaeyoung Kim and Jialing Zhang and Seung Woo Son},
keywords = {Smart farm, Lossy compression, IoT, Signal processing, Data fidelity},
abstract = {As the volume of data collected by various IoT sensors used in smart farm applications increases, the storing and processing of big data for agricultural applications become a huge challenge. The insight of this paper is that lossy compression can unleash the power of compression to IoT because, as compared with its counterpart (a lossless one), it can significantly reduce the data volume when the spatiotemporal characteristics of IoT sensor data are properly exploited. However, lossy compression faces the challenge of compressing too much data thus losing data fidelity, which might affect the quality of the data and potential analytics outcomes. To understand the impact of lossy compression on IoT data management and analytics, we evaluated four classification algorithms with reconstructed agricultural sensor data based on various energy concentration. Specifically, we applied three transformation-based lossy compression mechanisms to five real-world weather datasets collected at different sampling granularities from IoT weather stations. Our experimental results indicate that there is a strong positive correlation between the concentrated energy of the transformed coefficients and the compression ratio as well as the data quality. While we observed a general trend where much higher compression ratios can be achieved at the cost of a decrease in quality, we also observed that the impact on the classification accuracy varies among the data sets and algorithms we evaluated. Lastly, we show that the sampling granularity also influences the data fidelity in terms of the prediction performance and compression ratio.}
}
@article{XU2022105396,
title = {An overview of visualization and visual analytics applications in water resources management},
journal = {Environmental Modelling & Software},
pages = {105396},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105396},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222001025},
author = {Haowen Xu and Andy Berres and Yan Liu and Melissa R. Allen-Dumas and Jibonananda Sanyal},
abstract = {Recent advances in information, communication, and environmental monitoring technologies have increased the availability, spatiotemporal resolution, and quality of water-related data, thereby leading to the emergence of many innovative big data applications. Among these applications, visualization and visual analytics, also known as the visual computing techniques, empower the synergy of computational methods (e.g., machine learning and statistical models) with human reasoning to improve the understanding and solution toward complex science and engineering problems. These approaches are frequently integrated with geographic information systems and cyberinfrastructure to provide new opportunities and methods for enhancing water resources management. In this paper, we present a comprehensive review of recent hydroinformatics applications that employ visual computing techniques to (1) support complex data-driven research problems, and (2) support the communication and decision-makings in the water resources management sector. Then, we conduct a technical review of the state-of-the-art web-based visualization technologies and libraries to share our experiences on developing shareable, adaptive, and interactive visualizations and visual interfaces for resources management applications. We close with a vision that applies the emerging visual computing technologies and paradigms to develop the next generation of water resources management applications.}
}
@article{LI2020124178,
title = {Forecasting crude oil price with multilingual search engine data},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {551},
pages = {124178},
year = {2020},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2020.124178},
url = {https://www.sciencedirect.com/science/article/pii/S037843712030025X},
author = {Jingjing Li and Ling Tang and Shouyang Wang},
keywords = {Big data, Multilingual search engine index, Crude oil price forecasting, Google Trends, Artificial intelligence},
abstract = {In the big data era, search engine data (SED) have presented new opportunities for improving crude oil price prediction; however, the existing research were confined to single-language (mostly English) search keywords in SED collection. To address such a language bias and grasp worldwide investor attention, this study proposes a novel multilingual SED-driven forecasting methodology from a global perspective. The proposed methodology includes three main steps: (1) multilingual index construction, based on multilingual SED; (2) relationship investigation, between the multilingual index and crude oil price; and (3) oil price prediction, with the multilingual index as an informative predictor. With WTI spot price as studying samples, the empirical results indicate that SED have a powerful predictive power for crude oil price; nevertheless, multilingual SED statistically demonstrate better performance than single-language SED, in terms of enhancing prediction accuracy and model robustness.}
}
@article{KAMPKER2018120,
title = {Enabling Data Analytics in Large Scale Manufacturing},
journal = {Procedia Manufacturing},
volume = {24},
pages = {120-127},
year = {2018},
note = {4th International Conference on System-Integrated Intelligence: Intelligent, Flexible and Connected Systems in Products and Production},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918305341},
author = {Achim Kampker and Heiner Heimes and Ulrich Bührer and Christoph Lienemann and Stefan Krotil},
keywords = {Automotive, Manufacturing, Data Analytics, Big Data, Optimization},
abstract = {Companies of the manufacturing industry face increasing process complexity. To remain competitive, increasing the knowledge concerning innovative manufacturing processes is necessary. In other areas, data analytics methods have been successfully applied for this purpose. Currently, their application in large scale manufacturing is hampered by insufficient data availability. Therefore, this study presents a solution approach that enables adaptive data availability by establishing a data-use-case-matrix (DUCM), which allows use case prioritization to support dimensioning of control systems and IT infrastructures. In order to support technology development, further proposed is a scalable implementation of the prioritized use cases starting in early prototyping phases.}
}
@article{HE2019320,
title = {Network-wide identification of turn-level intersection congestion using only low-frequency probe vehicle data},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {108},
pages = {320-339},
year = {2019},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2019.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X18312543},
author = {Zhengbing He and Geqi Qi and Lili Lu and Yanyan Chen},
keywords = {Big data, Floating car data, Urban road network, Traffic congestion, Road intersection},
abstract = {Locating the bottlenecks in cities where traffic congestion usually occurs is essential prior to solving congestion problems. Therefore, this paper proposes a low-frequency probe vehicle data (PVD)-based method to identify turn-level intersection traffic congestion in an urban road network. This method initially divides an urban area into meter-scale square cells and maps PVD into those cells and then identifies the cells that correspond to road intersections by taking advantage of the fixed-location stop-and-go characteristics of traffic passing through intersections. With those rasterized road intersections, the proposed method recognizes probe vehicles’ turning directions and provides preliminary analysis of traffic conditions at all turning directions. The proposed method is map-independent (i.e., no digital map is needed) and computationally efficient and is able to rapidly screen most of the intersections for turn-level congestion in a road network. Thereby, this method is expected to greatly decrease traffic engineers’ workloads by providing information regarding where and when to investigate and solve traffic congestion problems.}
}
@article{JANSSEN2020101493,
title = {Data governance: Organizing data for trustworthy Artificial Intelligence},
journal = {Government Information Quarterly},
volume = {37},
number = {3},
pages = {101493},
year = {2020},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2020.101493},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X20302719},
author = {Marijn Janssen and Paul Brous and Elsa Estevez and Luis S. Barbosa and Tomasz Janowski},
keywords = {Big data, Data governance, AI, Algorithmic governance, Information sharing, Artificial Intelligence, Trusted frameworks},
abstract = {The rise of Big, Open and Linked Data (BOLD) enables Big Data Algorithmic Systems (BDAS) which are often based on machine learning, neural networks and other forms of Artificial Intelligence (AI). As such systems are increasingly requested to make decisions that are consequential to individuals, communities and society at large, their failures cannot be tolerated, and they are subject to stringent regulatory and ethical requirements. However, they all rely on data which is not only big, open and linked but varied, dynamic and streamed at high speeds in real-time. Managing such data is challenging. To overcome such challenges and utilize opportunities for BDAS, organizations are increasingly developing advanced data governance capabilities. This paper reviews challenges and approaches to data governance for such systems, and proposes a framework for data governance for trustworthy BDAS. The framework promotes the stewardship of data, processes and algorithms, the controlled opening of data and algorithms to enable external scrutiny, trusted information sharing within and between organizations, risk-based governance, system-level controls, and data control through shared ownership and self-sovereign identities. The framework is based on 13 design principles and is proposed incrementally, for a single organization and multiple networked organizations.}
}
@article{ZHANG2020102659,
title = {Design and application of a personal credit information sharing platform based on consortium blockchain},
journal = {Journal of Information Security and Applications},
volume = {55},
pages = {102659},
year = {2020},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2020.102659},
url = {https://www.sciencedirect.com/science/article/pii/S2214212620308139},
author = {Jing Zhang and Rong Tan and Chunhua Su and Wen Si},
keywords = {Consortium blockchain, Personal credit reporting, Credit information sharing, Big data crediting},
abstract = {The technical features of blockchain, including decentralization, data transparency, tamper-proofing, traceability, privacy protection and open-sourcing, make it a suitable technology for solving the information asymmetry problem in personal credit reporting transactions. Applying blockchain technology to credit reporting meets the needs of social credit system construction and may become an important technical direction in the future. This paper analyzed the problems faced by China’s personal credit reporting market, designed the framework of personal credit information sharing platform based on blockchain 3.0 architecture, studied the technical details of the platform and the technical advantages, and finally, applied the platform to the credit blacklist sharing transaction and explored the possible implementation approach. The in-depth integration of blockchain technology and personal credit reporting helps to realize the safe sharing of credit data and reduce the cost of credit data collection, thereby helping the technological and efficiency transformation of the personal credit reporting industry and promoting the overall development of the social credit system.}
}
@article{MOZZONI2022402,
title = {Transfer’s monitoring in bus transit services by Automatic Vehicle Location data},
journal = {Transportation Research Procedia},
volume = {60},
pages = {402-409},
year = {2022},
note = {New scenarios for safe mobility in urban areasProceedings of the XXV International Conference Living and Walking in Cities (LWC 2021), September 9-10, 2021, Brescia, Italy},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2021.12.052},
url = {https://www.sciencedirect.com/science/article/pii/S2352146521009534},
author = {Sara Mozzoni and Massimo Di Francesco and Giulio Maternini and Benedetto Barabino},
keywords = {Big Data, Transfer diagnosis, Automatic Vehicle Location Data},
abstract = {Since transfers increase the connectivity of routes, they improve the characteristics of transit networks. Designing and managing transfers are well-investigated issues arising at the tactical and operational level. Conversely, the monitoring phase was rarely faced to verify the consistency between well planned and/or delivered transfers. In this paper, we tailor an innovative methodology for measuring the rate of transfers between two routes by using archived Automatic Vehicle Location (AVL) data. This measurement is performed spatially, at shared and unshared (but reasonably quite close) bus stops, and temporally at each time period. The results are represented by easy-to-read control dashboards. This methodology is tested by about 240,000 AVL real records provided by the local bus operator of Cagliari (Italy) and provides valuable insights into the characterization of transfers.}
}
@incollection{MILOSEVIC201639,
title = {Chapter 2 - Real-Time Analytics},
editor = {Rajkumar Buyya and Rodrigo N. Calheiros and Amir Vahid Dastjerdi},
booktitle = {Big Data},
publisher = {Morgan Kaufmann},
pages = {39-61},
year = {2016},
isbn = {978-0-12-805394-2},
doi = {https://doi.org/10.1016/B978-0-12-805394-2.00002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053942000027},
author = {Z. Milosevic and W. Chen and A. Berry and F.A. Rabhi},
keywords = {Real-time analytics, Complex event processing, Streaming, Event processing, Advanced analytics, Data analysis, Machine learning, Finance, EventSwarm},
abstract = {Real-time analytics is a special kind of Big Data analytics in which data elements are required to be processed and analyzed as they arrive in real time. It is important in situations where real-time processing and analysis can deliver important insights and yield business value. This chapter provides an overview of current processing and analytics platforms needed to support such analysis, as well as analytics techniques that can be applied in such environments. The chapter looks beyond traditional event processing system technology to consider a broader big data context that involves “data at rest” platforms and solutions. The chapter includes a case study showing the use of EventSwarm complex event processing engine for a class of analytics problems in finance. The chapter concludes with several research challenges, such as the need for new approaches and algorithms required to support real-time data filtering, data exploration, statistical data analysis, and machine learning.}
}
@article{TAGLIAFERRI201873,
title = {A new standardized data collection system for interdisciplinary thyroid cancer management: Thyroid COBRA},
journal = {European Journal of Internal Medicine},
volume = {53},
pages = {73-78},
year = {2018},
issn = {0953-6205},
doi = {https://doi.org/10.1016/j.ejim.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0953620518300621},
author = {Luca Tagliaferri and Carlo Gobitti and Giuseppe Ferdinando Colloca and Luca Boldrini and Eleonora Farina and Carlo Furlan and Fabiola Paiar and Federica Vianello and Michela Basso and Lorenzo Cerizza and Fabio Monari and Gabriele Simontacchi and Maria Antonietta Gambacorta and Jacopo Lenkowicz and Nicola Dinapoli and Vito Lanzotti and Renzo Mazzarotto and Elvio Russi and Monica Mangoni},
keywords = {Big data, Data pooling, Personalized medicine, Radiotherapy, Thyroid, Cancer management},
abstract = {The big data approach offers a powerful alternative to Evidence-based medicine. This approach could guide cancer management thanks to machine learning application to large-scale data. Aim of the Thyroid CoBRA (Consortium for Brachytherapy Data Analysis) project is to develop a standardized web data collection system, focused on thyroid cancer. The Metabolic Radiotherapy Working Group of Italian Association of Radiation Oncology (AIRO) endorsed the implementation of a consortium directed to thyroid cancer management and data collection. The agreement conditions, the ontology of the collected data and the related software services were defined by a multicentre ad hoc working-group (WG). Six Italian cancer centres were firstly started the project, defined and signed the Thyroid COBRA consortium agreement. Three data set tiers were identified: Registry, Procedures and Research. The COBRA-Storage System (C-SS) appeared to be not time-consuming and to be privacy respecting, as data can be extracted directly from the single centre's storage platforms through a secured connection that ensures reliable encryption of sensible data. Automatic data archiving could be directly performed from Image Hospital Storage System or the Radiotherapy Treatment Planning Systems. The C-SS architecture will allow “Cloud storage way” or “distributed learning” approaches for predictive model definition and further clinical decision support tools development. The development of the Thyroid COBRA data Storage System C-SS through a multicentre consortium approach appeared to be a feasible tool in the setup of complex and privacy saving data sharing system oriented to the management of thyroid cancer and in the near future every cancer type.}
}
@incollection{KATARINA202283,
title = {Chapter 6 - Innovative technologies in precision healthcare},
editor = {Debmalya Barh},
booktitle = {Biotechnology in Healthcare},
publisher = {Academic Press},
pages = {83-102},
year = {2022},
isbn = {978-0-323-89837-9},
doi = {https://doi.org/10.1016/B978-0-323-89837-9.00016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323898379000164},
author = {Šoltýs Katarína and Kľoc Marek and Rabajdová Miroslava and Mareková Mária},
keywords = {Precision healthcare, biotechnology, emerging technologies, big data},
abstract = {Precision medicine is the intersection of data science, analytics, and biomedicine in creating a healthy learning system that conducts research in the context of clinical care while optimizing the tools and information used to provide better outcomes for patients. Emerging technologies represent a novel, innovative, and fast-evolving trend within a particular field. Among the latest trends as virtual reality, robotics, wearable, and implantable sensors, and removable tattoos, together with nanotechnologies, 3D printing, and others are considered. In addition, new advanced computing technologies including artificial intelligence, machine learning (ML), big data mining, and cloud computing form an integral part of personalized healthcare. Personalized medicine is not necessarily the same as precision medicine. From the point of view of technology development, precision medicine is an intermediate step to personalized medicine, which will be much more complex and will require even more data. There is a big challenge to combine multiomics approaches in analysis, as we can see in bioinformatics that there are a lot of techniques in one area. Analysis of more than one area such as the genome, transcriptome even microbiome, starts exponentially grown on science field. The integration of multiomics data analysis and machine learning can have led to the discovery of new biomarkers, and improve of differential diagnostics of latent diseases. In this chapter, we describe the use of emerging technologies as well as bioengineering and ML for precision healthcare.}
}
@article{JIANG2021172,
title = {Data consistency method of heterogeneous power IOT based on hybrid model},
journal = {ISA Transactions},
volume = {117},
pages = {172-179},
year = {2021},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2021.01.056},
url = {https://www.sciencedirect.com/science/article/pii/S0019057821000665},
author = {Haoyu Jiang and Kai Chen and Quanbo Ge and Jinqiang Xu and Yingying Fu and Chunxi Li},
keywords = {Power IOT system, Hybrid model, Heterogeneous data consistency, Machine learning combination method},
abstract = {The data of the power Internet of Things (IOT) system is transferred from the IaaS layer to the SaaS layer. The general data preprocessing method mainly solves the problem of big data anomalies and missing at the PaaS layer, but it still lacks the ability to judge the high error data that meets the timing characteristics, making it difficult to deal with heterogeneous power inconsistent issues. This paper shows this phenomenon and its physical mechanism, showing the difficulty of building a quantitative model forward. A data-driven method is needed to form a hybrid model to correct the data. The research object is the electricity meter data on both sides of a commercial building transformer, which comes from different power IOT systems. The low-voltage side was revised based on the high-voltage side. Compared with the correction method based on purely using neural networks, the combined method, Linear Regression (LS) + Differential Evolution (DE) + Extreme Learning Machine (ELM), further reduces the deviation from approximately 4% to 1%.}
}
@article{PATONAI2021e00203,
title = {Integrating trophic data from the literature: The Danube River food web},
journal = {Food Webs},
volume = {28},
pages = {e00203},
year = {2021},
issn = {2352-2496},
doi = {https://doi.org/10.1016/j.fooweb.2021.e00203},
url = {https://www.sciencedirect.com/science/article/pii/S2352249621000161},
author = {Katalin Patonai and Ferenc Jordán},
keywords = {Aggregation, Danube River, Food web, Incomplete data, Taxonomy},
abstract = {In the era of bioinformatics and big data, ecological research depends on large and easily accessible databases that make it possible to construct complex system models. Open-access data repositories for food webs via publications and ecological databases (e.g. EcoBase) are becoming increasingly common, yet certain ecosystem types are underrepresented (e.g. rivers). In this paper, we compile the trophic connections (predator-prey relationships) for the Danube River ecosystem as gathered from globally available literature data. Data are analyzed by Danube regions separately (Upper, Middle, Lower Danube) as well as an integrated master network version. The master version has been aggregated into larger taxonomic categories. Local and global metrics were used to analyze and compare each network. We find disparity between regions (the Middle Danube having most nodes, but still quite heterogenous), we identify the most important trophic groups, and explain ways on evaluating missing data using each aggregation stage. This data-driven approach, summarizing our presently documented knowledge, can be used for preparing preliminary models and to further refine the Danube River food web in the future.}
}
@article{MATTIOLI2022453,
title = {Information Quality: the cornerstone for AI-based Industry 4.0},
journal = {Procedia Computer Science},
volume = {201},
pages = {453-460},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.059},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004720},
author = {Juliette Mattioli and Pierre-Olivier Robic and Emeric Jesson},
keywords = {Industry 4.0, Data-driven AI, Knowledge-based AI, Data Quality, Information Quality},
abstract = {AI becomes a key enabler for Industry 4.0. Data / information quality become a real cornerstone on the overall process from user expectation to products / systems / solutions in a consistent perspective in order to ensure quality of the manufacturing production. This paper highlights some key characteristics in terms of information quality required to implement an effective AI based monitoring framework, in order to achieve operational excellence in Industry.}
}
@article{AGANY20201704,
title = {Assessment of vector-host-pathogen relationships using data mining and machine learning},
journal = {Computational and Structural Biotechnology Journal},
volume = {18},
pages = {1704-1721},
year = {2020},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2020.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S2001037020303202},
author = {Diing D.M. Agany and Jose E. Pietri and Etienne Z. Gnimpieba},
keywords = {Systems Bioscience, OMICs, Pathogenicity, Transmission, Adaptation, Data Mining, Big Data, Machine Learning, Association Mining, Host-Pathogen, Interaction, Infectious Disease, Vector-Borne Disease},
abstract = {Infectious diseases, including vector-borne diseases transmitted by arthropods, are a leading cause of morbidity and mortality worldwide. In the era of big data, addressing broad-scale, fundamental questions regarding the complex dynamics of these diseases will increasingly require the integration of diverse datasets to produce new biological knowledge. This review provides a current snapshot of the systematic assessment of the relationships between microbial pathogens, arthropod vectors and mammalian hosts using data mining and machine learning. We employ PRISMA to identify 32 key papers relevant to this topic. Our analysis shows an increasing use of data mining and machine learning tasks and techniques, including prediction, classification, clustering, association rules mining, and deep learning, over the last decade. However, it also reveals a number of critical challenges in applying these to the study of vector-host-pathogen interactions at various systems biology levels. Here, relevant studies, current limitations and future directions are discussed. Furthermore, the quality of data in relevant papers was assessed using the FAIR (Findable, Accessible, Interoperable, Reusable) compliance criteria to evaluate and encourage reproducibility and shareability of research outcomes. Although shortcomings in their application remain, data mining and machine learning have significant potential to break new ground in understanding fundamental aspects of vector-host-pathogen relationships and their application in this field should be encouraged. In particular, while predictive modeling, feature engineering and supervised machine learning are already being used in the field, other data mining and machine learning methods such as deep learning and association rules analysis lag behind and should be implemented in combination with established methods to accelerate hypothesis and knowledge generation in the domain.}
}
@article{LEE201820,
title = {Industrial Artificial Intelligence for industry 4.0-based manufacturing systems},
journal = {Manufacturing Letters},
volume = {18},
pages = {20-23},
year = {2018},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2018.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2213846318301081},
author = {Jay Lee and Hossein Davari and Jaskaran Singh and Vibhor Pandhare},
keywords = {Industrial AI, Industry 4.0, Big data, Smart manufacturing, Cyber physical systems},
abstract = {The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.}
}
@incollection{KRISHNAN202085,
title = {4 - Scientific research applications and usage},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {85-97},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00004-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000041},
author = {Krish Krishnan},
keywords = {CERN, God particle, Large electron–positron collider, Large hadron collider, Quarks, Scientific research, Standard model},
abstract = {Scientific research is one area of applications and usage of big data where we can generate lots of data in a single experiment and perform complex analytics on the same in the outcome of that experiment. The most famous example that we can talk about is the usage of all infrastructure technologies in the discovery of the “God particle” or “Higgs boson particle” which is leading us to uncover more exploration around the universe.}
}
@article{LEAL2021100172,
title = {Smart Pharmaceutical Manufacturing: Ensuring End-to-End Traceability and Data Integrity in Medicine Production},
journal = {Big Data Research},
volume = {24},
pages = {100172},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2020.100172},
url = {https://www.sciencedirect.com/science/article/pii/S221457962030040X},
author = {Fátima Leal and Adriana E. Chis and Simon Caton and Horacio González–Vélez and Juan M. García–Gómez and Marta Durá and Angel Sánchez–García and Carlos Sáez and Anthony Karageorgos and Vassilis C. Gerogiannis and Apostolos Xenakis and Efthymios Lallas and Theodoros Ntounas and Eleni Vasileiou and Georgios Mountzouris and Barbara Otti and Penelope Pucci and Rossano Papini and David Cerrai and Mariola Mier},
keywords = {ALCOA, Blockchain, Data anaytics, Data quality, Intelligent agents, Smart contracts},
abstract = {Production lines in pharmaceutical manufacturing generate numerous heterogeneous data sets from various embedded systems which control the multiple processes of medicine production. Such data sets should arguably ensure end-to-end traceability and data integrity in order to release a medicine batch, which is uniquely identified and tracked by its batch number/code. Consequently, auditable computerised systems are crucial on pharmaceutical production lines, since the industry is becoming increasingly regulated for product quality and patient health purposes. This paper describes the EU-funded SPuMoNI project, which aims to ensure the quality of large amounts of data produced by computerised production systems in representative pharmaceutical environments. Our initial results include significant progress in: (i) end-to-end verification taking advantage of blockchain properties and smart contracts to ensure data authenticity, transparency, and immutability; (ii) data quality assessment models to identify data behavioural patterns that can violate industry practices and/or international regulations; and (iii) intelligent agents to collect and manipulate data as well as perform smart decisions. By analysing multiple sensors in medicine production lines, manufacturing work centres, and quality control laboratories, our approach has been initially evaluated using representative industry-grade pharmaceutical manufacturing data sets generated at an IT environment with regulated processes inspected by regulatory and government agencies.}
}
@incollection{NASSEHI2022317,
title = {Chapter 11 - Review of machine learning technologies and artificial intelligence in modern manufacturing systems},
editor = {Dimitris Mourtzis},
booktitle = {Design and Operation of Production Networks for Mass Personalization in the Era of Cloud Technology},
publisher = {Elsevier},
pages = {317-348},
year = {2022},
isbn = {978-0-12-823657-4},
doi = {https://doi.org/10.1016/B978-0-12-823657-4.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128236574000026},
author = {Aydin Nassehi and Ray Y. Zhong and Xingyu Li and Bogdan I. Epureanu},
keywords = {Predictive maintenance, Artificial intelligence, Machine learning, Smart manufacturing, Industry 4.0},
abstract = {With the advent of new methods usually identified under the banners of artificial intelligence (AI) and machine learning (ML), statistical analysis methods of complex and uncertain manufacturing systems have been undergoing significant changes. Therefore, various definitions of AI, a brief history, and its differences with traditional statistics are presented. Moreover, ML is introduced to identify its place in data science and differences to topics such as big data analytics and manufacturing problems that use AI and ML are then characterized. Next, a lifecycle-based approach is adopted and the use of various methods in each phase is analyzed, identifying the most useful techniques and the unifying attributes of AI in manufacturing. Finally, the chapter maps out future developments of AI and the emerging trends and identifies a vision based on combining machine and human intelligence in a productive and empowering manner as well. This vision presents humans and increasingly more intelligent machines, not as competitors, but as partners allowing creative and innovative paradigms to emerge.}
}
@article{KARIM2016214,
title = {Maintenance Analytics – The New Know in Maintenance},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {28},
pages = {214-219},
year = {2016},
note = {3rd IFAC Workshop on Advanced Maintenance Engineering, Services and Technology AMEST 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.11.037},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316324612},
author = {Ramin Karim and Jesper Westerberg and Diego Galar and Uday Kumar},
keywords = {big data, maintenance analytics, eMaintenance, Knowledge discovery, maintenance decision support},
abstract = {Abstract:
Decision-making in maintenance has to be augmented to instantly understand and efficiently act, i.e. the new know. The new know in maintenance needs to focus on two aspects of knowing: 1) what can be known and 2) what must be known, in order to enable the maintenance decision-makers to take appropriate actions. Hence, the purpose of this paper is to propose a concept for knowledge discovery in maintenance with focus on Big Data and analytics. The concept is called Maintenance Analytics (MA). MA focuses in the new knowledge discovery in maintenance. MA addresses the process of discovery, understanding, and communication of maintenance data from four time-related perspectives, i.e. 1) “Maintenance Descriptive Analytics (monitoring)”; 2) “Maintenance Diagnostic Analytics”; 3) “Maintenance Predictive Analytics”; and 4) “Maintenance Prescriptive analytics”.}
}
@article{SETER201959,
title = {The data driven transport research train is leaving the station. Consultants all aboard?},
journal = {Transport Policy},
volume = {80},
pages = {59-69},
year = {2019},
issn = {0967-070X},
doi = {https://doi.org/10.1016/j.tranpol.2019.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0967070X17305589},
author = {Hanne Seter and Petter Arnesen and Odd André Hjelkrem},
abstract = {This study sets out to assess whether there is a knowledge gap between the research frontier and the consultation business in how transport data are collected, managed and analysed. The consulting business plays an important role in applying data and methods as they typically carry out public tasks in various parts of the transport system, which are becoming more and more specialised. At the same time, big data has emerged with the promise to provide new, more and better information to help understand society and execute policies more efficiently – what we refer to as the data driven transition. We conduct a literature review to identify the state of the art within international research and compare this with results from interviews and with a survey sent to representatives from the Norwegian consultation business. We find that there is a considerable gap between international researchers and the consulting business within the entire process of collection, management and analysis of traffic data, and that this gap is increasing with the emergence of the data driven transition. Finally, we argue that the results are applicable to other countries as well. Action should be taken to keep the consultants up to speed, which will require efforts from several actors, including governmental agencies, the education institutions, the consulting business and researchers.}
}
@article{HE201714946,
title = {Statistical Process Monitoring for IoT-Enabled Cybermanufacturing: Opportunities and Challenges},
journal = {IFAC-PapersOnLine},
volume = {50},
number = {1},
pages = {14946-14951},
year = {2017},
note = {20th IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2017.08.2546},
url = {https://www.sciencedirect.com/science/article/pii/S2405896317334717},
author = {Q. Peter He and Jin Wang and Devarshi Shah and Nader Vahdat},
keywords = {Cybermanufacturing, Internet of Things, sensors, statistical process monitoring, fault detection, fault diagnosis, statistics pattern analysis},
abstract = {Initiated from services and consumer products industries, there is a growing interest in using Internet of Things (IoT) technologies in various industries. In particular, IoT-enabled cybermanufacturing starts to draw increasing attention. Because IoT devices such as IoT sensors are usually much cheaper and smaller than the traditional sensors, there is a potential for instrumenting manufacturing systems with massive number of sensors. The premise is that the big data subsequently collected from IoT sensors can be utilized to advance manufacturing. Therefore, data-driven statistical process monitoring (SPM) is expected to contribute significantly to the advancement of cybermanufacturing. In this work, the state-of-the-art in cybermanufacturing is reviewed; an IoT-enabled manufacturing technology testbed (MTT) was built to explore the potential of IoT sensors for manufacturing, as well as to understand the characteristics of data produced by the IoT sensors; finally, the potentials and challenges associated with big data analytics presented by cybermanufacturing systems is discussed; and we propose statistics pattern analysis (SPA) as a promising SPM tool for cybermanufacturing.}
}
@article{LEE2020157,
title = {Machine learning for enterprises: Applications, algorithm selection, and challenges},
journal = {Business Horizons},
volume = {63},
number = {2},
pages = {157-170},
year = {2020},
note = {ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2019.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0007681319301521},
author = {In Lee and Yong Jae Shin},
keywords = {Machine learning, Artificial intelligence, Deep learning, Big data, Neural networks, Chatbot, Innovation capability, Resources and capabilities},
abstract = {Machine learning holds great promise for lowering product and service costs, speeding up business processes, and serving customers better. It is recognized as one of the most important application areas in this era of unprecedented technological development, and its adoption is gaining momentum across almost all industries. In view of this, we offer a brief discussion of categories of machine learning and then present three types of machine-learning usage at enterprises. We then discuss the trade-off between the accuracy and interpretability of machine-learning algorithms, a crucial consideration in selecting the right algorithm for the task at hand. We next outline three cases of machine-learning development in financial services. Finally, we discuss challenges all managers must confront in deploying machine-learning applications.}
}
@article{BOHNSACK2019799,
title = {What the hack? A growth hacking taxonomy and practical applications for firms},
journal = {Business Horizons},
volume = {62},
number = {6},
pages = {799-818},
year = {2019},
note = {Digital Transformation & Disruption},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2019.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007681319301247},
author = {René Bohnsack and Meike Malena Liesner},
keywords = {Growth hacking, Digital transformation, Lean startup, Digital marketing, Big data},
abstract = {As companies become increasingly digital, growth hacking emerged as a new way of scaling businesses. While the term is fashionable in business, many executives remain confused about the concept. Even if firms have an idea of what growth hacking is, they may still be puzzled as to how to do it, creating a strategy-execution gap. Our article assists firms by bridging the growth hacking strategy-execution gap. First, we provide a growth hacking framework and deconstruct its building blocks: marketing, data analysis, coding, and the lean startup philosophy. We then present a taxonomy of 34 growth hacking patterns along the customer lifecycle of acquisition, activation, revenue, retention, and referral; categorize them on the two dimensions of resource intensity and time lag; and provide an example of how to apply the taxonomy in the case of a fitness application. Finally, we discuss seven opportunities and challenges of growth hacking that firms should keep in mind.}
}
@article{EHRING2021163,
title = {SMART standards - concept for the automated transfer of standard contents into a machine-actionable form},
journal = {Procedia CIRP},
volume = {100},
pages = {163-168},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121004868},
author = {Dominik Ehring and Janosch Luttmer and Robin Pluhnau and Arun Nagarajah},
keywords = {SMART Standards, knowledge representation, automatic extraction, transfer, 3M model of Duisburg},
abstract = {Standards are - not directly visible to everyone – omnipresent in nearly every development process. In times of digitalization, where buzzwords such as "connectivity of machines", "artificial intelligence", “big data”, “cloud computing” or “smart factories” are often used, companies are still confronted with problems in handling standards throughout the entire product lifecycle. Today’s way of working with standards is characterized by manual viewing of documents, whereby a user searches for relevant information, such as formulas, and has to transfer this information to his process, method or tool. This manual process results in an increased time, loss of quality due to faulty manual transmission of information, a high adjustment effort for updates of standards and no guarantee for traceability. In order to reduce and minimize errors and needed time for work with information stored within standards, there is a need for a new form of knowledge representation for standards with sufficient data quality to ensure standard-compliant development activities. Consequently, there is a need for machine-actionable standards to ensure autonomous and efficient processes, whereby the effort for preparation is less than the benefit. The question arises how classified standards content can be represented in a machine-actionable way without loss of information. This paper shows a concept for the automatic extraction of standards content and their transfer into a machine-actionable knowledge representation. The concept, which is based on the “3M Framework of Duisburg” and thus answers questions of modularization, modeling and management, consists of six steps "extraction", "modeling", “modification”, "fusion and storage", "provision" and "application", to digitalize existing content, is presented and discussed.}
}
@article{RAJ2021103107,
title = {A survey on the role of Internet of Things for adopting and promoting Agriculture 4.0},
journal = {Journal of Network and Computer Applications},
volume = {187},
pages = {103107},
year = {2021},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103107},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521001284},
author = {Meghna Raj and Shashank Gupta and Vinay Chamola and Anubhav Elhence and Tanya Garg and Mohammed Atiquzzaman and Dusit Niyato},
abstract = {There is a rapid increase in the adoption of emerging technologies like the Internet of Things (IoT), Unmanned Aerial Vehicles (UAV), Internet of Underground Things (IoUT), Data analytics in the agriculture domain to meet the increased food demand to cater to the increasing population. Agriculture 4.0 is set to revolutionize agriculture productivity by using Precision Agriculture (PA), IoT, UAVs, IoUT, and other technologies to increase agriculture produce for growing demographics while addressing various farm-related issues. This survey provides a comprehensive overview of how multiple technologies such as IoT, UAVs, IoUT, Big Data Analytics, Deep Learning Techniques, and Machine Learning methods can be used to manage various farm-related operations. For each of these technologies, a detailed review is done on how the technology is being used in Agriculture 4.0. These discussions include an overview of relevant technologies, their use cases, existing case studies, and research works that demonstrate the use of these technologies in Agriculture 4.0. This paper also highlights the various future research gaps in the adoption of these technologies in Agriculture 4.0.}
}
@article{ABRAHAM2019424,
title = {Data governance: A conceptual framework, structured review, and research agenda},
journal = {International Journal of Information Management},
volume = {49},
pages = {424-438},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0268401219300787},
author = {Rene Abraham and Johannes Schneider and Jan {vom Brocke}},
keywords = {Data governance, Information governance, Conceptual framework, Literature review, Research agenda},
abstract = {Data governance refers to the exercise of authority and control over the management of data. The purpose of data governance is to increase the value of data and minimize data-related cost and risk. Despite data governance gaining in importance in recent years, a holistic view on data governance, which could guide both practitioners and researchers, is missing. In this review paper, we aim to close this gap and develop a conceptual framework for data governance, synthesize the literature, and provide a research agenda. We base our work on a structured literature review including 145 research papers and practitioner publications published during 2001-2019. We identify the major building blocks of data governance and decompose them along six dimensions. The paper supports future research on data governance by identifying five research areas and displaying a total of 15 research questions. Furthermore, the conceptual framework provides an overview of antecedents, scoping parameters, and governance mechanisms to assist practitioners in approaching data governance in a structured manner.}
}
@article{HAMILTON2020103926,
title = {Fast and automated sensory analysis: Using natural language processing for descriptive lexicon development},
journal = {Food Quality and Preference},
volume = {83},
pages = {103926},
year = {2020},
issn = {0950-3293},
doi = {https://doi.org/10.1016/j.foodqual.2020.103926},
url = {https://www.sciencedirect.com/science/article/pii/S0950329319308304},
author = {Leah M. Hamilton and Jacob Lahne},
keywords = {Natural language processing, Rapid descriptive methods, Big data, Whisky, Research methodology, Machine learning},
abstract = {As sensory evaluation relies upon humans accurately communicating their sensory experience, the diverse and overlapping vocabulary of flavor descriptors remains a major challenge. The lexicon generation protocols used in methods like Descriptive Analysis are expensive and time-consuming, while the post-facto analyses of natural vocabulary in “quick and dirty” methods like Free Choice or Flash Profiling require considerable subjective decision-making on the part of the analyst. A potential alternative for producing lexicons and analyzing the sensory attributes of products in nonstandardized text can be found in Natural Language Processing (NLP). NLP tools allow for the analysis of larger volumes of free text with fewer subjective decisions. This paper describes the steps necessary to automatically collect, clean, and analyze existing product descriptions from the web. As a case study, online reviews of international whiskies from two prominent websites (2309 reviews from WhiskyCast and 4289 reviews from WhiskyAdvocate) were collected, preprocessed to only retain potentially-descriptive nouns, adjectives, and verbs, and then the final term list was grouped into a flavor wheel using Correspondence Analysis and Agglomerative Hierarchical Clustering. The wheel is compared to an existing Scotch flavor wheel. The ease of collecting nonstandardized descriptions of products and the improved speed of automated methods can facilitate collection of descriptive sensory data for products where no lexicon exists. This has the potential to speed up and standardize many of the bottlenecks in rapid descriptive methods and facilitate the collection and use of very large datasets of product descriptions.}
}
@article{LU201968,
title = {Oil and Gas 4.0 era: A systematic review and outlook},
journal = {Computers in Industry},
volume = {111},
pages = {68-90},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519302064},
author = {Hongfang Lu and Lijun Guo and Mohammadamin Azimi and Kun Huang},
keywords = {Oil and Gas 4.0, Big data, Digitization, IIoT, Intelligentization},
abstract = {Recently, with the development of “Industry 4.0”, “Oil and Gas 4.0” has also been put on the agenda in the past two years. Some companies and experts believe that “Oil and Gas 4.0” can completely change the status quo of the oil and gas industry, which can bring huge benefits because it accelerates the digitization and intelligentization of the oil and gas industry. However, the “Oil and Gas 4.0” is still in its infancy. Therefore, this paper systematically introduces the concept and core technologies of “Oil and Gas 4.0”, such as big data and the industrial Internet of Things (IIoT). Moreover, this paper analyzes typical application scenarios of the oil and gas industry chain (upstream, midstream and downstream) through examples, such as intelligent oilfield, intelligent pipeline, and intelligent refinery. It is concluded that the essence of “Oil and Gas 4.0” is a data-driven intelligence system based on the highly digitization. To the best of our knowledge, this is the first academic peer-reviewed paper on the “Oil and Gas 4.0” era, aiming to let more oil and gas industry personnel understand its benefits and application scenarios, so as to better apply it to practical engineering in the future. In the discussion section, this paper also analyzes the opportunities and difficulties that may be brought about by the “Oil and Gas 4.0” era. Finally, relevant policy recommendations are proposed.}
}
@article{RIKHARDSSON201837,
title = {Business intelligence & analytics in management accounting research: Status and future focus},
journal = {International Journal of Accounting Information Systems},
volume = {29},
pages = {37-58},
year = {2018},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2018.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1467089516300616},
author = {Pall Rikhardsson and Ogan Yigitbasioglu},
keywords = {Business intelligence, Management accounting, Big data, Analytics},
abstract = {Executives see technology, data and analytics as a transforming force in business. Many organizations are therefore implementing business intelligence & analytics (BI&A) technologies to support reporting and decision-making. Traditionally, management accounting is the primary support for decision-making and control in an organization. As such, it has clear links to and can benefit from applying BI&A technologies. This indicates an interesting research area for accounting and AIS researchers. However, a review of the literature in top accounting and information systems journals indicates that to date, little research has focused on this link. This article reviews the literature, points to several research gaps and proposes a framework for studying the relationship between BI&A and management accounting.}
}
@article{QIU2020115,
title = {Research on Cost Management Optimization of Financial Sharing Center Based on RPA},
journal = {Procedia Computer Science},
volume = {166},
pages = {115-119},
year = {2020},
note = {Proceedings of the 3rd International Conference on Mechatronics and Intelligent Robotics (ICMIR-2019)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.031},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920301538},
author = {Yu Lian Qiu and Guo Fang Xiao},
keywords = {RPA, Financial shared service center, Cost management, Process optimization, Big data},
abstract = {With the development of artificial intelligence technology, the widespread application of robot process automation (RPA) in the future financial field has become an inevitable trend. Through the review of the current situation of cost management of A Group’s financial shared service center, the article deeply expounds the problems that the current cross-system data cannot be automatically collected, the cost accounting is not timely, and the cost analysis report mode is too fixed. Based on Robot Process Automation (RPA), cost management process optimization and improvement were made on the cross-system data acquisition, "Cloud Purchasing Platform" construction, and comprehensive multi-dimensional cost analysis. It is expected to provide reference for the robot process automation application of the financial shared service center.}
}
@article{CHOUVARDA201522,
title = {Connected health and integrated care: Toward new models for chronic disease management},
journal = {Maturitas},
volume = {82},
number = {1},
pages = {22-27},
year = {2015},
note = {PERSONALIZED HEALTHCARE FOR MIDLIFE AND BEYOND},
issn = {0378-5122},
doi = {https://doi.org/10.1016/j.maturitas.2015.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0378512215006052},
author = {Ioanna G. Chouvarda and Dimitrios G. Goulis and Irene Lambrinoudaki and Nicos Maglaveras},
keywords = {Connected health, Integrated care, Personal health system, Electronic health},
abstract = {The increasingly aging population in Europe and worldwide brings up the need for the restructuring of healthcare. Technological advancements in electronic health can be a driving force for new health management models, especially in chronic care. In a patient-centered e-health management model, communication and coordination between patient, healthcare professionals in primary care and hospitals can be facilitated, and medical decisions can be made timely and easily communicated. Bringing the right information to the right person at the right time is what connected health aims at, and this may set the basis for the investigation and deployment of the integrated care models. In this framework, an overview of the main technological axes and challenges around connected health technologies in chronic disease management are presented and discussed. A central concept is personal health system for the patient/citizen and three main application areas are identified. The connected health ecosystem is making progress, already shows benefits in (a) new biosensors, (b) data management, (c) data analytics, integration and feedback. Examples are illustrated in each case, while open issues and challenges for further research and development are pinpointed.}
}
@article{STEVENS201515,
title = {Sources of spatial animal and human health data: Casting the net wide to deal more effectively with increasingly complex disease problems},
journal = {Spatial and Spatio-temporal Epidemiology},
volume = {13},
pages = {15-29},
year = {2015},
issn = {1877-5845},
doi = {https://doi.org/10.1016/j.sste.2015.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1877584515000179},
author = {Kim B. Stevens and Dirk U. Pfeiffer},
keywords = {Big data, Data warehouse, Google Earth, mHealth, Spatial data, Volunteered geographic information},
abstract = {During the last 30years it has become commonplace for epidemiological studies to collect locational attributes of disease data. Although this advancement was driven largely by the introduction of handheld global positioning systems (GPS), and more recently, smartphones and tablets with built-in GPS, the collection of georeferenced disease data has moved beyond the use of handheld GPS devices and there now exist numerous sources of crowdsourced georeferenced disease data such as that available from georeferencing of Google search queries or Twitter messages. In addition, cartography has moved beyond the realm of professionals to crowdsourced mapping projects that play a crucial role in disease control and surveillance of outbreaks such as the 2014 West Africa Ebola epidemic. This paper provides a comprehensive review of a range of innovative sources of spatial animal and human health data including data warehouses, mHealth, Google Earth, volunteered geographic information and mining of internet-based big data sources such as Google and Twitter. We discuss the advantages, limitations and applications of each, and highlight studies where they have been used effectively.}
}
@article{POWELL2022100261,
title = {Garbage in garbage out: The precarious link between IoT and blockchain in food supply chains},
journal = {Journal of Industrial Information Integration},
volume = {25},
pages = {100261},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100261},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000595},
author = {Warwick Powell and Marcus Foth and Shoufeng Cao and Valéri Natanelov},
keywords = {Blockchain, Data quality, Distributed ledger technology, Meat industry, Internet of things, Food supply chains},
abstract = {The application of blockchain in food supply chains does not resolve conventional IoT data quality issues. Data on a blockchain may simply be immutable garbage. In response, this paper reports our observations and learnings from an ongoing beef supply chain project that integrates Blockchain and IoT for supply chain event tracking and beef provenance assurance and proposes two solutions for data integrity and trust in the Blockchain and IoT-enabled food supply chain. Rather than aiming for absolute truth, we explain how applying the notion of ‘common knowledge’ fundamentally changes oracle identity and data validity practices. Based on the learnings derived from leading an IoT supply chain project with a focus on beef exports from Australia to China, our findings unshackle IoT and Blockchain from being used merely to collect lag indicators of past states and liberate their potential as lead indicators of desired future states. This contributes: (a) to limit the possibility of capricious claims on IoT data performance, and; (b) to utilise mechanism design as an approach by which supply chain behaviours that increase the probability of desired future states being realised can be encouraged.}
}
@article{LIANG201887,
title = {Application and research of global grid database design based on geographic information},
journal = {Global Energy Interconnection},
volume = {1},
number = {1},
pages = {87-95},
year = {2018},
issn = {2096-5117},
doi = {https://doi.org/10.14171/j.2096-5117.gei.2018.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S2096511718300112},
author = {Xuming Liang},
keywords = {Big data collection, Geographic information, Grid database, Data mining},
abstract = {Energy crisis and climate change have become two seriously concerned issues universally. As a feasible solution, Global Energy Interconnection (GEI) has been highly praised and positively responded by the international community once proposed by China. From strategic conception to implementation, GEI development has entered a new phase of joint action now. Gathering and building a global grid database is a prerequisite for conducting research on GEI. Based on the requirement of global grid data management and application, combining with big data and geographic information technology, this paper studies the global grid data acquisition and analysis process, sorts out and designs the global grid database structure supporting GEI research, and builds a global grid database system.}
}
@article{DEY20191317,
title = {Artificial Intelligence in Cardiovascular Imaging: JACC State-of-the-Art Review},
journal = {Journal of the American College of Cardiology},
volume = {73},
number = {11},
pages = {1317-1335},
year = {2019},
issn = {0735-1097},
doi = {https://doi.org/10.1016/j.jacc.2018.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S0735109719302360},
author = {Damini Dey and Piotr J. Slomka and Paul Leeson and Dorin Comaniciu and Sirish Shrestha and Partho P. Sengupta and Thomas H. Marwick},
keywords = {artificial intelligence, cardiovascular imaging, deep learning, machine learning},
abstract = {Data science is likely to lead to major changes in cardiovascular imaging. Problems with timing, efficiency, and missed diagnoses occur at all stages of the imaging chain. The application of artificial intelligence (AI) is dependent on robust data; the application of appropriate computational approaches and tools; and validation of its clinical application to image segmentation, automated measurements, and eventually, automated diagnosis. AI may reduce cost and improve value at the stages of image acquisition, interpretation, and decision-making. Moreover, the precision now possible with cardiovascular imaging, combined with “big data” from the electronic health record and pathology, is likely to better characterize disease and personalize therapy. This review summarizes recent promising applications of AI in cardiology and cardiac imaging, which potentially add value to patient care.}
}
@incollection{WANG2018247,
title = {Chapter 8 - Real-Time Monitoring and Early Warning of a Train’s Running State and Operation Behavior},
editor = {Junfeng Wang},
booktitle = {Safety Theory and Control Technology of High-Speed Train Operation},
publisher = {Academic Press},
pages = {247-266},
year = {2018},
isbn = {978-0-12-813304-0},
doi = {https://doi.org/10.1016/B978-0-12-813304-0.00008-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128133040000086},
author = {Junfeng Wang},
keywords = {High-speed railway, big data, train running status, monitoring, early warning},
abstract = {In the view of the whole system this chapter introduces the train state monitoring and early warning, which is based on big data and combines big data theory with human and signaling systems, comprehensively considering the coordination among the TCC, CBI, CTC, and other subsystems. We analyze the factors that can affect the train state of operation systematically, including the operation action of the signaling system and the operator, realizing the real-time and online monitoring and early warning of train running state then to ensure the safety of train operation.}
}
@article{LAIFA2021981,
title = {Train delay prediction in Tunisian railway through LightGBM model},
journal = {Procedia Computer Science},
volume = {192},
pages = {981-990},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.101},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015891},
author = {Hassiba Laifa and Raoudha khcherif and Henda Hajjami {Ben Ghezalaa}},
keywords = {Delay prediction, Data Analysis, Machine learning, LightGBM},
abstract = {Train delays are one of the most important problems in the railway systems across the world, which urges the development of predictive analysis-based approaches to estimate it. In fact, with the advanced big data analysis and machine learning tools and technologies, the train delay-prediction systems can process and extract useful information from the large historical train movement data collected by the railway information system. Besides, accurate prediction of train delays can help train dispatchers make decisions through timetable rescheduling and service reliability improving. We propose, in this manuscript, a machine-learning model that captures the relationship between the arrival delay of passenger trains and the various characteristics of the railway system. We also apply, for the first time, lightGBM regressor based on optimal hyper-parameters to predict train delays. To evaluate the introduced model performance, the latter is compared with that of some other widely used existing models. Its R-squared, RMSE and RME were also compared with those of Support Vector Machine, Random Forest, XGBboost and Artificial Neural Network models. Statistical comparison indicates that the LightGBM outperforms the other models and is the fastest.}
}
@article{SAVOLAINEN202095,
title = {Organisational Constraints in Data-driven Maintenance: a case study in the automotive industry},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {95-100},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301592},
author = {P Savolainen and J Magnusson and M. Gopalakrishnan and E. {Turanoglu Bekar} and A. Skoogh},
keywords = {Maintenance Management, Decision Support, Data Quality, Data-driven Decisions, Organisational Factors, Smart Maintenance},
abstract = {Technological development and innovations has been the focus of research in the field of smart maintenance, whereas there is less research regarding how maintenance organisations adapt the development. This case study focuses to understand what constraints maintenance organisations in the transition into applying more data-driven decisions in maintenance. This paper aims to emphasize the organisational challenges in data-driven maintenance, such as trustworthiness of data-driven decisions, data quality, management and competences. Through a case study at a global company in the automotive industry these challenges are highlighted and discussed through a questionnaire survey participated by 72 people and interviews with 7 people from the maintenance organisation.}
}
@article{HURTER2014207,
title = {Interactive image-based information visualization for aircraft trajectory analysis},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {47},
pages = {207-227},
year = {2014},
note = {Special Issue: Emerging Technologies Special Issue of ICTIS 2013 – Guest Editors: Liping Fu and Ming Zhong and Special Issue: Visualization & Visual Analytics in Transportation – Guest Editors: Patricia S. Hu and Michael L. Pack},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2014.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X14000710},
author = {C. Hurter and S. Conversy and D. Gianazza and A.C. Telea},
keywords = {Trajectory manipulation, Air traffic control, Image based techniques, Information visualization},
abstract = {Objectives: The objective of the presented work is to present novel methods for big data exploration in the Air Traffic Control (ATC) domain. Data is formed by sets of airplane trajectories, or trails, which in turn records the positions of an aircraft in a given airspace at several time instants, and additional information such as flight height, speed, fuel consumption, and metadata (e.g. flight ID). Analyzing and understanding this time-dependent data poses several non-trivial challenges to information visualization. Materials and methods: To address this Big Data challenge, we present a set of novel methods to analyze aircraft trajectories with interactive image-based information visualization techniques.As a result, we address the scalability challenges in terms of data manipulation and open questions by presenting a set of related visual analysis methods that focus on decision-support in the ATC domain. All methods use image-based techniques, in order to outline the advantages of such techniques in our application context, and illustrated by means of use-cases from the ATC domain. Results: For each considered use-case, we outline the type of questions posed by domain experts, data involved in addressing these questions, and describe the specific image-based techniques we used to address these questions. Further, for each of the proposed techniques, we describe the visual representation and interaction mechanisms that have been used to address the above-mentioned goals. We illustrate these use-cases with real-life datasets from the ATC domain, and show how our techniques can help end-users in the ATC domain discover new insights, and solve problems, involving the presented datasets.}
}
@incollection{20151,
title = {Chapter One - Introduction},
editor = {Olivier Curé and Guillaume Blin},
booktitle = {RDF Database Systems},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-8},
year = {2015},
isbn = {978-0-12-799957-9},
doi = {https://doi.org/10.1016/B978-0-12-799957-9.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780127999579000018},
keywords = {RDF, Web of Data, Semantic Web, Big Data, Data management},
abstract = {This chapter motivates the importance of RDF data management through the Big Data and Web of Data/Semantic Web phenomena. It also provides some insights of existing RDF stores and presents the dimensions used in this book to compare these systems.}
}
@article{LAU2019357,
title = {A survey of data fusion in smart city applications},
journal = {Information Fusion},
volume = {52},
pages = {357-374},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519300326},
author = {Billy Pik Lik Lau and Sumudu Hasala Marakkalage and Yuren Zhou and Naveed Ul Hassan and Chau Yuen and Meng Zhang and U-Xuan Tan},
keywords = {Data fusion, Sensor fusion, Smart city, Big data, Internet of things, Multi-perspectives classification},
abstract = {The advancement of various research sectors such as Internet of Things (IoT), Machine Learning, Data Mining, Big Data, and Communication Technology has shed some light in transforming an urban city integrating the aforementioned techniques to a commonly known term - Smart City. With the emergence of smart city, plethora of data sources have been made available for wide variety of applications. The common technique for handling multiple data sources is data fusion, where it improves data output quality or extracts knowledge from the raw data. In order to cater evergrowing highly complicated applications, studies in smart city have to utilize data from various sources and evaluate their performance based on multiple aspects. To this end, we introduce a multi-perspectives classification of the data fusion to evaluate the smart city applications. Moreover, we applied the proposed multi-perspectives classification to evaluate selected applications in each domain of the smart city. We conclude the paper by discussing potential future direction and challenges of data fusion integration.}
}
@article{KOURTESIS2014307,
title = {Semantic-based QoS management in cloud systems: Current status and future challenges},
journal = {Future Generation Computer Systems},
volume = {32},
pages = {307-323},
year = {2014},
note = {Special Section: The Management of Cloud Systems, Special Section: Cyber-Physical Society and Special Section: Special Issue on Exploiting Semantic Technologies with Particularization on Linked Data over Grid and Cloud Architectures},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2013.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1300232X},
author = {Dimitrios Kourtesis and Jose María Alvarez-Rodríguez and Iraklis Paraskakis},
keywords = {Cloud systems, Quality of service, Service oriented architectures, Semantics, Ontologies, Linked data, Sensor data, Big data},
abstract = {Cloud Computing and Service Oriented Architectures have seen a dramatic increase of the amount of applications, services, management platforms, data, etc. gaining momentum for the necessity of new complex methods and techniques to deal with the vast heterogeneity of data sources or services. In this sense Quality of Service (QoS) seeks for providing an intelligent environment of self-management components based on domain knowledge in which cloud components can be optimized easing the transition to an advanced governance environment. On the other hand, semantics and ontologies have emerged to afford a common and standard data model that eases the interoperability, integration and monitoring of knowledge-based systems. Taking into account the necessity of an interoperable and intelligent system to manage QoS in cloud-based systems and the emerging application of semantics in different domains, this paper reviews the main approaches for semantic-based QoS management as well as the principal methods, techniques and standards for processing and exploiting diverse data providing advanced real-time monitoring services. A semantic-based framework for QoS management is also outlined taking advantage of semantic technologies and distributed datastream processing techniques. Finally a discussion of existing efforts and challenges is also provided to suggest future directions.}
}
@article{SCHLEICH20183,
title = {Geometrical Variations Management 4.0: towards next Generation Geometry Assurance},
journal = {Procedia CIRP},
volume = {75},
pages = {3-10},
year = {2018},
note = {The 15th CIRP Conference on Computer Aided Tolerancing, CIRP CAT 2018, 11-13 June 2018, Milan, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.04.078},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118305948},
author = {Benjamin Schleich and Kristina Wärmefjord and Rikard Söderberg and Sandro Wartzack},
keywords = {Industry 4.0, Digital Twin, Geometry Assurance},
abstract = {Product realization processes are undergoing radical change considering the increasing digitalization of manufacturing fostered by cyber-physical production systems, the internet of things, big data, cloud computing, and the advancing use of digital twins. These trends are subsumed under the term “industry 4.0” describing the vision of a digitally connected manufacturing environment. The contribution gives an overview of future challenges and potentials for next generation geometry assurance and geometrical variations management in the context of industry 4.0. Particularly, the focus is set on potentials and risks of increasingly available manufacturing data and the use of digital twins in geometrical variations management.}
}
@article{LOPEZROBLES201922,
title = {30 years of intelligence models in management and business: A bibliometric review},
journal = {International Journal of Information Management},
volume = {48},
pages = {22-38},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S026840121730244X},
author = {J.R. López-Robles and J.R. Otegi-Olaso and I. {Porto Gómez} and M.J. Cobo},
keywords = {Business Intelligence, Competitive Intelligence, Strategic Intelligence, Science information management, Mapping analysis},
abstract = {The critical factors in the big data era are collection, analysis, and dissemination of information to improve an organization’s competitive position and enhance its products and services. In this scenario, it is imperative that organizations use Intelligence, which is understood as a process of gathering, analyzing, interpreting, and disseminating high-value data and information at the right time for use in the decision-making process. Earlier, the concept of Intelligence was associated with the military and national security sector; however, in present times, and as organizations evolve, Intelligence has been defined in several ways for the purposes of different applications. Given that the purpose of Intelligence is to obtain real value from data, information, and the dynamism of the organizations, the study of this discipline provides an opportunity to analyze the core trends related to data collection and processing, information management, decision-making process, and organizational capabilities. Therefore, the present study makes a conceptual analysis of the existing definitions of intelligence in the literature by quantifying the main bibliometric performance indicators, identifying the main authors and research areas, and evaluating the development of the field using SciMAT as a bibliometric analysis software.}
}
@article{ROJO2019160,
title = {Near-ground effect of height on pollen exposure},
journal = {Environmental Research},
volume = {174},
pages = {160-169},
year = {2019},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2019.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S0013935119302439},
author = {Jesús Rojo and Jose Oteros and Rosa Pérez-Badia and Patricia Cervigón and Zuzana Ferencova and A. Monserrat Gutiérrez-Bustillo and Karl-Christian Bergmann and Gilles Oliver and Michel Thibaudon and Roberto Albertini and David {Rodríguez-De la Cruz} and Estefanía Sánchez-Reyes and José Sánchez-Sánchez and Anna-Mari Pessi and Jukka Reiniharju and Annika Saarto and M. Carmen Calderón and César Guerrero and Daniele Berra and Maira Bonini and Elena Chiodini and Delia Fernández-González and José García and M. Mar Trigo and Dorota Myszkowska and Santiago Fernández-Rodríguez and Rafael Tormo-Molina and Athanasios Damialis and Franziska Kolek and Claudia Traidl-Hoffmann and Elena Severova and Elsa Caeiro and Helena Ribeiro and Donát Magyar and László Makra and Orsolya Udvardy and Purificación Alcázar and Carmen Galán and Katarzyna Borycka and Idalia Kasprzyk and Ed Newbigin and Beverley Adams-Groom and Godfrey P. Apangu and Carl A. Frisk and Carsten A. Skjøth and Predrag Radišić and Branko Šikoparija and Sevcan Celenk and Carsten B. Schmidt-Weber and Jeroen Buters},
keywords = {Height, Pollen, Aerobiology, Monitoring network, Big data},
abstract = {The effect of height on pollen concentration is not well documented and little is known about the near-ground vertical profile of airborne pollen. This is important as most measuring stations are on roofs, but patient exposure is at ground level. Our study used a big data approach to estimate the near-ground vertical profile of pollen concentrations based on a global study of paired stations located at different heights. We analyzed paired sampling stations located at different heights between 1.5 and 50 m above ground level (AGL). This provided pollen data from 59 Hirst-type volumetric traps from 25 different areas, mainly in Europe, but also covering North America and Australia, resulting in about 2,000,000 daily pollen concentrations analyzed. The daily ratio of the amounts of pollen from different heights per location was used, and the values of the lower station were divided by the higher station. The lower station of paired traps recorded more pollen than the higher trap. However, while the effect of height on pollen concentration was clear, it was also limited (average ratio 1.3, range 0.7–2.2). The standard deviation of the pollen ratio was highly variable when the lower station was located close to the ground level (below 10 m AGL). We show that pollen concentrations measured at >10 m are representative for background near-ground levels.}
}
@article{MIA2022100238,
title = {A privacy-preserving National Clinical Data Warehouse: Architecture and analysis},
journal = {Smart Health},
volume = {23},
pages = {100238},
year = {2022},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2021.100238},
url = {https://www.sciencedirect.com/science/article/pii/S2352648321000544},
author = {Md Raihan Mia and Abu Sayed Md Latiful Hoque and Shahidul Islam Khan and Sheikh Iqbal Ahamed},
keywords = {Privacy, Standardization, Clinical data warehousing, Data modeling, Big data analytics, Decision supports},
abstract = {A centralized clinical data repository is essential for inspecting patients’ medical history, disease analysis, population-wide disease research, treatment decision support, and improving existing healthcare policies and services. Bangladesh, a rapidly developing country, poses several unusual challenges for developing such a centralized clinical data repository as the existing Electronic Health Records (EHR) are stored in unconnected, heterogeneous sources with no unique patient identifier and consistency. Data integration with secure record linkage, privacy preservation, quality control, and data standardization are the main challenges for developing a consistent and interoperable centralized clinical data repository. Based on the findings from our previous researches, we have designed an anonymous National Clinical Data Warehouse (NCDW) framework to reinforce research and analysis. The architecture of NCDW is divided into five stages to overcome the challenges: (1) Wrapper-based anonymous data acquisition; (2) Data loading and staging; (3) Transformation, standardization, and uploading to the data warehouse; (4) Management and monitoring; (5) Data Mart design, OLAP server, data mining, and applications. A prototype of NCDW has been developed with a complete pipeline from data collection to analytics by integrating three data sources. The proposed NCDW model facilitates regional and national decision support, intelligent disease analysis, knowledge discovery, and data-driven research. We have inspected the analytical efficacy of the framework by qualitative evaluation of the national decision support from two derived disease data marts. The experimental result based on the analysis is satisfactory to extend the NCDW on a large scale.}
}
@article{NYOMANKUTHAKRISNAWIJAYA2022106813,
title = {Data analytics platforms for agricultural systems: A systematic literature review},
journal = {Computers and Electronics in Agriculture},
volume = {195},
pages = {106813},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.106813},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922001302},
author = {Ngakan {Nyoman Kutha Krisnawijaya} and Bedir Tekinerdogan and Cagatay Catal and Rik van der Tol},
keywords = {Data analytics platforms, Agriculture, Systematic literature review, Big Data},
abstract = {With the rapid developments in ICT, the current agriculture businesses have become increasingly data-driven and are supported by advanced data analytics techniques. In this context, several studies have investigated the adopted data analytics platforms in the agricultural sector. However, the main characteristics and overall findings on these platforms are scattered over the various studies, and to the best of our knowledge, there has been no attempt yet to systematically synthesize the features and obstacles of the adopted data analytics platforms. This article presents the results of an in-depth systematic literature review (SLR) that has explicitly focused on the domains of the platforms, the stakeholders, the objectives, the adopted technologies, the data properties and the obstacles. According to the year-wise analysis, it is found that no relevant primary study between 2010 and 2013 was found. This implies that the research of data analytics in agricultural sectors is a popular topic from recent years, so the results from before 2010 are likely less relevant. In total, 535 papers published from 2010 to 2020 were retrieved using both automatic and manual search strategies, among which 45 journal articles were selected for further analysis. From these primary studies, 33 features and 34 different obstacles were identified. The identified features and obstacles help characterize the different data analytics platforms and pave the way for further research.}
}
@article{FAIEQ2017151,
title = {C2IoT: A framework for Cloud-based Context-aware Internet of Things services for smart cities},
journal = {Procedia Computer Science},
volume = {110},
pages = {151-158},
year = {2017},
note = {14th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2017) / 12th International Conference on Future Networks and Communications (FNC 2017) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.06.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917312486},
author = {Soufiane Faieq and Rajaa Saidi and Hamid Elghazi and Moulay Driss Rahmani},
keywords = {Internet of Things, Cloud Computing, Context-Awareness, Big Data, Service Composition, Smart City},
abstract = {The smart city vision was born by the integration of ICT in the day to day city management operations and citizens lives, owing to the need for novel and smart ways to manage the cities resources; making them more efficient, sustainable and transparent. However, the understanding of the crucial elements to this integration and how they can benefit from each other proves difficult and unclear. In this article, we investigate the intricate synergies between different technologies and paradigms involved in the smart city vision, to help design a robust framework, capable of handling the challenges impeding its successful implementation. To this end, we propose a context-aware centered approach to present a holistic view of a smart city as viewed from the different angles (Cloud, IoT, Big Data). We also propose a framework encompassing elements from the different enablers, leveraging their strengths to build and develop smart-x applications and services.}
}
@article{LI20191234,
title = {Internet of Things to network smart devices for ecosystem monitoring},
journal = {Science Bulletin},
volume = {64},
number = {17},
pages = {1234-1245},
year = {2019},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2019.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095927319304013},
author = {Xin Li and Ning Zhao and Rui Jin and Shaomin Liu and Xiaomin Sun and Xuefa Wen and Dongxiu Wu and Yan Zhou and Jianwen Guo and Shiping Chen and Ziwei Xu and Mingguo Ma and Tianming Wang and Yonghua Qu and Xinwei Wang and Fangming Wu and Yuke Zhou},
keywords = {Ecosystem monitoring, Fragile ecosystem, Internet of Things, Wireless sensor network, Smart device},
abstract = {Smart, real-time, low-cost, and distributed ecosystem monitoring is essential for understanding and managing rapidly changing ecosystems. However, new techniques in the big data era have rarely been introduced into operational ecosystem monitoring, particularly for fragile ecosystems in remote areas. We introduce the Internet of Things (IoT) techniques to establish a prototype ecosystem monitoring system by developing innovative smart devices and using IoT technologies for ecosystem monitoring in isolated environments. The developed smart devices include four categories: large-scale and nonintrusive instruments to measure evapotranspiration and soil moisture, in situ observing systems for CO2 and δ13C associated with soil respiration, portable and distributed devices for monitoring vegetation variables, and Bi-CMOS cameras and pressure trigger sensors for terrestrial vertebrate monitoring. These new devices outperform conventional devices and are connected to each other via wireless communication networks. The breakthroughs in the ecosystem monitoring IoT include new data loggers and long-distance wireless sensor network technology that supports the rapid transmission of data from devices to wireless networks. The applicability of this ecosystem monitoring IoT is verified in three fragile ecosystems, including a karst rocky desertification area, the National Park for Amur Tigers, and the oasis-desert ecotone in China. By integrating these devices and technologies with an ecosystem monitoring information system, a seamless data acquisition, transmission, processing, and application IoT is created. The establishment of this ecosystem monitoring IoT will serve as a new paradigm for ecosystem monitoring and therefore provide a platform for ecosystem management and decision making in the era of big data.}
}
@article{CHEN2020104344,
title = {Robust Bayesian networks for low-quality data modeling and process monitoring applications},
journal = {Control Engineering Practice},
volume = {97},
pages = {104344},
year = {2020},
issn = {0967-0661},
doi = {https://doi.org/10.1016/j.conengprac.2020.104344},
url = {https://www.sciencedirect.com/science/article/pii/S0967066120300289},
author = {Guangjie Chen and Zhiqiang Ge},
keywords = {Robust Bayesian network, Data quality feature, Process monitoring, Fault diagnosis},
abstract = {In this paper, a novel robust Bayesian network is proposed for process modeling with low-quality data. Since unreliable data can cause model parameters to deviate from the real distributions and make network structures unable to characterize the true causalities, data quality feature is utilized to improve the process modeling and monitoring performance. With a predetermined trustworthy center, the data quality measurement results can be evaluated through an exponential function with Mahalanobis distances. The conventional Bayesian network learning algorithms including structure learning and parameter learning are modified by the quality feature in a weighting form, intending to extract useful information and make a reasonable model. The effectiveness of the proposed method is demonstrated through TE benchmark process and a real industrial process.}
}
@article{VIAL2019113133,
title = {Reflections on quality requirements for digital trace data in IS research},
journal = {Decision Support Systems},
volume = {126},
pages = {113133},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113133},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301629},
author = {Gregory Vial},
keywords = {Digital trace data, Data quality, GitHub},
abstract = {In recent years an increasing number of academic disciplines, including IS, have sourced digital trace data for their research. Notwithstanding the potential of such data in (re)investigations of various phenomena of interest that would otherwise be difficult or impossible to study using other sources of data, we view the quality of digital trace data as an underappreciated issue in IS research. To initiate a discussion of how to evaluate and report on the quality of digital trace data in IS research, we couch our arguments within the broader tradition of research on data quality. We explain how the uncontrolled nature of digital trace data creates unique challenges for IS researchers, who need to collect, store, retrieve, and transform those data for the purpose of numerical analysis. We then draw parallels with concepts and patterns commonly used in data analysis projects and argue that, although IS researchers probably apply such concepts and patterns, this is not reported in publications, undermining the reader's ability to assess the reliability, statistical power and replicability of the findings. Using the case of GitHub to illustrate such challenges, we develop a preliminary set of guidelines to help researchers consider and report on the quality of the digital trace data they use in their research. Our work contributes to the debate on data quality and provides relevant recommendations for scholars and IS journals at a time when a growing number of publications are relying on digital trace data.}
}
@article{NABATI2017160,
title = {Data Driven Decision Making in Planning the Maintenance Activities of Off-shore Wind Energy},
journal = {Procedia CIRP},
volume = {59},
pages = {160-165},
year = {2017},
note = {Proceedings of the 5th International Conference in Through-life Engineering Services Cranfield University, 1st and 2nd November 2016},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S221282711630960X},
author = {Elaheh Gholamzadeh Nabati and Klaus-Dieter Thoben},
keywords = {Maintenance, (Big) data analysis, Off-shore wind turbines, Decision making},
abstract = {Planning and scheduling for wind farms play a critical role in the costs of maintenance. The use and analysis of field data or so-called Product Use Information (PUI) to improve maintenance activities and to reduce the costs has gained attention in the recent years. The product use data consist of sources such as measure of sensors on the turbines, the alarms information or signals from the condition monitoring, Supervisory Control and Data Acquisition (SCADA) systems, which are currently used in maintenance activities. However, those data have the potential to offer alternative solutions to improve processes and provide better decisions, by transforming them into actionable knowledge. In order to make the right decision it is important to understand, which PUI data source and which data analysis methods, are suitable for what kind of decision making task. The aim of this study is to discover, how analysis of PUI can help in the maintenance processes of off-shore wind power. The techniques from the field of big data analytics for analyzing the PUI are here addressed. The results of this study contain suggestions on the basis of algorithms of data analytics, suitable for each decision type.}
}
@article{ZARKOWSKY2021260,
title = {Artificial intelligence's role in vascular surgery decision-making},
journal = {Seminars in Vascular Surgery},
volume = {34},
number = {4},
pages = {260-267},
year = {2021},
issn = {0895-7967},
doi = {https://doi.org/10.1053/j.semvascsurg.2021.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0895796721000624},
author = {Devin S. Zarkowsky and David P. Stonko},
abstract = {ABSTRACT
Artificial intelligence (AI) is the next great advance informing medical science. Several disciplines, including vascular surgery, use AI-based decision-making tools to improve clinical performance. Although applied widely, AI functions best when confronted with voluminous, accurate data. Consistent, predictable analytic technique selection also challenges researchers. This article contextualizes AI analyses within evidence-based medicine, focusing on “big data” and health services research, as well as discussing opportunities to improve data collection and realize AI's promise.}
}
@article{AMUTHABALA2019233,
title = {Robust analysis and optimization of a novel efficient quality assurance model in data warehousing},
journal = {Computers & Electrical Engineering},
volume = {74},
pages = {233-244},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0045790618318470},
author = {P. Amuthabala and R. Santhosh},
keywords = {Data warehouse, Distributed, Data complexity, Data quality, Quality assurance, Optimization, Machine learning},
abstract = {The significance of distributed data warehouses is to initiate the proliferation of various analytical applications. However, with the increase of ubiquitous devices, it is likely that massive volumes of data will be generated, which poses further problems based on the degradation of data quality. The practical reasons for the degradation of data quality in distributed warehouses are identified as heterogeneous data, uncertain inferior data which further affect predictions. The proposed system presents an integrated optimization model to address all the quality degradation problems and to provide a better computational model which effectively incorporates a higher degree of quality assurance. An analytical methodology is adopted in order to develop the proposed quality assurance model for distributed data warehouses.}
}
@article{EHWERHEMUEPHA2022108120,
title = {Cerner real-world data (CRWD) - A de-identified multicenter electronic health records database},
journal = {Data in Brief},
volume = {42},
pages = {108120},
year = {2022},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2022.108120},
url = {https://www.sciencedirect.com/science/article/pii/S2352340922003304},
author = {Louis Ehwerhemuepha and Kimberly Carlson and Ryan Moog and Ben Bondurant and Cheryl Akridge and Tatiana Moreno and Gary Gasperino and William Feaster},
keywords = {Cerner Real-World Data(CRWD), COVID-19, SARS-CoV-2, Electronic Health Records (EHR), HealtheIntent, HealtheDataLab™, Cerner learning Health Network (LHN)},
abstract = {Cerner Real-World DataTM (CRWD) is a de-identified big data source of multicenter electronic health records. Cerner Corporation secured appropriate data use agreements and permissions from more than 100 health systems in the United States contributing to the database as of March 2022. A subset of the database was extracted to include data from only patients with SARS-CoV-2 infections and is referred to as the Cerner COVID-19 Dataset. The December 2021 version of CRWD consists of 100 million patients and 1.5 billion encounters across all care settings. There are 2.3 billion, 2.9 billion, 486 million, and 11.5 billion records in the condition, medication, procedure, and lab (laboratory test) tables respectively. The 2021 Q3 COVID-19 Dataset consists of 130.1 million encounters from 3.8 million patients. The size and longitudinal nature of CRWD can be leveraged for advanced analytics and artificial intelligence in medical research across all specialties and is a rich source of novel discoveries on a wide range of conditions including but not limited to COVID-19.}
}
@article{HARRISON2020102672,
title = {New and emerging data forms in transportation planning and policy: Opportunities and challenges for “Track and Trace” data},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {117},
pages = {102672},
year = {2020},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2020.102672},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X20305878},
author = {Gillian Harrison and Susan M. Grant-Muller and Frances C. Hodgson},
keywords = {Transport policy, Track and Trace, Mobile phone data, Mobility profile, Big Data},
abstract = {High quality, reliable data and robust models are central to the development and appraisal of transportation planning and policy. Although conventional data may offer good ‘content’, it is widely observed that it lacks context i.e. who and why people are travelling. Transportation modelling has developed within these boundaries, with implications for the planning, design and management of transportation systems and policy-making. This paper establishes the potential of passively collected GPS-based “Track & Trace” (T&T) datasets of individual mobility profiles towards enhancing transportation modelling and policy-making. T&T is a type of New and Emerging Data Form (NEDF), lying within the broader ‘Big Data’ paradigm, and is typically collected using mobile phone sensors and related technologies. These capture highly grained mobility content and can be linked to the phone owner/user behavioural choices and other individual context. Our meta-analysis of existing literature related to spatio-temporal mobile phone data demonstrates that NEDF’s, and in particular T&T data, have had little mention to date within an applied transportation planning and policy context. We thus establish there is an opportunity for policy-makers, transportation modellers, researchers and a wide range of stakeholders to collaborate in developing new analytic approaches, revise existing models and build the skills and related capacity needed to lever greatest value from the data, as well as to adopt new business models that could revolutionise citizen participation in policy-making. This is of particular importance due to the growing awareness in many countries for a need to develop and monitor efficient cross-sectoral policies to deliver sustainable communities.}
}
@article{MICHAILIDOU2022101953,
title = {EQUALITY: Quality-aware intensive analytics on the edge},
journal = {Information Systems},
volume = {105},
pages = {101953},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101953},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001496},
author = {Anna-Valentini Michailidou and Anastasios Gounaris and Moysis Symeonides and Demetris Trihinas},
keywords = {Fog computing, Optimization, Sensors, Data quality},
abstract = {Our work is motivated by the fact that there is an increasing need to perform complex analytics jobs over streaming data as close to the edge devices as possible and, in parallel, it is important that data quality is considered as an optimization objective along with performance metrics. In this work, we develop a solution that trades latency for an increased fraction of incoming data, for which data quality-related measurements and operations are performed, in jobs running over geo-distributed heterogeneous and constrained resources. Our solution is hybrid: on the one hand, we perform search heuristics over locally optimal partial solutions to yield an enhanced global solution regarding task allocations; on the other hand, we employ a spring relaxation algorithm to avoid unnecessarily increased degree of partitioned parallelism. Through thorough experiments, we show that we can improve upon state-of-the-art solutions in terms of our objective function that combines latency and extent of quality checks by up to 2.56X. Moreover, we implement our solution within Apache Storm, and we perform experiments in an emulated setting. The results show that we can reduce the latency in 86.9% of the cases examined, while latency is up to 8 times lower compared to the built-in Storm scheduler, with the average latency reduction being 52.5%.}
}
@incollection{BOREK201423,
title = {Chapter 2 - Enterprise Information Management},
editor = {Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall},
booktitle = {Total Information Risk Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {23-38},
year = {2014},
isbn = {978-0-12-405547-6},
doi = {https://doi.org/10.1016/B978-0-12-405547-6.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012405547600002X},
author = {Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall},
keywords = {EIM Strategy, EIM Governance, EIM Components, Big Data and EIM, Challenges for EIM},
abstract = {This chapter gives an introduction to concept of enterprise information management, investigates the influence of Big Data on EIM, and discusses today's key challenges and pressures for EIM.}
}
@article{VILLAHENRIKSEN202060,
title = {Internet of Things in arable farming: Implementation, applications, challenges and potential},
journal = {Biosystems Engineering},
volume = {191},
pages = {60-84},
year = {2020},
issn = {1537-5110},
doi = {https://doi.org/10.1016/j.biosystemseng.2019.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1537511020300039},
author = {Andrés Villa-Henriksen and Gareth T.C. Edwards and Liisa A. Pesonen and Ole Green and Claus Aage Grøn Sørensen},
keywords = {Smart farming, Internet of things, Wireless sensor network, Farm management information system, Big data, Machine learning},
abstract = {The Internet of Things is allowing agriculture, here specifically arable farming, to become data-driven, leading to more timely and cost-effective production and management of farms, and at the same time reducing their environmental impact. This review is addressing an analytical survey of the current and potential application of Internet of Things in arable farming, where spatial data, highly varying environments, task diversity and mobile devices pose unique challenges to be overcome compared to other agricultural systems. The review contributes an overview of the state of the art of technologies deployed. It provides an outline of the current and potential applications, and discusses the challenges and possible solutions and implementations. Lastly, it presents some future directions for the Internet of Things in arable farming. Current issues such as smart phones, intelligent management of Wireless Sensor Networks, middleware platforms, integrated Farm Management Information Systems across the supply chain, or autonomous vehicles and robotics stand out because of their potential to lead arable farming to smart arable farming. During the implementation, different challenges are encountered, and here interoperability is a key major hurdle throughout all the layers in the architecture of an Internet of Things system, which can be addressed by shared standards and protocols. Challenges such as affordability, device power consumption, network latency, Big Data analysis, data privacy and security, among others, have been identified by the articles reviewed and are discussed in detail. Different solutions to all identified challenges are presented addressing technologies such as machine learning, middleware platforms, or intelligent data management.}
}
@article{LIOUTAS2021103023,
title = {Enhancing the ability of agriculture to cope with major crises or disasters: What the experience of COVID-19 teaches us},
journal = {Agricultural Systems},
volume = {187},
pages = {103023},
year = {2021},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2020.103023},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X20308842},
author = {Evagelos D. Lioutas and Chrysanthi Charatsari},
keywords = {Agriculture, COVID-19, Major crises, Smart technology, Community marketing, Resilience},
abstract = {The COVID-19 outbreak was an unprecedented situation that uncovered forgotten interconnections and interdependencies between agriculture, society, and economy, whereas it also brought to the fore the vulnerability of agrifood production to external disturbances. Building upon the ongoing experience of the COVID-19 pandemic, in this short communication, we discuss three potential mechanisms that, in our opinion, can mitigate the impacts of major crises or disasters in agriculture: resilience-promoting policies, community marketing schemes, and smart farming technology. We argue that resilience-promoting policies should focus on the development of crisis management plans and enhance farmers' capacity to cope with external disturbances. We also stress the need to promote community marketing conduits that ensure an income floor for farmers while in parallel facilitating consumer access to agrifood products when mainstream distribution channels under-serve them. Finally, we discuss some issues that need to be solved to ensure that smart technology and big data can help farmers overcome external shocks.}
}
@article{ANDRADE2019102352,
title = {Cognitive security: A comprehensive study of cognitive science in cybersecurity},
journal = {Journal of Information Security and Applications},
volume = {48},
pages = {102352},
year = {2019},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2019.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S2214212618307804},
author = {Roberto O Andrade and Sang Guun Yoo},
keywords = {Cognitive security, Cognitive science, Situation awareness, Cyber operations},
abstract = {Nowadays, IoT, cloud computing, mobile and social networks are generating a transformation in social processes. Nevertheless, this technological change rise to new threats and security attacks that produce new and complex cybersecurity scenarios with large volumes of data and different attack vectors that can exceeded the cognitive skills of security analysts. In this context, cognitive sciences can enhance the cognitive processes, which can help to security analysts to establish actions in less time and more efficiently within cybersecurity operations. This works presents a cognitive security model that integrates technological solutions such as Big Data, Machine Learning, and Support Decision Systems with the cognitive processes of security analysts used to generate knowledge, understanding and execution of security response actions. The model considers alternatives to establish the automation process in the execution of cognitive tasks defined in the cyber operations processes and includes the analyst as the central axis in the processes of validation and decision making through the use of MAPE-K, OODA and Human in the Loop.}
}
@article{OMRI202023,
title = {Industrial data management strategy towards an SME-oriented PHM},
journal = {Journal of Manufacturing Systems},
volume = {56},
pages = {23-36},
year = {2020},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520300467},
author = {N. Omri and Z. {Al Masry} and N. Mairot and S. Giampiccolo and N. Zerhouni},
keywords = {Small and medium-sized enterprises, Data-driven PHM, Industrial data management, Data quality metrics, PHM implementation strategy},
abstract = {The fourth industrial revolution is derived from advances in digitization and prognostic and health management (PHM) disciplines to make plants smarter and more efficient. However, an adapted approach for data-driven PHM process implementation in small and medium-sized enterprises (SMEs) has not been yet discussed. This research gap is due to the specificities of SMEs and the lack of documentation. In this paper, we examine existing standards for implementing PHM in the industrial field and discuss the limitations within SMEs. Based on that, a novel strategy to implement a data-driven PHM approach in SMEs is proposed. Accordingly, the data management process and the impact of data quality are reviewed to address some critical data problems in SMEs (e.g., data volume and data accuracy). A first set of simulations was carried out to study the impact of the data volume and percentage of missing data on classification problems in PHM. A general model of the evolution of the results accuracy in function of data volume and missing data is then generated, and an economic data volume notion is proposed for data infrastructure resizing. The proposed strategy and the developed models are then applied to the Scoder enterprise, which is a French SME. The feedback on the first results of this application is reported and discussed.}
}
@article{TSENG2021108244,
title = {Smart product service system hierarchical model in banking industry under uncertainties},
journal = {International Journal of Production Economics},
volume = {240},
pages = {108244},
year = {2021},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2021.108244},
url = {https://www.sciencedirect.com/science/article/pii/S0925527321002206},
author = {Ming-Lang Tseng and Tat-Dat Bui and Shulin Lan and Ming K. Lim and Abu Hashan Md Mashud},
keywords = {Smart product-service systems, Digital technology, Sustainable innovation, Fuzzy delphi method, Decision-making trial and evaluation laboratory (DEMATEL), Diffusion of innovation theory},
abstract = {This study adopts the diffusion of innovation theory as to develop the smart product service system model in banking industry due to prior studies are lacking in identifying the attributes. The smart product service system functions are bearing high uncertainty and system complexity; hence, the hybrid method of fuzzy Delphi method and fuzzy decision-making trial and evaluation laboratory to construct a valid hierarchical model and identified the causal interrelationships among the attributes. The smart product service system hierarchical model with eight aspects and 41 criteria are proposed enriching the existing literature and that identify appropriate strategies to achieve operational performance. The results show that seven aspects and 22 criteria are determined as the valid hierarchical model. The institutional compression, digital platform operation, and e-knowledge management are the causing aspects helps to form smart product service system operational performance in high uncertainty. For practices, the banking decision-makers should develop innovative actions relied on the forcible compression, cyber-physical systems, industrial big data, cloud service allocation and sharing, and transparency improvement as they are most importance criteria playing a decisive role in a successful SPSS. This provides guidelines for banking industry practice in Taiwan encouraging the miscellany of digital technology accomplishment for sustainable target.}
}
@article{SHARMA2018103,
title = {Big GIS analytics framework for agriculture supply chains: A literature review identifying the current trends and future perspectives},
journal = {Computers and Electronics in Agriculture},
volume = {155},
pages = {103-120},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918311311},
author = {Rohit Sharma and Sachin S. Kamble and Angappa Gunasekaran},
keywords = {Agriculture supply chain, GIS analytics, Big data analytics, Internet of things, Drones, Smart farming},
abstract = {The world population is estimated to reach nine billion by 2050. Many challenges are adding pressure on the current agriculture supply chains that include shrinking land sizes, ever increasing demand for natural resources and environmental issues. The agriculture systems need a major transformation from the traditional practices to precision agriculture or smart farming practices to overcome these challenges. Geographic information system (GIS) is one such technology that pushes the current methods to precision agriculture. In this paper, we present a systematic literature review (SLR) of 120 research papers on various applications of big GIS analytics (BGA) in agriculture. The selected papers are classified into two broad categories; the level of analytics and GIS applications in agriculture. The GIS applications viz., land suitability, site search and selection, resource allocation, impact assessment, land allocation, and knowledge-based systems are considered in this study. The outcome of this study is a proposed BGA framework for agriculture supply chain. This framework identifies big data analytics to play a significant role in improving the quality of GIS application in agriculture and provides the researchers, practitioners, and policymakers with guidelines on the successful management of big GIS data for improved agricultural productivity.}
}
@incollection{BRAVOMERODIO2021191,
title = {Chapter Four - Translational biomarkers in the era of precision medicine},
editor = {Gregory S. Makowski},
series = {Advances in Clinical Chemistry},
publisher = {Elsevier},
volume = {102},
pages = {191-232},
year = {2021},
issn = {0065-2423},
doi = {https://doi.org/10.1016/bs.acc.2020.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065242320300913},
author = {Laura Bravo-Merodio and Animesh Acharjee and Dominic Russ and Vartika Bisht and John A. Williams and Loukia G. Tsaprouni and Georgios V. Gkoutos},
keywords = {Translational biomarkers, Omics, Big data, Artificial intelligence, Clinical trials},
abstract = {In this chapter we discuss the past, present and future of clinical biomarker development. We explore the advent of new technologies, paving the way in which health, medicine and disease is understood. This review includes the identification of physicochemical assays, current regulations, the development and reproducibility of clinical trials, as well as, the revolution of omics technologies and state-of-the-art integration and analysis approaches.}
}
@article{KRISTOFFERSEN2021120957,
title = {Towards a business analytics capability for the circular economy},
journal = {Technological Forecasting and Social Change},
volume = {171},
pages = {120957},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120957},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521003899},
author = {Eivind Kristoffersen and Patrick Mikalef and Fenna Blomsma and Jingyue Li},
keywords = {Digital circular economy, Sustainability, Big data analytics, Competitive advantage, Resource-based view, Expert interviews},
abstract = {Digital technologies are growing in importance for accelerating firms’ circular economy transition. However, so far, the focus has primarily been on the technical aspects of implementing these technologies with limited research on the organizational resources and capabilities required for successfully leveraging digital technologies for circular economy. To address this gap, this paper explores the business analytics resources firms should develop and how these should be orchestrated towards a firm-wide capability. The paper proposes a conceptual model highlighting eight business analytics resources that, in combination, build a business analytics capability for the circular economy and how this relates to firms’ circular economy implementation, resource orchestration capability, and competitive performance. The model is based on the results of a thematic analysis of 15 semi-structured expert interviews with key positions in industry. Our approach is informed by and further develops, the theory of the resource-based view and the resource orchestration view. Based on the results, we develop a deeper understanding of the importance of taking a holistic approach to business analytics when leveraging data and analytics towards a more efficient and effective digital-enabled circular economy, the smart circular economy.}
}
@article{MONTANS2019845,
title = {Data-driven modeling and learning in science and engineering},
journal = {Comptes Rendus Mécanique},
volume = {347},
number = {11},
pages = {845-855},
year = {2019},
note = {Data-Based Engineering Science and Technology},
issn = {1631-0721},
doi = {https://doi.org/10.1016/j.crme.2019.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1631072119301809},
author = {Francisco J. Montáns and Francisco Chinesta and Rafael Gómez-Bombarelli and J. Nathan Kutz},
keywords = {Data-driven science, Data-driven modeling, Artificial intelligence, Machine learning, Data-science, Big data},
abstract = {In the past, data in which science and engineering is based, was scarce and frequently obtained by experiments proposed to verify a given hypothesis. Each experiment was able to yield only very limited data. Today, data is abundant and abundantly collected in each single experiment at a very small cost. Data-driven modeling and scientific discovery is a change of paradigm on how many problems, both in science and engineering, are addressed. Some scientific fields have been using artificial intelligence for some time due to the inherent difficulty in obtaining laws and equations to describe some phenomena. However, today data-driven approaches are also flooding fields like mechanics and materials science, where the traditional approach seemed to be highly satisfactory. In this paper we review the application of data-driven modeling and model learning procedures to different fields in science and engineering.}
}
@article{RUMSON2019598,
title = {Innovations in the use of data facilitating insurance as a resilience mechanism for coastal flood risk},
journal = {Science of The Total Environment},
volume = {661},
pages = {598-612},
year = {2019},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2019.01.114},
url = {https://www.sciencedirect.com/science/article/pii/S0048969719301317},
author = {Alexander G. Rumson and Stephen H. Hallett},
keywords = {Risk analytics, Adaptation, Remote sensing, Big Data},
abstract = {Insurance plays a crucial role in human efforts to adapt to environmental hazards. Effective insurance can serve as both a measure to distribute, and a method to communicate risk. In order for insurance to fulfil these roles successfully, policy pricing and cover choices must be risk-based and founded on accurate information. This is reliant on a robust evidence base forming the foundation of policy choices. This paper focuses on the evidence available to insurers and emergent innovation in the use of data. The main risk considered is coastal flooding, for which the insurance sector offers an option for potential adaptation, capable of increasing resilience. However, inadequate supply and analysis of data have been highlighted as factors preventing insurance from fulfilling this role. Research was undertaken to evaluate how data are currently, and could potentially, be used within risk evaluations for the insurance industry. This comprised of 50 interviews with those working and associated with the London insurance market. The research reveals new opportunities, which could facilitate improvements in risk-reflective pricing of policies. These relate to a new generation of data collection techniques and analytics, such as those associated with satellite-derived data, IoT (Internet of Things) sensors, cloud computing, and Big Data solutions. Such technologies present opportunities to reduce moral hazard through basing predictions and pricing of risk on large empirical datasets. The value of insurers' claims data is also revealed, and is shown to have the potential to refine, calibrate, and validate models and methods. The adoption of such data-driven techniques could enable insurers to re-evaluate risk ratings, and in some instances, extend coverage to locations and developments, previously rated as too high a risk to insure. Conversely, other areas may be revealed more vulnerable, which could generate negative impacts for residents in these regions, such as increased premiums. However, the enhanced risk awareness generated, by new technology, data and data analytics, could positively alter future planning, development and investment decisions.}
}
@article{SINGH20191147,
title = {A Systemic Cybercrime Stakeholders Architectural Model},
journal = {Procedia Computer Science},
volume = {161},
pages = {1147-1155},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.227},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919319386},
author = {Manmeet Mahinderjit Singh and Anizah Abu Bakar},
keywords = {Big data, Internet of Things (IoT), Cyberspace, Routine Activity Theory, Systemic},
abstract = {The increased of cybercrime incidents taking place in the world is at its perilous magnitude causing losses in term of money and trust. Even though there are various cybersecurity solutions in place; the threat of cybercrime is still a hard problem. Exploration of cybercrime challenges, especially the preventions and detections of the cybercrime should be investigated by composing all the stakeholders and players of a cybercrime issue. In this paper; an exploration of several cybercrime stakeholders is done. It is argued that cybercrime is a systemic threat and cannot be tackled with cybersecurity and legal systems. The architectural model proposed is significant and should become one of the considered milestones in designing security control in tackling cybercrime globally.}
}
@article{ALASHHAB2021100059,
title = {Impact of coronavirus pandemic crisis on technologies and cloud computing applications},
journal = {Journal of Electronic Science and Technology},
volume = {19},
number = {1},
pages = {100059},
year = {2021},
note = {Special Section on In Silico Research on Microbiology and Public Health},
issn = {1674-862X},
doi = {https://doi.org/10.1016/j.jnlest.2020.100059},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X20300665},
author = {Ziyad R. Alashhab and Mohammed Anbar and Manmeet Mahinderjit Singh and Yu-Beng Leau and Zaher Ali Al-Sai and Sami {Abu Alhayja’a}},
keywords = {Big data privacy, Cloud computing (CC) applications, COVID-19, Digital transformation, Security challenge, Work from home},
abstract = {In light of the COVID-19 outbreak caused by the novel coronavirus, companies and institutions have instructed their employees to work from home as a precautionary measure to reduce the risk of contagion. Employees, however, have been exposed to different security risks because of working from home. Moreover, the rapid global spread of COVID-19 has increased the volume of data generated from various sources. Working from home depends mainly on cloud computing (CC) applications that help employees to efficiently accomplish their tasks. The cloud computing environment (CCE) is an unsung hero in the COVID-19 pandemic crisis. It consists of the fast-paced practices for services that reflect the trend of rapidly deployable applications for maintaining data. Despite the increase in the use of CC applications, there is an ongoing research challenge in the domains of CCE concerning data, guaranteeing security, and the availability of CC applications. This paper, to the best of our knowledge, is the first paper that thoroughly explains the impact of the COVID-19 pandemic on CCE. Additionally, this paper also highlights the security risks of working from home during the COVID-19 pandemic.}
}
@article{LEE2020101426,
title = {Determining causal relationships in leadership research using Machine Learning: The powerful synergy of experiments and data science},
journal = {The Leadership Quarterly},
pages = {101426},
year = {2020},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2020.101426},
url = {https://www.sciencedirect.com/science/article/pii/S1048984320300539},
author = {Allan Lee and Ilke Inceoglu and Oliver Hauser and Michael Greene},
keywords = {Leadership effectiveness, Leadership processes, Machine Learning, Artificial intelligence, Causality, Experiments, Big Data, Heterogeneous treatment effects},
abstract = {Machine Learning (ML) techniques offer exciting new avenues for leadership research. In this paper we discuss how ML techniques can be used to inform predictive and causal models of leadership effects and clarify why both types of model are important for leadership research. We propose combining ML and experimental designs to draw causal inferences by introducing a recently developed technique to isolate “heterogeneous treatment effects.” We provide a step-by-step guide on how to design studies that combine field experiments with the application of ML to establish causal relationships with maximal predictive power. Drawing on examples in the leadership literature, we illustrate how the suggested approach can be applied to examine the impact of, for example, leadership behavior on follower outcomes. We also discuss how ML can be used to advance leadership research from theoretical, methodological and practical perspectives and consider limitations.}
}
@article{WANG2020119852,
title = {Safety informatics as a new, promising and sustainable area of safety science in the information age},
journal = {Journal of Cleaner Production},
volume = {252},
pages = {119852},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.119852},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619347225},
author = {Bing Wang and Chao Wu},
keywords = {Safety science, Information science, Safety information, Safety informatics, Safety 4.0},
abstract = {Safety is a central dimension in contemporary debates on human health, loss prevention, environmental protection, sustainability, and cleaner production. In the information age, especially in the era of big data, safety information is an essential strategy for safety, and safety informatics has become a major research interest and a popular issue in the field of safety science. In recent years, safety informatics—a new area of safety science—has received increasing attention, developing greatly with successful research on the subject. The three key purposes of this paper are: (i) to analyze the historical development of safety informatics, (ii) to review the research progress of safety informatics, and (iii) to review limitations and propose future directions in the field of safety informatics. First, the development process of safety informatics is divided into four typical stages: (i) the embryonic stage (1940–1980), (ii) the initial stage (1980–1990), (iii) the formation stage (1990–2010), and (iv) the deepening stage (2010–present). Then, a review of safety informatics research is provided from seven aspects, including: (i) the discipline construction of safety informatics, (ii) theoretical safety information model, (iii) accident causation model from a safety information perspective, (iv) safety management based on safety information, (v) safety big data, (vi) safety intelligence, and (vii) safety information technology. Finally, limitations and future research directions in the safety informatics area are briefly discussed.}
}
@article{FAN201575,
title = {Temporal knowledge discovery in big BAS data for building energy management},
journal = {Energy and Buildings},
volume = {109},
pages = {75-89},
year = {2015},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2015.09.060},
url = {https://www.sciencedirect.com/science/article/pii/S0378778815302991},
author = {Cheng Fan and Fu Xiao and Henrik Madsen and Dan Wang},
keywords = {Temporal knowledge discovery, Time series data mining, Big data, Building automation system, Building energy management},
abstract = {With the advances of information technologies, today's building automation systems (BASs) are capable of managing building operational performance in an efficient and convenient way. Meanwhile, the amount of real-time monitoring and control data in BASs grows continually in the building lifecycle, which stimulates an intense demand for powerful big data analysis tools in BASs. Existing big data analytics adopted in the building automation industry focus on mining cross-sectional relationships, whereas the temporal relationships, i.e., the relationships over time, are usually overlooked. However, building operations are typically dynamic and BAS data are essentially multivariate time series data. This paper presents a time series data mining methodology for temporal knowledge discovery in big BAS data. A number of time series data mining techniques are explored and carefully assembled, including the Symbolic Aggregate approXimation (SAX), motif discovery, and temporal association rule mining. This study also develops two methods for the efficient post-processing of knowledge discovered. The methodology has been applied to analyze the BAS data retrieved from a real building. The temporal knowledge discovered is valuable to identify dynamics, patterns and anomalies in building operations, derive temporal association rules within and between subsystems, assess building system performance and spot opportunities in energy conservation.}
}
@article{SALIM2020106964,
title = {Modelling urban-scale occupant behaviour, mobility, and energy in buildings: A survey},
journal = {Building and Environment},
volume = {183},
pages = {106964},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2020.106964},
url = {https://www.sciencedirect.com/science/article/pii/S0360132320303231},
author = {Flora D. Salim and Bing Dong and Mohamed Ouf and Qi Wang and Ilaria Pigliautile and Xuyuan Kang and Tianzhen Hong and Wenbo Wu and Yapan Liu and Shakila Khan Rumi and Mohammad Saiedur Rahaman and Jingjing An and Hengfang Deng and Wei Shao and Jakub Dziedzic and Fisayo Caleb Sangogboye and Mikkel Baun Kjærgaard and Meng Kong and Claudia Fabiani and Anna Laura Pisello and Da Yan},
keywords = {Big data, Occupant behaviour, Energy modelling, Mobility, Urban data, Sensors, Machine learning, Energy in buildings, Energy in cities},
abstract = {The proliferation of urban sensing, IoT, and big data in cities provides unprecedented opportunities for a deeper understanding of occupant behaviour and energy usage patterns at the urban scale. This enables data-driven building and energy models to capture the urban dynamics, specifically the intrinsic occupant and energy use behavioural profiles that are not usually considered in traditional models. Although there are related reviews, none investigated urban data for use in modelling occupant behaviour and energy use at multiple scales, from buildings to neighbourhood to city. This survey paper aims to fill this gap by providing a critical summary and analysis of the works reported in the literature. We present the different sources of occupant-centric urban data that are useful for data-driven modelling and categorise the range of applications and recent data-driven modelling techniques for urban behaviour and energy modelling, along with the traditional stochastic and simulation-based approaches. Finally, we present a set of recommendations for future directions in data-driven modelling of occupant behaviour and energy in buildings at the urban scale.}
}
@article{MAO2021103052,
title = {Comprehensive strategies of machine-learning-based quantitative structure-activity relationship models},
journal = {iScience},
volume = {24},
number = {9},
pages = {103052},
year = {2021},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2021.103052},
url = {https://www.sciencedirect.com/science/article/pii/S2589004221010208},
author = {Jiashun Mao and Javed Akhtar and Xiao Zhang and Liang Sun and Shenghui Guan and Xinyu Li and Guangming Chen and Jiaxin Liu and Hyeon-Nae Jeon and Min Sung Kim and Kyoung Tai No and Guanyu Wang},
keywords = {Data analysis in structural biology, Machine learning, Structural biology},
abstract = {Summary
Early quantitative structure-activity relationship (QSAR) technologies have unsatisfactory versatility and accuracy in fields such as drug discovery because they are based on traditional machine learning and interpretive expert features. The development of Big Data and deep learning technologies significantly improve the processing of unstructured data and unleash the great potential of QSAR. Here we discuss the integration of wet experiments (which provide experimental data and reliable verification), molecular dynamics simulation (which provides mechanistic interpretation at the atomic/molecular levels), and machine learning (including deep learning) techniques to improve QSAR models. We first review the history of traditional QSAR and point out its problems. We then propose a better QSAR model characterized by a new iterative framework to integrate machine learning with disparate data input. Finally, we discuss the application of QSAR and machine learning to many practical research fields, including drug development and clinical trials.}
}
@article{HUSEIEN2022100116,
title = {A review on 5G technology for smart energy management and smart buildings in Singapore},
journal = {Energy and AI},
volume = {7},
pages = {100116},
year = {2022},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100116},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000653},
author = {Ghasan Fahim Huseien and Kwok Wei Shah},
keywords = {5G technology, Sustainability, Smart building, Facilities management, Build environment},
abstract = {Sustainable and smart building is a recent concept that is gaining momentum in public opinion, and thus, it is making its way into the agendas of researchers and city authorities all over the world. To move towards sustainable development goals, 5G technology would make significant impacts are building construction, operation, and management by facilitating high-class services, providing efficient functionalities. It's well known that the Singapore is one of top smart cities in this world and from the first counties that adopted of 5G technology in various sectors including smart buildings. Based on these facts, this paper discusses the international trends in 5G applications for smart buildings, and R&D and test bedding works conducted in 5G labs. As well as, the manuscript widely reviewed and discussed the 5G technology development, use cases, applications and future projects which supported by Singapore government. Finally, the 5G use cases for smart buildings and build environment improvement application were discussed. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.}
}
@article{BURNS2022420,
title = {Real-World Evidence for Regulatory Decision-Making: Guidance From Around the World},
journal = {Clinical Therapeutics},
volume = {44},
number = {3},
pages = {420-437},
year = {2022},
issn = {0149-2918},
doi = {https://doi.org/10.1016/j.clinthera.2022.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0149291822000170},
author = {Leah Burns and Nadege Le Roux and Robert Kalesnik-Orszulak and Jennifer Christian and Mathias Hukkelhoven and Frank Rockhold and John O'Donnell},
keywords = {Efficiency, Product effectiveness, Real-world evidence, Regulatory decision making},
abstract = {Purpose
Interest in leveraging real-world evidence (RWE) to support regulatory decision making for product effectiveness has been increasing globally as evident by the increasing number of regulatory frameworks and guidance documents. However, acceptance of RWE, especially before marketing for regulatory approval, differs across countries. In addition, guidance on the design and conduct of innovative clinical trials, such as randomized controlled registry studies, pragmatic trials, and other hybrid studies, is lacking.
Methods
We assessed the global regulatory environment with regard to RWE based on regional availability of the following 3 key regulatory elements: (1) RWE regulatory framework, (2) data quality and standards guidance. and (3) study methods guidance.
Findings
This article reviews the available frameworks and existing guidance from across the globe and discusses the observed gaps and opportunities for further development and harmonization.
Implications
Cross-country collaborations are encouraged to further shape and align RWE policies and help establish frameworks in countries without current policies with the goal of creating efficiencies when considering RWE to support regulatory decision-making globally.}
}
@article{MINET2017126,
title = {Crowdsourcing for agricultural applications: A review of uses and opportunities for a farmsourcing approach},
journal = {Computers and Electronics in Agriculture},
volume = {142},
pages = {126-138},
year = {2017},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2017.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917300479},
author = {Julien Minet and Yannick Curnel and Anne Gobin and Jean-Pierre Goffart and François Mélard and Bernard Tychon and Joost Wellens and Pierre Defourny},
keywords = {Crowdsourcing, Citizen science, Smart farming, Participatory approaches, Big data, ICT, Data collection},
abstract = {Crowdsourcing, understood as outsourcing tasks or data collection by a large group of non-professionals, is increasingly used in scientific research and operational applications. In this paper, we reviewed crowdsourcing initiatives in agricultural science and farming activities and further discussed the particular characteristics of this approach in the field of agriculture. On-going crowdsourcing initiatives in agriculture were analysed and categorised according to their crowdsourcing component. We identified eight types of agricultural data and information that can be generated from crowdsourcing initiatives. Subsequently we described existing methods of quality control of the crowdsourced data. We analysed the profiles of potential contributors in crowdsourcing initiatives in agriculture, suggested ways for increasing farmers’ participation, and discussed the on-going initiatives in the light of their target beneficiaries. While crowdsourcing is reported to be an efficient way of collecting observations relevant to environmental monitoring and contributing to science in general, we pointed out that crowdsourcing applications in agriculture may be hampered by privacy issues and other barriers to participation. Close connections with the farming sector, including extension services and farm advisory companies, could leverage the potential of crowdsourcing for both agricultural research and farming applications. This paper coins the term of farmsourcing asa professional crowdsourcing strategy in farming activities and provides a source of recommendations and inspirations for future collaborative actions in agricultural crowdsourcing.}
}
@article{HE2021102867,
title = {State-of-health estimation based on real data of electric vehicles concerning user behavior},
journal = {Journal of Energy Storage},
volume = {41},
pages = {102867},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.102867},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21005892},
author = {Zhigang He and Xiaoyu Shen and Yanyan Sun and Shichao Zhao and Bin Fan and Chaofeng Pan},
keywords = {Electric vehicles, SOH, User behavior, LWLR, LSTM},
abstract = {State of health (SOH) of lithium-ion battery pack directly determines the driving mileage and output power of the electric vehicle. With the development of big data storage and analysis technology, using big data to off-line estimate battery pack SOH is more feasible than before. This paper proposes a SOH estimation method based on real data of electric vehicles concerning user behavior. The charging capacity is calculated by historical charging data, and locally weighted linear regression (LWLR) algorithm is used to qualitatively characterize the capacity decline trend. The health features are extracted from historical operating data, maximal information coefficient (MIC) algorithm is used to measure the correlation between health features and capacity. Then, long and short-term memory (LSTM)-based neural network will further learn the nonlinear degradation relationship between capacity and health features. Bayesian optimization algorithm is used to ensure the generalization of the model when different electric vehicles produce different user behaviors. The estimation method is validated by the 300 days historical dataset from 100 vehicles with different driving behavior. The results indicates that the maximum relative error of estimating SOH is 0.2%.}
}
@article{AMARATUNGA2020100027,
title = {Uses and opportunities for machine learning in hypertension research},
journal = {International Journal of Cardiology Hypertension},
volume = {5},
pages = {100027},
year = {2020},
issn = {2590-0862},
doi = {https://doi.org/10.1016/j.ijchy.2020.100027},
url = {https://www.sciencedirect.com/science/article/pii/S2590086220300045},
author = {Dhammika Amaratunga and Javier Cabrera and Davit Sargsyan and John B. Kostis and Stavros Zinonos and William J. Kostis},
keywords = {Machine learning, Deep neural networks, Hypertension, Disease management, Personalized disease network},
abstract = {Background
Artificial intelligence (AI) promises to provide useful information to clinicians specializing in hypertension. Already, there are some significant AI applications on large validated data sets.
Methods and results
This review presents the use of AI to predict clinical outcomes in big data i.e. data with high volume, variety, veracity, velocity and value. Four examples are included in this review. In the first example, deep learning and support vector machine (SVM) predicted the occurrence of cardiovascular events with 56%–57% accuracy. In the second example, in a data base of 378,256 patients, a neural network algorithm predicted the occurrence of cardiovascular events during 10 year follow up with sensitivity (68%) and specificity (71%). In the third example, a machine learning algorithm classified 1,504,437 patients on the presence or absence of hypertension with 51% sensitivity, 99% specificity and area under the curve 87%. In example four, wearable biosensors and portable devices were used in assessing a person's risk of developing hypertension using photoplethysmography to separate persons who were at risk of developing hypertension with sensitivity higher than 80% and positive predictive value higher than 90%. The results of the above studies were adjusted for demographics and the traditional risk factors for atherosclerotic disease.
Conclusion
These examples describe the use of artificial intelligence methods in the field of hypertension.}
}
@article{PERERA2016512,
title = {Marine Engine Operating Regions under Principal Component Analysis to evaluate Ship Performance and Navigation Behavior},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {23},
pages = {512-517},
year = {2016},
note = {10th IFAC Conference on Control Applications in Marine SystemsCAMS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.10.487},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316320778},
author = {Lokukaluge P. Perera and Brage Mo},
keywords = {Principal Component Analysis, Big Data, Marine Engine Operations, Ship Performance Monitoring, Structured Data},
abstract = {Abstract:
Marine engine operating regions under principal component analysis (PCA) to evaluate ship performance and navigation behavior are presented in this study. A data set with ship performance and navigation information (i.e. a selected vessel) is considered to identify its hidden structure with respect to a selected operating region of the marine engine. Firstly, the data set is classified with respect to the engine operating points (i.e. operating modes), identifying three operating regions for the main engine. Secondly, one engine operating region (i.e. a data cluster) is analyzed to calculate the respective principal components (PCs). These PCs represent various relationships among ship performance and navigation parameters of the vessel and those relationships with respect to the marine engine operating region are used to evaluate ship performance and navigation behavior. Furthermore, such knowledge (i.e. PCs and parameter behavior) can also be used for sensor fault identification and data compression/expansion types of applications as a big data solution in shipping.}
}
@article{GACUTAN2022150742,
title = {Continental patterns in marine debris revealed by a decade of citizen science},
journal = {Science of The Total Environment},
volume = {807},
pages = {150742},
year = {2022},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2021.150742},
url = {https://www.sciencedirect.com/science/article/pii/S0048969721058204},
author = {Jordan Gacutan and Emma L. Johnston and Heidi Tait and Wally Smith and Graeme F. Clark},
keywords = {Environmental monitoring, Plastic pollution, Bioregional management, Litter, Citizen science, Marine debris},
abstract = {Anthropogenic marine debris is a persistent threat to oceans, imposing risks to ecosystems and the communities they support. Whilst an understanding of marine debris risks is steadily advancing, monitoring at spatial and temporal scales relevant to management remains limited. Citizen science projects address this shortcoming but are often critiqued on data accuracy and potential bias in sampling efforts. Here we present 10-years of Australia's largest marine debris database - the Australian Marine Debris Initiative (AMDI), in which we perform systematic data filtering, test for differences between collecting groups, and report patterns in marine debris. We defined five stages of data filtering to address issues in data quality and to limit inference to ocean-facing sandy beaches. Significant differences were observed in the average accumulation of items between filtered and remaining data. Further, differences in sampling were compared between collecting groups at the same site (e.g., government, NGOs, and schools), where no significant differences were observed. The filtering process removed 21% of events due to data quality issues and a further 42% of events to restrict analyses to ocean-facing sandy beaches. The remaining 7275 events across 852 sites allowed for an assessment of debris patterns at an unprecedented spatial and temporal resolution. Hard plastics were the most common material found on beaches both nationally and regionally, consisting of up to 75% of total debris. Nationally, land and sea-sourced items accounted for 48% and 7% of debris, respectively, with most debris found on the east coast of Australia. This study demonstrates the value of citizen science datasets with broad spatial and temporal coverage, and the importance of data filtering to improve data quality. The citizen science presented provides an understanding of debris patterns on Australia's ocean beaches and can serve as a foundation for future source reduction plans.}
}
@article{CHE2023513,
title = {Impacts of pollution heterogeneity on population exposure in dense urban areas using ultra-fine resolution air quality data},
journal = {Journal of Environmental Sciences},
volume = {125},
pages = {513-523},
year = {2023},
issn = {1001-0742},
doi = {https://doi.org/10.1016/j.jes.2022.02.041},
url = {https://www.sciencedirect.com/science/article/pii/S1001074222001061},
author = {Wenwei Che and Yumiao Zhang and Changqing Lin and Yik Him Fung and Jimmy C.H. Fung and Alexis K.H. Lau},
keywords = {Particulate matter, Nitrogen dioxide, Ozone, Pollution heterogeneity, Urban area},
abstract = {Traditional air quality data have a spatial resolution of 1 km or above, making it challenging to resolve detailed air pollution exposure in complex urban areas. Combining urban morphology, dynamic traffic emission, regional and local meteorology, physicochemical transformations in air quality models using big data fusion technology, an ultra-fine resolution modeling system was developed to provide air quality data down to street level. Based on one-year ultra-fine resolution data, this study investigated the effects of pollution heterogeneity on the individual and population exposure to particulate matter (PM2.5 and PM10), nitrogen dioxide (NO2), and ozone (O3) in Hong Kong, one of the most densely populated and urbanized cities. Sharp fine-scale variabilities in air pollution were revealed within individual city blocks. Using traditional 1 km average to represent individual exposure resulted in a positively skewed deviation of up to 200% for high-end exposure individuals. Citizens were disproportionally affected by air pollution, with annual pollutant concentrations varied by factors of 2 to 5 among 452 District Council Constituency Areas (DCCAs) in Hong Kong, indicating great environmental inequities among the population. Unfavorable city planning resulted in a positive spatial coincidence between pollution and population, which increased public exposure to air pollutants by as large as 46% among districts in Hong Kong. Our results highlight the importance of ultra-fine pollutant data in quantifying the heterogeneity in pollution exposure in the dense urban area and the critical role of smart urban planning in reducing exposure inequities.}
}
@article{YAO202154,
title = {Application of artificial intelligence in renal disease},
journal = {Clinical eHealth},
volume = {4},
pages = {54-61},
year = {2021},
issn = {2588-9141},
doi = {https://doi.org/10.1016/j.ceh.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2588914121000083},
author = {Lijing Yao and Hengyuan Zhang and Mengqin Zhang and Xing Chen and Jun Zhang and Jiyi Huang and Lu Zhang},
keywords = {Artificial intelligence (AI), Machine learning (ML), Artificial neural network (ANN), Convolution neural network (CNN), Nephrology},
abstract = {Artificial intelligence (AI) has been applied widely in almost every area of our daily lives, due to the growth of computing power, advances in methods and techniques, and the explosion of data, it also plays a critical role in academic disciplines, medicine is not an exception. AI can augment the intelligence of clinicians in diagnosis, prognosis, and treatment decisions.Kidney disease causes great economic burden worldwide, with both acute kidney injury and chronic kidney disease bringing about high morbidity and mortality. Outstanding challenges in nephrology may be addressed by leveraging big data and AI.In this review, we summarized advances in machine learning (ML), artificial neural network (ANN), convolution neural network (CNN) and deep learning (DL), with a special focus on acute kidney injury (AKI), chronic kidney disease (CKD), end-stage renal disease (ESRD), dialysis, kidney transplantation and nephropathology. AI may not be anticipated to replace the nephrologists’ medical decision-making for now, but instead assisting them in providing optimal personalized therapy for patients.}
}
@article{KAMAL2016191,
title = {A MapReduce approach to diminish imbalance parameters for big deoxyribonucleic acid dataset},
journal = {Computer Methods and Programs in Biomedicine},
volume = {131},
pages = {191-206},
year = {2016},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2016.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0169260715304119},
author = {Sarwar Kamal and Shamim Hasnat Ripon and Nilanjan Dey and Amira S. Ashour and V. Santhi},
keywords = {MapReduce, K-nearest neighbor, Big data, DNA (deoxyribonucleic acid), Computational biology, Imbalance data},
abstract = {Background
In the age of information superhighway, big data play a significant role in information processing, extractions, retrieving and management. In computational biology, the continuous challenge is to manage the biological data. Data mining techniques are sometimes imperfect for new space and time requirements. Thus, it is critical to process massive amounts of data to retrieve knowledge. The existing software and automated tools to handle big data sets are not sufficient. As a result, an expandable mining technique that enfolds the large storage and processing capability of distributed or parallel processing platforms is essential.
Method
In this analysis, a contemporary distributed clustering methodology for imbalance data reduction using k-nearest neighbor (K-NN) classification approach has been introduced. The pivotal objective of this work is to illustrate real training data sets with reduced amount of elements or instances. These reduced amounts of data sets will ensure faster data classification and standard storage management with less sensitivity. However, general data reduction methods cannot manage very big data sets. To minimize these difficulties, a MapReduce-oriented framework is designed using various clusters of automated contents, comprising multiple algorithmic approaches.
Results
To test the proposed approach, a real DNA (deoxyribonucleic acid) dataset that consists of 90 million pairs has been used. The proposed model reduces the imbalance data sets from large-scale data sets without loss of its accuracy.
Conclusions
The obtained results depict that MapReduce based K-NN classifier provided accurate results for big data of DNA.}
}
@article{YEBENES2019614,
title = {Towards a Data Governance Framework for Third Generation Platforms},
journal = {Procedia Computer Science},
volume = {151},
pages = {614-621},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.082},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305447},
author = {Juan Yebenes and Marta Zorrilla},
keywords = {Data governance, Industry 4.0, data bus architecture, cloud computing, IoT, big data},
abstract = {The fourth industrial revolution considers data as a business asset and therefore this is placed as a central element of the software architecture (data as a service) that will support the horizontal and vertical digitalization of industrial processes. The large volume of data that the environment generates, its heterogeneity and complexity, as well as its reuse for later processes (e.g. analytics, IA) requires the adoption of policies, directives and standards for its right governance. Furthermore, the issues related to the use of resources in the cloud computing must be taken into account with the aim of meeting the requirements of performance and security of the different processes. This article, in the absence of frameworks adapted to this new architecture, proposes an initial schema for developing an effective data governance programme for third generation platforms, that means, a conceptual tool which guides organizations to define, design, develop and deploy services aligned with its vision and business goals in I4.0 era.}
}
@article{LAKIND2019302,
title = {ExpoQual: Evaluating measured and modeled human exposure data},
journal = {Environmental Research},
volume = {171},
pages = {302-312},
year = {2019},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2019.01.039},
url = {https://www.sciencedirect.com/science/article/pii/S0013935119300465},
author = {Judy S. LaKind and Cian O’Mahony and Thomas Armstrong and Rosalie Tibaldi and Benjamin C. Blount and Daniel Q. Naiman},
keywords = {ExpoQual, BEES-C, Exposure, Human, Quality, Fit-for-purpose, Instrument, Biomonitoring, Model uncertainty},
abstract = {Recent rapid technological advances are producing exposure data sets for which there are no available data quality assessment tools. At the same time, regulatory agencies are moving in the direction of data quality assessment for environmental risk assessment and decision-making. A transparent and systematic approach to evaluating exposure data will aid in those efforts. Any approach to assessing data quality must consider the level of quality needed for the ultimate use of the data. While various fields have developed approaches to assess data quality, there is as yet no general, user-friendly approach to assess both measured and modeled data in the context of a fit-for-purpose risk assessment. Here we describe ExpoQual, an instrument developed for this purpose which applies recognized parameters and exposure data quality elements from existing approaches for assessing exposure data quality. Broad data streams such as quantitative measured and modeled human exposure data as well as newer and developing approaches can be evaluated. The key strength of ExpoQual is that it facilitates a structured, reproducible and transparent approach to exposure data quality evaluation and provides for an explicit fit-for-purpose determination. ExpoQual was designed to minimize subjectivity and to include transparency in aspects based on professional judgment. ExpoQual is freely available on-line for testing and user feedback (exposurequality.com).}
}
@article{SUN2022191,
title = {Advances in optical phenotyping of cereal crops},
journal = {Trends in Plant Science},
volume = {27},
number = {2},
pages = {191-208},
year = {2022},
issn = {1360-1385},
doi = {https://doi.org/10.1016/j.tplants.2021.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S1360138521002028},
author = {Dawei Sun and Kelly Robbins and Nicolas Morales and Qingyao Shu and Haiyan Cen},
keywords = {cereal crops, high-throughput phenotyping, optical sensors, traits},
abstract = {Optical sensors and sensing-based phenotyping techniques have become mainstream approaches in high-throughput phenotyping for improving trait selection and genetic gains in crops. We review recent progress and contemporary applications of optical sensing-based phenotyping (OSP) techniques in cereal crops and highlight optical sensing principles for spectral response and sensor specifications. Further, we group phenotypic traits determined by OSP into four categories – morphological, biochemical, physiological, and performance traits – and illustrate appropriate sensors for each extraction. In addition to the current status, we discuss the challenges of OSP and provide possible solutions. We propose that optical sensing-based traits need to be explored further, and that standardization of the language of phenotyping and worldwide collaboration between phenotyping researchers and other fields need to be established.}
}
@article{CHOW2017455,
title = {Internet-based computer technology on radiotherapy},
journal = {Reports of Practical Oncology & Radiotherapy},
volume = {22},
number = {6},
pages = {455-462},
year = {2017},
issn = {1507-1367},
doi = {https://doi.org/10.1016/j.rpor.2017.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1507136716301602},
author = {James C.L. Chow},
keywords = {Radiotherapy, Computer technology, Cloud computing, Machine learning, Big data},
abstract = {Recent rapid development of Internet-based computer technologies has made possible many novel applications in radiation dose delivery. However, translational speed of applying these new technologies in radiotherapy could hardly catch up due to the complex commissioning process and quality assurance protocol. Implementing novel Internet-based technology in radiotherapy requires corresponding design of algorithm and infrastructure of the application, set up of related clinical policies, purchase and development of software and hardware, computer programming and debugging, and national to international collaboration. Although such implementation processes are time consuming, some recent computer advancements in the radiation dose delivery are still noticeable. In this review, we will present the background and concept of some recent Internet-based computer technologies such as cloud computing, big data processing and machine learning, followed by their potential applications in radiotherapy, such as treatment planning and dose delivery. We will also discuss the current progress of these applications and their impacts on radiotherapy. We will explore and evaluate the expected benefits and challenges in implementation as well.}
}
@article{GRANELL20161,
title = {Future Internet technologies for environmental applications},
journal = {Environmental Modelling & Software},
volume = {78},
pages = {1-15},
year = {2016},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215301298},
author = {Carlos Granell and Denis Havlik and Sven Schade and Zoheir Sabeur and Conor Delaney and Jasmin Pielorz and Thomas Usländer and Paolo Mazzetti and Katharina Schleidt and Mike Kobernus and Fuada Havlik and Nils Rune Bodsberg and Arne Berre and Jose Lorenzo Mon},
keywords = {Environmental informatics, Environmental observation web, Future internet, Cloud computing, Internet of things, Big data, Environmental specific enablers, Volunteered geographic information, Crowdtasking},
abstract = {This paper investigates the usability of Future Internet technologies (aka “Generic Enablers of the Future Internet”) in the context of environmental applications. The paper incorporates the best aspects of the state-of-the-art in environmental informatics with geospatial solutions and scalable processing capabilities of Internet-based tools. It specifically targets the promotion of the “Environmental Observation Web” as an observation-centric paradigm for building the next generation of environmental applications. In the Environmental Observation Web, the great majority of data are considered as observations. These can be generated from sensors (hardware), numerical simulations (models), as well as by humans (human sensors). Independently from the observation provenance and application scope, data can be represented and processed in a standardised way in order to understand environmental processes and their interdependencies. The development of cross-domain applications is then leveraged by technologies such as Cloud Computing, Internet of Things, Big Data Processing and Analytics. For example, “the cloud” can satisfy the peak-performance needs of applications which may occasionally use large amounts of processing power at a fraction of the price of a dedicated server farm. The paper also addresses the need for Specific Enablers that connect mainstream Future Internet capabilities with sensor and geospatial technologies. Main categories of such Specific Enablers are described with an overall architectural approach for developing environmental applications and exemplar use cases.}
}
@article{YE2019936,
title = {Improved population mapping for China using remotely sensed and points-of-interest data within a random forests model},
journal = {Science of The Total Environment},
volume = {658},
pages = {936-946},
year = {2019},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2018.12.276},
url = {https://www.sciencedirect.com/science/article/pii/S0048969718351489},
author = {Tingting Ye and Naizhuo Zhao and Xuchao Yang and Zutao Ouyang and Xiaoping Liu and Qian Chen and Kejia Hu and Wenze Yue and Jiaguo Qi and Zhansheng Li and Peng Jia},
keywords = {Points of interest, Population, Random forests, Nighttime light, China},
abstract = {Remote sensing image products (e.g. brightness of nighttime lights and land cover/land use types) have been widely used to disaggregate census data to produce gridded population maps for large geographic areas. The advent of the geospatial big data revolution has created additional opportunities to map population distributions at fine resolutions with high accuracy. A considerable proportion of the geospatial data contains semantic information that indicates different categories of human activities occurring at exact geographic locations. Such information is often lacking in remote sensing data. In addition, the remarkable progress in machine learning provides toolkits for demographers to model complex nonlinear correlations between population and heterogeneous geographic covariates. In this study, a typical type of geospatial big data, points-of-interest (POIs), was combined with multi-source remote sensing data in a random forests model to disaggregate the 2010 county-level census population data to 100 × 100 m grids. Compared with the WorldPop population dataset, our population map showed higher accuracy. The root mean square error for population estimates in Beijing, Shanghai, Guangzhou, and Chongqing for this method and WorldPop were 27,829 and 34,193, respectively. The large under-allocation of the population in urban areas and over-allocation in rural areas in the WorldPop dataset was greatly reduced in this new population map. Apart from revealing the effectiveness of POIs in improving population mapping, this study promises the potential of geospatial big data for mapping other socioeconomic parameters in the future.}
}
@incollection{CINNAMON2020121,
title = {Humanitarian Mapping},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {121-128},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10559-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105591},
author = {Jonathan Cinnamon},
keywords = {Big data, Cartography, Crisis, Crowdsourcing, Disaster, Emergency, Geospatial web, Relief, Spatial data, Web mapping},
abstract = {Humanitarian mapping refers to the production of spatial data and cartographic products to improve situational awareness and decision-making around humanitarian issues from acute events such as natural disasters and public health emergencies to longer term events such as refugee crises and political unrest. Mapping is a key part of the broader area of humanitarian information management, which has traditionally been undertaken by governments and international humanitarian organizations. As a core aspect of the field of digital humanitarianism, mapping activities are now widely undertaken by smaller organizations and networks of volunteers who produce spatial data and maps on the ground and remotely via the use of Web mapping and mobile phone technologies. Big data based on location and behavioral attributes produced online and through interaction with digital systems and networks can also be exploited to enhance information environments. Together, these new developments signal new possibilities for improved risk and crisis management, based on up-to-date high resolution spatial and temporal evidence. Research in human geography, geographic information science, and related disciplines focuses on tracing benefits such as increased speed and low costs, as well as the risks of relying on distributed volunteers and new sources of data of questionable accuracy and validity.}
}
@article{MOSAVI2022503,
title = {Intelligent energy management using data mining techniques at Bosch Car Multimedia Portugal facilities},
journal = {Procedia Computer Science},
volume = {201},
pages = {503-510},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.065},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004781},
author = {Nasim Sadat Mosavi and Francisco Freitas and Rogério Pires and César Rodrigues and Isabel Silva and Manuel Santos and Paulo Novais},
keywords = {Energy consumption, Prediction, Optimization, Data Mining, Machine Learning, Forecasting, Industry 4.0},
abstract = {The fusion of emerged technologies such as Artificial Intelligence, cloud computing, big data, and the Internet of Things in manufacturing has pioneered this industry to meet the fourth stage of the industrial revolution (industry 4.0). One major approach to keeping this sector sustainable and productive is intelligent energy demand planning. Monitoring and controlling the consumption of energy under industry 4.0, directly results in minimizing the cost of operation and maximizing efficiency. To advance the research on the adoption of industry 4.0, this study examines CRISP-DM methodology to project data mining approach over data from 2020 to 2021 which was collected from industrial sensors to predict/forecast future electrical consumption at Bosch car multimedia facilities located at Braga, Portugal. Moreover, the influence of indicators such as humidity and temperature on electrical energy consumption was investigated. This study employed five promising regression algorithms and FaceBook prophet (FB prophet) to apply over data belonging to two HVAC (heating, ventilation, and air conditioning) sensors (E333, 3260). Results indicate Random Forest (RF) algorithms as a potential regression approach for prediction and the outcome of FB prophet to forecast the demand of future usage of electrical energy associated with HVAC presented. Based on that, it was concluded that predicting the usage of electrical energy for both data points requires time series techniques. Where “timestamp” was identified as the most effective feature to predict consume of electrical energy by regression technique (RF). The result of this study was integrated with Intelligent Industrial Management System (IIMS) at Bosch Portugal.}
}
@incollection{LEUNG2021197,
title = {Chapter 13 - A support vector machine–based voice disorders detection using human voice signal},
editor = {Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui},
booktitle = {Artificial Intelligence and Big Data Analytics for Smart Healthcare},
publisher = {Academic Press},
pages = {197-208},
year = {2021},
series = {Next Gen Tech Driven Personalized Med&Smart Healthcare},
isbn = {978-0-12-822060-3},
doi = {https://doi.org/10.1016/B978-0-12-822060-3.00014-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220603000140},
author = {Pak Ho Leung and Kwok Tai Chui and Kenneth Lo and Patricia Ordóñez {de Pablos}},
keywords = {Artificial intelligence, big data, human voice, imbalanced classification, machine learning, medical screening, smart city, smart healthcare, support vector machine, voice disorders},
abstract = {Voice disorders are common diseases; most of the people have had experienced in their life. Voice disorder sufferers are usually not seeking medical consultation attributable to time-consuming and costly medical expenditure. Recently, researchers have proposed various machine learning algorithms for rapid detection of voice disorders based on the analysis of human voice. In this chapter, we have taken the pronunciation of vowel /a/ as the input of support vector machine algorithm. The research problem is formulated as binary classification which output will be either healthy or pathological status. Our work achieves an accuracy of 69.3% (sensitivity of 83.3% and specificity of 33.3%) which improves by 6.4%–19.3% compared with existing works. The implication of research work suggests tackling the imbalanced classification by adding penalty or generating new training data to class of smaller size. Everybody could contribute the voice signal of vowel /a/ and serving as big data pool.}
}
@article{SHARMA2021101624,
title = {Implementing challenges of artificial intelligence: Evidence from public manufacturing sector of an emerging economy},
journal = {Government Information Quarterly},
pages = {101624},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101624},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000605},
author = {Manu Sharma and Sunil Luthra and Sudhanshu Joshi and Anil Kumar},
keywords = {Artificial intelligence, Implementing challenges, Public manufacturing sector, AI enabled systems, Emerging economies},
abstract = {The growing Artificial Intelligence (AI) age has been flooded with several innovations in algorithmic machine learning that may bring significant impacts to industries such as healthcare, agriculture, education, manufacturing, retail etc. But challenges such as data quality, privacy and lack of a skilled workforce limit the scope of AI implementation in emerging economies, particularly in the Public Manufacturing Sector (PMS). Therefore, to enhance the body of relevant literature, this study examines the existing challenges of AI implementation in PMS of India and explores the inter-relationships among them. The study has utilized the DEMATEL method for identification of the cause-and-effect group factors. The findings reveal that poor data quality, managers' lack of understanding of cognitive technologies, data privacy, problems in integrating cognitive projects and expensive technologies are the main challenges for AI implementation in PMS of India. Moreover, a model is proposed for industrial decision-makers and managers to take appropriate decisions to develop intelligent AI enabled systems for manufacturing organizations in emerging economies.}
}
@article{MACIAS2022,
title = {Nowcasting food inflation with a massive amount of online prices},
journal = {International Journal of Forecasting},
year = {2022},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2022.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S016920702200036X},
author = {Paweł Macias and Damian Stelmasiak and Karol Szafranek},
keywords = {Inflation nowcasting, Online prices, Big data, Nowcasting competition, Web scraping},
abstract = {The consensus in the literature on providing accurate inflation forecasts underlines the importance of precise nowcasts. In this paper, we focus on this issue by employing a unique, extensive dataset of online food and non-alcoholic beverages prices gathered automatically from the webpages of major online retailers in Poland since 2009. We perform a real-time nowcasting experiment by using a highly disaggregated framework among popular, simple univariate approaches. We demonstrate that pure estimates of online price changes are already effective in nowcasting food inflation, but accounting for online food prices in a simple, recursively optimized model delivers further gains in the nowcast accuracy. Our framework outperforms various other approaches, including judgmental methods, traditional benchmarks, and model combinations. After the outbreak of the COVID-19 pandemic, its nowcasting quality has improved compared to other approaches and remained comparable with judgmental nowcasts. We also show that nowcast accuracy increases with the volume of online data, but their quality and relevance are essential for providing accurate in-sample fit and out-of-sample nowcasts. We conclude that online prices can markedly aid the decision-making process at central banks.}
}
@incollection{ZEITOUNI2020159,
title = {Chapter 8 - Query Processing and Access Methods for Big Astro and Geo Databases},
editor = {Petr Škoda and Fathalrahman Adam},
booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
publisher = {Elsevier},
pages = {159-171},
year = {2020},
isbn = {978-0-12-819154-5},
doi = {https://doi.org/10.1016/B978-0-12-819154-5.00018-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191545000187},
author = {Karine Zeitouni and Mariem Brahem and Laurent Yeh and Atanas Hristov},
keywords = {Spatial databases, Spatial access methods, Query optimization, Big Data, System architecture},
abstract = {In spite of their development in different communities, either astro-informatics or geo-informatics, data management and analytics of astronomical and geospatial data share the same characteristics, and raise the same challenges when it comes to access, query, or analysis of the spatial features over Big Data. The very first challenge is to deal with the data volume, which is tremendous in many geo and astro datasets. In this chapter, we highlight their main specificity and outline the main steps of query processing in big geospatial and astronomical data servers. Through the review of the state of the art, we show the advance in the topic of Big Data management in both contexts of geospatial and sky surveying, while highlighting their similarity. This progress notwithstanding, several issues remain to deal with the variety (such as multidimensional arrays) of the data.}
}
@article{ECKARDT2020406,
title = {Opioid use disorder research and the Council for the Advancement of Nursing Science priority areas},
journal = {Nursing Outlook},
volume = {68},
number = {4},
pages = {406-416},
year = {2020},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2020.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0029655419306931},
author = {Patricia Eckardt and Donald Bailey and Holli A. DeVon and Cynthia Dougherty and Pamela Ginex and Cheryl A. Krause-Parello and Rita H. Pickler and Therese S. Richmond and Eleanor Rivera and Carol F. Roye and Nancy Redeker},
keywords = {Precision health, Big data and Data analytics, Determinants of health, Global health, Opioid use disorder research},
abstract = {Background
Chronic diseases, such as opioid use disorder (OUD) require a multifaceted scientific approach to address their evolving complexity. The Council for the Advancement of Nursing Science's (Council) four nursing science priority areas (precision health; global health, determinants of health, and big data/data analytics) were established to provide a framework to address current complex health problems.
Purpose
To examine OUD research through the nursing science priority areas and evaluate the appropriateness of the priority areas as a framework for research on complex health conditions.
Method
OUD was used as an exemplar to explore the relevance of the nursing science priorities for future research.
Findings
Research in the four priority areas is advancing knowledge in OUD identification, prevention, and treatment. Intersection of OUD research population focus and methodological approach was identified among the priority areas.
Discussion
The Council priorities provide a relevant framework for nurse scientists to address complex health problems like OUD.}
}
@article{ZHANG2020100715,
title = {Knowledge mapping of tourism demand forecasting research},
journal = {Tourism Management Perspectives},
volume = {35},
pages = {100715},
year = {2020},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2020.100715},
url = {https://www.sciencedirect.com/science/article/pii/S2211973620300829},
author = {Chengyuan Zhang and Shouyang Wang and Shaolong Sun and Yunjie Wei},
keywords = {Bibliometric analysis, Tourist arrival, Hospitality demand, Knowledge map, CiteSpace, Infographic},
abstract = {Utilizing a scientometric review of global trends and structure from 388 bibliographic records over two decades (1999–2018), this study seeks to advance the building of comprehensive knowledge maps that draw upon global travel demand studies. The study, using the techniques of co-citation analysis, collaboration network and emerging trends analysis, identified major disciplines that provide knowledge and theories for tourism demand forecasting, many trending research topics, the most critical countries, institutions, publications, and articles, and the most influential researchers. The increasing interest and output for big data and machine learning techniques in the field were visualized via comprehensive knowledge maps. This research provides meaningful guidance for researchers, operators and decision makers who wish to improve the accuracy of tourism demand forecasting.}
}
@article{SHAH2019562,
title = {An Internet-of-things Enabled Smart Manufacturing Testbed},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {1},
pages = {562-567},
year = {2019},
note = {12th IFAC Symposium on Dynamics and Control of Process Systems, including Biosystems DYCOPS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.06.122},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319302083},
author = {Devarshi Shah and Jin Wang and Q. Peter He},
keywords = {Internet-of-things, smart manufacturing, big data, data analytics, statistical analysis, vibration, soft sensor, process monitoring},
abstract = {The emergence of the industrial Internet of Things (IoT) and ever advancing computing and communication technologies have fueled a new industrial revolution which is happening worldwide to make current manufacturing systems smarter, safer, and more efficient. Although many general frameworks have been proposed for IoT enabled systems for industrial application, there is limited literature on demonstrations or testbeds of such systems. In addition, there is a lack of systematic study on the characteristics of IoT sensors and data analytics challenges associated with IoT sensor data. This study is an attempt to help fill this gap by exploring the characteristics of IoT vibration sensors and show how IoT sensors and big data analytics can be used to develop real time monitoring frameworks.}
}
@article{PASIDIS2019301,
title = {Congestion by accident? A two-way relationship for highways in England},
journal = {Journal of Transport Geography},
volume = {76},
pages = {301-314},
year = {2019},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2017.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0966692317300704},
author = {Ilias Pasidis},
keywords = {Accidents, Traffic congestion, Big data, Highways, England},
abstract = {This paper aims to estimate the causal effect of accidents on traffic congestion and vice versa. In order to identify both effects of this two-way relationship, I use dynamic panel data techniques and open access ‘big data’ of highway traffic and accidents in England for the period 2012–2014. The research design is based on the daily-and-hourly specific mean reversion pattern of highway traffic, which can be used to define a recurrent congestion benchmark. Using this benchmark, I am able to identify the causal effect of accidents on non-recurrent traffic congestion. A positive relationship between traffic congestion and road accidents would yield multiplicative benefits for policies that aim at reducing either of these issues. Additionally, I explore the duration of the effect of an accident on congestion, the ‘rubbernecking’ effect, as well as heterogeneous effects in the most congested highway segments. Then, I test the use of methods which employ the bulk of information in big data and other methods using a very reduced sample. In my application, both approaches produce similar results. Finally, I find a non-linear negative effect of traffic congestion on the probability of an accident.}
}
@incollection{BERMAN20181,
title = {1 - Introduction},
editor = {Jules J. Berman},
booktitle = {Principles and Practice of Big Data (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {1-13},
year = {2018},
isbn = {978-0-12-815609-4},
doi = {https://doi.org/10.1016/B978-0-12-815609-4.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128156094000017},
author = {Jules J. Berman},
keywords = {Big data definition, Small data, Data filtering, Data reduction},
abstract = {Big Data is not synonymous with lots and lots of data. Useful Big Data resources adhere to a set of data management principles that are fundamentally different from the traditional practices followed for small data projects. The areas of difference include: data collection; data annotation (including metadata and identifiers); location and distribution of stored data; classification of data; data access rules; data curation; data immutability; data permanence; verification and validity methods for the contained data; analytic methods; costs; and incumbent legal, social, and ethical issues. Skilled professionals who are adept in the design and management of small data resources may be unprepared for the unique challenges posed by Big Data. This chapter is an introduction to topics that will be fully explained in later chapters.}
}
@article{ZHANG2018149,
title = {Product features characterization and customers’ preferences prediction based on purchasing data},
journal = {CIRP Annals},
volume = {67},
number = {1},
pages = {149-152},
year = {2018},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2018.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0007850618300441},
author = {Jian Zhang and Alessandro Simeone and Peihua Gu and Bo Hong},
keywords = {Design, Product development, Big data},
abstract = {Big data of online product purchases is an emerging source for obtaining customers’ preferences of product features for new product development. This paper proposes a framework and associated method for product features characterization and customers’ preference prediction based on online product purchase data. Specifications and components of products are firstly analyzed and the relationships between product specifications and components are then established for features characterization. The customers preferred specifications, features and their combinations are predicted for development of new products. The features characterization and customers’ preferences prediction of toy cars were used as an example of illustrating the proposed method.}
}
@incollection{MCGILVRAY2021253,
title = {Chapter 5 - Structuring Your Project},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {253-267},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.00001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150000013},
author = {Danette McGilvray},
keywords = {Solution Development Life Cycle (SDLC), Agile, Scrum, sequential, waterfall, project objectives, project team, project roles, project timing, project approach},
abstract = {It is essential that those using the Ten Steps do a good job of organizing their work. This chapter guides readers’ choices when setting up their projects and assembling a project team. Three general types of projects are detailed: 1) Focused data quality improvement project, 2) Data quality activities in another project, such as application development, data migration or integration of any kind, and 3) Ad hoc use of data quality steps, activities, or techniques from the Ten Steps. Additional information is given for incorporating data quality activities into another project using various SDLCs (solution/system/software life cycles). Relevant data quality activities from the Ten Steps can be incorporated into any SDLC that is the basis for the larger project (Agile, sequential, hybrid, etc.). To that end, several tables list data governance, stewardship, data quality and readiness activities and where they would take place in typical SDLC phases. A table with Agile Scrum activities are cross-referenced to the same SDLC phases. The chapter concludes with general tips for project timing, communication, and engagement.}
}
@article{CAPPIELLO2022101874,
title = {Assessing and improving measurability of process performance indicators based on quality of logs},
journal = {Information Systems},
volume = {103},
pages = {101874},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101874},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921000995},
author = {Cinzia Cappiello and Marco Comuzzi and Pierluigi Plebani and Matheus Fim},
keywords = {Business process, Event log, Data quality assessment, Data quality improvement},
abstract = {The efficiency and effectiveness of business processes are usually evaluated by Process Performance Indicators (PPIs), which are computed using process event logs. PPIs can be insightful only when they are measurable, i.e., reliable. This paper proposes to define PPI measurability on the basis of the quality of the data in the process logs. Then, based on this definition, a framework for PPI measurability assessment and improvement is presented. For the assessment, we propose novel definitions of PPI accuracy, completeness, consistency, timeliness and volume that contextualise the traditional definitions in the data quality literature to the case of process logs. For the improvement, we define a set of guidelines for improving the measurability of a PPI. These guidelines may concern improving existing event logs, for instance through data imputation, implementation or enhancement of the process monitoring systems, or updating the PPI definitions. A case study in a large-sized institution is discussed to show the feasibility and the practical value of the proposed framework.}
}
@article{DETRE2018541,
title = {Handling veracity in multi-criteria decision-making: A multi-dimensional approach},
journal = {Information Sciences},
volume = {460-461},
pages = {541-554},
year = {2018},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2017.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0020025517309337},
author = {Guy {De Tré} and Robin {De Mol} and Antoon Bronselaer},
keywords = {Multi-criteria decision-making, Veracity in ‘big’ data, Data quality assessment, Data quality handling},
abstract = {Decision support systems aim to help a decision maker with selecting the option from a set of available options that best meets her or his needs. In multi-criteria based decision support approaches, a suitability degree is computed for each option, reflecting how suitable that option is considering the preferences of the decision maker. Nowadays, it becomes more and more common that data of different quality, originating from different data sets and different data providers have to be integrated and processed in order to compute the suitability degrees. Also, data sets can be very large such that their data become commonly prone to incompleteness, imprecision and uncertainty. Hence, not all data used for decision making can be trusted to the same extent and consequently, neither the results of computations with such data can be trusted to the same extent. For this reason, data quality assessment becomes an important aspect of a decision making process. To correctly inform the users, it is essential to communicate not only the computed suitability degrees of the available options, but also the confidence about these suitability degrees as can be derived from data quality assessment. In this paper a novel multi-dimensional approach for data quality assessment in multi-criteria decision making, supporting the computation of associated confidence degrees, is presented. Providing confidence information adds an extra dimension to the decision making process and leads to more soundly decisions. The added value of the approach is illustrated with aspects of a geographic decision making process.}
}
@article{SAYAD2019130,
title = {Predictive modeling of wildfires: A new dataset and machine learning approach},
journal = {Fire Safety Journal},
volume = {104},
pages = {130-146},
year = {2019},
issn = {0379-7112},
doi = {https://doi.org/10.1016/j.firesaf.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0379711218303941},
author = {Younes Oulad Sayad and Hajar Mousannif and Hassan {Al Moatassime}},
keywords = {Big data, Remote sensing, Machine learning, Wildfire prediction, Data mining, Artificial intelligence},
abstract = {Wildfires, whether natural or caused by humans, are considered among the most dangerous and devastating disasters around the world. Their complexity comes from the fact that they are hard to predict, hard to extinguish and cause enormous financial losses. To address this issue, many research efforts have been conducted in order to monitor, predict and prevent wildfires using several Artificial Intelligence techniques and strategies such as Big Data, Machine Learning, and Remote Sensing. The latter offers a rich source of satellite images, from which we can retrieve a huge amount of data that can be used to monitor wildfires. The method used in this paper combines Big Data, Remote Sensing and Data Mining algorithms (Artificial Neural Network and SVM) to process data collected from satellite images over large areas and extract insights from them to predict the occurrence of wildfires and avoid such disasters. For this reason, we implemented a methodology that serves this purpose by building a dataset based on Remote Sensing data related to the state of the crops (NDVI), meteorological conditions (LST), as well as the fire indicator “Thermal Anomalies”, these data, were acquired from “MODIS” (Moderate Resolution Imaging Spectroradiometer), a key instrument aboard the Terra and Aqua satellites. This dataset is available on GitHub via this link (https://github.com/ouladsayadyounes/Wildfires). Experiments were made using the big data platform “Databricks”. Experimental results gave high prediction accuracy (98.32%). These results were assessed using several validation strategies (e.g., classification metrics, cross-validation, and regularization) as well as a comparison with some wildfire early warning systems.}
}
@incollection{DEMCHENKO201721,
title = {Chapter 2 - Cloud Computing Infrastructure for Data Intensive Applications},
editor = {Hui-Huang Hsu and Chuan-Yu Chang and Ching-Hsien Hsu},
booktitle = {Big Data Analytics for Sensor-Network Collected Intelligence},
publisher = {Academic Press},
pages = {21-62},
year = {2017},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-809393-1},
doi = {https://doi.org/10.1016/B978-0-12-809393-1.00002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093931000027},
author = {Yuri Demchenko and Fatih Turkmen and Cees {de Laat} and Ching-Hsien Hsu and Christophe Blanchet and Charles Loomis},
keywords = {Big Data, Big Data Architecture Framework (BDAF), Big data infrastructure, NIST Big Data Architecture (BDRA), Cloud computing, Intercloud Architecture Framework (ICAF), Cloud powered design, SlipStream cloud automation platform},
abstract = {This chapter describes the general architecture and functional components of the cloud-based big data infrastructure (BDI). The chapter starts with the analysis of emerging Big Data and data intensive technologies and provides the general definition of the Big Data Architecture Framework (BDAF) that includes the following components: Big Data definition, Data Management including data lifecycle and data structures, generically cloud based BDI, Data Analytics technologies and platforms, and Big Data security, compliance, and privacy. The chapter refers to NIST Big Data Reference Architecture (BDRA) and summarizes general requirements to Big Data systems described in NIST documents. The proposed BDI and its cloud-based components are defined in accordance with the NIST BDRA and BDAF. This chapter provides detailed analysis of the two bioinformatics use cases as typical example of the Big Data applications that have being developed by the authors in the framework of the CYCLONE project. The effective use of cloud for bioinformatics applications requires maximum automation of the applications deployment and management that may include resources from multiple clouds and providers. The proposed CYCLONE platform for multicloud multiprovider applications deployment and management is based on the SlipStream cloud automation platform and includes all necessary components to build and operate complex scientific applications. The chapter discusses existing platforms for cloud powered applications development and deployment automation, in particularly referring to the SlipStream cloud automation platform, which allows multicloud applications deployment and management. The chapter also includes a short overview of the existing Big Data platforms and services provided by the major cloud services providers which can be used for fast deployment of customer Big Data applications using the benefits of cloud technologies and global cloud infrastructure.}
}
@article{KARAFILI201852,
title = {An argumentation reasoning approach for data processing},
journal = {Computers in Industry},
volume = {94},
pages = {52-61},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S016636151730338X},
author = {Erisa Karafili and Konstantina Spanaki and Emil C. Lupu},
keywords = {Data processing, Data quality, Usage control, Argumentation reasoning, Data manufacturing, Case scenarios},
abstract = {Data-intensive environments enable us to capture information and knowledge about the physical surroundings, to optimise our resources, enjoy personalised services and gain unprecedented insights into our lives. However, to obtain these endeavours extracted from the data, this data should be generated, collected and the insight should be exploited. Following an argumentation reasoning approach for data processing and building on the theoretical background of data management, we highlight the importance of data sharing agreements (DSAs) and quality attributes for the proposed data processing mechanism. The proposed approach is taking into account the DSAs and usage policies as well as the quality attributes of the data, which were previously neglected compared to existing methods in the data processing and management field. Previous research provided techniques towards this direction; however, a more intensive research approach for processing techniques should be introduced for the future to enhance the value creation from the data and new strategies should be formed around this data generated daily from various devices and sources.}
}
@article{GHORBANIAN2020276,
title = {Improved land cover map of Iran using Sentinel imagery within Google Earth Engine and a novel automatic workflow for land cover classification using migrated training samples},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {167},
pages = {276-288},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620302008},
author = {Arsalan Ghorbanian and Mohammad Kakooei and Meisam Amani and Sahel Mahdavi and Ali Mohammadzadeh and Mahdi Hasanlou},
keywords = {Land cover classification, Sentinel, Google Earth Engine, Big data, Remote sensing, Iran},
abstract = {Accurate information about the location, extent, and type of Land Cover (LC) is essential for various applications. The only recent available country-wide LC map of Iran was generated in 2016 by the Iranian Space Agency (ISA) using Moderate Resolution Imaging Spectroradiometer (MODIS) images with a considerably low accuracy. Therefore, the production of an up-to-date and accurate Iran-wide LC map using the most recent remote sensing, machine learning, and big data processing algorithms is required. Moreover, it is important to develop an efficient method for automatic LC generation for various time periods without the need to collect additional ground truth data from this immense country. Therefore, this study was conducted to fulfill two objectives. First, an improved Iranian LC map with 13 LC classes and a spatial resolution of 10 m was produced using multi-temporal synergy of Sentinel-1 and Sentinel-2 satellite datasets applied to an object-based Random forest (RF) algorithm. For this purpose, 2,869 Sentinel-1 and 11,994 Sentinel-2 scenes acquired in 2017 were processed and classified within the Google Earth Engine (GEE) cloud computing platform allowing big geospatial data analysis. The Overall Accuracy (OA) and Kappa Coefficient (KC) of the final Iran-wide LC map for 2017 was 95.6% and 0.95, respectively, indicating the considerable potential of the proposed big data processing method. Second, an efficient automatic method was developed based on Sentinel-2 images to migrate ground truth samples from a reference year to automatically generate an LC map for any target year. The OA and KC for the LC map produced for the target year 2019 were 91.35% and 0.91, respectively, demonstrating the efficiency of the proposed method for automatic LC mapping. Based on the obtained accuracies, this method can potentially be applied to other regions of interest for LC mapping without the need for ground truth data from the target year.}
}
@incollection{SEBASTIANCOLEMAN202269,
title = {Chapter 4 - The Data Challenge: The Mechanics of Meaning},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {69-92},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000043},
author = {Laura Sebastian-Coleman},
keywords = {History of data, statistics, scientific data, organizational data, relational data, characteristics of data},
abstract = {This chapter presents an extended definition of the concept of data, through the lens of history. All forms of data encode information about the real world. Using data always involves interpretation, so it is important to understand how data works, to understand “data as data.” But what we mean by data and how we create and use it in science, statistics, and commerce have changed over time. Many assumptions about data quality are rooted in this evolution. A better understanding of the evolution of data helps us define and manage specific expectations related to data quality.}
}
@article{ROBERTSON2020214,
title = {An integrated environmental analytics system (IDEAS) based on a DGGS},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {162},
pages = {214-228},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620300502},
author = {Colin Robertson and Chiranjib Chaudhuri and Majid Hojati and Steven A. Roberts},
keywords = {DGGS, Data model, Big data, Spatial data, Analytics, Environment},
abstract = {Discrete global grid systems (DGGS) have been proposed as a data model for a digital earth framework. We introduce a new data model and analytics system called IDEAS – integrated discrete environmental analysis system to create an operational DGGS-based GIS which is suitable for large scale environmental modelling and analysis. Our analysis demonstrates that DGGS-based GIS is feasible within a relational database environment incorporating common data analytics tools. Common GIS operations implemented in our DGGS data model outperformed the same operations computed using traditional geospatial data types. A case study into wildfire modelling demonstrates the capability for data integration and supporting big data geospatial analytics. These results indicate that DGGS data models have significant capability to solve some of the key outstanding problems related to geospatial data analytics, providing a common representation upon which fast and scalable algorithms can be built.}
}
@article{GUNDLA2016460,
title = {Creating NoSQL Biological Databases with Ontologies for Query Relaxation},
journal = {Procedia Computer Science},
volume = {91},
pages = {460-469},
year = {2016},
note = {Promoting Business Analytics and Quantitative Management of Technology: 4th International Conference on Information Technology and Quantitative Management (ITQM 2016)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.07.120},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916313138},
author = {Naresh Kumar Gundla and Zhengxin Chen},
keywords = {NoSQL databases, Query Relaxation, Ontology, MongoDB, AllgroGraph},
abstract = {The complexity of building biological databases is well-known and ontologies play an extremely important role in biological databases. However, much of the emphasis on the role of ontologies in biological databases has been on the construction of databases. In this paper, we explore a somewhat overlooked aspect regarding ontologies in biological databases, namely, how ontologies can be used to assist better database retrieval. In particular, we show how ontologies can be used to revise user submitted queries for query relaxation. In addition, since our research is conducted at today's “big data” era, our investigation is centered on NoSQL databases which serve as a kind of “representatives” of big data. This paper contains two major parts: First we describe our methodology of building two NoSQL application databases (MongoDB and AllegroGraph) using GO ontology, and then discuss how to achieve query relaxation through GO ontology. We report our experiments and show sample queries and results. Our research on query relaxation on NoSQL databases is complementary to existing work in big data and in biological databases and deserves further exploration.}
}
@article{MAHDAVINEJAD2018161,
title = {Machine learning for internet of things data analysis: a survey},
journal = {Digital Communications and Networks},
volume = {4},
number = {3},
pages = {161-175},
year = {2018},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2017.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S235286481730247X},
author = {Mohammad Saeid Mahdavinejad and Mohammadreza Rezvan and Mohammadamin Barekatain and Peyman Adibi and Payam Barnaghi and Amit P. Sheth},
keywords = {Machine learning, Internet of Things, Smart data, Smart City},
abstract = {Rapid developments in hardware, software, and communication technologies have facilitated the emergence of Internet-connected sensory devices that provide observations and data measurements from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As these numbers grow and technologies become more mature, the volume of data being published will increase. The technology of Internet-connected devices, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interactions between the physical and cyber worlds. In addition to an increased volume, the IoT generates big data characterized by its velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this big data are the key to developing smart IoT applications. This article assesses the various machine learning methods that deal with the challenges presented by IoT data by considering smart cities as the main use case. The key contribution of this study is the presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying a Support Vector Machine (SVM) to Aarhus smart city traffic data is presented for a more detailed exploration.}
}
@article{ALTURJMAN2020357,
title = {Intelligence and security in big 5G-oriented IoNT: An overview},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {357-368},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19301074},
author = {Fadi Al-Turjman},
keywords = {IoNT, Security, Big data, Design factors},
abstract = {Internet of Nano-Things (IoNT) overcomes critical difficulties and additionally open doors for wearable sensor based huge information examination. Conventional computing and/or communication systems do not offer enough flexibility and adaptability to deal with the gigantic amount of assorted information nowadays. This creates the need for legitimate components that can efficiently investigate and communicate the huge data while maintaining security and quality of service. In addition, while developing the ultra-wide Heterogeneous Networks (HetNets) associated with the ongoing Big Data project and 5G-based IoNT, it is required to resolve the emerging difficulties as well. Accordingly, these difficulties and other relevant design issues have been comprehensively reported in this survey. It mainly focuses on security issues and associated intelligence to be considered while managing these issues.}
}
@article{XIE202272,
title = {Real-World Data for Healthcare Research in China: Call for Actions},
journal = {Value in Health Regional Issues},
volume = {27},
pages = {72-81},
year = {2022},
issn = {2212-1099},
doi = {https://doi.org/10.1016/j.vhri.2021.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212109921000765},
author = {Jipan Xie and Eric Q. Wu and Shan Wang and Tao Cheng and Zhou Zhou and Jia Zhong and Larry Liu},
keywords = {administrative claims, data access, electronic health records, real-world data},
abstract = {Objectives
This study aimed to provide an overview of major data sources in China that can be potentially used for epidemiology, health economics, and outcomes research; compare them with similar data sources in other countries; and discuss future directions of healthcare data development in China.
Methods
The study was conducted in 2 phases. First, various data sources were identified through a targeted literature review and recommendations by experts. Second, an in-depth assessment was conducted to evaluate the strengths and limitations of administrative claims and electronic health record data, which were further compared with similar data sources in developed countries.
Results
Secondary databases, including administrative claims and electronic health records, are the major types of real-world data in China. There are substantial variations in available data elements even within the same type of databases. Compared with similar databases in developed countries, the secondary databases in China have some general limitations such as variations in data quality, unclear data usage mechanism, and lack of longitudinal follow-up data. In contrast, the large sample size and the potential to collect additional data based on research needs present opportunities to further improve real-world data in China.
Conclusions
Although healthcare data have expanded substantially in China, high-quality real-world evidence that can be used to facilitate decision making remains limited in China. To support the generation of real-world evidence, 2 fundamental issues in existing databases need to be addressed—data access/sharing and data quality.}
}
@article{SHET2021311,
title = {Examining the determinants of successful adoption of data analytics in human resource management – A framework for implications},
journal = {Journal of Business Research},
volume = {131},
pages = {311-326},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.03.054},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321002174},
author = {Sateesh.V. Shet and Tanuj Poddar and Fosso {Wamba Samuel} and Yogesh K. Dwivedi},
keywords = {Human resource analytics, HRM analytics, People analytics, Adoption of HR analytics, Challenges, Implementation of HR analytics, Big data, Data analytics, Framework synthesis},
abstract = {Data analytics has gained importance in human resource management (HRM) for its ability to provide insights based on data-driven decision-making processes. However, integrating an analytics-based approach in HRM is a complex process, and hence, many organizations are unable to adopt HR Analytics (HRA). Using a framework synthesis approach, we first identify the challenges that hinder the practice of HRA and then develop a framework to explain the different factors that impact the adoption of HRA within organizations. This study identifies the key aspects related to the technological, organizational, environmental, data governance, and individual factors that influence the adoption of HRA. In addition, this paper determines 23 sub-dimensions of these five factors as the crucial aspects for successfully implementing and practicing HRA within organizations. We also discuss the implications of the framework for HR leaders, HR Managers, CEOs, IT Managers and consulting practitioners for effective adoption of HRA in organization.}
}
@article{PAIS2019100194,
title = {An automated workflow for MALDI-ToF mass spectra pattern identification on large data sets: An application to detect aneuploidies from pregnancy urine},
journal = {Informatics in Medicine Unlocked},
volume = {16},
pages = {100194},
year = {2019},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2019.100194},
url = {https://www.sciencedirect.com/science/article/pii/S2352914819300851},
author = {Ricardo J. Pais and R. Zmuidinaite and S.A. Butler and R.K. Iles},
keywords = {MALDI-ToF, Pattern recognition, Quality control, Comparative intensity data, Automated processing},
abstract = {Urine from first trimester pregnancies has been found to be rich in information related to aneuploidies and other clinical conditions. Mass spectral analysis derived from matrix assisted laser desorption ionization (MALDI) time of flight (ToF) data has been proven to be a cost effective method for clinical diagnostics. However, urine mass spectra are complex and require data modelling frameworks. Therefore, computational approaches that systematically analyse big data generated from MALDI-ToF mass spectra are essential. To address this issue, we developed an automated workflow that successfully processed large data sets from MALDI-ToF which is 100-fold faster than using a common software tool. Our method performs accurate data quality control decisions, and generates a comparative analysis to extract peak intensity patterns from a data set. We successfully applied our framework to the identification of peak intensity patterns for Trisomy 21 and Trisomy 18 gestations on data sets from maternal pregnancy urines obtained in the UK and China. The results from our automated comparative analysis have shown characteristic patterns associated with aneuploidies in the first trimester pregnancy. Moreover, we have shown that the intensity patterns depended on the population origin, gestational age, and MALDI-ToF instrument.}
}
@article{JEONG2019358,
title = {Cohort profile: Beyond birth cohort study – The Korean CHildren's ENvironmental health Study (Ko-CHENS)},
journal = {Environmental Research},
volume = {172},
pages = {358-366},
year = {2019},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2018.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0013935118306388},
author = {Kyoung Sook Jeong and Suejin Kim and Woo Jin Kim and Hwan-Cheol Kim and Jisuk Bae and Yun-Chul Hong and Mina Ha and Kangmo Ahn and Ji-Young Lee and Yangho Kim and Eunhee Ha},
keywords = {Ko-CHENS, Children, Environment, Cohort profile, Birth cohort},
abstract = {The Korean CHildren's ENvironmental health Study (Ko-CHENS) is a nationwide prospective birth cohort showing the correlation between the environmental exposures and the health effects to prevent the environmental diseases in children, and it provides the guidelines for the environmental hazardous factors, applying the life-course approach to the environmental-health management system. The Ko-CHENS consists of 5000 Core and 65,000 Main Cohorts. The children in the Core Cohort are followed up at 6 months, every year before their admission into the elementary school, and every 3 years from the first year after this admission. The children in the Cohort will be followed up through the data links (Statistics Korea, National Health Insurance Service [NHIS], and Ministry of Education). The individual biospecimens will be analyzed for 19 substances. The long-term-storage biological samples will be used for the further substance analysis. The Ko-CHENS will investigate whether the environmental variables including the perinatal outdoor and indoor factors and the greenness contribute causally to the health outcomes in the children and adolescents. In addition to the individual surveys, the assessments of the outdoor exposures and health outcomes will use the national air-quality monitoring data and claim data of the NHIS, respectively. The two big-data forms of the Ko-CHENS are as follows: The Ko-CHENS data that can be linked with the nationally registered NHIS health-related database, including the medical utilization and the periodic health screening, and the birth/mortality database in the Statistics; the other is the Big-CHENS dataset that is based on the NHIS mother delivery code, for which the follow-up of almost 97% of the total birth population is expected. The Ko-CHENS is a very cost-effective study that fully exploits the existing national big-data systems with the data linkage.}
}
@article{AYVAZ2021114598,
title = {Predictive maintenance system for production lines in manufacturing: A machine learning approach using IoT data in real-time},
journal = {Expert Systems with Applications},
volume = {173},
pages = {114598},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114598},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421000397},
author = {Serkan Ayvaz and Koray Alpay},
keywords = {Predictive maintenance, Internet of things, Manufacturing systems, Artificial intelligence, Machine learning, Big data},
abstract = {In this study, a data driven predictive maintenance system was developed for production lines in manufacturing. By utilizing the data generated from IoT sensors in real-time, the system aims to detect signals for potential failures before they occur by using machine learning methods. Consequently, it helps address the issues by notifying operators early such that preventive actions can be taken prior to a production stop. In current study, the effectiveness of the system was also assessed using real-world manufacturing system IoT data. The evaluation results indicated that the predictive maintenance system was successful in identifying the indicators of potential failures and it can help prevent some production stops from happening. The findings of comparative evaluations of machine learning algorithms indicated that models of Random Forest, a bagging ensemble algorithm, and XGBoost, a boosting method, appeared to outperform the individual algorithms in the assessment. The best performing machine learning models in this study have been integrated into the production system in the factory.}
}
@article{FERREIRA2021757,
title = {How do data scientists and managers influence machine learning value creation?},
journal = {Procedia Computer Science},
volume = {181},
pages = {757-764},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.228},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002714},
author = {Humberto Ferreira and Pedro Ruivo and Carolina Reis},
keywords = {machine learning, business value, data scientists, managers, people factor},
abstract = {Corporations are leveraging machine learning (ML) to create business value (BV). So, it becomes relevant to not only ponder the antecedents that influence the ML BV process but also, the main actors that influence the creation of such value within organizations: data scientists and managers. Grounded in the dynamic-capabilities theory, a model is proposed and tested with 319 responses to a survey. While for both groups, platform maturity and data quality are equally important factors for financial performance, information intensity is an equally important factor for organizational performance. On one hand, data scientists care more about the catalytic effect of data quality on the relationship between platform maturity and financial performance, and the compatibility factor for organizational performance. On the other hand, managers care more about the feasibility factor for financial performance. The findings presented here offer insights on how data scientists and managers perceive the ML BV creation process.}
}
@article{POLYVYANYY2019345,
title = {A systematic approach for discovering causal dependencies between observations and incidents in the health and safety domain},
journal = {Safety Science},
volume = {118},
pages = {345-354},
year = {2019},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2019.04.045},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518316230},
author = {Artem Polyvyanyy and Anastasiia Pika and Moe T. Wynn and Arthur H.M. {ter Hofstede}},
keywords = {Big data, Data mining, Process mining, Proximity of events, Causality, Health and safety, Cause of incidents},
abstract = {The paper at hand motivates, proposes, demonstrates, and evaluates a novel systematic approach to discovering causal dependencies between events encoded in large arrays of data, called causality mining. The approach has emerged in the discussions with our project partner, an Australian public energy company. It was successfully evaluated in a case study with the project partner to extract valuable, and otherwise unknown, information on the causal dependencies between observations reported by the company’s employees as part of the organizational health and safety management practices and incidents that had occurred at the organization’s sites. The dependencies were derived based on the notion of proximity of the observations and incidents. The setup and results of the evaluation are reported in this paper. The new approach and the delivered insights aim at improving the overall health and safety culture of the project partner practices, as they can be applied to caution and, thus, prevent future incidents.}
}
@article{SOUIFI2022103666,
title = {Uncertainty of key performance indicators for Industry 4.0: A methodology based on the theory of belief functions},
journal = {Computers in Industry},
volume = {140},
pages = {103666},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103666},
url = {https://www.sciencedirect.com/science/article/pii/S016636152200063X},
author = {Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar},
keywords = {Industry 4.0, Performance management, Decision support, Big Data, Uncertainty modeling},
abstract = {For the past few years, we have been hearing about Industry 4.0 (or the fourth industrial revolution), which promises to improve productivity, flexibility, quality, customer satisfaction and employee well-being. To assess whether these goals are achieved, it is necessary to implement a performance management system (PMS). However, a PMS must take into account the various challenges associated with Industry 4.0, including the availability of large amounts of data. While it represents an opportunity for companies to improve performance, big data does not necessarily mean good data. It can be uncertain, imprecise, ambiguous, etc. Uncertainty is one of the major challenges and it is essential to take it into account when computing performance indicators to increase confidence in decision making. To address this issue, we propose a method to model uncertainty in key performance indicators (KPIs). Our work allows associating with each indicator an uncertainty noted m, computed on the basis of the theory of belief functions. The KPI and its associated uncertainty form a pair (KP I, m). The method developed allows calculating this uncertainty m for the input data of the performance management system. We show how these modeled uncertainties should be propagated to the KPIs. For these KPI uncertainties, we have defined rules to support decision-making. The method developed, based on the theory of belief functions, is part of a methodology we propose to define and extract smart data from massive data. To our knowledge, this is the first attempt to use this theory to model uncertain performance indicators. Our work has shown its effectiveness and its applicability to a case of bottle filling line simulation. In addition to these results, this work opens up new perspectives, particularly for taking uncertainty into account in expert opinions and in industrial risk assessment.}
}
@article{MORAN201942,
title = {Curious Feature Selection},
journal = {Information Sciences},
volume = {485},
pages = {42-54},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519301100},
author = {Michal Moran and Goren Gordon},
keywords = {Intrinsic motivation learning, Curiosity loop, Reinforcement learning, Big data, Data science, Feature selection},
abstract = {In state-of-the-art big-data applications, the process of building machine learning models can be very challenging due to continuous changes in data structures and the need for human interaction to tune the variables and models over time. Hence, expedited learning in rapidly changing environments is required. In this work, we address this challenge by implementing concepts from the field of intrinsically motivated computational learning, also known as artificial curiosity (AC). In AC, an autonomous agent acts to optimize its learning about itself and its environment by receiving internal rewards based on prediction errors. We present a novel method of intrinsically motivated learning, based on the curiosity loop, to learn the data structures in large and varied datasets. An autonomous agent learns to select a subset of relevant features in the data, i.e., feature selection, to be used later for model construction. The agent optimizes its learning about the data structure over time without requiring external supervision. We show that our method, called the Curious Feature Selection (CFS) algorithm, positively impacts the accuracy of learning models on three public datasets.}
}
@article{FRONZETTICOLLADON2019113075,
title = {Using social network and semantic analysis to analyze online travel forums and forecast tourism demand},
journal = {Decision Support Systems},
volume = {123},
pages = {113075},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113075},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301046},
author = {Andrea {Fronzetti Colladon} and Barbara Guardabascio and Rosy Innarella},
keywords = {Tourism forecasting, Social network analysis, Semantic analysis, Online community, Text mining, Big data},
abstract = {Forecasting tourism demand has important implications for both policy makers and companies operating in the tourism industry. In this research, we applied methods and tools of social network and semantic analysis to study user-generated content retrieved from online communities which interacted on the TripAdvisor travel forum. We analyzed the forums of 7 major European capital cities, over a period of 10 years, collecting more than 2,660,000 posts, written by about 147,000 users. We present a new methodology of analysis of tourism-related big data and a set of variables which could be integrated into traditional forecasting models. We implemented Factor Augmented Autoregressive and Bridge models with social network and semantic variables which often led to a better forecasting performance than univariate models and models based on Google Trend data. Forum language complexity and the centralization of the communication network – i.e. the presence of eminent contributors – were the variables that contributed more to the forecasting of international airport arrivals.}
}
@article{GE20201883,
title = {Developing the Quality Model for Collaborative Open Data},
journal = {Procedia Computer Science},
volume = {176},
pages = {1883-1892},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.228},
url = {https://www.sciencedirect.com/science/article/pii/S187705092032130X},
author = {Mouzhi Ge and Włodzimierz Lewoniewski},
keywords = {Data Quality, Quality Assessment, Collaborative Open Data, Wikipedia, Quality Model},
abstract = {Nowadays, the development of data sharing technologies allows to involve more people to collaboratively contribute knowledge on the Web. The shared knowledge is usually represented as Collaborative Open Data (COD), for example, Wikipedia is one of the well-known sources for COD. The Wikipedia articles can be written in different languages, updated in real time, and originated from a vast variety of editors. However, COD also bring different data quality problems such as data inconsistency and low data objectiveness due to the crowd-based and dynamic nature. These data quality problems such as biased information may lead to sentimental changes or social impacts. This paper therefore proposes a new measurement model to assess the quality of COD. In order to evaluate the proposed model, A preliminary experiment is conducted with a large scale of Wikipedia articles to validate the applicability and efficiency of this proposed quality model in the real-world scenario.}
}
@incollection{BERANGER201697,
title = {2 - Ethical Development of the Medical Datasphere},
editor = {Jérôme Béranger},
booktitle = {Big Data and Ethics},
publisher = {Elsevier},
pages = {97-166},
year = {2016},
isbn = {978-1-78548-025-6},
doi = {https://doi.org/10.1016/B978-1-78548-025-6.50002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480256500026},
author = {Jérôme Béranger},
keywords = {Algorithmic ethics, Architecture, Complex data, Ethical data mining, Ethical issues, Ethical-technical guidance, Evaluation, Medical datasphere, Neo-Platonic modeling},
abstract = {Abstract:
As Lucy Suchmann observed in 2011, through Lévi-Strauss, “…we are our tools…” and our personal health data are an integral part of us. In these circumstances, it becomes necessary to question the value of Big Data in the health sphere.}
}
@article{XU2021100860,
title = {Comparing differences in the spatiotemporal patterns between resident tourists and non-resident tourists using hotel check-in registers},
journal = {Tourism Management Perspectives},
volume = {39},
pages = {100860},
year = {2021},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2021.100860},
url = {https://www.sciencedirect.com/science/article/pii/S2211973621000738},
author = {Yuquan Xu and Xiaobin Ran and Yuewen Liu and Wei Huang},
keywords = {Big data, Spatiotemporal patterns, Resident tourists and non-resident tourists, Multiple city travel, Hotel check-in registers},
abstract = {Previous research studied the spatiotemporal patterns in different visitor segments but lacks evidence of the segmentation of resident tourists and non-resident tourists in multi-city travel. To fill this gap, this study conducts a big data study using hotel check-in registers. The exploratory data analysis visualizes the spatiotemporal patterns and the differences between resident tourists and non-resident tourists. Then, the spatiotemporal patterns are measured by the length of stay and the number of visited cities. The regression shows that both the length of stay and the number of visited cities of non-resident tourists are higher than those of resident tourists. Moreover, non-resident tourists reduce their length of stay and their number of visited cities more than resident tourists on three-day holidays, while they increase their number of visited cities less than resident tourists on seven-day holidays. This study has significant implications for understanding spatiotemporal patterns and visitors' segmentations.}
}
@article{AUFAURE2016100,
title = {From Business Intelligence to semantic data stream management},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {100-107},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003635},
author = {Marie-Aude Aufaure and Raja Chiky and Olivier Curé and Houda Khrouf and Gabriel Kepeklian},
keywords = {Data stream, Linked Data, Business Intelligence, Stream reasoning},
abstract = {The Semantic Web technologies are being increasingly used for exploiting relations between data. In addition, new tendencies of real-time systems, such as social networks, sensors, cameras or weather information, are continuously generating data. This implies that data and links between them are becoming extremely vast. Such huge quantity of data needs to be analyzed, processed, as well as stored if necessary. In this position paper, we will introduce recent work on Real-Time Business Intelligence combined with semantic data stream management. We will present underlying approaches such as continuous queries, data summarization and matching, and stream reasoning.}
}
@article{SHAO2022102736,
title = {IoT data visualization for business intelligence in corporate finance},
journal = {Information Processing & Management},
volume = {59},
number = {1},
pages = {102736},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102736},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321002181},
author = {Cuili Shao and Yonggang Yang and Sapna Juneja and Tamizharasi GSeetharam},
keywords = {IoT, Data visualization, Business intelligence, Corporate finance},
abstract = {Business intelligence (BI) incorporates business research, data mining, data visualization, data tools,infrastructure, and best practices to help businesses make more data-driven choices.Business intelligence's challenging characteristics include data breaches, difficulty in analyzing different data sources, and poor data quality is consideredessential factors. In this paper, IoT-based Efficient Data Visualization Framework (IoT- EDVF) has been proposed to strengthen leaks' risk, analyze multiple data sources, and data quality management for business intelligence in corporate finance.Corporate analytics management is introduced to enhance the data analysis system's risk, and the complexity of different sources can allow accessing Business Intelligence. Financial risk analysis is implemented to improve data quality management initiative helps use main metrics of success, which are essential to the individual needs and objectives. The statistical outcomes of the simulation analysis show the increasedperformance with a lower delay response of 5ms and improved revenue analysis with the improvement of 29.42% over existing models proving the proposed framework's reliability.}
}
@article{KAYABAY2022121264,
title = {Data science roadmapping: An architectural framework for facilitating transformation towards a data-driven organization},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121264},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121264},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006983},
author = {Kerem Kayabay and Mert Onuralp Gökalp and Ebru Gökalp and P. {Erhan Eren} and Altan Koçyiğit},
keywords = {Technology roadmapping, Technology management, Data science, Digital transformation, Data-driven organization, Big data},
abstract = {Leveraging data science can enable businesses to exploit data for competitive advantage by generating valuable insights. However, many industries cannot effectively incorporate data science into their business processes, as there is no comprehensive approach that allows strategic planning for organization-wide data science efforts and data assets. Accordingly, this study explores the Data Science Roadmapping (DSR) to guide organizations in aligning their business strategies with data-related, technological, and organizational resources. The proposed approach is built on the widely adopted technology roadmapping framework and customizes its context, architecture, and process by synthesizing data science, big data, and data-driven organization literature. Based on industry collaborations, the framework provides a hybrid and agile methodology comprising the recommended steps. We applied DSR with a research group with sector experience to create a comprehensive data science roadmap to validate and refine the framework. The results indicate that the framework facilitates DSR initiatives by creating a comprehensive roadmap capturing strategy, data, technology, and organizational perspectives. The contemporary literature illustrates prebuilt roadmaps to help businesses become data-driven. However, becoming data-driven also necessitates significant social change toward openness and trust. The DSR initiative can facilitate this social change by opening communication channels, aligning perspectives, and generating consensus among stakeholders.}
}
@article{WANG202151,
title = {The national multi-center artificial intelligent myopia prevention and control project},
journal = {Intelligent Medicine},
volume = {1},
number = {2},
pages = {51-55},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621000085},
author = {Xun Wang and Yahan Yang and Yuxuan Wu and Wenbin Wei and Li Dong and Yang Li and Xingping Tan and Hankun Cao and Hong Zhang and Xiaodan Ma and Qin Jiang and Yunfan Zhou and Weihua Yang and Chaoyu Li and Yu Gu and Lin Ding and Yanli Qin and Qi Chen and Lili Li and Mingyue Lian and Jin Ma and Dongmei Cui and Yuanzhou Huang and Wenyan Liu and Xiao Yang and Shuiming Yu and Jingjing Chen and Dongni Wang and Zhenzhe Lin and Pisong Yan and Haotian Lin},
keywords = {Myopia prevention and control, Artificial intelligent, National multicenter project},
abstract = {In recent years, the incidence of myopia has increased at an alarming rate among children and adolescents in China. The exploration of an effective prevention and control method for myopia is in urgent need. With the development of information technology in the past decade, artificial intelligence with the Internet of Things technology (AIoT) is characterized by strong computing power, advanced algorithm, continuous monitoring, and accurate prediction of long-term progression. Therefore, big data and artificial intelligence technology have the potential to be applied to data mining of myopia etiology and prediction of myopia occurrence and development. More recently, there has been a growing recognition that myopia study involving AIoT needs to undergo a rigorous evaluation to demonstrate robust results.}
}
@article{SMIDT20211018,
title = {The challenge of privacy and security when using technology to track people in times of COVID-19 pandemic},
journal = {Procedia Computer Science},
volume = {181},
pages = {1018-1026},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.281},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003306},
author = {Hermanus J Smidt and Osden Jokonya},
keywords = {COVID 19, tracking, society, technology, privacy},
abstract = {Since the start of the Coronavirus disease 2019 (COVID-19) governments and health authorities across the world have find it very difficult in controlling infections. Digital technologies such as artificial intelligence (AI), big data, cloud computing, blockchain and 5G have effectively improved the efficiency of efforts in epidemic monitoring, virus tracking, prevention, control and treatment. Surveillance to halt COVID-19 has raised privacy concerns, as many governments are willing to overlook privacy implications to save lives. The purpose of this paper is to conduct a focused Systematic Literature Review (SLR), to explore the potential benefits and implications of using digital technologies such as AI, big data and cloud to track COVID-19 amongst people in different societies. The aim is to highlight the risks of security and privacy to personal data when using technology to track COVID-19 in societies and identify ways to govern these risks. The paper uses the SLR approach to examine 40 articles published during 2020, ultimately down selecting to the most relevant 24 studies. In this SLR approach we adopted the following steps; formulated the problem, searched the literature, gathered information from studies, evaluated the quality of studies, analysed and integrated the outcomes of studies while concluding by interpreting the evidence and presenting the results. Papers were classified into different categories such as technology use, impact on society and governance. The study highlighted the challenge for government to balance the need of what is good for public health versus individual privacy and freedoms. The findings revealed that although the use of technology help governments and health agencies reduce the spread of the COVID-19 virus, government surveillance to halt has sparked privacy concerns. We suggest some requirements for government policy to be ethical and capable of commanding the trust of the public and present some research questions for future research.}
}
@article{ESCOBAR2020103378,
title = {Adding value to Linked Open Data using a multidimensional model approach based on the RDF Data Cube vocabulary},
journal = {Computer Standards & Interfaces},
volume = {68},
pages = {103378},
year = {2020},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2019.103378},
url = {https://www.sciencedirect.com/science/article/pii/S0920548919300480},
author = {Pilar Escobar and Gustavo Candela and Juan Trujillo and Manuel Marco-Such and Jesús Peral},
keywords = {Linked Open Data, Multidimensional modelling, Conceptual modelling, RDF Data Cube vocabulary, Semantic web, Big data},
abstract = {Most organisations using Open Data currently focus on data processing and analysis. However, although Open Data may be available online, these data are generally of poor quality, thus discouraging others from contributing to and reusing them. This paper describes an approach to publish statistical data from public repositories by using Semantic Web standards published by the W3C, such as RDF and SPARQL, in order to facilitate the analysis of multidimensional models. We have defined a framework based on the entire lifecycle of data publication including a novel step of Linked Open Data assessment and the use of external repositories as knowledge base for data enrichment. As a result, users are able to interact with the data generated according to the RDF Data Cube vocabulary, which makes it possible for general users to avoid the complexity of SPARQL when analysing data. The use case was applied to the Barcelona Open Data platform and revealed the benefits of the application of our approach, such as helping in the decision-making process.}
}
@incollection{GUDIVADA201731,
title = {Chapter 2 - Data Analytics: Fundamentals},
editor = {Mashrur Chowdhury and Amy Apon and Kakan Dey},
booktitle = {Data Analytics for Intelligent Transportation Systems},
publisher = {Elsevier},
pages = {31-67},
year = {2017},
isbn = {978-0-12-809715-1},
doi = {https://doi.org/10.1016/B978-0-12-809715-1.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012809715100002X},
author = {Venkat N. Gudivada},
keywords = {Data analytics, data science, data mining, clustering, classification, model building},
abstract = {This chapter provides a comprehensive and unified view of data analytics fundamentals. Four functional facets of data analytics—descriptive, diagnostic, predictive, and prescriptive—are described. The evolution of data analytics from SQL analytics, business analytics, visual analytics, big data analytics, to cognitive analytics is presented. Data science as the foundational discipline for the current generation of data analytics systems is explored in this chapter. Data lifecycle and data quality issues are outlined. Open source tools and resources for developing data analytics systems are listed. Future directions in data analytics are indicated. The chapter concludes by providing a summary. To reinforce and enhance the reader’s data analytics knowledge and tools, questions and exercise problems are provided at the end of the chapter.}
}
@article{LARSON2016700,
title = {A review and future direction of agile, business intelligence, analytics and data science},
journal = {International Journal of Information Management},
volume = {36},
number = {5},
pages = {700-710},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S026840121630233X},
author = {Deanne Larson and Victor Chang},
keywords = {Agile methodologies, Business intelligence (BI), Analytics and big data, Lifecycle for BI and Big Data},
abstract = {Agile methodologies were introduced in 2001. Since this time, practitioners have applied Agile methodologies to many delivery disciplines. This article explores the application of Agile methodologies and principles to business intelligence delivery and how Agile has changed with the evolution of business intelligence. Business intelligence has evolved because the amount of data generated through the internet and smart devices has grown exponentially altering how organizations and individuals use information. The practice of business intelligence delivery with an Agile methodology has matured; however, business intelligence has evolved altering the use of Agile principles and practices. The Big Data phenomenon, the volume, variety, and velocity of data, has impacted business intelligence and the use of information. New trends such as fast analytics and data science have emerged as part of business intelligence. This paper addresses how Agile principles and practices have evolved with business intelligence, as well as its challenges and future directions.}
}
@incollection{HOVENGA2022209,
title = {Chapter 9 - Quality data, design, implementation, and governance},
editor = {Evelyn Hovenga and Heather Grain},
booktitle = {Roadmap to Successful Digital Health Ecosystems},
publisher = {Academic Press},
pages = {209-237},
year = {2022},
isbn = {978-0-12-823413-6},
doi = {https://doi.org/10.1016/B978-0-12-823413-6.00013-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234136000136},
author = {Evelyn Hovenga and Heather Grain},
keywords = {Data quality, Data design, System implementation, Information governance, Leadership, Data supply chain},
abstract = {Data is the core for digital health systems; that data needs to be accurate, consistent, and available. The health workforce needs to understand how to define and govern data quality throughout the data supply chain, from origin through sharing, aggregated reporting, and eventually to enable the discovery of new knowledge based upon that data. Data quality applies to the data supply chain as a whole. Data needs to be collectable and useful at origin and able to represent and explain things that may not be known at the time of collection. Consistency of concept representation throughout the data supply chain needs to be clearly specified and transparent. Professional bodies need to provide leadership into the specification and governance of data, information, and computable knowledge, augmenting their traditional role of knowledge acquisition and publication based upon evidence. Throughout all levels of healthcare, the necessity of data quality needs to be better understood and managed.}
}
@incollection{SVAB2021274,
title = {Complexity of Patient Data in Primary Care Practice},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {274-282},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11590-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383115909},
author = {Igor Švab},
keywords = {Big data, Digital health, Family medicine, Genomics, Primary care},
abstract = {The chapter deals with the practical implications of big data in clinical practice, especially primary care. Family medicine has always advocated individualized approach to patient care. Medicine is changing rapidly for different reasons. One of the reasons is the development of new technologies which is going to radically change medical practice in the future. One of the key changes will involve the importance and practice of data management. The traditional data management that was based on paper records is being changed to the electronic medical record which offers great potential for patient management. This transition will also give rise to new challenges to the practising physician. We are facing the challenge of new sources of data, their increase and variety. Currently, all these data is stored in different locations and there is no consensus whether one single profession is going to take responsibility for managing the data of the patient. If this is decided, the primary care practice would seem a logical solution. In order to do this would involve challenges to healthcare policy and infrastructure. The big data approach to medical care gives rise to new ethical challenges that we would have to address. Existing and future physicians will have to be educated in order to address all these issues for the benefit of their patients. Nevertheless, the physicians should still remember that even with the vast development of precision medicine, the patient will still be more than just a collection of data.}
}
@article{CHENG2021102938,
title = {Construction of a service quality scale for the online food delivery industry},
journal = {International Journal of Hospitality Management},
volume = {95},
pages = {102938},
year = {2021},
issn = {0278-4319},
doi = {https://doi.org/10.1016/j.ijhm.2021.102938},
url = {https://www.sciencedirect.com/science/article/pii/S0278431921000815},
author = {Ching-Chan Cheng and Ya-Yuan Chang and Cheng-Ta Chen},
keywords = {Online food delivery, Service quality, Big data analytic, OFD service quality scale},
abstract = {The main purpose of this study is based on qualitative and quantitative research procedures, and integrates the key service factors for the online food delivery (OFD) industry extracted by Internet Big Data Analytics (IBDA) to construct a OFD service quality scale (OFD-SERV). This study takes OFD customers in Taipei City as the objects. The results show that 20 key service factors for the OFD industry are extracted through IBDA. The OFD-SERV scale contains six dimensions including reliability, maintenance of meal quality and hygiene, assurance, security, system operation and traceability, a total of 28 items. The results from the structural equation modeling showed that the reliability, assurance and system operation have a positive impact on customer satisfaction. Finally, the findings provide knowledge and inspiration for the current OFD, and enable OFD operators and future researchers to more accurately identify the deficiency of service quality.}
}
@article{VIDGEN2017626,
title = {Management challenges in creating value from business analytics},
journal = {European Journal of Operational Research},
volume = {261},
number = {2},
pages = {626-639},
year = {2017},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2017.02.023},
url = {https://www.sciencedirect.com/science/article/pii/S0377221717301455},
author = {Richard Vidgen and Sarah Shaw and David B. Grant},
keywords = {Analytics, Delphi, Management challenges, Value creation, Ecosystem},
abstract = {The popularity of big data and business analytics has increased tremendously in the last decade and a key challenge for organizations is in understanding how to leverage them to create business value. However, while the literature acknowledges the importance of these topics little work has addressed them from the organization's point of view. This paper investigates the challenges faced by organizational managers seeking to become more data and information-driven in order to create value. Empirical research comprised a mixed methods approach using (1) a Delphi study with practitioners through various forums and (2) interviews with business analytics managers in three case organizations. The case studies reinforced the Delphi findings and highlighted several challenge focal areas: organizations need a clear data and analytics strategy, the right people to effect a data-driven cultural change, and to consider data and information ethics when using data for competitive advantage. Further, becoming data-driven is not merely a technical issue and demands that organizations firstly organize their business analytics departments to comprise business analysts, data scientists, and IT personnel, and secondly align that business analytics capability with their business strategy in order to tackle the analytics challenge in a systemic and joined-up way. As a result, this paper presents a business analytics ecosystem for organizations that contributes to the body of scholarly knowledge by identifying key business areas and functions to address to achieve this transformation.}
}
@incollection{KRISHNAN2020157,
title = {9 - Governance},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {157-174},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00009-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000090},
author = {Krish Krishnan},
keywords = {Big data application, Data management, Data-driven architecture, Governance, Machine learning, Master data, Metadata},
abstract = {Building the big data application is very interesting and can provide multiple users with multiple perspectives, data discovery to end state analytics and beyond is very much what everybody wants to achieve. Enterprises are ready to spend millions of dollars to get a share of your wallet, they want to be a part of your life and be present at every event that gets your attention. They want to leverage their partnerships and influence you, how do they make this all happen? The most successful companies will tell you their story is built on governance. The aspect of governance is very critical to the success of this journey whether internal or external. What governance are we talking about? How do we implement the same? This chapter will focus on those aspects.}
}
@article{CORRALESGARAY201977,
title = {Knowledge areas, themes and future research on open data: A co-word analysis},
journal = {Government Information Quarterly},
volume = {36},
number = {1},
pages = {77-87},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18303216},
author = {Diego Corrales-Garay and Marta Ortiz-de-Urbina-Criado and Eva-María Mora-Valentín},
keywords = {Open data, Bibliometric analysis, Co-word analysis, Science map, Knowledge areas, Most-studied themes, Future trends},
abstract = {This paper aims to contribute to a better understanding of the literature on open data in three ways. The first is to develop a descriptive analysis of journals and authors to identify the knowledge areas in which open data are applied. The second is to analyse the conceptual structure of the field using a bibliometric technique. The co-word analysis enabled us to create a map of the main themes that have been studied, identifying their importance and relevance. These themes were analysed and grouped. The third is to propose future research trends. According to our results, the main knowledge areas are Engineering, Health, Public Administration, Management and Education. The main themes are big data, open-linked data and data reuse. Finally, several research questions are proposed according to knowledge area and theme.}
}
@article{LI20161,
title = {A snail shell process model for knowledge discovery via data analytics},
journal = {Decision Support Systems},
volume = {91},
pages = {1-12},
year = {2016},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2016.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167923616301233},
author = {Yan Li and Manoj A. Thomas and Kweku-Muata Osei-Bryson},
keywords = {Knowledge discovery via data analytics, Snail shell process model, KDDA, Big data analytics, Data-driven decision making},
abstract = {The rapid growth of big data environment imposes new challenges that traditional knowledge discovery and data mining process (KDDM) models are not adequately suited to address. We propose a snail shell process model for knowledge discovery via data analytics (KDDA) to address these challenges. We evaluate the utility of the KDDA process model using real-world analytic case studies at a global multi-media company. By comparing against traditional KDDM models, we demonstrate the need and relevance of the snail shell model, particularly in addressing faster turnaround and frequent model updates that characterize knowledge discovery in the big data environment.}
}
@incollection{FARRE2022197,
title = {Chapter 7 - Data-driven policy evaluation},
editor = {Didier Grimaldi and Carlos Carrasco-Farré},
booktitle = {Implementing Data-Driven Strategies in Smart Cities},
publisher = {Elsevier},
pages = {197-225},
year = {2022},
isbn = {978-0-12-821122-9},
doi = {https://doi.org/10.1016/B978-0-12-821122-9.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128211229000026},
author = {Marçal Farré and Federico Todeschini and Didier Grimaldi and Carlos Carrasco-Farré},
keywords = {Survey data, Administrative data, Big data, Policy evaluation, Impact evaluation, Process evaluation},
abstract = {Public policies should be designed and implemented, whenever possible, using evidence as rigorous as possible. Urban interventions then should be no exception. In recent times, we have witnessed increasing efforts to transform information into knowledge, and thus help policymakers make better decisions. In this chapter, we will explore how public policy evaluation helps municipal governments tackle social problems and how big data can improve the design and implementation of more effective, efficient, and transparent policies.}
}
@article{ZHANG2020104512,
title = {Blockchain-based life cycle assessment: An implementation framework and system architecture},
journal = {Resources, Conservation and Recycling},
volume = {152},
pages = {104512},
year = {2020},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2019.104512},
url = {https://www.sciencedirect.com/science/article/pii/S0921344919304185},
author = {Abraham Zhang and Ray Y Zhong and Muhammad Farooque and Kai Kang and V G Venkatesh},
keywords = {Blockchain, Life cycle assessment, Supply chain sustainability, Environmental sustainability, Operational excellence},
abstract = {Life cycle assessment (LCA) is widely used for assessing the environmental impacts of a product or service. Collecting reliable data is a major challenge in LCA due to the complexities involved in the tracking and quantifying inputs and outputs at multiple supply chain stages. Blockchain technology offers an ideal solution to overcome the challenge in sustainable supply chain management. Its use in combination with internet-of-things (IoT) and big data analytics and visualization can help organizations achieve operational excellence in conducting LCA for improving supply chain sustainability. This research develops a framework to guide the implementation of Blockchain-based LCA. It proposes a system architecture that integrates the use of Blockchain, IoT, and big data analytics and visualization. The proposed implementation framework and system architecture were validated by practitioners who were experienced with Blockchain applications. The research also analyzes system implementation costs and discusses potential issues and solutions, as well as managerial and policy implications.}
}
@article{BUTTERWORTH2018257,
title = {The ICO and artificial intelligence: The role of fairness in the GDPR framework},
journal = {Computer Law & Security Review},
volume = {34},
number = {2},
pages = {257-268},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2018.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S026736491830044X},
author = {Michael Butterworth},
keywords = {Artificial intelligence (AI), Big data analytics, General Data Protection Regulation (GDPR), Fairness, Regulations, Collective rights, Data ethics},
abstract = {The year 2017 has seen many EU and UK legislative initiatives and proposals to consider and address the impact of artificial intelligence on society, covering questions of liability, legal personality and other ethical and legal issues, including in the context of data processing. In March 2017, the Information Commissioner's Office (UK) updated its big data guidance to address the development of artificial intelligence and machine learning, and to provide (GDPR), which will apply from 25 May 2018. This paper situates the ICO's guidance in the context of wider legal and ethical considerations and provides a critique of the position adopted by the ICO. On the ICO's analysis, the key challenge for artificial intelligence processing personal data is in establishing that such processing is fair. This shift reflects the potential for artificial intelligence to have negative social consequences (whether intended or unintended) that are not otherwise addressed by the GDPR. The question of ‘fairness’ is an important one, to address the imbalance between big data organisations and individual data subjects, with a number of ethical and social impacts that need to be evaluated.}
}
@article{APPELBAUM201729,
title = {Impact of business analytics and enterprise systems on managerial accounting},
journal = {International Journal of Accounting Information Systems},
volume = {25},
pages = {29-44},
year = {2017},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2017.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1467089517300490},
author = {Deniz Appelbaum and Alexander Kogan and Miklos Vasarhelyi and Zhaokai Yan},
keywords = {Managerial accounting, Business analytics, Big data, Enterprise systems, Business intelligence},
abstract = {The nature of management accountants' responsibility is evolving from merely reporting aggregated historical value to also including organizational performance measurement and providing management with decision related information. Corporate information systems such as enterprise resource planning (ERP) systems have provided management accountants with both expanded data storage power and enhanced computational power. With big data extracted from both internal and external data sources, management accountants now could utilize data analytics techniques to answer the questions including: what has happened (descriptive analytics), what will happen (predictive analytics), and what is the optimized solution (prescriptive analytics). However, research shows that the nature and scope of managerial accounting has barely changed and that management accountants employ mostly descriptive analytics, some predictive analytics, and a bare minimum of prescriptive analytics. This paper proposes a Managerial Accounting Data Analytics (MADA) framework based on the balanced scorecard theory in a business intelligence context. MADA provides management accountants the ability to utilize comprehensive business analytics to conduct performance measurement and provide decision related information. With MADA, three types of business analytics (descriptive, predictive, and prescriptive) are implemented into four corporate performance measurement perspectives (financial, customer, internal process, and learning and growth) in an enterprise system environment. Other related issues that affect the successful utilization of business analytics within a corporate-wide business intelligence (BI) system, such as data quality and data integrity, are also discussed. This paper contributes to the literature by discussing the impact of business analytics on managerial accounting from an enterprise systems and BI perspective and by providing the Managerial Accounting Data Analytics (MADA) framework that incorporates balanced scorecard methodology.}
}
@article{ESCOBAR2021748,
title = {Quality 4.0 — Green, Black and Master Black Belt Curricula},
journal = {Procedia Manufacturing},
volume = {53},
pages = {748-759},
year = {2021},
note = {49th SME North American Manufacturing Research Conference (NAMRC 49, 2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.06.085},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921001086},
author = {Carlos A. Escobar and Debejyo Chakraborty and Megan McGovern and Daniela Macias and Ruben Morales-Menendez},
keywords = {Quality 4.0, Certification, Smart manufacturing, Artificial intelligence, Big data},
abstract = {Industrial Big Data (IBD) and Artificial Intelligence (AI) are propelling the new era of manufacturing - smart manufacturing. Manufacturing companies can competitively position themselves amongst the most advanced and influential companies by successfully implementing Quality 4.0 practices. Despite the global impact of COVID-19 and the low deployment success rate, industrialization of the AI mega-trend has dominated the business landscape in 2020. Although these technologies have the potential to advance quality standards, it is not a trivial task. A significant portion of quality leaders do not yet have a clear deployment strategy and universally cite difficulty in harnessing such technologies. The lack of people power is one of the biggest challenges. From a career development standpoint, the higher-educated employees (such as engineers) are the most exposed to, and thus affected by, these new technologies. 79% of young professionals have reported receiving training outside of formal schooling to acquire the necessary skills for Industry 4.0. Strategically investing in training is thus important for manufacturing companies to generate value from IBD and AI. Following the path traced by Six Sigma, this article presents a certification curricula for Green, Black, and Master Black Belts. The proposed curriculum combines six areas of knowledge: statistics, quality, manufacturing, programming, learning, and optimization. These areas, along with an ad hoc 7-step problem solving strategy, must be mastered to obtain a certification. Certified professionals will be well positioned to deploy Quality 4.0 technologies and strategies. They will have the capacity to identify engineering intractable problems that can be formulated as machine learning problems and successfully solve them. These certifications are an efficient and effective way for professionals to advance in their career and thrive in Industry 4.0.}
}
@article{CHEN20151331,
title = {Towards Integrated Study of Data Management and Data Mining},
journal = {Procedia Computer Science},
volume = {55},
pages = {1331-1339},
year = {2015},
note = {3rd International Conference on Information Technology and Quantitative Management, ITQM 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.117},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915015926},
author = {Zhengxin Chen},
keywords = {Big Data, granular computing, granularity, databases, rough set theory, database keyword search (DBKWS)},
abstract = {From very beginning, research and practice of database management systems (DBMSs) have been cantered on handling granulation and granularities at various levels, thus sharing common interests with granular computing (GrC). Although DBMS and GrC have different focuses, the advent of Big Data has brought these two research areas closer to each other, because Big Data requires integrated study of data storage and analysis. In this paper, we explore this issue. Starting with an examination of granularities from a database perspective, we discuss new challenges of Big Data. We then turn to data management issues related to GrC. As an example of possible cross-fertilization of these two fields, we examine the recent development of database keyword search (DBKWS). Even research in DBKWS is largely independent to GrC, DBKWS has to handle various issues related to granularity handling. In particular, aggregation of DBKWS results is closely related to studies in granularities and granulation, which echoes L. Zadeh's famous formula: Granulation = Summarization. We present our proposed approach, termed as extended keyword search, which illustrates that an integrated study of data management and data mining/analysis is not restricted to GrC or rough set theory}
}
@article{RAGUSEO2021103451,
title = {Streams of digital data and competitive advantage: The mediation effects of process efficiency and product effectiveness},
journal = {Information & Management},
volume = {58},
number = {4},
pages = {103451},
year = {2021},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2021.103451},
url = {https://www.sciencedirect.com/science/article/pii/S0378720621000252},
author = {Elisabetta Raguseo and Federico Pigni and Claudio Vitari},
keywords = {Streams of big data, Process efficiency, Product effectiveness, Competitive advantage},
abstract = {Firms can achieve a competitive advantage by leveraging real-time Digital Data Streams (DDSs). The ability to profit from DDSs is emerging as a critical competency for firms and a novel area for Information Technology (IT) investments. We examine the relationship between DDS readiness and competitive advantage by studying the mediation effect of product effectiveness and process efficiency. The research model is tested with data obtained from 302 companies, and the results confirm the existence of the mediation effects. Interestingly, we confirm that competitive advantage is more significantly impacted by IT investments affecting product effectiveness than those affecting process efficiency.}
}
@article{SILVA2015289,
title = {Workshop Synthesis: Respondent/Survey Interaction in a World of Web and Smartphone Apps},
journal = {Transportation Research Procedia},
volume = {11},
pages = {289-296},
year = {2015},
note = {Transport Survey Methods: Embracing Behavioural and Technological Changes Selected contributions from the 10th International Conference on Transport Survey Methods 16-21 November 2014, Leura, Australia},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2015.12.025},
url = {https://www.sciencedirect.com/science/article/pii/S2352146515003166},
author = {João de Abreu e Silva and Mark Davis},
keywords = {web based surveys, smartphone surveys, respondent interaction, respondent burden},
abstract = {Web and smartphone surveys are increasingly being used to collect travel information. This workshop explored respondent interaction with these tools, covering a range of research concerns. While smartphone surveys facilitate real-time passive collection of continuous data, thereby reducing respondent burden, their use raises many issues common with those present in web surveys. These include survey design, sample representativeness, privacy, respondent burden, data quality and validation. Workshop participants considered possible areas for future research on these issues and others such as provision of feedback to respondents, linking with big data and focusing on attitudinal and behavioural motivations.}
}
@article{OZKAN2019208,
title = {Criminology in the age of data explosion: New directions},
journal = {The Social Science Journal},
volume = {56},
number = {2},
pages = {208-219},
year = {2019},
issn = {0362-3319},
doi = {https://doi.org/10.1016/j.soscij.2018.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0362331918301514},
author = {Turgut Ozkan},
keywords = {Social science, Big data, Crime, Social media, Data-driven social science},
abstract = {This review discusses practical benefits and limitations of novel data-driven research for social scientists in general and criminologists in particular by providing a comprehensive examination of the matter. Specifically, this study is an attempt to critically evaluate ‘big data’, data-driven perspectives, and their epistemological value for both scholars and practitioners, particularly those working on crime. It serves as guidance for those who are interested in data-driven research by pointing out new research avenues. In addition to the benefits, the drawbacks associated with data-driven approaches are also discussed. Finally, critical problems that are emerging in this era, such as privacy and ethical concerns are highlighted.}
}
@article{ANJUM2018326,
title = {Privacy preserving data by conceptualizing smart cities using MIDR-Angelization},
journal = {Sustainable Cities and Society},
volume = {40},
pages = {326-334},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2018.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717314646},
author = {Adeel Anjum and Tahir Ahmed and Abid Khan and Naveed Ahmad and Mansoor Ahmad and Muhammad Asif and Alavalapati Goutham Reddy and Tanzila Saba and Nayma Farooq},
keywords = {Big data, IoT data management, Disclosure risk, HIPAA, Patient privacy, Re-identification risk, Smart city},
abstract = {Smart City and IoT improves the performance of health, transportation, energy and reduce the consumption of resources. Among the smart city services, Big Data analytics is one of the imperative technologies that have a vast perspective to reach sustainability, enhanced resilience, effective quality of life and quick management of resources. This paper focuses on the privacy of big data in the context of smart health to support smart cities. Furthermore, the trade-off between the data privacy and utility in big data analytics is the foremost concern for the stakeholders of a smart city. The majority of smart city application databases focus on preserving the privacy of individuals with different disease data. In this paper, we propose a trust-based hybrid data privacy approach named as “MIDR-Angelization” to assure privacy and utility in big data analytics when sharing same disease data of patients in IoT industry. Above all, this study suggests that privacy-preserving policies and practices to share disease and health information of patients having the same disease should consider detailed disease information to enhance data utility. An extensive experimental study performed on a real-world dataset to measure instance disclosure risk which shows that the proposed scheme outperforms its counterpart in terms of data utility and privacy.}
}
@article{REIS2015238,
title = {Integrating modelling and smart sensors for environmental and human health},
journal = {Environmental Modelling & Software},
volume = {74},
pages = {238-246},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S136481521500167X},
author = {Stefan Reis and Edmund Seto and Amanda Northcross and Nigel W.T. Quinn and Matteo Convertino and Rod L. Jones and Holger R. Maier and Uwe Schlink and Susanne Steinle and Massimo Vieno and Michael C. Wimberly},
keywords = {Integrated modelling, Environmental sensors, Population health, Environmental health, Big data},
abstract = {Sensors are becoming ubiquitous in everyday life, generating data at an unprecedented rate and scale. However, models that assess impacts of human activities on environmental and human health, have typically been developed in contexts where data scarcity is the norm. Models are essential tools to understand processes, identify relationships, associations and causality, formalize stakeholder mental models, and to quantify the effects of prevention and interventions. They can help to explain data, as well as inform the deployment and location of sensors by identifying hotspots and areas of interest where data collection may achieve the best results. We identify a paradigm shift in how the integration of models and sensors can contribute to harnessing ‘Big Data’ and, more importantly, make the vital step from ‘Big Data’ to ‘Big Information’. In this paper, we illustrate current developments and identify key research needs using human and environmental health challenges as an example.}
}
@article{WU2020116388,
title = {Impact factors of the real-world fuel consumption rate of light duty vehicles in China},
journal = {Energy},
volume = {190},
pages = {116388},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2019.116388},
url = {https://www.sciencedirect.com/science/article/pii/S0360544219320833},
author = {Tian Wu and Xiao Han and M. Mocarlo Zheng and Xunmin Ou and Hongbo Sun and Xiong Zhang},
keywords = {Real-world fuel consumption rate, Energy consumption, Private passenger vehicles, Big data, China},
abstract = {Measuring real-world fuel consumption of light duty vehicles can be challenging due to the limited collection of actual data. In this paper, we use big data retrieved from the record of real-world fuel consumptions of different brands of vehicles in different areas (n = 106,809 samples from 201 brands of vehicles and 34 cities) in China to build up a real-world fuel consumption rate (RFCR) model to estimate the fuel consumption given the driving conditions and figure out the main factors that affect actual fuel consumption in the real world. We find the average deviation of actual fuel consumptions and the fitting results of RFCR model is 4.22% , which does not significantly differ from zero, and the fuel consumptions calculated by RFCR model tend to be 1.40 L/100 km (about 25%) higher than the official reported data. Furthermore, we find that annual average temperature and altitude factors significantly influence the fuel consumption rate. The results indicate that there is a real world performance discrepancy between the theoretical fuel consumption released by authorities and that in the real world, and some green behaviors (choose light duty vehicles, reduce the use of air conditioning and change to manual transmission type) can reduce energy consumption of vehicles.}
}
@article{KUMAR202185,
title = {Analysis of Barriers to Industry 4.0 adoption in Manufacturing Organizations: an ISM Approach},
journal = {Procedia CIRP},
volume = {98},
pages = {85-90},
year = {2021},
note = {The 28th CIRP Conference on Life Cycle Engineering, March 10 – 12, 2021, Jaipur, India},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121000330},
author = {Pramod Kumar and Jaiprakash Bhamu and Kuldip Singh Sangwan},
keywords = {Industry 4.0, interpretive structural modeling, digital manufacturing, barriers, MICMAC analysis},
abstract = {Industry 4.0 has enabled technological integration of cyber physical systems and internet based communication in manufacturing value creation processes. As of now, many people use it as a collective term for advanced technologies, i.e. advanced robotics, artificial intelligence, machine learning, big data analytics, cloud computing, smart sensors, internet of things, augmented reality, etc. This substantially improves flexibility, quality, productivity, cost, and customer satisfaction by transforming existing centralized manufacturing systems towards digital and decentralized one. Despite having potential benefits of industry 4.0, the organizations are facing typical obstacles and challenges in adopting new technologies and successful implementation in their business models. This paper aims to identify potential barriers which may hinder the implementation of industry 4.0 in manufacturing organizations. The identified barriers, through comprehensive literature review and on the basis of opinions collected from industry experts, are: poor value-chain integration, cyber-security challenges, uncertainty about economic benefits, lack of adequate skills in workforce, high investment requirements, lack of infrastructure, jobs disruptions, challenges in data management and data quality, lack of secure standards and norms, and resistance to change. Interpretive Structural Modeling (ISM) is used to establish relationships among these barriers to develop a hierarchical model and MICMAC analysis for further classification of identified barriers for better understanding. An analysis of driving and dependence of the barriers may help in clear understanding of these for successful implementation of Industry 4.0 practices in the organizations.}
}
@incollection{KHAN2022,
title = {Bivariate, cluster, and suitability analysis of NoSQL solutions for big graph applications},
series = {Advances in Computers},
publisher = {Elsevier},
year = {2022},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2021.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0065245821000590},
author = {Samiya Khan and Xiufeng Liu and Syed Arshad Ali and Mansaf Alam},
keywords = {NoSQL, Big data system, Storage solution, Bivariate analysis, Cluster analysis, Classification},
abstract = {With the explosion of social media, the Web, Internet of Things, and the proliferation of smart devices, large amounts of data are being generated each day. However, traditional data management technologies are increasingly inadequate to cope with this growth in data. NoSQL has become increasingly popular as this technology can provide consistent, scalable and available solutions for the ever-growing heterogeneous data. Recent years have seen growing applications shifting from traditional data management systems to NoSQL solutions. However, there is limited in-depth literature reporting on NoSQL storage technologies for big graph and their applications in various fields. This chapter fills this gap by conducting a comprehensive study of 80 state-of-the-art NoSQL technologies. In this chapter, we first present a feature analysis of the NoSQL solutions and then generate a data set of the investigated solutions for further analysis in order to better understand and select the technologies. We perform a clustering analysis to segment the NoSQL solutions, compare the classified solutions based on their storage data models and Brewer's CAP theorem, and examine big graph applications in six specific domains. To help users select appropriate NoSQL solutions, we have developed a decision tree model and a web-based user interface to facilitate this process. In addition, the significance, challenges, applications and categories of storage technologies are discussed as well.}
}
@incollection{GROOT2017127,
title = {Chapter 5 - Data Management Tools and Techniques},
editor = {Martijn Groot},
booktitle = {A Primer in Financial Data Management},
publisher = {Academic Press},
pages = {127-177},
year = {2017},
isbn = {978-0-12-809776-2},
doi = {https://doi.org/10.1016/B978-0-12-809776-2.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128097762000053},
author = {Martijn Groot},
keywords = {data management technology, databases, big data, financial analytics, IT management},
abstract = {In this chapter we look at the technology and tooling available for data management. We start with a discussion on the different components of an IT infrastructure and how to describe them. We provide a short taxonomy of tools from analytics and data distribution to data governance and end-user tools. This is followed by a discussion on data models and storage models, from traditional relational databases to different challenger technologies including NoSQL databases. We discuss data quality management and curation processes and data storage at different scales from data marts and warehouses to data lakes. We then discuss data analytics and big data technologies and what some of the use cases and implications for financial services firms are. After discussing privacy and security aspects, and the promising application areas of blockchain technology in master data, we discuss cloud storage models and what the cloud trend means for banks and asset managers. We end with a discussion on IT sourcing options, IT management, and IT maturity models before concluding with a look ahead.}
}
@incollection{WU202257,
title = {Chapter 3 - CTDA methodology},
editor = {Jiaping Wu and Junyu He and George Christakos},
booktitle = {Quantitative Analysis and Modeling of Earth and Environmental Data},
publisher = {Elsevier},
pages = {57-100},
year = {2022},
isbn = {978-0-12-816341-2},
doi = {https://doi.org/10.1016/B978-0-12-816341-2.00010-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128163412000101},
author = {Jiaping Wu and Junyu He and George Christakos},
keywords = {Methodological chain, Knowledge bases, Big data, Scales, Visualization, Chronotopologic statistics},
abstract = {Abstract The methodological characteristics of the chronotopologic data analysis chain are discussed. Various kinds of knowledge are considered and properly classified, and several illustrative examples in applied sciences are presented. Big data and data-driven analyses are critically reviewed, and their implementation carefully assessed. Data scale types are classifications considered in property- and attribute-oriented settings. Classical statistics inadequacies are pointed out and the need of a chronotopology-dependent statistics is outlined. The chronotopologic visualization thinking mode and techniques are briefly reviewed.}
}
@article{OIEN20211334,
title = {An approach to data structuring and predictive analysis in discrete manufacturing},
journal = {Procedia CIRP},
volume = {104},
pages = {1334-1338},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.224},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121011227},
author = {Christian Dalheim Øien and Sebastian Dransfeld},
keywords = {Anomaly Detection, Predictive Maintenance, Discrete Manufacturing, Big Data Analytics, Adaptive Self-learning Systems},
abstract = {In discrete manufacturing the variation in process parameters and duration is often large. Common data storage and analytics systems primarily store data in univariate time series, and when analysing machine components of strongly varying lifetime and behaviour this causes a challenge. This paper presents a data structure and an analysis method for outlier detection which intends to deal with this challenge, as an alternative to predictive maintenance which often requires more data with higher quality than what is available. A case study in aluminium extrusion billet manufacturing is used to demonstrate the approach, predominantly detecting anomalies at the end of a critical component’s lifetime.}
}
@article{LUO2021197,
title = {Towards high quality mobile crowdsensing: Incentive mechanism design based on fine-grained ability reputation},
journal = {Computer Communications},
volume = {180},
pages = {197-209},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421003637},
author = {Zhuangye Luo and Jia Xu and Pengcheng Zhao and Dejun Yang and Lijie Xu and Jian Luo},
keywords = {Mobile crowdsensing, Incentive mechanism, Quality-aware, Reputation system},
abstract = {Mobile crowdsensing has become an efficient paradigm for performing large-scale sensing tasks. Many quality-aware incentive mechanisms for mobile crowdsensing have been proposed. However, most of them measure the data quality by one single metric from a specific perspective. Moreover, they usually use the real-time quality, which cannot provide sufficient incentive for the workers with long-term high quality. In this paper, we refine the generalized data quality into the fine-grained ability requirement. We present a mobile crowdsensing system to achieve the fine-grained quality control, and formulate the problem of maximizing the social cost such that the fine-grained ability requirement of all sensing tasks can be satisfied. To stimulate the workers with long-term high quality, we design two ability reputation systems to assess workers’ fine-grained abilities online. The incentive mechanism based on the reverse auction and fine-grained ability reputation system is proposed. We design a greedy algorithm to select the winners and determine the payment based on the bids and fine-grained ability reputation of workers. Through both rigorous theoretical analysis and extensive simulations, we demonstrate that the proposed mechanisms achieve computational efficiency, individual rationality, truthfulness, whitewashing proof, and guaranteed approximation. Moreover, the designed mechanisms show prominent advantage in terms of social cost and average ability achievement ratio.}
}
@article{GUO2021107627,
title = {Automated pressure transient analysis: A cloud-based approach},
journal = {Journal of Petroleum Science and Engineering},
volume = {196},
pages = {107627},
year = {2021},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2020.107627},
url = {https://www.sciencedirect.com/science/article/pii/S0920410520306951},
author = {Yonggui Guo and Ibrahim Mohamed and Ali Zidane and Yashesh Panchal and Omar Abou-Sayed and Ahmed Abou-Sayed},
keywords = {Pressure transient analysis, Cloud computing, ISIP, Flow regime, G-function, Web-based application},
abstract = {Pressure transient analysis provides essential information to evaluate the dimensions of injection induced fractures, permeability damage near the wellbore, and pressure elevation in the injection horizon. For injection wells, shut-in data can be collected and analyzed after each injection cycle to evaluate the well injectivity and predict the well longevity. However, any interactive analysis of the pressure data could be subjective and time-consuming. In this study a novel cloud-based approach to automatically analyzing pressure data is presented, which aims to improve the reliability and efficiency of pressure transient analysis. There are two fundamental requirements for automated pressure transient analysis: 1) Pressure data needs to be automatically retrieved from field sites and fed to the analyzer; 2) The engineer can automatically select instantaneous shut-in pressure (ISIP), identify flow regimes, and determine the fracture closure point if any. To meet these requirements as well as to take advantage of cloud storage and computing technologies, a web-based application has been developed to pull real time injection data from any field sites and push it to a cloud database. A built-in pressure transient workflow has been also proposed to detect any stored or real-time pressure data and perform pressure analysis automatically if the required data is available. The automated pressure transient analysis technology has been applied to multiple injection projects. In general, the analysis results including formation and fracture properties (i.e. permeability, fracture half length, skin factor, and fracture closure pressure) are comparable to results from interactive analysis. Any discrepancies are mainly caused by poor data quality. Issues such as inconsistent selections of ISIP and different slopes defined for pre and after closure analyses also contribute to the divergence. Overall, the automated pressure transient analysis provides consistent results as the exact same criteria are applied to the pressure data, and analysis results are independent of the analyzer's experience and knowledge. As data from oil/gas industry increases exponentially over time, automated data transmission, storage, analysis and access are becoming necessary to maximize the value of the data and reduce operation cost. The automated pressure transient analysis presented here demonstrates that cloud storage and computing combined with automated analysis tools is a viable way to overcome big data challenges faced by oil/gas industry professionals.}
}
@article{ZHANG202124,
title = {Thinking on the informatization development of China's healthcare system in the post-COVID-19 era},
journal = {Intelligent Medicine},
volume = {1},
number = {1},
pages = {24-28},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621000097},
author = {Ming Zhang and Danyun Dai and Siliang Hou and Wei Liu and Feng Gao and Dong Xu and Yu Hu},
keywords = {Coronavirus disease 2019, Healthcare system, Informatization},
abstract = {With the application of Internet of Things, big data, cloud computing, artificial intelligence, and other cutting-edge technologies, China's medical informatization is developing rapidly. In this paper, we summaried the role of information technology in healthcare sector's battle against the coronavirus disease 2019 (COVID-19) from the perspectives of early warning and monitoring, screening and diagnosis, medical treatment and scientific research, analyzes the bottlenecks of the development of information technology in the post-COVID-19 era, and puts forward feasible suggestions for further promoting the construction of medical informatization from the perspectives of sharing, convenience, and safety.}
}
@article{WANG2018440,
title = {Research on the Theory and Method of Grid Data Asset Management},
journal = {Procedia Computer Science},
volume = {139},
pages = {440-447},
year = {2018},
note = {6th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.258},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918319288},
author = {Jun Wang and Yun-si Li and Wei Song and Ai-hua Li},
keywords = {big data, grid data asset, asset management, data governance},
abstract = {In the era of Big Data, data assets have become a strategic resource which cannot be overlooked by both society and enterprises. However, data is not equal to the assets. This paper first introduces the necessary conditions of data assetization and discriminates the concepts of data governance, data management and data asset management. Then it focuses on the unique connotation and characteristics of grid data assets. With reference to the mainstream theory of data management, the framework for the grid data asset management is set up in the combination of the characteristics of data assets, business needs and the actual situation in the power supply enterprises. Finally, this paper puts forward higher system requirements and technical requirements for China’s power supply enterprises to conduct data asset management.}
}
@article{AKTER201985,
title = {Analytics-based decision-making for service systems: A qualitative study and agenda for future research},
journal = {International Journal of Information Management},
volume = {48},
pages = {85-95},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218312696},
author = {Shahriar Akter and Ruwan Bandara and Umme Hani and Samuel {Fosso Wamba} and Cyril Foropon and Thanos Papadopoulos},
keywords = {Big data analytics, Decision-making, Service systems},
abstract = {While the use of big data tends to add value for business throughout the entire value chain, the integration of big data analytics (BDA) to the decision-making process remains a challenge. This study, based on a systematic literature review, thematic analysis and qualitative interview findings, proposes a set of six-steps to establish both rigor and relevance in the process of analytics-driven decision-making. Our findings illuminate the key steps in this decision process including problem definition, review of past findings, model development, data collection, data analysis as well as actions on insights in the context of service systems. Although findings have been discussed in a sequence of steps, the study identifies them as interdependent and iterative. The proposed six-step analytics-driven decision-making process, practical evidence from service systems, and future research agenda, provide altogether the foundation for future scholarly research and can serve as a step-wise guide for industry practitioners.}
}
@article{SKARPATHIOTAKI2022100274,
title = {Cross-Industry Process Standardization for Text Analytics},
journal = {Big Data Research},
volume = {27},
pages = {100274},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100274},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000915},
author = {Christina G. Skarpathiotaki and Konstantinos E. Psannis},
keywords = {Big data analytics, Advanced analysis, Artificial intelligence, Machine learning, Text analytics, Cross-industry processes},
abstract = {We are living in a world where everything computes, everyone and everything is connected and sharing data. Going beyond just capturing and managing data, enterprises are tapping into IoT and Artificial Intelligence (AI) to create insights and intelligence in a revolutionary way that was not possible before. For instance, by analyzing unstructured data (such as text), call centers can extract entities, concepts, themes which can enable them to get faster insights that only few years back was not feasible. Public safety and law enforcement are only few of the examples that benefit from text analytics used to strengthen crime investigation. Sentiment Analysis, Content Classification, Language Detection and Intent Detection are just some of the Text Classification applications. The overall process model of such applications considering the complexity of the unstructured data, can be definitely challenging. In response to the chaotic emerging science of unstructured data analysis, the main goal of this paper is to first contribute to the gap of no existing methodology approach for Text Analytics projects, by introducing a methodology approach based on one of the most widely accepted and used methodology approach of CRISP-DM.}
}
@article{YAISH20142168,
title = {Multi-tenant Elastic Extension Tables Data Management},
journal = {Procedia Computer Science},
volume = {29},
pages = {2168-2181},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.202},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914003792},
author = {Haitham Yaish and Madhu Goyal and George Feuerlicht},
keywords = {Cloud Computing, Software as a Service, Big Data, Elastic Extension Tables, Multi-tenant Database, Relational Tables, Virtual Relational Tables.},
abstract = {Multi-tenant database is a new database solution which is significant for Software as a service (SaaS) and Big Data applications in the context of cloud computing paradigm. This multi-tenant database has significant design challenges to develop a solution that ensures a high level of data quality, accessibility, and manageability for the tenants using this database. In this paper, we propose a multi-tenant data management service called Elastic Extension Tables Schema Handler Service (EETSHS), which is based on a multi-tenant database schema called Elastic Extension Tables (EET). This data management service satisfies tenants’ different business requirements, by creating, managing, organizing, and administratinglarge volumes of structured, semi-structured, and unstructured data. Furthermore, it combines traditional relational data with virtual relational data in a single database schema and allows tenants to manage this data by calling functions from this service. We present algorithms for frequently used functions of this service, and perform several experiments to measure the feasibility and effectiveness of managing multi-tenant data using these functions. We report experimental results of query execution timesfor managing tenants’ virtual and traditional relational data showing that EET schema is a good candidate for the management of multi-tenant data for SaaS and Big Data applications.}
}
@article{ANG20161,
title = {Big Sensor Data Applications in Urban Environments},
journal = {Big Data Research},
volume = {4},
pages = {1-12},
year = {2016},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2015.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615300241},
author = {Li-Minn Ang and Kah Phooi Seng},
keywords = {Big data, Sensor-based systems, Survey, Application, Challenges},
abstract = {The emergence of new technologies such as Internet/Web/Network-of-Things and large scale wireless sensor systems enables the collection of data from an increasing volume and variety of networked sensors for analysis. In this review article, we summarize the latest developments of big sensor data systems (a term to conceptualize the application of the big data model towards networked sensor systems) in various representative studies for urban environments, including for air pollution monitoring, assistive living, disaster management systems, and intelligent transportation. An important focus is the inclusion of how value is extracted from the big data system. We also discuss some recent techniques for big data acquisition, cleaning, aggregation, modeling, and interpretation in large scale sensor-based systems. We conclude the paper with a discussion on future perspectives and challenges of sensor-based data systems in the big data era.}
}
@article{LIU2021589,
title = {Toward intelligent wireless communications: Deep learning - based physical layer technologies},
journal = {Digital Communications and Networks},
volume = {7},
number = {4},
pages = {589-597},
year = {2021},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2021.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S2352864821000742},
author = {Siqi Liu and Tianyu Wang and Shaowei Wang},
keywords = {Data-driven, Deep learning, Physical layer, Wireless communications},
abstract = {Advanced technologies are required in future mobile wireless networks to support services with highly diverse requirements in terms of high data rate and reliability, low latency, and massive access. Deep Learning (DL), one of the most exciting developments in machine learning and big data, has recently shown great potential in the study of wireless communications. In this article, we provide a literature review on the applications of DL in the physical layer. First, we analyze the limitations of existing signal processing techniques in terms of model accuracy, global optimality, and computational scalability. Next, we provide a brief review of classical DL frameworks. Subsequently, we discuss recent DL-based physical layer technologies, including both DL-based signal processing modules and end-to-end systems. Deep neural networks are used to replace a single or several conventional functional modules, whereas the objective of the latter is to replace the entire transceiver structure. Lastly, we discuss the open issues and research directions of the DL-based physical layer in terms of model complexity, data quality, data representation, and algorithm reliability.}
}
@article{ARIMURA2020100212,
title = {Changes in urban mobility in Sapporo city, Japan due to the Covid-19 emergency declarations},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {7},
pages = {100212},
year = {2020},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2020.100212},
url = {https://www.sciencedirect.com/science/article/pii/S2590198220301238},
author = {Mikiharu Arimura and Tran Vinh Ha and Kota Okumura and Takumi Asada},
keywords = {Covid-19, Moving pattern, Mobile spatial statistics, Population concentration, Big data},
abstract = {At the time of writing, the world is facing the new coronavirus pandemic, which has been declared one of the most dangerous disasters of the 21st century. All nations and communities have applied many countermeasures to control the spread of the epidemic. In terms of countermeasures, lockdowns and reductions of social activities are meant to flatten the curve of infection. Nevertheless, to date, there has been no evaluation of the effectiveness of these methods. Thus, the present study aims to interpret the change in the population density of Sapporo city in the emergency's period declaration using big data obtained from mobile spatial statistics. The results indicate that, in the time of refraining from traveling, the city's residents have been more likely to stay home and less likely to travel to the center area. This has led to a decrease of up to 90% of the population density in crowded areas. The study's outcomes partly explain the statement of reducing 70%–80% of contact between people in line with the purpose of the emergency declaration. Moreover, these findings establish the primary step for further analysis of estimating the efficiency of policy in controlling the epidemic.}
}
@article{NG2017939,
title = {A Master Data Management Solution to Unlock the Value of Big Infrastructure Data for Smart, Sustainable and Resilient City Planning},
journal = {Procedia Engineering},
volume = {196},
pages = {939-947},
year = {2017},
note = {Creative Construction Conference 2017, CCC 2017, 19-22 June 2017, Primosten, Croatia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.08.034},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817331569},
author = {S. Thomas Ng and Frank J. Xu and Yifan Yang and Mengxue Lu},
keywords = {Big data, integrated infrastructure asset management, master data management, smart city, smart infrastructure},
abstract = {In recent years, many governments have launched various smart city or smart infrastructure initiatives to improve the quality of citizens’ life and help city managers / planners optimize the operation and management of urban infrastructures. By deploying internet of things (IoT) to infrastructure systems, high-volume and high-variety of data pertinent to the condition and performance of infrastructure systems along with the behaviors of citizens can be gathered, processed, integrated and analyzed through cloud-based infrastructure asset management systems, ubiquitous mobile applications and big data analytics platforms. Nonetheless, how to fully exploit the value of ‘big infrastructure data’ is still a key challenge facing most stakeholders. Unless data is shared by different infrastructure systems in an interoperable and consistent manner, it is difficult to realize the smart infrastructure concept for efficient smart city planning, not to mention about developing appropriate resilience and sustainable programs. To unlock the value of big infrastructure data for smart, sustainable and resilient city planning, a master data management (MDM) solution is proposed in this paper. MDM has been adopted in the business sector to orchestrate operational and analytical big data applications. In order to derive a suitable MDM solution for smart, sustainable and resilient city planning, commercial and open source MDM systems, smart city standards, smart city concept models, smart community infrastructure frameworks, semantic web technologies will be critically reviewed, and feedback and requirements will be gathered from experts who are responsible for developing smart, sustainable and resilient city programs. A case study which focuses on the building and transportation infrastructures of a selected community in Hong Kong will be conducted to pilot the proposed MDM solution.}
}
@article{WANG201714,
title = {GSA: Genome Sequence Archive*},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {15},
number = {1},
pages = {14-18},
year = {2017},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2017.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1672022917300025},
author = {Yanqing Wang and Fuhai Song and Junwei Zhu and Sisi Zhang and Yadong Yang and Tingting Chen and Bixia Tang and Lili Dong and Nan Ding and Qian Zhang and Zhouxian Bai and Xunong Dong and Huanxin Chen and Mingyuan Sun and Shuang Zhai and Yubin Sun and Lei Yu and Li Lan and Jingfa Xiao and Xiangdong Fang and Hongxing Lei and Zhang Zhang and Wenming Zhao},
keywords = {Genome Sequence Archive, GSA, Big data, Raw sequence data, INSDC},
abstract = {With the rapid development of sequencing technologies towards higher throughput and lower cost, sequence data are generated at an unprecedentedly explosive rate. To provide an efficient and easy-to-use platform for managing huge sequence data, here we present Genome Sequence Archive (GSA; http://bigd.big.ac.cn/gsa or http://gsa.big.ac.cn), a data repository for archiving raw sequence data. In compliance with data standards and structures of the International Nucleotide Sequence Database Collaboration (INSDC), GSA adopts four data objects (BioProject, BioSample, Experiment, and Run) for data organization, accepts raw sequence reads produced by a variety of sequencing platforms, stores both sequence reads and metadata submitted from all over the world, and makes all these data publicly available to worldwide scientific communities. In the era of big data, GSA is not only an important complement to existing INSDC members by alleviating the increasing burdens of handling sequence data deluge, but also takes the significant responsibility for global big data archive and provides free unrestricted access to all publicly available data in support of research activities throughout the world.}
}
@incollection{FORTSON2021185,
title = {Chapter 10 - From Green Peas to STEVE: Citizen Science Engagement in Space Science},
editor = {Amy Paige Kaminski},
booktitle = {Space Science and Public Engagement},
publisher = {Elsevier},
pages = {185-219},
year = {2021},
isbn = {978-0-12-817390-9},
doi = {https://doi.org/10.1016/B978-0-12-817390-9.00009-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128173909000099},
author = {Lucy Fortson},
keywords = {Citizen Science, Crowdsourcing, Machine learning, Volunteer engagement},
abstract = {The past two decades has seen a tremendous rise in citizen science and crowdsourcing techniques as a means to carry out ground-breaking research while at the same time engage the general public in the wonders of space science. This article reviews some of the recent advances made in this realm as well as lessons learned from the unique perspective of the author’s role as a cofounder of the Zooniverse citizen science platform and practicing astrophysics researcher. I briefly describe the factors that led to the recent rise of citizen science including the formation of governance bodies at national and international levels, and the adoption by Federal Agencies within the United States government. I address concerns raised by research colleagues on the validity of citizen science as a research methodology, and then describe several key metrics for the success of citizen science including the link between data quality and publications, and the critical role that motivation and engagement of volunteer participants play in project success. I use the Green Pea galaxies discovered by Galaxy Zoo volunteers and an aurora-like phenomenon known as STEVE discovered by Aurorasaurus volunteers as examples of how, with the right tools and support, non-professional volunteers can make key contributions to space science. I then describe the role that machine learning can play when judiciously teamed with citizen scientists to tackle the ever-growing challenge of big data and close with some reflections on what it takes to support and manage a large platform like the Zooniverse.}
}
@article{DEMOULIN2020103120,
title = {Acceptance of text-mining systems: The signaling role of information quality},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103120},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308765},
author = {Nathalie T.M. Demoulin and Kristof Coussement},
keywords = {Technology acceptance model (TAM), Text mining, Big data, Information quality, Top management support},
abstract = {The popularity of the big data domain has boosted corporate interest in collecting and storing tremendous amounts of consumers’ textual information. However, decision makers are often overwhelmed by the abundance of information, and the usage of text mining (TM) tools is still at its infancy. This study validates an extended technology acceptance model integrating information quality (IQ) and top management support. Results confirm that IQ influences behavioral intentions and TM tools usage, through perceptions of external control, perceived ease of use, and perceived usefulness; top management support also has a key role in determining the usage of TM tools.}
}
@article{CHRISTOPOULOS201570,
title = {Extraction of the global absolute temperature for Northern Hemisphere using a set of 6190 meteorological stations from 1800 to 2013},
journal = {Journal of Atmospheric and Solar-Terrestrial Physics},
volume = {128},
pages = {70-83},
year = {2015},
issn = {1364-6826},
doi = {https://doi.org/10.1016/j.jastp.2015.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364682615000577},
author = {Demetris T. Christopoulos},
keywords = {Absolute temperature, Northern Hemisphere, Valid station, Data quality, Seasonal bias, Extreme values distribution, Missing records, Big data analysis},
abstract = {Starting from a set of 6190 meteorological stations we are choosing 6130 of them and only for Northern Hemisphere we are computing average values for absolute annual Mean, Minimum, Q1, Median, Q3, Maximum temperature plus their standard deviations for years 1800–2013, while we use 4887 stations and 389467 rows of complete yearly data. The data quality and the seasonal bias indices are defined and used in order to evaluate our dataset. After the year 1969 the data quality is monotonically decreasing while the seasonal bias is positive in most of the cases. An Extreme Value Distribution estimation is performed for minimum and maximum values, giving some upper bounds for both of them and indicating a big magnitude for temperature changes. Finally suggestions for improving the quality of meteorological data are presented.}
}
@article{LIN2018293,
title = {DTRM: A new reputation mechanism to enhance data trustworthiness for high-performance cloud computing},
journal = {Future Generation Computer Systems},
volume = {83},
pages = {293-302},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.01.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17317247},
author = {Hui Lin and Jia Hu and Chuanfeng Xu and Jianfeng Ma and Mengyang Yu},
keywords = {Cloud computing, Reputation mechanism, Trustworthiness, Data veracity},
abstract = {Cloud computing and the mobile Internet have been the two most influential information technology revolutions, which intersect in mobile cloud computing (MCC). The burgeoning MCC enables the large-scale collection and processing of big data, which demand trusted, authentic, and accurate data to ensure an important but often overlooked aspect of big data — data veracity. Troublesome internal attacks launched by internal malicious users is one key problem that reduces data veracity and remains difficult to handle. To enhance data veracity and thus improve the performance of big data computing in MCC, this paper proposes a Data Trustworthiness enhanced Reputation Mechanism (DTRM) which can be used to defend against internal attacks. In the DTRM, the sensitivity-level based data category, Metagraph theory based user group division, and reputation transferring methods are integrated into the reputation query and evaluation process. The extensive simulation results based on real datasets show that the DTRM outperforms existing classic reputation mechanisms under bad mouthing attacks and mobile attacks.}
}
@article{NASCIMENTO202097,
title = {Estimating record linkage costs in distributed environments},
journal = {Journal of Parallel and Distributed Computing},
volume = {143},
pages = {97-106},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731520302756},
author = {Dimas Cassimiro Nascimento and Carlos Eduardo Santos Pires and Tiago Brasileiro Araujo and Demetrio Gomes Mestre},
keywords = {Record linkage, Theoretical model, Data quality, Cloud computing},
abstract = {Record Linkage (RL) is the task of identifying duplicate entities in a dataset or multiple datasets. In the era of Big Data, this task has gained notorious attention due to the intrinsic quadratic complexity of the problem in relation to the size of the dataset. In practice, this task can be outsourced to a cloud service, and thus, a service customer may be interested in estimating the costs of a record linkage solution before executing it. Since the execution time of a record linkage solution depends on a combination of various algorithms, their respective parameter values and the employed cloud infrastructure, in practice it is hard to perform an a priori estimation of infrastructure costs for executing a record linkage task. Besides estimating customer costs, the estimation of record linkage costs is also important to evaluate whether (or not) the application of a set of RL parameter values will satisfy predefined time and budget restrictions. Aiming to tackle these challenges, we propose a theoretical model for estimating RL costs taking into account the main steps that may influence the execution time of the RL task. We also propose an algorithm, denoted as TBF, for evaluating the feasibility of RL parameter values, given a set of predefined customer restrictions. We evaluate the efficacy of the proposed model combined with regression techniques using record linkage results processed in real distributed environments. Based on the experimental results, we show that the employed regression technique has significant influence over the estimated record linkage costs. Moreover, we conclude that specific regression techniques are more suitable for estimating record linkage costs, depending on the evaluated scenario.}
}
@article{SOLANKI2019476,
title = {Towards a knowledge driven framework for bridging the gap between software and data engineering},
journal = {Journal of Systems and Software},
volume = {149},
pages = {476-484},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302772},
author = {Monika Solanki and Bojan Božić and Christian Dirschl and Rob Brennan},
keywords = {Ontologies, Data engineering, Software engineering, Alignment, Integration},
abstract = {In this paper we present a collection of ontologies specifically designed to model the information exchange needs of combined software and data engineering. Effective, collaborative integration of software and big data engineering for Web-scale systems, is now a crucial technical and economic challenge. This requires new combined data and software engineering processes and tools. Our proposed models have been deployed to enable: tool-chain integration, such as the exchange of data quality reports; cross-domain communication, such as interlinked data and software unit testing; mediation of the system design process through the capture of design intents and as a source of context for model-driven software engineering processes. These ontologies are deployed in web-scale, data-intensive, system development environments in both the commercial and academic domains. We exemplify the usage of the suite on case-studies emerging from two complex collaborative software and data engineering scenarios: one from the legal sector and the other from the Social sciences and Humanities domain.}
}
@article{XIANG20191180,
title = {Dynamic cooperation strategies of the closed-loop supply chain involving the internet service platform},
journal = {Journal of Cleaner Production},
volume = {220},
pages = {1180-1193},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.01.310},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619303373},
author = {Zehua Xiang and Minli Xu},
keywords = {Big data marketing, Differential game, Closed-loop supply chain, Internet service platform},
abstract = {In the age of “Internet+”, many Internet service platforms (ISPs) in China have been widely introduced to the closed-loop supply chain (CLSC). To further study the role of the Internet service platform, this paper considers a CLSC composed of a manufacturer, a retailer and an Internet service platform who invests in research and development (R&D), advertising and Big Data marketing, and develops the goodwill dynamic model based on the differential game theory. The construction of a goodwill dynamic model has two purposes, namely, to increase sales and the return rate. The optimal decisions for 3 players under two different cooperative scenarios are obtained, namely, the retailer payment scenario (scenario D) and the manufacturer cost-sharing scenario (scenario S). The supply chain members gain more profit or achieve a higher level of goodwill for products under certain conditions, i.e., a high residual value from remanufacturing, a high sharing rate of residual value from the retailer's recycled products, and a low recycling cost. Interestingly, the wholesale price increases with the residual value of recycled products when goodwill effectiveness is low, while the price declines when goodwill effectiveness is high. After comparing two cooperative scenarios, the result shows that an Internet service platform will invest more in Big Data marketing under the manufacturer cost-sharing scenario, and cooperation between the manufacturer and the Internet service platform can help improve the goodwill of enterprises or products. Moreover, the manufacturer cost-sharing scenario is payoff-Pareto-improving in most cases through the coordination of a cost-sharing rate, and the effectiveness of Big Data marketing exerts a positive effect on goodwill and the development of the industry. In addition, the retailer has “free rider” tendencies in the manufacturer cost-sharing scenario. The results encourage more enterprises to enhance the value of goodwill through cooperation with Internet service platforms because Internet service platforms conveniently utilize Big Data marketing to increase the sales of products and the collecting rate of used products, which in turn helps environmental sustainability.}
}
@article{ESKANDARITORBAGHAN2022106543,
title = {Understanding the potential of emerging digital technologies for improving road safety},
journal = {Accident Analysis & Prevention},
volume = {166},
pages = {106543},
year = {2022},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2021.106543},
url = {https://www.sciencedirect.com/science/article/pii/S0001457521005741},
author = {Mehran {Eskandari Torbaghan} and Manu Sasidharan and Louise Reardon and Leila C.W. Muchanga-Hvelplund},
keywords = {Safety, Road, Transport, Digital technology, Information},
abstract = {Each year, 1.35 million people are killed on the world’s roads and another 20–50 million are seriously injured. Morbidity or serious injury from road traffic collisions is estimated to increase to 265 million people between 2015 and 2030. Current road safety management systems rely heavily on manual data collection, visual inspection and subjective expert judgment for their effectiveness, which is costly, time-consuming, and sometimes ineffective due to under-reporting and the poor quality of the data. A range of innovations offers the potential to provide more comprehensive and effective data collection and analysis to improve road safety. However, there has been no systematic analysis of this evidence base. To this end, this paper provides a systematic review of the state of the art. It identifies that digital technologies - Artificial Intelligence (AI), Machine-Learning, Image-Processing, Internet-of-Things (IoT), Smartphone applications, Geographic Information System (GIS), Global Positioning System (GPS), Drones, Social Media, Virtual-reality, Simulator, Radar, Sensor, Big Data – provide useful means for identifying and providing information on road safety factors including road user behaviour, road characteristics and operational environment. Moreover, the results show that digital technologies such as AI, Image processing and IoT have been widely applied to enhance road safety, due to their ability to automatically capture and analyse data while preventing the possibility of human error. However, a key gap in the literature remains their effectiveness in real-world environments. This limits their potential to be utilised by policymakers and practitioners.}
}
@article{TSAI2021105421,
title = {Sustainable supply chain management trends in world regions: A data-driven analysis},
journal = {Resources, Conservation and Recycling},
volume = {167},
pages = {105421},
year = {2021},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2021.105421},
url = {https://www.sciencedirect.com/science/article/pii/S0921344921000288},
author = {Feng Ming Tsai and Tat-Dat Bui and Ming-Lang Tseng and Mohd Helmi Ali and Ming K. Lim and Anthony SF Chiu},
keywords = {Sustainable supply chain management, Data-driven analysis, Fuzzy Delphi method, Entropy weight method, Fuzzy decision-making trial and evaluation laboratory},
abstract = {This study proposes a data-driven analysis that describes the overall situation and reveals the factors hindering improvement in the sustainable supply chain management field. The literature has presented a summary of the evolution of sustainable supply chain management across attributes. Prior studies have evaluated different parts of the supply chain as independent entities. An integrated systematic assessment is absent in the extant literature and makes it necessary to identify potential opportunities for research direction. A hybrid of data-driven analysis, the fuzzy Delphi method, the entropy weight method and fuzzy decision-making trial and evaluation laboratory is adopted to address uncertainty and complexity. This study contributes to locating the boundary of fundamental knowledge to advance future research and support practical execution. Valuable direction is provided by reviewing the existing literature to identify the critical indicators that need further examination. The results show that big data, closed-loop supply chains, industry 4.0, policy, remanufacturing, and supply chain network design are the most important indicators of future trends and disputes. The challenges and gaps among different geographical regions is offered that provides both a local viewpoint and a state-of-the-art advanced sustainable supply chain management assessment.}
}
@article{KATARIA2019101429,
title = {Emerging role of eHealth in the identification of very early inflammatory rheumatic diseases},
journal = {Best Practice & Research Clinical Rheumatology},
volume = {33},
number = {4},
pages = {101429},
year = {2019},
note = {How to Investigate: Very Early Inflammatory Rheumatic Diseases},
issn = {1521-6942},
doi = {https://doi.org/10.1016/j.berh.2019.101429},
url = {https://www.sciencedirect.com/science/article/pii/S1521694219300981},
author = {Suchitra Kataria and Vinod Ravindran},
keywords = {Artificial intelligence, Big data, Machine learning, Data analytics, Wearable devices, Robotics, Digital health},
abstract = {Digital health or eHealth technologies, notably pervasive computing, robotics, big-data, wearable devices, machine learning, and artificial intelligence (AI), have opened unprecedented opportunities as to how the diseases are diagnosed and managed with active patient engagement. Patient-related data have provided insights (real world data) into understanding the disease processes. Advanced analytics have refined these insights further to draw dynamic algorithms aiding clinicians in making more accurate diagnosis with the help of machine learning. AI is another tool, which, although is still in the evolution stage, has the potential to help identify early signs even before the clinical features are apparent. The evolving digital developments pose challenges on allowing access to health-related data for further research but, at the same time, protecting each patient's privacy. This review focuses on the recent technological advances and their applications and highlights the immense potential to enable early diagnosis of rheumatological diseases.}
}
@article{MOLL2019100833,
title = {The role of internet-related technologies in shaping the work of accountants: New directions for accounting research},
journal = {The British Accounting Review},
volume = {51},
number = {6},
pages = {100833},
year = {2019},
note = {Innovative Governance and Sustainable Pathways in a Disruptive Environment},
issn = {0890-8389},
doi = {https://doi.org/10.1016/j.bar.2019.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0890838919300459},
author = {Jodie Moll and Ogan Yigitbasioglu},
keywords = {Accounting profession, Cloud, Big data, Blockchain, Artificial intelligence},
abstract = {This paper reviews the accounting literature that focuses on four Internet-related technologies that have the potential to dramatically change and disrupt the work of accountants and accounting researchers in the near future. These include cloud, big data, blockchain, and artificial intelligence (AI). For instance, access to distributed ledgers (blockchain) and big data supported by cloud-based analytics tools and AI will automate decision making to a large extent. These technologies may significantly improve financial visibility and allow more timely intervention due to the perpetual nature of accounting. However, given the number of tasks technology has relieved of accountants, these technologies may also lead to concerns about the profession's legitimacy. The findings suggest that scholars have not given sufficient attention to these technologies and how these technologies affect the everyday work of accountants. Research is urgently needed to understand the new kinds of accounting required to manage firms in the changing digital economy and to determine the new skills and competencies accountants may need to master to remain relevant and add value. The paper outlines a set of questions to guide future research.}
}
@article{MERHI2021121180,
title = {Evaluating the critical success factors of data intelligence implementation in the public sector using analytical hierarchy process},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121180},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121180},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006132},
author = {Mohammad I. Merhi},
keywords = {Data intelligence, Systems implementation, Data analytics, Success factors, Public sector, AHP},
abstract = {This study aims to fill a gap in the literature by identifying, defining, and evaluating the critical success factors that impact the implementation of data intelligence in the public sector. Fourteen factors were identified, and then divided into three categories: organization, process, and technology. We used the analytical hierarchy process, a quantitative method of decision-making, to evaluate the importance of the factors presented in the study using data collected from nine experts. The results showed that technology, as a category, is the most important. The analysis also indicated that project management, information systems & data, and data quality are the most important factors among all fourteen critical success factors. We discuss the implications of the analysis for practitioners and researchers in the paper.}
}
@article{CINNAMON2016253,
title = {Evidence and future potential of mobile phone data for disease disaster management},
journal = {Geoforum},
volume = {75},
pages = {253-264},
year = {2016},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2016.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0016718516301981},
author = {Jonathan Cinnamon and Sarah K. Jones and W. Neil Adger},
keywords = {Mobile phone, Call detail records, SMS, Disaster, Disease, Big data},
abstract = {Global health threats such as the recent Ebola and Zika virus outbreaks require rapid and robust responses to prevent, reduce and recover from disease dispersion. As part of broader big data and digital humanitarianism discourses, there is an emerging interest in data produced through mobile phone communications for enhancing the data environment in such circumstances. This paper assembles user perspectives and critically examines existing evidence and future potential of mobile phone data derived from call detail records (CDRs) and two-way short message service (SMS) platforms, for managing and responding to humanitarian disasters caused by communicable disease outbreaks. We undertake a scoping review of relevant literature and in-depth interviews with key informants to ascertain the: (i) information that can be gathered from CDRs or SMS data; (ii) phase(s) in the disease disaster management cycle when mobile data may be useful; (iii) value added over conventional approaches to data collection and transfer; (iv) barriers and enablers to use of mobile data in disaster contexts; and (v) the social and ethical challenges. Based on this evidence we develop a typology of mobile phone data sources, types, and end-uses, and a decision-tree for mobile data use, designed to enable effective use of mobile data for disease disaster management. We show that mobile data holds great potential for improving the quality, quantity and timing of selected information required for disaster management, but that testing and evaluation of the benefits, constraints and limitations of mobile data use in a wider range of mobile-user and disaster contexts is needed to fully understand its utility, validity, and limitations.}
}
@article{RIESENER2019304,
title = {Framework for defining information quality based on data attributes within the digital shadow using LDA},
journal = {Procedia CIRP},
volume = {83},
pages = {304-310},
year = {2019},
note = {11th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.131},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119304366},
author = {Michael Riesener and Christian Dölle and Günther Schuh and Christian Tönnes},
keywords = {Digital Shadow, Information quality, Data quality, Latent dirichlet allocation (LDA)},
abstract = {The amount of data, which is created in companies is increasing due to modern communication technologies and decreasing costs for storing data. This leads to an advancement of methods for data analyses as well as to an increasing awareness of benefits resulting from data-based knowledge. In the context of product service systems and product development, there are two major concepts for providing product information. The digital twin collects every information possible, while the digital shadow provides a sufficient and content-related picture of the product. Since these concepts merge data from different sources, comprehension about information quality and its relation to the data quality becomes immanently important. This paper introduces a framework to determine information quality with respect to data-related and system-related attributes. An extensive literature review with focus on “information quality” and “data quality” identifies the important approaches for describing information and data quality. A latent dirichlet allocation (LDA) algorithm is applied on 371 definitions and identify 12 data-related and system-related attributes for information quality. Those attributes are assigned to six dimensions for information quality. So the proposed framework depicts the relationships between data attributes and the influence on information quality.}
}
@article{NEWHART2019498,
title = {Data-driven performance analyses of wastewater treatment plants: A review},
journal = {Water Research},
volume = {157},
pages = {498-513},
year = {2019},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2019.03.030},
url = {https://www.sciencedirect.com/science/article/pii/S0043135419302490},
author = {Kathryn B. Newhart and Ryan W. Holloway and Amanda S. Hering and Tzahi Y. Cath},
keywords = {Wastewater treatment, Big data, Statistical process control, Process optimization, Monitoring},
abstract = {Recent advancements in data-driven process control and performance analysis could provide the wastewater treatment industry with an opportunity to reduce costs and improve operations. However, big data in wastewater treatment plants (WWTP) is widely underutilized, due in part to a workforce that lacks background knowledge of data science required to fully analyze the unique characteristics of WWTP. Wastewater treatment processes exhibit nonlinear, nonstationary, autocorrelated, and co-correlated behavior that (i) is very difficult to model using first principals and (ii) must be considered when implementing data-driven methods. This review provides an overview of data-driven methods of achieving fault detection, variable prediction, and advanced control of WWTP. We present how big data has been used in the context of WWTP, and much of the discussion can also be applied to water treatment. Due to the assumptions inherent in different data-driven modeling approaches (e.g., control charts, statistical process control, model predictive control, neural networks, transfer functions, fuzzy logic), not all methods are appropriate for every goal or every dataset. Practical guidance is given for matching a desired goal with a particular methodology along with considerations regarding the assumed data structure. References for further reading are provided, and an overall analysis framework is presented.}
}
@incollection{MOISE2015279,
title = {Chapter 12 - Terabyte-Scale Image Similarity Search},
editor = {Venu Govindaraju and Vijay V. Raghavan and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {33},
pages = {279-301},
year = {2015},
booktitle = {Big Data Analytics},
issn = {0169-7161},
doi = {https://doi.org/10.1016/B978-0-444-63492-4.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444634924000125},
author = {Diana Moise and Denis Shestakov},
keywords = {Big Data, Hadoop, MapReduce, Image search, Multimedia retrieval, Smart deployment, SIFT, HDFS, Hadoop deployment, Hadoop configuration, Hadoop performance, Map waves},
abstract = {While the past decade has witnessed an unprecedented growth of data generated and collected all over the world, existing data management approaches lack the ability to address the challenges of Big Data. One of the most promising tools for Big Data processing is the MapReduce paradigm. Although it has its limitations, the MapReduce programming model has laid the foundations for answering some of the Big Data challenges. In this chapter, we focus on Hadoop, the open-source implementation of the MapReduce paradigm. Using as case study a Hadoop-based application, i.e., image similarity search, we present our experiences with the Hadoop framework when processing terabytes of data. The scale of the data and the application workload allowed us to test the limits of Hadoop and the efficiency of the tools it provides. We present a wide collection of experiments and the practical lessons we have drawn from our experience with the Hadoop environment. Our findings can be shared as best practices and recommendations to the Big Data researchers and practitioners.}
}
@article{MUNINGER2022140,
title = {Social media use: A review of innovation management practices},
journal = {Journal of Business Research},
volume = {143},
pages = {140-156},
year = {2022},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2022.01.039},
url = {https://www.sciencedirect.com/science/article/pii/S0148296322000510},
author = {Marie-Isabelle Muninger and Dominik Mahr and Wafa Hammedi},
keywords = {Social media, Innovation, Systematic review, Framework and research agenda},
abstract = {The use of social media for innovation requires firms to manage rapid information transfers, big data, and multiway communication. Yet managers lack clear insights on the way social media should be managed and current literature is dispersed across various research streams. In this article, the authors aim to develop a better understanding of how social media use should be leveraged for innovation. To achieve this objective, they build a systematic review of evidence from 177 scientific articles across four key management disciplines. They analyze research perspectives and conceptualizations of social media use for innovation and provide a framework of the drivers, contingencies and outcomes related to this topic. Next, they attempt to identify what is currently known about social media use for innovation. Last, they suggest critical areas for future inquiry on this important subject.}
}
@article{LLAVE2018516,
title = {Data lakes in business intelligence: reporting from the trenches},
journal = {Procedia Computer Science},
volume = {138},
pages = {516-524},
year = {2018},
note = {CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.071},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918317046},
author = {Marilex Rea Llave},
keywords = {Business intelligence, big data, data lake, BI architecture},
abstract = {The data lake approach has emerged as a promising way to handle large volumes of structured and unstructured data. This big data technology enables enterprises to profoundly improve their Business Intelligence. However, there is a lack of empirical studies on the use of the data lake approach in enterprises. This paper provides the results of an exploratory study designed to improve the understanding of the use of the data lake approach in enterprises. I interviewed 12 experts who had implemented this approach in various enterprises and identified three important purposes of implementing data lakes: (1) as staging areas or sources for data warehouses, (2) as a platform for experimentation for data scientists and analysts, and (3) as a direct source for self-service business intelligence. The study also identifies several perceived benefits and challenges of the data lake approach. The results may be beneficial for both academics and practitioners. Further, suggestions for future research is presented.}
}
@article{HASSANI2021121111,
title = {The science of statistics versus data science: What is the future?},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121111},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121111},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521005448},
author = {Hossein Hassani and Christina Beneki and Emmanuel Sirimal Silva and Nicolas Vandeput and Dag Øivind Madsen},
keywords = {Perspective, Science, Statistics, Data science, Similarities, Differences, Pragmatism},
abstract = {The importance and relevance of the discipline of statistics with the merits of the evolving field of data science continues to be debated in academia and industry. Following a narrative literature review with over 100 scholarly and practitioner-oriented publications from statistics and data science, this article generates a pragmatic perspective on the relationships and differences between statistics and data science. Some data scientists argue that statistics is not necessary for data science as statistics delivers simple explanations and data science delivers results. Therefore, this article aims to stimulate debate and discourse among both academics and practitioners in these fields. The findings reveal the need for stakeholders to accept the inherent advantages and disadvantages within the science of statistics and data science. The science of statistics enables data science (aiding its reliability and validity), and data science expands the application of statistics to Big Data. Data scientists should accept the contribution and importance of statistics and statisticians must humbly acknowledge the novel capabilities made possible through data science and support this field of study with their theoretical and pragmatic expertise. Indeed, the emergence of data science does pose a threat to statisticians, but the opportunities for synergies are far greater.}
}
@article{CAO2020107850,
title = {Online investigation of vibration serviceability limitations using smartphones},
journal = {Measurement},
volume = {162},
pages = {107850},
year = {2020},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2020.107850},
url = {https://www.sciencedirect.com/science/article/pii/S0263224120303882},
author = {Lei Cao and Jun Chen},
keywords = {Vibration serviceability, Online sampling, Big data, Data cleaning},
abstract = {Vibration serviceability issue has attracted increasing attentions recently. Many studies on vibration serviceability limitations have been performed in labs using simulation. The proposed limits were incompatible and lacked details about the physiological and environmental factors because of small sample sizes and unrealistic environments. This study proposes a novel online big data approach for investigating vibration serviceability limits in real environment. A smartphone-based application (App) was designed and spread to volunteers to collect multi-source heterogeneous data including questionnaires of personal judgement on vibration level, vibration signals, environmental and biological factors in their daily life. So far, 8521 records have been received. Data cleaning was performed and a qualified database with large volume and various types of factor information was produced. Analysis of the database showed that vibration limits given by the new method were compatible with previous results, but with more abundant details that were ignored in previous studies.}
}
@article{MATHEUS2020101284,
title = {Data science empowering the public: Data-driven dashboards for transparent and accountable decision-making in smart cities},
journal = {Government Information Quarterly},
volume = {37},
number = {3},
pages = {101284},
year = {2020},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18300303},
author = {Ricardo Matheus and Marijn Janssen and Devender Maheshwari},
keywords = {Data science, Dashboards, E-government, Open government, Open data, Big data, Smart City, Design principles, Transparency, Accountability, Trust, Policy-making, Decision-making},
abstract = {Dashboards visualize a consolidated set data for a certain purpose which enables users to see what is happening and to initiate actions. Dashboards can be used by governments to support their decision-making and policy processes or to communicate and interact with the public. The objective of this paper is to understand and to support the design of dashboards for creating transparency and accountability. Two smart city cases are investigated showing that dashboards can improve transparency and accountability, however, realizing these benefits was cumbersome and encountered various risks and challenges. Challenges include insufficient data quality, lack of understanding of data, poor analysis, wrong interpretation, confusion about the outcomes, and imposing a pre-defined view. These challenges can easily result in misconceptions, wrong decision-making, creating a blurred picture resulting in less transparency and accountability, and ultimately in even less trust in the government. Principles guiding the design of dashboards are presented. Dashboards need to be complemented by mechanisms supporting citizens' engagement, data interpretation, governance and institutional arrangements.}
}
@incollection{PEZOULAS202019,
title = {Chapter 2 - Types and sources of medical and other related data},
editor = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
booktitle = {Medical Data Sharing, Harmonization and Analytics},
publisher = {Academic Press},
pages = {19-65},
year = {2020},
isbn = {978-0-12-816507-2},
doi = {https://doi.org/10.1016/B978-0-12-816507-2.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165072000025},
author = {Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis},
keywords = {Big data, Cohorts, Medical data acquisition, Sources of medical data, Types of medical data},
abstract = {This chapter presents the origin of medical data that comprises the core of any federated cloud platform that deals with medical data sharing and analytics. The different types and sources of medical data are extensively described along with applications and standard data acquisition protocols. Emphasis is given on the definition and the impact of the cohorts in clinical research, as well as the importance of the big data in medicine. The impact of the big data in medicine is also discussed along with emerging opportunities and challenges. The need to develop data standardization protocols across heterogeneous and dispersed data sources is finally highlighted, to enable the analysis of different types of medical big data.}
}
@article{GAHA2021216,
title = {Towards the implementation of the Digital Twin in CMM inspection process: opportunities, challenges and proposals},
journal = {Procedia Manufacturing},
volume = {54},
pages = {216-221},
year = {2021},
note = {10th CIRP Sponsored Conference on Digital Enterprise Technologies (DET 2020) – Digital Technologies as Enablers of Industrial Competitiveness and Sustainability},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2021.07.033},
url = {https://www.sciencedirect.com/science/article/pii/S2351978921001694},
author = {Raoudha Gaha and Alexandre Durupt and Benoit Eynard},
keywords = {Digital Twin, CMM, inspection, Model-based-defintion, Digital thread},
abstract = {The use of Digital Twin (DT) is adopted by manufacturers and have positive effects on the product manufacturing process. The aim of this paper is to define a Coordinate Measuring Machine (CMM) inspection DT model, based on inspection process digitalized functionalities, from one side, and Industry 4.0 opportunities (Digital thread, Big data, etc.), from the other side. A review about DT definition, is firstly presented. Secondly, we review related studies based on existing DT orientations and usages for CMM inspection. Thirdly, challenges related to the variation management are presented. Finally, a discussion about possible DT functionalities and opportunities is conducted then a CMM inspection DT model is presented.}
}
@article{AHMAD2022112128,
title = {Data-driven probabilistic machine learning in sustainable smart energy/smart energy systems: Key developments, challenges, and future research opportunities in the context of smart grid paradigm},
journal = {Renewable and Sustainable Energy Reviews},
volume = {160},
pages = {112128},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112128},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122000569},
author = {Tanveer Ahmad and Rafal Madonski and Dongdong Zhang and Chao Huang and Asad Mujeeb},
keywords = {Data-driven probabilistic machine learning, Energy distribution, Discovery and design of energy materials, Big data analytics and smart grid, Strategic energy planning and smart manufacturing, Energy demand-side response},
abstract = {The current trend indicates that energy demand and supply will eventually be controlled by autonomous software that optimizes decision-making and energy distribution operations. New state-of-the-art machine learning (ML) technologies are integral in optimizing decision-making in energy distribution networks and systems. This study was conducted on data-driven probabilistic ML techniques and their real-time applications to smart energy systems and networks to highlight the urgency of this area of research. This study focused on two key areas: i) the use of ML in core energy technologies and ii) the use cases of ML for energy distribution utilities. The core energy technologies include the use of ML in advanced energy materials, energy systems and storage devices, energy efficiency, smart energy material manufacturing in the smart grid paradigm, strategic energy planning, integration of renewable energy, and big data analytics in the smart grid environment. The investigated ML area in energy distribution systems includes energy consumption and price forecasting, the merit order of energy price forecasting, and the consumer lifetime value. Cybersecurity topics for power delivery and utilization, grid edge systems and distributed energy resources, power transmission, and distribution systems are also briefly studied. The primary goal of this work was to identify common issues useful in future studies on ML for smooth energy distribution operations. This study was concluded with many energy perspectives on significant opportunities and challenges. It is noted that if the smart ML automation is used in its targeting energy systems, the utility sector and energy industry could potentially save from $237 billion up to $813 billion.}
}
@article{GARCIABERNARDO2018164,
title = {The effects of data quality on the analysis of corporate board interlock networks},
journal = {Information Systems},
volume = {78},
pages = {164-172},
year = {2018},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917302272},
author = {Javier Garcia-Bernardo and Frank W. Takes},
abstract = {Nowadays, social network data of ever increasing size is gathered, stored and analyzed by researchers from a range of disciplines. This data is often automatically gathered from API’s, websites or existing databases. As a result, the quality of this data is typically not manually validated, and the resulting social networks may be based on false, biased or incomplete data. In this paper, we investigate the effect of data quality issues on the analysis of large networks. We focus on the global board interlock network, in which nodes represent firms across the globe, and edges model social ties between firms – shared board members holding a position at both firms. First, we demonstrate how we can automatically assess the completeness of a large dataset of 160 million firms, in which data is missing not at random. Second, we present a novel method to increase the accuracy of the entries in our data. By comparing the expected and empirical characteristics of the resulting network topology, we develop a technique that automatically prunes and merges duplicate nodes and edges. Third, we use a case study of the board interlock network of Sweden to show how poor quality data results in distorted network topologies, incorrect community division, biased centrality values and abnormal influence spread under a well-known diffusion model. Finally, we demonstrate how the proposed data quality assessment methods help restore the network structure, ultimately allowing us to derive meaningful and correct results from the analysis of the network.}
}
@article{PRODHAN2022105327,
title = {A review of machine learning methods for drought hazard monitoring and forecasting: Current research trends, challenges, and future research directions},
journal = {Environmental Modelling & Software},
volume = {149},
pages = {105327},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105327},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222000330},
author = {Foyez Ahmed Prodhan and Jiahua Zhang and Shaikh Shamim Hasan and Til Prasad {Pangali Sharma} and Hasiba Pervin Mohana},
keywords = {Machine learning, Deep learning, Forecasting, Drought, Big data},
abstract = {Machine learning is a dynamic field with wide-ranging applications, including drought modeling and forecasting. Drought is a complex, devastating natural disaster for which it is challenging to develop effective prediction models. Therefore, our review focuses on basic information about machine learning methods (MLMs) and their potential applications in developing efficient and effective drought forecasting models. We observed that MLMs have achieved significant advances in the robustness, effectiveness, and accuracy of the algorithms for drought modelling in recent years. The performance comparison of MLMs with other models provides a comprehensive conception of different model evaluation metrics. Further challenges of MLMs, such as inadequate training data sets, noise, outliers, and observation bias for spatial data sets, are explored. Finally, our review conveys in-depth understanding to researchers on machine learning applications in forecasting and modeling and provides drought mitigation strategy guidance for policymakers.}
}
@article{DO2020100018,
title = {Data quality analysis of interregional travel demand: Extracting travel patterns using matrix decomposition},
journal = {Asian Transport Studies},
volume = {6},
pages = {100018},
year = {2020},
issn = {2185-5560},
doi = {https://doi.org/10.1016/j.eastsj.2020.100018},
url = {https://www.sciencedirect.com/science/article/pii/S2185556020300183},
author = {Canh Xuan Do and Makoto Tsukai and Akimasa Fujiwara},
keywords = {Interregional travel survey, Web-based survey, Mobile phone data, Data quality, Nonnegative matrix factorization},
abstract = {The Interregional Travel Survey in Japan (formerly the Net Passenger Transportation Survey [NPTS]) still has some limitations. New data sources have recently emerged, e.g., massive data from web-based surveys (WEB) or collecting passive mobile phone data (MOBI). Using or not using these data sources have been questioned for data integration or model estimation and validation. Therefore, as an initial step, the data quality of new data sources was evaluated to identify the potential for data integration with NPTS or new data collection methods to replace NPTS. This study focused on finding out the similarities in travel patterns extracted from these data sources using a nonnegative matrix factorization method. This study found that origin–destination pairs in the MOBI travel patterns were significantly different from those of NPTS and WEB, while there were some similarities between NPTS and WEB. However, some issues have been remaining and should be resolved in the future.}
}
@incollection{BERMAN2013183,
title = {Chapter 13 - Legalities},
editor = {Jules J. Berman},
booktitle = {Principles of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {183-199},
year = {2013},
isbn = {978-0-12-404576-7},
doi = {https://doi.org/10.1016/B978-0-12-404576-7.00013-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124045767000137},
author = {Jules J. Berman},
keywords = {Feist Publishing, Inc. v. Rural Telephone Service Co., Data Quality Act, Freedom of Information Act, limited data use agreements, tort, patents, intellectual property, informed consent, data ownership, copyright, infringement, fair use},
abstract = {Big Data projects always incur some legal risk. It is impossible to know all the data contained in a Big Data project, and it is impossible to know every purpose to which Big Data is used. Hence, the entities that produce Big Data may unknowingly contribute to a variety of illegal activities, chiefly copyright and other intellectual property infringements, breaches of confidentiality, and privacy invasions. In addition, issues of data quality, data availability, and data documentation may contribute to the legal or regulatory disqualification of Big Data as a resource suitable for its intended purposes. In this chapter, four issues will be discussed in detail: (1) responsibility for the accuracy of the contained data; (2) rights to create, use, and share the data held in the resource; (3) intellectual property encumbrances incurred from the use of standards required for data representation and data exchange; and (4) protections for individuals whose personal information is used in the resource. Big Data managers contend with a wide assortment of legal issues, but these four problems never seem to go away.}
}
@article{PUTHAL201722,
title = {A dynamic prime number based efficient security mechanism for big sensing data streams},
journal = {Journal of Computer and System Sciences},
volume = {83},
number = {1},
pages = {22-42},
year = {2017},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2016.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0022000016000209},
author = {Deepak Puthal and Surya Nepal and Rajiv Ranjan and Jinjun Chen},
keywords = {Security, Sensor networks, Big data stream, Key exchange, Security verification},
abstract = {Big data streaming has become an important paradigm for real-time processing of massive continuous data flows in large scale sensing networks. While dealing with big sensing data streams, a Data Stream Manager (DSM) must always verify the security (i.e. authenticity, integrity, and confidentiality) to ensure end-to-end security and maintain data quality. Existing technologies are not suitable, because real time introduces delay in data stream. In this paper, we propose a Dynamic Prime Number Based Security Verification (DPBSV) scheme for big data streams. Our scheme is based on a common shared key that updated dynamically by generating synchronized prime numbers. The common shared key updates at both ends, i.e., source sensing devices and DSM, without further communication after handshaking. Theoretical analyses and experimental results of our DPBSV scheme show that it can significantly improve the efficiency of verification process by reducing the time and utilizing a smaller buffer size in DSM.}
}
@article{D2021,
title = {A study on artificial intelligence for monitoring smart environments},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.06.046},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321043911},
author = {Karthika D.},
keywords = {Big data analytics, Machine learning, Internet of things, Smart cities, Wireless technologies},
abstract = {Wireless networking has made enormous improvements. These developments have brought about new paradigms of wireless networking and communications Environmental protection has been in recent years a more intelligent and linked system for all facets of a global city. With the rise in data gathering, machine learning (ML) approaches can be used to boost the knowledge and the skill of an application. As the numbers increase and technology develops, the number of available data increases. Smart collection and interpretation of these Big Data is the underground to the rising of smart Internet of Things IoT apps. This study discovers the diverse methods of machine learning that resolve data difficulties in smart cities. The discussion takes place on applications such as air quality, water pollution, radiation pollution, smart buildings, smart transport, etc., which pose genuine environmental challenges. Adequate monitoring is needed to ensure sustainable growth in the world by safeguarding a healthy society. The potential and challenges in particular the role of machine learning technology for the Internet of Things and Big Data Analytics.}
}
@article{FUCHS2014198,
title = {Big data analytics for knowledge generation in tourism destinations – A case from Sweden},
journal = {Journal of Destination Marketing & Management},
volume = {3},
number = {4},
pages = {198-209},
year = {2014},
issn = {2212-571X},
doi = {https://doi.org/10.1016/j.jdmm.2014.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212571X14000353},
author = {Matthias Fuchs and Wolfram Höpken and Maria Lexhagen},
keywords = {Big data analytics, Tourism destination, Destination management information system, Business intelligence, Data mining, Online Analytical Processing (OLAP)},
abstract = {This paper presents a knowledge infrastructure which has recently been implemented as a genuine novelty at the leading Swedish mountain tourism destination, Åre. By applying a Business Intelligence approach, the Destination Management Information System Åre (DMIS-Åre) drives knowledge creation and application as a precondition for organizational learning at tourism destinations. Schianetz, Kavanagh, and Lockington’s (2007) concept of the ‘Learning Tourism Destination’ and the ‘Knowledge Destination Framework’ introduced by Höpken, Fuchs, Keil, and Lexhagen (2011) build the theoretical fundament for the technical architecture of the presented Business Intelligence application. After having introduced the development process of indicators measuring destination performance as well as customer behaviour and experience, the paper highlights how DMIS-Åre can be used by tourism managers to gain new knowledge about customer-based destination processes focused on pre- and post-travel phases, like “Web-Navigation”, “Booking” and “Feedback”. After a concluding discussion about the various components building the prototypically implemented BI-based DMIS infrastructure with data from destination stakeholders, the agenda of future research is sketched. The agenda considers, for instance, the application of real-time Business Intelligence to gain real-time knowledge on tourists’ on-site behaviour at tourism destinations.}
}
@article{OYEDELE2021100158,
title = {Machine learning predictions for lost time injuries in power transmission and distribution projects},
journal = {Machine Learning with Applications},
volume = {6},
pages = {100158},
year = {2021},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2021.100158},
url = {https://www.sciencedirect.com/science/article/pii/S2666827021000797},
author = {Ahmed O. Oyedele and Anuoluwapo O. Ajayi and Lukumon O. Oyedele},
keywords = {Hazard assessment, Deep learning, Big data predictive analytics, Power infrastructure, Zero count data},
abstract = {Although advanced machine learning algorithms are predominantly used for predicting outcomes in many fields, their utilisation in predicting incident outcome in construction safety is still relatively new. This study harnesses Big Data with Deep Learning to develop a robust safety management system by analysing unstructured incident datasets consisting of 168,574 data points from power transmission and distribution projects delivered across the UK from 2004 to 2016. This study compared Deep Learning performance with popular machine learning algorithms (support vector machine, random forests, multivariate adaptive regression splines, generalised linear model, and their ensembles) concerning lost time injury and risk assessment in power utility projects. Deep Learning gave the best prediction for safety outcomes with high skills (AUC = 0.95, R2 = 0.88, and multi-class ROC = 0.93), thus outperforming the other algorithms. The results from this study also highlight the significance of quantitative analysis of empirical data in safety science and contribute to an enhanced understanding of injury patterns using predictive analytics in conjunction with safety experts’ perspectives. Additionally, the results will enhance the skills of safety managers in the power utility domain to advance safety intervention efforts.}
}
@article{REIS2020232,
title = {Assessing the drivers of machine learning business value},
journal = {Journal of Business Research},
volume = {117},
pages = {232-243},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.05.053},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320303581},
author = {Carolina Reis and Pedro Ruivo and Tiago Oliveira and Paulo Faroleiro},
keywords = {Machine learning, Business value, Competitive advantage, Dynamic capabilities theory},
abstract = {Machine learning (ML) is expected to transform the business landscape in the near future completely. Hitherto, some successful ML case-stories have emerged. However, how organizations can derive business value (BV) from ML has not yet been substantiated. We assemble a conceptual model, grounded on the dynamic capabilities theory, to uncover key drivers of ML BV, in terms of financial and strategic performance. The proposed model was assessed by surveying 319 corporations. Our findings are that ML use, big data analytics maturity, platform maturity, top management support, and process complexity are, to some extent, drivers of ML BV. We also find that platform maturity has, to some degree, a moderator influence between ML use and ML BV, and between big data analytics maturity and ML BV. To the best of our knowledge, this is the first research to deliver such findings in the ML field.}
}
@article{LI202156,
title = {Construction of an artificial intelligence system in dermatology: effectiveness and consideration of Chinese Skin Image Database (CSID)},
journal = {Intelligent Medicine},
volume = {1},
number = {2},
pages = {56-60},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S266710262100005X},
author = {Chengxu Li and Wenmin Fei and Yang Han and Xiaoli Ning and Ziyi Wang and Keke Li and Ke Xue and Jingkai Xu and Ruixing Yu and Rusong Meng and Feng Xu and Weimin Ma and Yong Cui},
keywords = {Artificial intelligence, Dermatology, Skin image, Chinese Skin Image Database},
abstract = {After more than 60 years of development, artificial intelligence (AI) has been widely used in various fields. Especially in recent years, with the development of deep learning, AI has made many remarkable achievements in the medical field. Dermatology, as a clinical discipline with morphology as its main feature, is particularly suitable for the development of AI. The rapid development of skin imaging technology has helped dermatologists to assist in the diagnosis of diseases and has greatly improved the accuracy of diagnosis. Skin imaging data have natural big data attributes, which is important for AI research. The establishment of the Chinese Skin Image Database (CSID) has solved many problems such as isolated data islands and inconsistent data quality. Based on the CSID, many pioneering achievements have been made in the research and development of AI-assisted decision-making software, the establishment of expert organizations, personnel training, scientific research, and so on. At present, there are still many problems with AI in the field of dermatology, such as clinical validation, medical device licensing, interdisciplinary, and standard formulation, which urgently need to be solved by joint efforts of all parties.}
}
@article{HUARD201518,
title = {The data quality paradox},
journal = {Network Security},
volume = {2015},
number = {6},
pages = {18-20},
year = {2015},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(15)30051-9},
url = {https://www.sciencedirect.com/science/article/pii/S1353485815300519},
author = {Boris Huard},
abstract = {After its people, data is arguably an organisation's most valuable asset. According to recent figures by the Networked Systems and Services department at SINTEF, some 90% of all the data in the world has been created in the past two years. That shouldn't be too much of a surprise to us given the data-driven world we now live in; but what is promising is that companies are increasingly switching on to its strategic and commercial value. The challenge is how do we extract value from this data in a way that empowers people and organisations alike?}
}
@article{CHO2022102477,
title = {What's driving the diffusion of next-generation digital technologies?},
journal = {Technovation},
pages = {102477},
year = {2022},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2022.102477},
url = {https://www.sciencedirect.com/science/article/pii/S0166497222000244},
author = {Jaehan Cho and Timothy DeStefano and Hanhin Kim and Inchul Kim and Jin Hyun Paik},
abstract = {The recent development and diffusion of next-generation digital technologies (NGDTs) such as artificial intelligence, the Internet of Things, big data, 3D printing, and so on are expected to have an immense impact on businesses, innovation, and society. While we know from extant research that a firm's R&D investment, intangible assets, and productivity are factors that influence technology use more generally, to date there is little known about the factors that determine how these emerging tools are used, and by who. Using Probit and OLS modeling on a survey of 12,579 South Korean firms in 2017, we conduct one of the first comprehensive examinations highlighting various firm characteristics that drive NGDT implementation. While much of the literature assesses the use of individual technologies, our research attempts to unveil the extent to which firms implement NGDTs in bundles. Our investigation shows that more than half of the firms that use NGDTs deployed multiple technologies simultaneously. One of the insightful complementarities identified in this research exists amongst technologies that generate, facilitate and demand large sums of data, including big data, IoT, cloud computing and AI. Such technologies also appear important for innovative tools such as 3D printing and robotics.}
}
@article{ZHOU2021103342,
title = {Remaining useful life prediction with probability distribution for lithium-ion batteries based on edge and cloud collaborative computation},
journal = {Journal of Energy Storage},
volume = {44},
pages = {103342},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.103342},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21010331},
author = {Yong Zhou and Huanghui Gu and Teng Su and Xuebing Han and Languang Lu and Yuejiu Zheng},
keywords = {Battery life prediction, RVM optimization prediction, Parameter transfer: RUL probability prediction},
abstract = {This paper proposes the architecture of the combination of the battery management system (BMS) and the cloud big data platform. Firstly, BMS measures and extracts the mean voltage falloff (MVF). A regression model of capacity and MVF based on historical data is established with generalized Box-Cox Transformation and least squares. The capacity and MVF are uploaded to the cloud big data platform, and then the mean and variance of the MVF is predicted based on the relevance vector machine, thereby realizing the 2σ range prediction of the lithium battery's state of health and the probability density function prediction of the remaining useful life. This paper makes two contributions to the data-driven prediction method. First, the edge-cloud collaborative computing architecture combining BMS and cloud is proposed, which effectively utilizes the advantages of BMS data quality and cloud computing power. Second, through the combination of relevance vector machine with particle swarm optimization and horizontal parameter transfer, the number of samples required for model learning is reduced to 30% and has better accuracy and robustness. Through the verification of NASA data, the results show that the average error is less than 2.18%.}
}
@article{BUI2021109392,
title = {Advanced data analytics for ship performance monitoring under localized operational conditions},
journal = {Ocean Engineering},
volume = {235},
pages = {109392},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109392},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821008040},
author = {Khanh Q. Bui and Lokukaluge P. Perera},
keywords = {Big data analytics, Machine learning, Ship performance monitoring, Energy efficiency, Emission control, Data anomaly detection},
abstract = {Improving the operational energy efficiency of existing ships is attracting considerable interests to reduce the environmental footprint due to air emissions. As the shipping industry is entering into Shipping 4.0 with digitalization as a disruptive force, an intriguing area in the field of ship’s operational energy efficiency is big data analytics. This paper proposes a big data analytics framework for ship performance monitoring under localized operational conditions with the help of appropriate data analytics together with domain knowledge. The proposed framework is showcased through a data set obtained from a bulk carrier pertaining the detection of data anomalies, the investigation of the ship’s localized operational conditions, the identification of the relative correlations among parameters and the quantification of the ship’s performance in each of the respective conditions. The novelty of this study is to provide a KPI (i.e. key performance indicator) for ship performance quantification in order to identify the best performance trim-draft mode under the engine modes of the case study ship. The proposed framework has the features to serve as an operational energy efficiency measure to provide data quality evaluation and decision support for ship performance monitoring that is of value for both ship operators and decision-makers.}
}
@article{RIVAS201794,
title = {Towards a service architecture for master data exchange based on ISO 8000 with support to process large datasets},
journal = {Computer Standards & Interfaces},
volume = {54},
pages = {94-104},
year = {2017},
note = {SI: New modeling in Big Data},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2016.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0920548916301192},
author = {Bibiano Rivas and Jorge Merino and Ismael Caballero and Manuel Serrano and Mario Piattini},
keywords = {Master data, Data quality, ISO 8000, Big data},
abstract = {During the execution of business processes involving various organizations, Master Data is usually shared and exchanged. It is necessary to keep appropriate levels of quality in these Master Data in order to prevent problems in the business processes. Organizations can be benefitted from having information about the level of quality of master data along with the master data to support decision about the usage of data in business processes is to include information about the level of quality alongside the Master Data. ISO 8000-1x0 specifies how to add this information to the master data messages. From the clauses stated in the various part of standard we developed a reference architecture, enhanced with big data technologies to better support the management of large datasets The main contribution of this paper is a service architecture for Master Data Exchange supporting the requirements stated by the different parts of the standard like the development of a data dictionary with master data terms; a communication protocol; an API to manage the master data messages; and the algorithms in MapReduce to measure the data quality.}
}
@article{KNEPPER201792,
title = {Forward Observer system for radar data workflows: Big data management in the field},
journal = {Future Generation Computer Systems},
volume = {76},
pages = {92-97},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17310567},
author = {Richard Knepper and Matthew Standish},
keywords = {Microcomputers, Information storage, Physical sciences and engineering},
abstract = {There are unique challenges in managing data collection and management from instruments in the field in general. These issues become extreme when “in the field” means “in a plane over the Antarctic”. In this paper we present the design and function of the Forward Observer a computer cluster and data analysis system that flies in a plane in the Arctic and Antarctic to collect, analyze in real time, and store Synthetic Aperture Radar (SAR) data. SAR is used to analyze the thickness and structure of polar ice sheets. We also discuss the processing of data once it is returned to the continental US and made available via data grids. The needs for in-flight data analysis and storage in the Antarctic and Arctic are highly unusual, and we have developed a novel system to meet those needs. We describe the constraints and requirements that led to the creation of this system and the general functionality which it applies to any instrument. We discuss the main means for handling replication and creating checksum information to ensure that data collected in polar regions are returned safely to mainland US for analysis. So far, not a single byte of data collected in the field has failed to make it home to the US for analysis (although many particular data storage devices have failed or been damaged due to the challenges of the extreme environments in which this system is used). While the Forward Observer system is developed for the extreme situation of data management in the field in the Antarctic, the technology and solutions we have developed are applicable and potentially usable in many situations where researchers wish to do real time data management in the field in areas that are constrained in terms of electrical supply.}
}
@article{AMEEN2021106761,
title = {Consumer interaction with cutting-edge technologies: Implications for future research},
journal = {Computers in Human Behavior},
volume = {120},
pages = {106761},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106761},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221000832},
author = {Nisreen Ameen and Sameer Hosany and Ali Tarhini},
keywords = {Consumer interaction, Cutting-edge technologies, Artificial intelligence, Virtual reality and augmented reality, Robotics, Wearable technology, Big data analytics},
abstract = {This article provides an overview of extant literature addressing consumer interaction with cutting-edge technologies. Six focal cutting-edge technologies are identified: artificial intelligence, augmented reality, virtual reality, wearable technology, robotics and big data analytics. Our analysis shows research on consumer interaction with cutting-edge technologies is at a nascent stage, and there are several gaps requiring attention. To further advance knowledge, our article offers avenues for future interdisciplinary research addressing implications of consumer interaction with cutting-edge technologies. More specifically, we propose six main areas for future research namely: rethinking consumer behaviour models, identifying behavioural differences among different generations of consumers, understanding how consumers interact with automated services, ethics, privacy and the blackbox, consumer security concerns and consumer interaction with new-age technologies during and after a major global crisis such as the COVID-19 pandemic.}
}
@incollection{SHANG2022203,
title = {Chapter 7 - Data mining technologies for Mobility-as-a-Service (MaaS)},
editor = {Haoran Zhang and Xuan Song and Ryosuke Shibasaki},
booktitle = {Big Data and Mobility as a Service},
publisher = {Elsevier},
pages = {203-228},
year = {2022},
isbn = {978-0-323-90169-7},
doi = {https://doi.org/10.1016/B978-0-323-90169-7.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901697000087},
author = {Wen-Long Shang and Haoran Zhang and Yi Sui},
keywords = {MaaS, Big data, Data mining, Support vector machine, Linear regression, Decision tree, Clustering, Bike-sharing, COVID-19},
abstract = {This chapter mainly introduces big data technologies for MaaS. Firstly, the development, definition, and purpose of MaaS and the significance of data mining technologies for MaaS are introduced briefly. Following this, the definition of data mining, its processing objects, classical steps and processes, and types of traffic big data are reviewed. Afterward, data mining technologies such as support vector machine, linear regression, decision tree, and clustering analysis are introduced. Finally, a case study of data mining technology used for bike-sharing in Beijing during the Covid-19 pandemic is presented to demonstrate the role of data mining technologies in travel behaviors. This chapter mainly provides a clue or reference for the exploration of big data analysis of MaaS.}
}
@article{MARSDEN2019113172,
title = {Perspectives on numerical data quality in IS research},
journal = {Decision Support Systems},
volume = {126},
pages = {113172},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113172},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619302015},
author = {James R. Marsden and David E. Pingry and Jason B. Thatcher}
}
@article{SZYMANSKA20181,
title = {Modern data science for analytical chemical data – A comprehensive review},
journal = {Analytica Chimica Acta},
volume = {1028},
pages = {1-10},
year = {2018},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2018.05.038},
url = {https://www.sciencedirect.com/science/article/pii/S0003267018306421},
author = {Ewa Szymańska},
keywords = {Chemometrics, Data science, Big data, Chemical analytical data, Methodology},
abstract = {Efficient and reliable analysis of chemical analytical data is a great challenge due to the increase in data size, variety and velocity. New methodologies, approaches and methods are being proposed not only by chemometrics but also by other data scientific communities to extract relevant information from big datasets and provide their value to different applications. Besides common goal of big data analysis, different perspectives and terms on big data are being discussed in scientific literature and public media. The aim of this comprehensive review is to present common trends in the analysis of chemical analytical data across different data scientific fields together with their data type-specific and generic challenges. Firstly, common data science terms used in different data scientific fields are summarized and discussed. Secondly, systematic methodologies to plan and run big data analysis projects are presented together with their steps. Moreover, different analysis aspects like assessing data quality, selecting data pre-processing strategies, data visualization and model validation are considered in more detail. Finally, an overview of standard and new data analysis methods is provided and their suitability for big analytical chemical datasets shortly discussed.}
}
@article{JUNG2020112,
title = {Ten-year patient journey of stage III non-small cell lung cancer patients: A single-center, observational, retrospective study in Korea (Realtime autOmatically updated data warehOuse in healTh care; UNIVERSE-ROOT study)},
journal = {Lung Cancer},
volume = {146},
pages = {112-119},
year = {2020},
issn = {0169-5002},
doi = {https://doi.org/10.1016/j.lungcan.2020.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0169500220304670},
author = {Hyun Ae Jung and Jong-Mu Sun and Se-Hoon Lee and Jin Seok Ahn and Myung-Ju Ahn and Keunchil Park},
keywords = {Real-time updated system, Big data, Real-world data, NSCLC, Treatment},
abstract = {Introduction
Until the recent approval of immunotherapy after completing concurrent chemoradiotherapy (CCRT), there has been little progress in treating unresectable stage III non-small cell lung cancer (NSCLC). This prompted us to search real-world data (RWD) to better understand diagnosis and treatment patterns, and outcomes.
Methods
This non-interventional observational study used a unique, novel algorithm for big data analysis to collect and assess anonymized patient electronic medical records from a clinical data warehouse (CDW) over a 10-year period to capture real-world patterns of diagnosis, treatment, and outcomes of stage III NSCLC patients. We describe real-world patterns of diagnosis and treatment of patients with newly-diagnosed stage III NSCLC, and patients’ characteristics, and assessment of treatment outcomes.
Results
We analyzed clinical variables from 23,735 NSCLC patients. Stage III patients (N = 4138, 18.2 %) were diagnosed as IIIA (N = 2,547, 11.2 %) or IIIB (N = 1,591. 7.0 %). Treated stage III patients (N = 2530, 61.1 %) had a median age of 64.2 years, were mostly male (78.5 %) and had an ECOG performance status of 1 (65.2 %). Treatment comprised curative-intent surgery (N = 1,254, 49.6 %) with 705 receiving neoadjuvant therapy; definitive CRT (N = 648, 25.6 %); palliative CT (N = 270, 10.7 %), or thoracic RT (N = 170, 6.7 %). Median OS (range) for neoadjuvant, surgery, CRT, palliative chemotherapy, lung RT alone, and supportive care was 49.2 (42.0–56.5), 52.5 (43.1–61.9), 30.3 (26.6–34.0), 14.7 (13.0–16.4), 8.8 (6.2–11.3), and 2.0 (1.0–3.0) months, respectively.
Conclusions
This unique in-house algorithm enabled a rapid and comprehensive analysis of big data through a CDW, with daily automatic updates that documented real-world PFS and OS consistent with the published literature, and real-world treatment patterns and clinical outcomes in stage III NSCLC patients.}
}
@article{WANG2021189,
title = {Safety intelligence as an essential perspective for safety management in the era of Safety 4.0: From a theoretical to a practical framework},
journal = {Process Safety and Environmental Protection},
volume = {148},
pages = {189-199},
year = {2021},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2020.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0957582020318000},
author = {Bing Wang},
keywords = {Safety intelligence (SI), Safety big data, Safety 4.0, Safety management, Safety decision-making},
abstract = {In the age of big data, intelligence, and Industry 4.0, intelligence plays an increasingly significant role in management or, more specifically, decision making; thus, it becomes a popular topic and is recognised as an important discipline. Hence, safety intelligence (SI) as a new safety concept and term was proposed. SI aims to transform raw safety data and information into meaningful and actionable information for safety management; it is considered an essential perspective for safety management in the era of Safety 4.0 (computational safety science—a new paradigm for safety science in the age of big data, intelligence, and Industry 4.0). However, thus far, no existing research provides a framework that comprehensively describes SI and guides the implementation of SI practices in organisations. To address this research gap and to provide a framework for SI and its practice in the context of safety management, based on a systematic and comprehensive explanation on SI from different perspectives, this study attempts to propose a theoretical framework for SI from a safety management perspective and then presents an SI practice model aimed at supporting safety management in organisations.}
}
@article{FRANCIA2021299,
title = {Making data platforms smarter with MOSES},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {299-313},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002260},
author = {Matteo Francia and Enrico Gallinucci and Matteo Golfarelli and Anna Giulia Leoni and Stefano Rizzi and Nicola Santolini},
keywords = {Data lake, Metadata, Big data, Data platform},
abstract = {The rise of data platforms has enabled the collection and processing of huge volumes of data, but has opened to the risk of losing their control. Collecting proper metadata about raw data and transformations can significantly reduce this risk. In this paper we propose MOSES, a technology-agnostic, extensible, and customizable framework for metadata handling in big data platforms. The framework hinges on a metadata repository that stores information about the objects in the big data platform and the processes that transform them. MOSES provides a wide range of functionalities to different types of users of the platform. Differently from previous high-level proposals, MOSES is fully implemented and it was not conceived for a specific technology. Besides discussing the rationale and the features of MOSES, in this paper we describe its implementation and we test it on a real case study. The ultimate goal is to take a significant step forward towards proving that metadata handling in big data platforms is feasible and beneficial.}
}
@article{KARKOUCH201657,
title = {Data quality in internet of things: A state-of-the-art survey},
journal = {Journal of Network and Computer Applications},
volume = {73},
pages = {57-81},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516301564},
author = {Aimad Karkouch and Hajar Mousannif and Hassan {Al Moatassime} and Thomas Noel},
keywords = {Internet of things, Data quality, Data cleaning, Outlier detection},
abstract = {In the Internet of Things (IoT), data gathered from a global-scale deployment of smart-things, are the base for making intelligent decisions and providing services. If data are of poor quality, decisions are likely to be unsound. Data quality (DQ) is crucial to gain user engagement and acceptance of the IoT paradigm and services. This paper aims at enhancing DQ in IoT by providing an overview of its state-of-the-art. Data properties and their new lifecycle in IoT are surveyed. The concept of DQ is defined and a set of generic and domain-specific DQ dimensions, fit for use in assessing IoT's DQ, are selected. IoT-related factors endangering the DQ and their impact on various DQ dimensions and on the overall DQ are exhaustively analyzed. DQ problems manifestations are discussed and their symptoms identified. Data outliers, as a major DQ problem manifestation, their underlying knowledge and their impact in the context of IoT and its applications are studied. Techniques for enhancing DQ are presented with a special focus on data cleaning techniques which are reviewed and compared using an extended taxonomy to outline their characteristics and their fitness for use for IoT. Finally, open challenges and possible future research directions are discussed.}
}
@article{DONG2015278,
title = {Traffic zone division based on big data from mobile phone base stations},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {58},
pages = {278-291},
year = {2015},
note = {Big Data in Transportation and Traffic Engineering},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2015.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X15002223},
author = {Honghui Dong and Mingchao Wu and Xiaoqing Ding and Lianyu Chu and Limin Jia and Yong Qin and Xuesong Zhou},
keywords = {Mobile telephones, Call detail record (CDR) data, Traffic semantic analysis, Traffic zone division, Traffic zone attribute index, Travel patterns},
abstract = {Call detail record (CDR) data from mobile communication carriers offer an emerging and promising source of information for analysis of traffic problems. To date, research on insights and information to be gleaned from CDR data for transportation analysis has been slow, and there has been little progress on development of specific applications. This paper proposes the traffic semantic concept to extract traffic commuters’ origins and destinations information from the mobile phone CDR data and then use the extracted data for traffic zone division. A K-means clustering method was used to classify a cell-area (the area covered by a base stations) and tag a certain land use category or traffic semantic attribute (such as working, residential, or urban road) based on four feature data (including real-time user volume, inflow, outflow, and incremental flow) extracted from the CDR data. By combining the geographic information of mobile phone base stations, the roadway network within Beijing’s Sixth Ring Road was divided into a total of 73 traffic zones using another K-means clustering algorithm. Additionally, we proposed a traffic zone attribute-index to measure tendency of traffic zones to be residential or working. The calculated attribute-index values of 73 traffic zones in Beijing were consistent with the actual traffic and land-use data. The case study demonstrates that effective traffic and travel data can be obtained from mobile phones as portable sensors and base stations as fixed sensors, providing an opportunity to improve the analysis of complex travel patterns and behaviors for travel demand modeling and transportation planning.}
}
@article{KEBISEK202011168,
title = {Artificial Intelligence Platform Proposal for Paint Structure Quality Prediction within the Industry 4.0 Concept},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {11168-11174},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.299},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320305796},
author = {M. Kebisek and P. Tanuska and L. Spendla and J. Kotianova and P. Strelec},
keywords = {artificial intelligence, automotive, big data analytics, industry 4.0, knowledge discovery, neural networks, prediction, principal component analysis},
abstract = {This article provides an artificial intelligence platform proposal for paint structure quality prediction using Big Data analytics methodologies. The whole proposal fits into the current trends that are outlined in the Industry 4.0 concept. The painting process is very complex, producing huge volumes of data, but the main problem is that the data comes from different data sources, often heterogeneous, and it is necessary to propose a way to collect and integrate them into a common repository. The motivation for this work were the industry requirements to solve specific problems that cannot be solved by standard methods but require a sophisticated and holistic approach. It is the application of artificial intelligence that suggests a solution that is not otherwise visible, and the use of standard methods would not give any satisfactory results. The result is the design of an artificial intelligence platform that has been deployed in a real manufacturing process, and the initial results confirm the correctness and validity of this step. We also present a data collection and integration architecture, which is an integral part of every big data analytics solution, and a principal component analysis that was used to reduce the dimensionality of the large number of production process data.}
}
@article{EVANGELISTA2018112,
title = {Topological support and data quality can only be assessed through multiple tests in reviewing Blattodea phylogeny},
journal = {Molecular Phylogenetics and Evolution},
volume = {128},
pages = {112-122},
year = {2018},
issn = {1055-7903},
doi = {https://doi.org/10.1016/j.ympev.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S1055790318300186},
author = {Dominic Evangelista and France Thouzé and Manpreet Kaur Kohli and Philippe Lopez and Frédéric Legendre},
keywords = {Phylogenetic signal, mtDNA, Termite, Dictyoptera, SAMS, Rogue taxa, Long branch attraction, Signal analysis},
abstract = {Assessing support for molecular phylogenies is difficult because the data is heterogeneous in quality and overwhelming in quantity. Traditionally, node support values (bootstrap frequency, Bayesian posterior probability) are used to assess confidence in tree topologies. Other analyses to assess the quality of phylogenetic data (e.g. Lento plots, saturation plots, trait consistency) and the resulting phylogenetic trees (e.g. internode certainty, parameter permutation tests, topological tests) exist but are rarely applied. Here we argue that a single qualitative analysis is insufficient to assess support of a phylogenetic hypothesis and relate data quality to tree quality. We use six molecular markers to infer the phylogeny of Blattodea and apply various tests to assess relationship support, locus quality, and the relationship between the two. We use internode-certainty calculations in conjunction with bootstrap scores, alignment permutations, and an approximately unbiased (AU) test to assess if the molecular data unambiguously support the phylogenetic relationships found. Our results show higher support for the position of Lamproblattidae, high support for the termite phylogeny, and low support for the position of Anaplectidae, Corydioidea and phylogeny of Blaberoidea. We use Lento plots in conjunction with mutation-saturation plots, calculations of locus homoplasy to assess locus quality, identify long branch attraction, and decide if the tree’s relationships are the result of data biases. We conclude that multiple tests and metrics need to be taken into account to assess tree support and data robustness.}
}
@article{KOLOSSA2018775,
title = {Data quality over data quantity in computational cognitive neuroscience},
journal = {NeuroImage},
volume = {172},
pages = {775-785},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918300053},
author = {Antonio Kolossa and Bruno Kopp},
keywords = {Computational modeling, Functional brain imaging, Signal-to-noise ratio, Reliability, Replicability},
abstract = {We analyzed factors that may hamper the advancement of computational cognitive neuroscience (CCN). These factors include a particular statistical mindset, which paves the way for the dominance of statistical power theory and a preoccupation with statistical replicability in the behavioral and neural sciences. Exclusive statistical concerns about sampling error occur at the cost of an inadequate representation of the problem of measurement error. We contrasted the manipulation of data quantity (sampling error, by varying the number of subjects) against the manipulation of data quality (measurement error, by varying the number of data per subject) in a simulated Bayesian model identifiability study. The results were clear-cut in showing that - across all levels of signal-to-noise ratios - varying the number of subjects was completely inconsequential, whereas the number of data per subject exerted massive effects on model identifiability. These results emphasize data quality over data quantity, and they call for the integration of statistics and measurement theory.}
}
@article{NEETHIRAJAN2021100408,
title = {Digital Livestock Farming},
journal = {Sensing and Bio-Sensing Research},
volume = {32},
pages = {100408},
year = {2021},
issn = {2214-1804},
doi = {https://doi.org/10.1016/j.sbsr.2021.100408},
url = {https://www.sciencedirect.com/science/article/pii/S2214180421000131},
author = {Suresh Neethirajan and Bas Kemp},
keywords = {Precision Livestock Farming, digitalization, Digital Technologies in Livestock Systems, sensor technology, big data, blockchain, data models, livestock agriculture},
abstract = {As the global human population increases, livestock agriculture must adapt to provide more livestock products and with improved efficiency while also addressing concerns about animal welfare, environmental sustainability, and public health. The purpose of this paper is to critically review the current state of the art in digitalizing animal agriculture with Precision Livestock Farming (PLF) technologies, specifically biometric sensors, big data, and blockchain technology. Biometric sensors include either noninvasive or invasive sensors that monitor an individual animal’s health and behavior in real time, allowing farmers to integrate this data for population-level analyses. Real-time information from biometric sensors is processed and integrated using big data analytics systems that rely on statistical algorithms to sort through large, complex data sets to provide farmers with relevant trending patterns and decision-making tools. Sensors enabled blockchain technology affords secure and guaranteed traceability of animal products from farm to table, a key advantage in monitoring disease outbreaks and preventing related economic losses and food-related health pandemics. Thanks to PLF technologies, livestock agriculture has the potential to address the abovementioned pressing concerns by becoming more transparent and fostering increased consumer trust. However, new PLF technologies are still evolving and core component technologies (such as blockchain) are still in their infancy and insufficiently validated at scale. The next generation of PLF technologies calls for preventive and predictive analytics platforms that can sort through massive amounts of data while accounting for specific variables accurately and accessibly. Issues with data privacy, security, and integration need to be addressed before the deployment of multi-farm shared PLF solutions becomes commercially feasible.}
}
@article{DAVIS2017224,
title = {Residential land values in the Washington, DC metro area: New insights from big data},
journal = {Regional Science and Urban Economics},
volume = {66},
pages = {224-246},
year = {2017},
issn = {0166-0462},
doi = {https://doi.org/10.1016/j.regsciurbeco.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0166046216301508},
author = {Morris A. Davis and Stephen D. Oliner and Edward J. Pinto and Sankar Bokka},
keywords = {Land, Housing, House prices, Housing boom and bust, Financial crisis},
abstract = {We use a new property-level data set and an innovative methodology to estimate the price of land from 2000 to 2013 for nearly the universe of detached single-family homes in the Washington, DC metro area and to characterize the boom-bust cycle in land and house prices at a fine geography. The results show that land prices were more volatile than house prices everywhere, but especially so in the areas where land was inexpensive in 2000. We demonstrate that the change in the land share of house value during the boom was a significant predictor of the decline in house prices during the bust, highlighting the value of focusing on land in assessing house-price risk.}
}
@incollection{TALBURT2015191,
title = {Chapter 11 - ISO Data Quality Standards for Master Data},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {191-205},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378000119},
author = {John R. Talburt and Yinle Zhou},
keywords = {ISO, ANSI, ISO 8000, ISO 22745, Semantic Encoding},
abstract = {This chapter provides a discussion of the new International Organization for Standardization (ISO) standards related to the exchange of master data. It includes an in-depth look at the ISO 8000 family of standards, including ISO 8000-110, -120, -130, and -140, and their relationship to the ISO 22745-10, -30, and -40 standards. Also an explanation is given of simple versus strong ISO 8000-110 compliance, and the value proposition for ISO 8000 compliance is discussed.}
}
@incollection{FAULKNER202081,
title = {4 - Data Fundamentals},
editor = {Alastair Faulkner and Mark Nicholson},
booktitle = {Data-Centric Safety},
publisher = {Elsevier},
pages = {81-92},
year = {2020},
isbn = {978-0-12-820790-1},
doi = {https://doi.org/10.1016/B978-0-12-820790-1.00017-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207901000176},
author = {Alastair Faulkner and Mark Nicholson},
keywords = {Data ecosystems, Data context, Data architectures, Data Integrity, Data Quality},
abstract = {Data is one component in a system. It has value. The economics of increasingly data-centric systems is explored. There is a growing reliance on data to create systems that are larger scale, have wider scope and are more complex. As reliance on the data component increases, a self-reinforcing problem of implementing checks and balances necessary to enforce appropriate levels of risk reduction arises. This chapter introduces data architecture elements of container and content. It places this architecture within an appropriate data context. It introduces the concepts of data quality and data integrity. Data integrity is placed within the Safety Management and Safety Assurance processes. Big data and machine learning are considered in this context.}
}
@article{HARRIGAN2021102246,
title = {Identifying influencers on social media},
journal = {International Journal of Information Management},
volume = {56},
pages = {102246},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102246},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220314456},
author = {Paul Harrigan and Timothy M. Daly and Kristof Coussement and Julie A. Lee and Geoffrey N. Soutar and Uwana Evers},
keywords = {Influencers, Market mavens, Big data, Social media, Twitter},
abstract = {The increased availability of social media big data has created a unique challenge for marketing decision-makers; turning this data into useful information. One of the significant areas of opportunity in digital marketing is influencer marketing, but identifying these influencers from big data sets is a continual challenge. This research illustrates how one type of influencer, the market maven, can be identified using big data. Using a mixed-method combination of both self-report survey data and publicly accessible big data, we gathered 556,150 tweets from 370 active Twitter users. We then proposed and tested a range of social-media-based metrics to identify market mavens. Findings show that market mavens (when compared to non-mavens) have more followers, post more often, have less readable posts, use more uppercase letters, use less distinct words, and use hashtags more often. These metrics are openly available from public Twitter accounts and could integrate into a broad-scale decision support system for marketing and information systems managers. These findings have the potential to improve influencer identification effectiveness and efficiency, and thus improve influencer marketing.}
}
@article{KIM201818,
title = {Exploring Determinants of Semantic Web Technology Adoption from IT Professionals' Perspective: Industry Competition, Organization Innovativeness, and Data Management Capability},
journal = {Computers in Human Behavior},
volume = {86},
pages = {18-33},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S074756321830178X},
author = {Dan J. Kim and John Hebeler and Victoria Yoon and Fred Davis},
keywords = {Semantic web, IT professionals' perspective technology adoption, Technology-organization-environment framework, Innovation diffusion theory},
abstract = {The scale and complexity of big data quickly exceed the reach of direct human comprehension and increasingly require machine assistance to semantically analyze, organize, and interpret vast and diverse sources of big data in order to unlock its strategic value. Due to its volume, velocity, variety, and veracity, big data integration challenges overwhelm traditional integration approaches leaving many integration possibilities out of reach. Unlocking the value of big data requires innovative technology. Organizations must have the innovativeness and data capability to adopt the technology and harness its potential value. The Semantic Web (SW) technology has demonstrated its potential for integrating big data and has become important technology for tackling big data. Despite its importance to manage big data, little research has examined the determinants affecting SW adoption. Drawing upon the technology–organization–environment framework as a theory base, this study develops a research model explaining the factors affecting the adoption of SW technology from IT professionals' perspective, specifically in the context of corporate computing enterprises. We validate the proposed model using a set of empirical data collected from IT professionals including IT managers, system architects, software developers, and web developers. The findings suggest that perceived usefulness, perceived ease of use, organization's innovativeness, organization's data capability, and applicability to data management are important drivers of SW adoption. This study provides new insights on theories of organizational IT adoption from IT professionals' perspectives tailored to the context of SW technology.}
}
@article{HEINRICH201895,
title = {Assessing data quality – A probability-based metric for semantic consistency},
journal = {Decision Support Systems},
volume = {110},
pages = {95-106},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618300599},
author = {Bernd Heinrich and Mathias Klier and Alexander Schiller and Gerit Wagner},
keywords = {Data quality, Data quality assessment, Data quality metric, Data consistency},
abstract = {We present a probability-based metric for semantic consistency using a set of uncertain rules. As opposed to existing metrics for semantic consistency, our metric allows to consider rules that are expected to be fulfilled with specific probabilities. The resulting metric values represent the probability that the assessed dataset is free of internal contradictions with regard to the uncertain rules and thus have a clear interpretation. The theoretical basis for determining the metric values are statistical tests and the concept of the p-value, allowing the interpretation of the metric value as a probability. We demonstrate the practical applicability and effectiveness of the metric in a real-world setting by analyzing a customer dataset of an insurance company. Here, the metric was applied to identify semantic consistency problems in the data and to support decision-making, for instance, when offering individual products to customers.}
}
@article{STOGER2021105587,
title = {Legal aspects of data cleansing in medical AI},
journal = {Computer Law & Security Review},
volume = {42},
pages = {105587},
year = {2021},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105587},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921000601},
author = {Karl Stöger and David Schneeberger and Peter Kieseberg and Andreas Holzinger},
keywords = {Data cleansing, Data quality, Medical AI, Medical devices},
abstract = {Data quality is of paramount importance for the smooth functioning of modern data-driven AI applications with machine learning as a core technology. This is also true for medical AI, where malfunctions due to "dirty data" can have particularly dramatic harmful implications. Consequently, data cleansing is an important part in improving the usability of (Big) Data for medical AI systems. However, it should not be overlooked that data cleansing can also have negative effects on data quality if not performed carefully. This paper takes an interdisciplinary look at some of the technical and legal challenges of data cleansing against the background of European medical device law, with the key message that technical and legal aspects must always be considered together in such a sensitive context.}
}
@article{FAN2021123651,
title = {The future of Internet of Things in agriculture: Plant high-throughput phenotypic platform},
journal = {Journal of Cleaner Production},
volume = {280},
pages = {123651},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123651},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620336969},
author = {Jiangchuan Fan and Ying Zhang and Weiliang Wen and Shenghao Gu and Xianju Lu and Xinyu Guo},
keywords = {Internet of things in agriculture, Big data, High-throughput phenotype, Data mining},
abstract = {With continuous collaborative research in sensor technology, communication technology, plant science, computer science and engineering science, Internet of Things (IoT) in agriculture has made a qualitative leap through environmental sensor networks, non-destructive imaging, spectral analysis, robotics, machine vision and laser radar technology. Physical and chemical analysis can continuously obtain environmental data, experimental metadata (including text, image and spectral, 3D point cloud and real-time growth data) through integrated automation platform equipment and technical means. Based on data on multi-scale, multi-environmental and multi-mode plant traits that constitute big data on plant phenotypes, genotype–phenotype–envirotype relationship in the omics system can be explored deeply. Detailed information on the formation mechanism of specific biological traits can promote the process of functional genomics, plant molecular breeding and efficient cultivation. This study summarises the development background, research process and characteristics of high-throughput plant phenotypes. A systematic review of the research progress of IoT in agriculture and plant high-throughput phenotypes is conducted, including the acquisition and analysis of plant phenotype big data, phenotypic trait prediction and multi-recombination analysis based on plant phenomics. This study proposes key techniques for current plant phenotypes, and looks forward to the research on plant phenotype detection technology in the field environment, fusion and data mining of plant phenotype multivariate data, simultaneous observation of multi-scale phenotype platform and promotion of a comprehensive high-throughput phenotype technology.}
}
@article{AHMAD2021125834,
title = {Artificial intelligence in sustainable energy industry: Status Quo, challenges and opportunities},
journal = {Journal of Cleaner Production},
volume = {289},
pages = {125834},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.125834},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621000548},
author = {Tanveer Ahmad and Dongdong Zhang and Chao Huang and Hongcai Zhang and Ningyi Dai and Yonghua Song and Huanxin Chen},
keywords = {Artificial intelligence, Renewable energy, Energy demand, Decision making, Big data, Energy digitization},
abstract = {The energy industry is at a crossroads. Digital technological developments have the potential to change our energy supply, trade, and consumption dramatically. The new digitalization model is powered by the artificial intelligence (AI) technology. The integration of energy supply, demand, and renewable sources into the power grid will be controlled autonomously by smart software that optimizes decision-making and operations. AI will play an integral role in achieving this goal. This study focuses on the use of AI techniques in the energy sector. This study aims to present a realistic baseline that allows researchers and readers to compare their AI efforts, ambitions, new state-of-the-art applications, challenges, and global roles in policymaking. We covered three major aspects, including: i) the use of AI in solar and hydrogen power generation; (ii) the use of AI in supply and demand management control; and (iii) recent advances in AI technology. This study explored how AI techniques outperform traditional models in controllability, big data handling, cyberattack prevention, smart grid, IoT, robotics, energy efficiency optimization, predictive maintenance control, and computational efficiency. Big data, the development of a machine learning model, and AI will play an important role in the future energy market. Our study’s findings show that AI is becoming a key enabler of a complex, new and data-related energy industry, providing a key magic tool to increase operational performance and efficiency in an increasingly cut-throat environment. As a result, the energy industry, utilities, power system operators, and independent power producers may need to focus more on AI technologies if they want meaningful results to remain competitive. New competitors, new business strategies, and a more active approach to customers would require informed and flexible regulatory engagement with the associated complexities of customer safety, privacy, and information security. Given the pace of development in information technology, AI and data analysis, regulatory approvals for new services and products in the new Era of digital energy markets can be enforced as quickly and efficiently as possible.}
}
@article{MADHIKERMI2016145,
title = {Data quality assessment of maintenance reporting procedures},
journal = {Expert Systems with Applications},
volume = {63},
pages = {145-164},
year = {2016},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.06.043},
url = {https://www.sciencedirect.com/science/article/pii/S095741741630330X},
author = {Manik Madhikermi and Sylvain Kubler and Jérémy Robert and Andrea Buda and Kary Främling},
keywords = {Data quality, Information quality, Multi-criteria decision making, Analytic hierarchy process, Decision support systems, Maintenance},
abstract = {Today’s largest and fastest growing companies’ assets are no longer physical, but rather digital (software, algorithms...). This is all the more true in the manufacturing, and particularly in the maintenance sector where quality of enterprise maintenance services are closely linked to the quality of maintenance data reporting procedures. If quality of the reported data is too low, it can results in wrong decision-making and loss of money. Furthermore, various maintenance experts are involved and directly concerned about the quality of enterprises’ daily maintenance data reporting (e.g., maintenance planners, plant managers...), each one having specific needs and responsibilities. To address this Multi-Criteria Decision Making (MCDM) problem, and since data quality is hardly considered in existing expert maintenance systems, this paper develops a maintenance reporting quality assessment (MRQA) dashboard that enables any company stakeholder to easily – and in real-time – assess/rank company branch offices in terms of maintenance reporting quality. From a theoretical standpoint, AHP is used to integrate various data quality dimensions as well as expert preferences. A use case describes how the proposed MRQA dashboard is being used by a Finnish multinational equipment manufacturer to assess and enhance reporting practices in a specific or a group of branch offices.}
}
@article{BELLINI2019521,
title = {Data quality and blockchain technology},
journal = {Anaesthesia Critical Care & Pain Medicine},
volume = {38},
number = {5},
pages = {521-522},
year = {2019},
issn = {2352-5568},
doi = {https://doi.org/10.1016/j.accpm.2018.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S2352556818305368},
author = {Valentina Bellini and Alberto Petroni and Giuseppina Palumbo and Elena Bignami},
keywords = {Machine learning, Artificial intelligence, Blockchain technology}
}
@article{HOSEINZADEH2020101518,
title = {Quality of location-based crowdsourced speed data on surface streets: A case study of Waze and Bluetooth speed data in Sevierville, TN},
journal = {Computers, Environment and Urban Systems},
volume = {83},
pages = {101518},
year = {2020},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2020.101518},
url = {https://www.sciencedirect.com/science/article/pii/S0198971520302519},
author = {Nima Hoseinzadeh and Yuandong Liu and Lee D. Han and Candace Brakewood and Amin Mohammadnazar},
keywords = {Location-based data, Crowdsourced data, Waze, Bluetooth, Big data, Smart cities, Surface streets},
abstract = {Obtaining accurate speed and travel time information is a challenge for researchers, geographers, and transportation agencies. In the past, traffic data were usually acquired and disseminated by government agencies through fixed-location sensors. High costs, infrastructure demands, and low coverage levels of these sensor devices require agencies and researchers to look beyond the traditional approaches. With the emergence of smartphones and navigation apps, location-based and crowdsourced Big Data are receiving increased attention. In this regard, location-based big data (LocBigData) collected from probe vehicles and road users can be used to provide speed and travel time information in different locations. Examining the quality of crowdsourced data is essential for researchers and agencies before using them. This study assessed the quality of Waze speed data from surface streets and conducted a case study in Sevierville, Tennessee. Typically, examining the quality of these data in surface streets and arterials is more challenging than freeways data. This research used Bluetooth speed data as the ground truth, which is independent of Waze data. In this study, three steps of methodology were used. In the first step, Waze speed data was compared to Bluetooth data in terms of accuracy, mean difference, and distribution similarity. In the second step, a k-means algorithm was used to categorize Waze data quality, and a multinomial logistics regression model was performed to explore the significant factors that impact data quality. Finally, in the third step, machine learning techniques were conducted to predict the data quality in different conditions. The result of the comparison showed a similar pattern and a slight difference between datasets, which verified the quality of Waze speed data. The statistical model indicates that that Waze speed data are more accurate in peak hours than in night hours. Also, the traffic speed, traffic volume, and segment length have a significant association on the accuracy of Waze data on surface streets. Finally, the result of machine learning prediction showed that a KNN method performed the highest prediction accuracy of 84.5% and 82.9% of the time for training and test datasets, respectively. Overall, the study results suggest that Waze speed data is a promising data source for surface streets.}
}
@article{LI20189,
title = {Big enterprise registration data imputation: Supporting spatiotemporal analysis of industries in China},
journal = {Computers, Environment and Urban Systems},
volume = {70},
pages = {9-23},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0198971517301916},
author = {Fa Li and Zhipeng Gui and Huayi Wu and Jianya Gong and Yuan Wang and Siyu Tian and Jiawen Zhang},
keywords = {Geocoding, Missing values imputation, High Performance Computing, Industrial spatial distribution, Urban spatial structure, Short text classification},
abstract = {Big, fine-grained enterprise registration data that includes time and location information enables us to quantitatively analyze, visualize, and understand the patterns of industries at multiple scales across time and space. However, data quality issues like incompleteness and ambiguity, hinder such analysis and application. These issues become more challenging when the volume of data is immense and constantly growing. High Performance Computing (HPC) frameworks can tackle big data computational issues, but few studies have systematically investigated imputation methods for enterprise registration data in this type of computing environment. In this paper, we propose a big data imputation workflow based on Apache Spark as well as a bare-metal computing cluster, to impute enterprise registration data. We integrated external data sources, employed Natural Language Processing (NLP), and compared several machine-learning methods to address incompleteness and ambiguity problems found in enterprise registration data. Experimental results illustrate the feasibility, efficiency, and scalability of the proposed HPC-based imputation framework, which also provides a reference for other big georeferenced text data processing. Using these imputation results, we visualize and briefly discuss the spatiotemporal distribution of industries in China, demonstrating the potential applications of such data when quality issues are resolved.}
}
@article{LIU201990,
title = {Constructing Large Scale Cohort for Clinical Study on Heart Failure with Electronic Health Record in Regional Healthcare Platform: Challenges and Strategies in Data Reuse},
journal = {Chinese Medical Sciences Journal},
volume = {34},
number = {2},
pages = {90-102},
year = {2019},
issn = {1001-9294},
doi = {https://doi.org/10.24920/003579},
url = {https://www.sciencedirect.com/science/article/pii/S1001929419300318},
author = {Daowen Liu and Liqi Lei and Tong Ruan and Ping He},
keywords = {electronic health records, clinical terminology knowledge graph, clinical special disease case repository, evaluation of data quality, large scale cohort study},
abstract = {Regional healthcare platforms collect clinical data from hospitals in specific areas for the purpose of healthcare management. It is a common requirement to reuse the data for clinical research. However, we have to face challenges like the inconsistence of terminology in electronic health records (EHR) and the complexities in data quality and data formats in regional healthcare platform. In this paper, we propose methodology and process on constructing large scale cohorts which forms the basis of causality and comparative effectiveness relationship in epidemiology. We firstly constructed a Chinese terminology knowledge graph to deal with the diversity of vocabularies on regional platform. Secondly, we built special disease case repositories (i.e., heart failure repository) that utilize the graph to search the related patients and to normalize the data. Based on the requirements of the clinical research which aimed to explore the effectiveness of taking statin on 180-days readmission in patients with heart failure, we built a large-scale retrospective cohort with 29647 cases of heart failure patients from the heart failure repository. After the propensity score matching, the study group (n=6346) and the control group (n=6346) with parallel clinical characteristics were acquired. Logistic regression analysis showed that taking statins had a negative correlation with 180-days readmission in heart failure patients. This paper presents the workflow and application example of big data mining based on regional EHR data.}
}
@article{ALWIS2022103624,
title = {A survey on smart farming data, applications and techniques},
journal = {Computers in Industry},
volume = {138},
pages = {103624},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103624},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000197},
author = {Sandya De Alwis and Ziwei Hou and Yishuo Zhang and Myung Hwan Na and Bahadorreza Ofoghi and Atul Sajjanhar},
keywords = {Smart farming, Data analysis, Big data, Machine learning, Digital farming, Predictive farming, Farming industry},
abstract = {The Internet of Things (IoT) and the relevant technologies have had a significant impact on smart farming as a major sub-domain within the field of agriculture. Modern technology supports data collection from IoT devices through several farming processes. The extensive amount of collected smart farming data can be utilized for daily decision making and analysis such as yield prediction, growth analysis, quality maintenance, animal and aquaculture, as well as farm management. This survey focuses on three major aspects of contemporary smart farming. First, it highlights various types of big data generated through smart farming and makes a broad categorization of such data. Second, this paper discusses a comprehensive set of typical applications of big data in smart farming. Third, it identifies and introduces the principal big data and machine learning techniques that are utilized in smart farming data analysis. In doing so, this survey also identifies some of the major, current challenges in smart farming big data analysis.This paper provides a discussion on potential pathways toward more effective smart farming through relevant analytics-guided decision making.}
}
@incollection{SEBASTIANCOLEMAN2013173,
title = {Chapter 13 - Directives for Data Quality Strategy},
editor = {Laura Sebastian-Coleman},
booktitle = {Measuring Data Quality for Ongoing Improvement},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {173-192},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-397033-6},
doi = {https://doi.org/10.1016/B978-0-12-397033-6.00014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123970336000146},
author = {Laura Sebastian-Coleman}
}
@article{ZHANG2021101336,
title = {A framework of energy-consumption driven discrete manufacturing system},
journal = {Sustainable Energy Technologies and Assessments},
volume = {47},
pages = {101336},
year = {2021},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.101336},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821003465},
author = {Tao Zhang and Weixi Ji and Yongtao Qiu},
keywords = {Energy-efficient optimization, Discrete manufacturing system, Data preprocessing, Data mining},
abstract = {Because of big data on energy consumption, there is a lack of research on the discrete manufacturing system. The discrete manufacturing system has plenty of multi-source and heterogeneous data; it was challenging to collect real-time data. Recently, low carbon and green manufacturing is a hot field; especially, it can save electrical energy. This paper proposes a significant energy consumption data of a data-driven analysis framework, which promoting the energy efficiency of discrete manufacturing plant, equipment, and workshop production process. Firstly, put forward the evaluation standards of energy efficiency for discrete manufacturing shops. Then make energy-consumption data preprocessing. Efficiency optimization of big data mining method is put forward based on grid computing function. Design the discrete manufacturing system energy-consumption parameter values, then summarizes prediction algorithms and models in order to predict the results and the trends. Finally, introduce the application of a mobile phone shell manufacturing shop to verify the proposed framework. Further research will focus on energy-consumption data mining processing.}
}
@incollection{WANG2021295,
title = {Chapter 13 - Artificial Intelligence for Flood Observation},
editor = {Guy J-P. Schumann},
booktitle = {Earth Observation for Flood Applications},
publisher = {Elsevier},
pages = {295-304},
year = {2021},
series = {Earth Observation},
isbn = {978-0-12-819412-6},
doi = {https://doi.org/10.1016/B978-0-12-819412-6.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128194126000134},
author = {Ruo-Qian Wang},
keywords = {artificial intelligence, natural language processing, computer vision, machine learning, Big Data, Internet of Things, crowdsourcing, citizen science, surveillance video},
abstract = {Artificial intelligence (AI) is fundamentally changing our society, benefiting from the big data revolution and dramatical  declination in the Internet of Things (IoT) costs. Flood research and applications will progress with this emerging technology, as AI is creating new flood data sources, enhancing our capability to analyze the data, and improving our accuracy of flood predictions. This chapter introduces the basic concepts of AI and its technical frontier. Using the method of “review of the reviews” with example highlight the emerging AI applications in the field of flood hazards is summarized in terms of the data sources, including crowdsourcing and surveillance camera videos. The use of the AI-enabled big data is also discussed. The opportunities and barriers of this new technology are summarized. At the end of the chapter, the trend and the research gaps are identified in this field.}
}
@article{YANG2021,
title = {Standardization of collection, storage, annotation, and management of data related to medical artificial intelligence},
journal = {Intelligent Medicine},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621001200},
author = {Yahan Yang and Ruiyang Li and Yifan Xiang and Duoru Lin and Anqi Yan and Wenben Chen and Zhongwen Li and Weiyi Lai and Xiaohang Wu and Cheng Wan and Wei Bai and Xiucheng Huang and Qiang Li and Wenrui Deng and Xiyang Liu and Yucong Lin and Pisong Yan and Haotian Lin},
keywords = {Artificial intelligence, Big data, Intelligent medicine, Data collection, Data storage, Data annotation, Data management},
abstract = {Medical artificial intelligence (AI) and big data technology have rapidly advanced in recent years, and they are now routinely used for image-based diagnosis. China has a massive amount of medical data. However, a uniform criteria for medical data quality have yet to be established. Therefore, this review aimed to develop a standardized and detailed set of quality criteria for medical data collection, storage, annotation, and management related to medical AI. This will greatly improve the process of medical data resource sharing and the use of AI in clinical medicine.}
}
@article{VIDAURRE2018646,
title = {Discovering dynamic brain networks from big data in rest and task},
journal = {NeuroImage},
volume = {180},
pages = {646-656},
year = {2018},
note = {Brain Connectivity Dynamics},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.06.077},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917305487},
author = {Diego Vidaurre and Romesh Abeysuriya and Robert Becker and Andrew J. Quinn and Fidel Alfaro-Almagro and Stephen M. Smith and Mark W. Woolrich},
abstract = {Brain activity is a dynamic combination of the responses to sensory inputs and its own spontaneous processing. Consequently, such brain activity is continuously changing whether or not one is focusing on an externally imposed task. Previously, we have introduced an analysis method that allows us, using Hidden Markov Models (HMM), to model task or rest brain activity as a dynamic sequence of distinct brain networks, overcoming many of the limitations posed by sliding window approaches. Here, we present an advance that enables the HMM to handle very large amounts of data, making possible the inference of very reproducible and interpretable dynamic brain networks in a range of different datasets, including task, rest, MEG and fMRI, with potentially thousands of subjects. We anticipate that the generation of large and publicly available datasets from initiatives such as the Human Connectome Project and UK Biobank, in combination with computational methods that can work at this scale, will bring a breakthrough in our understanding of brain function in both health and disease.}
}
@article{SADIQ2017150,
title = {Open data: Quality over quantity},
journal = {International Journal of Information Management},
volume = {37},
number = {3},
pages = {150-154},
year = {2017},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216309021},
author = {Shazia Sadiq and Marta Indulska},
keywords = {Open data, Data quality},
abstract = {Open data aims to unlock the innovation potential of businesses, governments, and entrepreneurs, yet it also harbours significant challenges for its effective use. While numerous innovation successes exist that are based on the open data paradigm, there is uncertainty over the data quality of such datasets. This data quality uncertainty is a threat to the value that can be generated from such data. Data quality has been studied extensively over many decades and many approaches to data quality management have been proposed. However, these approaches are typically based on datasets internal to organizations, with known metadata, and domain knowledge of the data semantics. Open data, on the other hand, are often unfamiliar to the user and may lack metadata. The aim of this research note is to outline the challenges in dealing with data quality of open datasets, and to set an agenda for future research to address this risk to deriving value from open data investments.}
}
@article{WANG2022e97,
title = {Registries, Databases and Repositories for Developing Artificial Intelligence in Cancer Care},
journal = {Clinical Oncology},
volume = {34},
number = {2},
pages = {e97-e103},
year = {2022},
note = {Artificial Intelligence in Radiation Therapy},
issn = {0936-6555},
doi = {https://doi.org/10.1016/j.clon.2021.11.040},
url = {https://www.sciencedirect.com/science/article/pii/S0936655521004593},
author = {J.W. Wang and M. Williams},
keywords = {Artificial intelligence, Big Data, database, deep learning, registries, repository},
abstract = {Modern artificial intelligence techniques have solved some previously intractable problems and produced impressive results in selected medical domains. One of their drawbacks is that they often need very large amounts of data. Pre-existing datasets in the form of national cancer registries, image/genetic depositories and clinical datasets already exist and have been used for research. In theory, the combination of healthcare Big Data with modern, data-hungry artificial intelligence techniques should offer significant opportunities for artificial intelligence development, but this has not yet happened. Here we discuss some of the structural reasons for this, barriers preventing artificial intelligence from making full use of existing datasets, and make suggestions as to enable progress. To do this, we use the framework of the 6Vs of Big Data and the FAIR criteria for data sharing and availability (Findability, Accessibility, Interoperability, and Reuse). We share our experience in navigating these barriers through The Brain Tumour Data Accelerator, a Brain Tumour Charity-supported initiative to integrate fragmented patient data into an enriched dataset. We conclude with some comments as to the limits of such approaches.}
}
@article{LOOTEN2019104825,
title = {What can millions of laboratory test results tell us about the temporal aspect of data quality? Study of data spanning 17 years in a clinical data warehouse},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104825},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.12.030},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718307089},
author = {Vincent Looten and Liliane {Kong Win Chang} and Antoine Neuraz and Marie-Anne Landau-Loriot and Benoit Vedie and Jean-Louis Paul and Laëtitia Mauge and Nadia Rivet and Angela Bonifati and Gilles Chatellier and Anita Burgun and Bastien Rance},
keywords = {Quality control, Computational biology/methods*, Information storage and retrieval, Humans, Clinical laboratory information systems},
abstract = {Objective
To identify common temporal evolution profiles in biological data and propose a semi-automated method to these patterns in a clinical data warehouse (CDW).
Materials and Methods
We leveraged the CDW of the European Hospital Georges Pompidou and tracked the evolution of 192 biological parameters over a period of 17 years (for 445,000 + patients, and 131 million laboratory test results).
Results
We identified three common profiles of evolution: discretization, breakpoints, and trends. We developed computational and statistical methods to identify these profiles in the CDW. Overall, of the 192 observed biological parameters (87,814,136 values), 135 presented at least one evolution. We identified breakpoints in 30 distinct parameters, discretizations in 32, and trends in 79.
Discussion and conclusion
our method allowed the identification of several temporal events in the data. Considering the distribution over time of these events, we identified probable causes for the observed profiles: instruments or software upgrades and changes in computation formulas. We evaluated the potential impact for data reuse. Finally, we formulated recommendations to enable safe use and sharing of biological data collection to limit the impact of data evolution in retrospective and federated studies (e.g. the annotation of laboratory parameters presenting breakpoints or trends).}
}
@article{JAGATHEESAPERUMAL2022107691,
title = {A holistic survey on the use of emerging technologies to provision secure healthcare solutions},
journal = {Computers and Electrical Engineering},
volume = {99},
pages = {107691},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107691},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622000131},
author = {Senthil Kumar Jagatheesaperumal and Preeti Mishra and Nour Moustafa and Rahul Chauhan},
keywords = {Healthcare, Security, Internet of Things, Artificial intelligence, Machine learning, Deep learning, 5G networks},
abstract = {Healthcare applications demand systematic approaches to eradicate inevitable human errors to design a framework that systematically eliminates cyber-threats. The key focus of this paper is to provide a comprehensive survey on the use of modern enabling technologies, such as the Internet of Things (IoT), 5G networks, artificial intelligence (AI), and big data analytics, for providing secure and resilient healthcare solutions. A detailed taxonomy of existing technologies has been demonstrated for tackling various healthcare problems, along with their security-related issues in handling healthcare data. The application areas of each of the emerging technologies, along with their security aspects, are explained. Furthermore, an IoT-enabled smart pill bottle prototype is designed and illustrated as a case study for providing better understanding of the subject. Finally, various key research challenges are summarized with future research directions.}
}
@article{R2020235,
title = {Weibull Cumulative Distribution based real-time response and performance capacity modeling of Cyber–Physical Systems through software defined networking},
journal = {Computer Communications},
volume = {150},
pages = {235-244},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419311673},
author = {Gifty R. and Bharathi R.},
keywords = {Cyber–Physical Systems (CPS), Weibull Cumulative Distribution, Big data, Response time},
abstract = {Huge volumes of data are generated at rates faster than the speed of computing resources and executing processors available in market place. This anticipates a draft of information challenges associated with the performance capacity and the ability of big data processing systems to retort in real-time. Moreover, the elapsed time between probabilistic failures drops as the scale of information increases. An error occurred at a specific cluster node of a large Cyber–Physical System influences the overall computation requires to unfold big data transactions. Numerous failure characteristics, statistical response time and lifetime evaluation can be modeled through Weibull Distribution. In this paper, to scrutinize the latency for a data infrastructure, the three-parameter Weibull Cumulative Distribution is used through software defined networking in cyber–physical system. This speculation predicts that the shape of the response time distribution confide in the shape of the learning curve and depicts its parameters to the criterion of the input distribution.}
}
@article{BUFFAT2017277,
title = {Big data GIS analysis for novel approaches in building stock modelling},
journal = {Applied Energy},
volume = {208},
pages = {277-290},
year = {2017},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2017.10.041},
url = {https://www.sciencedirect.com/science/article/pii/S030626191731454X},
author = {René Buffat and Andreas Froemelt and Niko Heeren and Martin Raubal and Stefanie Hellweg},
keywords = {Building heat demand, Big data, Large scale modelling, Bottom-up modelling, GIS, Climate data, Spatio-temporal modelling},
abstract = {Building heat demand is responsible for a significant share of the total global final energy consumption. Building stock models with a high spatio-temporal resolution are a powerful tool to investigate the effects of new building policies aimed at increasing energy efficiency, the introduction of new heating technologies or the integration of buildings within an energy system based on renewable energy sources. Therefore, building stock models have to be able to model the improvements and variation of used materials in buildings. In this paper, we propose a method based on generalized large-scale geographic information system (GIS) to model building heat demand of large regions with a high temporal resolution. In contrast to existing building stock models, our approach allows to derive the envelope of all buildings from digital elevation models and to model location dependent effects such as shadowing due to the topography and climate conditions. We integrate spatio-temporal climate data for temperature and solar radiation to model climate effects of complex terrain. The model is validated against a database containing the measured energy demand of 1845 buildings of the city of St. Gallen, Switzerland and 120 buildings of the Alpine village of Zernez, Switzerland. The proposed model is able to assess and investigate large regions by using spatial data describing natural and anthropogenic land features. The validation resulted in an average goodness of fit (R2) of 0.6.}
}
@article{BRONSELAER201895,
title = {An incremental approach for data quality measurement with insufficient information},
journal = {International Journal of Approximate Reasoning},
volume = {96},
pages = {95-111},
year = {2018},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2018.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X17307478},
author = {A. Bronselaer and J. Nielandt and G. {De Tré}},
keywords = {Data quality measurement, Uncertainty modelling, Insufficient information, Possibility theory},
abstract = {Recently, a fundamental study on measurement of data quality introduced an ordinal-scaled procedure of measurement. Besides the pure ordinal information about the level of quality, numerical information is induced when considering uncertainty involved during measurement. In the case where uncertainty is modelled as probability, this numerical information is ratio-scaled. An essential property of the mentioned approach is that the application of a measure on a large collection of data can be represented efficiently in the sense that (i) the representation has a low storage complexity and (ii) it can be updated incrementally when new data are observed. However, this property only holds when the evaluation of predicates is clear and does not deal with uncertainty. For some dimensions of quality, this assumption is far too strong and uncertainty comes into play almost naturally. In this paper, we investigate how the presence of uncertainty influences the efficiency of a measurement procedure. Hereby, we focus specifically on the case where uncertainty is caused by insufficient information and is thus modelled by means of possibility theory. It is shown that the amount of data that reaches a certain level of quality, can be summarized as a possibility distribution over the set of natural numbers. We investigate an approximation of this distribution that has a controllable loss of information, allows for incremental updates and exhibits a low space complexity.}
}
@article{WIBISONO201633,
title = {Traffic big data prediction and visualization using Fast Incremental Model Trees-Drift Detection (FIMT-DD)},
journal = {Knowledge-Based Systems},
volume = {93},
pages = {33-46},
year = {2016},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2015.10.028},
url = {https://www.sciencedirect.com/science/article/pii/S0950705115004165},
author = {Ari Wibisono and Wisnu Jatmiko and Hanief Arief Wisesa and Benny Hardjono and Petrus Mursanto},
keywords = {Intelligent traffic systems, Data stream, Traffic prediction, Traffic visualization},
abstract = {Information extraction using distributed sensors has been widely used to obtain information knowledge from various regions or areas. Vehicle traffic data extraction is one of the ways to gather information in order to get the traffic condition information. This research intends to predict and visualize the traffic conditions in a particular road region. Traffic data was obtained from Department of Transport UK. These data are collected using hundreds of sensors for 24 h. Thus, the size of data is very huge. In order to get the behavior of the traffic condition, we need to analyze the huge dataset which was obtained from the sensors. The uses of conventional data mining methods are not sufficient to use, due to the process of knowledge building that should store data temporary in the memory. The fact that data is continuously becoming larger over time, therefore we need to find a method that could automatically adapt to process data in the form of streams. We use method called FIMT-DD (Fast Incremental Model Trees-Drift Detection) to analyze and predict the very large traffic dataset. Based on the prediction system that we have developed, we also visualize the prediction of traffic flow condition within generated sensor point in the real map simulation.}
}
@article{THOMAS2021101994,
title = {Advances in monitoring and evaluation in low- and middle-income countries},
journal = {Evaluation and Program Planning},
volume = {89},
pages = {101994},
year = {2021},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2021.101994},
url = {https://www.sciencedirect.com/science/article/pii/S0149718921000896},
author = {James C. Thomas and Kathy Doherty and Stephanie Watson-Grant and Manish Kumar},
keywords = {Monitoring and evaluation, Health information systems, Developing countries},
abstract = {Data to inform and improve health care systems in low- and middle-income countries (LMICs) has been facilitated by the development of monitoring and evaluation (M&E) systems. The drivers of change in M&E systems over the last 50 years have included a series of health concerns that have animated global donors (e.g., family planning, vaccination campaigns, and HIV/AIDS); the data requirements of donors; improved national economies enabling LMICs to invest more in M&E systems; and rapid advances in digital technologies. Progress has included the training and expansion of an M&E workforce, the creation of systems for data collection and use, and processes for assessing and ensuring data quality. Controversies have included the development of disease-specific systems that do not coordinate with each other, and a growing burden on health care deliverers to collect data for a proliferating number of health and process indicators. Digital technologies offer the promise of real time data and quick adaptation but also raise ethical and privacy concerns. The desire for speed can cast large-scale evaluations, considered by some to be the gold standard, in an unfavorable light as slow and expensive. Accordingly, there is a growing demand for speedy evaluations that rely on routine health information systems and privately collected “big data” from electronic health records and social media.}
}
@article{VALENCA2021100008,
title = {Main challenges and opportunities to dynamic road space allocation: From static to dynamic urban designs},
journal = {Journal of Urban Mobility},
volume = {1},
pages = {100008},
year = {2021},
issn = {2667-0917},
doi = {https://doi.org/10.1016/j.urbmob.2021.100008},
url = {https://www.sciencedirect.com/science/article/pii/S266709172100008X},
author = {Gabriel Valença and Filipe Moura and Ana {Morais de Sá}},
keywords = {Dynamic road space allocation, Big data, Smart cities, Intelligent transportation systems, Information and communication technology, Urban planning},
abstract = {Urban planning has focused on reallocating road space from automobile to more sustainable transport modes in many cities worldwide. Mostly in urban areas, road space (from façade to façade) is highly disputed by different urban activities and functions. Nonetheless, there are varying demand periods during the day in which road space is underutilized due to its static design. Underutilized spaces could be used for other mobility or access purposes to improve efficiency. Sensing road space, using big data and transport demand management tools, may characterize different demand patterns, adapt the road space dynamically and, ultimately, promote efficiency in using a scarce resource, such as urban road space. This approach also reinforces short-term flexibility in urban planning, allowing for better responses to unpredictable events. This paper defines the concept of dynamic road space allocation by discussing the previous literature on dynamic allocation of space. We propose a methodological framework and discuss the technological solutions as well as the many challenges of implementing dynamic road space allocation.}
}
@article{SHENG2019321,
title = {Technology in the 21st century: New challenges and opportunities},
journal = {Technological Forecasting and Social Change},
volume = {143},
pages = {321-335},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517311319},
author = {Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang},
keywords = {Business intelligence, Big data, Big data analytics, Advanced techniques, Decision-making},
abstract = {Although big data, big data analytics (BDA) and business intelligence have attracted growing attention of both academics and practitioners, a lack of clarity persists about how BDA has been applied in business and management domains. In reflecting on Professor Ayre's contributions, we want to extend his ideas on technological change by incorporating the discourses around big data, BDA and business intelligence. With this in mind, we integrate the burgeoning but disjointed streams of research on big data, BDA and business intelligence to develop unified frameworks. Our review takes on both technical and managerial perspectives to explore the complex nature of big data, techniques in big data analytics and utilisation of big data in business and management community. The advanced analytics techniques appear pivotal in bridging big data and business intelligence. The study of advanced analytics techniques and their applications in big data analytics led to identification of promising avenues for future research.}
}
@article{KHALOUFI2018294,
title = {Security model for Big Healthcare Data Lifecycle},
journal = {Procedia Computer Science},
volume = {141},
pages = {294-301},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.199},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318520},
author = {Hayat Khaloufi and Karim Abouelmehdi and Abderrahim Beni-hssane and Mostafa Saadi},
keywords = {Big data Security, Big data in healthcare, Big data lifecycle, Security threat model},
abstract = {Big data is a concept that aimed at collecting, storing, processing and transforming large amount of data into value using new combination of strategies and technologies. Big data is characterized by data that have a large volume, massive velocity, numerous variety, useful value, and veracity. Big Data Analytics offers tremendous insights to different organizations especially in healthcare. Currently, Big healthcare data has the highest potential for improving patient outcomes, gaining valuable insights, predicting outbreaks of epidemics, avoiding preventable diseases and effectively minimizing the cost of healthcare delivery. However, the dynamic nature of health data presents various conceptual, technical, legal and ethical challenges associated with the data processing and analysis activities. The big data security and privacy concepts are some of the most pertinent issues and have become increasingly significant associated with big healthcare data in the modern world. In this paper, we give an overview of big data characteristics and challenges in healthcare and present big healthcare data lifecycle integrated with security threats and attacks to provide encompass policies and mechanisms that aim at solving the various security challenges in each step of big data lifecycle. The focus is also placed on the description of the recently proposed techniques related to authentication, encryption, anonymization, access control, and privacy. We finally propose an approach to secure threat model for big healthcare data lifecycle as a main contribution of this paper.}
}
@article{DUVIER2018358,
title = {Data quality and governance in a UK social housing initiative: Implications for smart sustainable cities},
journal = {Sustainable Cities and Society},
volume = {39},
pages = {358-365},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2018.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717312520},
author = {Caroline Duvier and P.B. Anand and Crina Oltean-Dumbrava},
keywords = {Data quality, Data interoperability, Social housing, Smart sustainable cities, Business intelligence},
abstract = {Smart Sustainable Cities (SSC) consist of multiple stakeholders, who must cooperate in order for SSCs to be successful. Housing is an important challenge and in many cities, therefore, a key stakeholder are social housing organisations. This paper introduces a qualitative case study of a social housing provider in the UK who implemented a business intelligence project (a method to assess data networks within an organisation) to increase data quality and data interoperability. Our analysis suggests that creating pathways for different information systems within an organisation to ‘talk to’ each other is the first step. Some of the issues during the project implementation include the lack of training and development, organisational reluctance to change, and the lack of a project plan. The challenges faced by the organisation during this project can be helpful for those implementing SSCs. Currently, many SSC frameworks and models exist, yet most seem to neglect localised challenges faced by the different stakeholders. This paper hopes to help bridge this gap in the SSC research agenda.}
}
@article{KREGEL2021107083,
title = {Process Mining for Six Sigma: Utilising Digital Traces},
journal = {Computers & Industrial Engineering},
volume = {153},
pages = {107083},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.107083},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220307531},
author = {I. Kregel and D. Stemann and J. Koch and A. Coners},
keywords = {process mining, six sigma, DMAIC, big data analytics, data science, project management},
abstract = {Six Sigma is one of the most successful quality management philosophies of the past 20 years. However, the current challenges facing companies, such as rising process and supply chain complexity, as well as high volumes of unstructured data, cannot easily be answered by relying on traditional Six Sigma tools. Instead, the Process Mining (PM) technology using big data analytics promises valuable support for 6S and its data analysis capabilities. The article presents a design science research project in which a method for the integration of PM in Six Sigma’s DMAIC project structure was developed. This method could be extended, refined and tested during three evaluation cycles: an expert evaluation with Six Sigma professionals, a technical experiment and finally a multi case study in a company. The method therefore was eventually endorsed by 6S experts and successfully applied in a first pilot setting. This article presents the first developed method for the integration of PM and Six Sigma. It follows the recommendations of many researchers to test Six Sigma as an application field of PM as well as using the potential of big data analytics. The method can be used by researchers and practitioners alike to implement, test and verify its design in organisations.}
}
@article{LOPEZ20162128,
title = {Data Quality Control for St. Petersburg Flood Warning System},
journal = {Procedia Computer Science},
volume = {80},
pages = {2128-2140},
year = {2016},
note = {International Conference on Computational Science 2016, ICCS 2016, 6-8 June 2016, San Diego, California, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.532},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916310225},
author = {Jose Luis Araya Lopez and Anna V. Kalyuzhnaya and Sergey S. Kosukhin and Sergey V. Ivanov},
keywords = {outliers, quality-control, principal components, gap filling},
abstract = {This paper focuses on techniques for dealing with imperfect data in a frame of early warning system (EWS). Despite the fact that data may be technically damaged by presenting noise, outliers or missing values, met-ocean simulation systems have to deal with them to provide data transaction between models, real time data assimilation, calibration, etc. In this context data quality-control becomes one of the most important parts of EWS. St. Petersburg FWS was considered as an example of EWS. Quality control in St. Petersburg FWS contains blocks of technical control, human mistakes control, statistical control of simulated fields, statistical control and restoration of measurements and control using alternative models. Domain specific quality control was presented as two types of procedures based on theoretically proved methods were applied. The first procedure is based on probabilistic model of dynamical system, where processes are spatially interrelated and could be implemented in a form of multivariate regression (MRM). The second procedure is based on principal component analysis extended for taking into account temporal relations in data set (ePCA).}
}
@incollection{MCKNIGHT201432,
title = {Chapter Four - Data Quality: Passing the Standard},
editor = {William McKnight},
booktitle = {Information Management},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {32-43},
year = {2014},
isbn = {978-0-12-408056-0},
doi = {https://doi.org/10.1016/B978-0-12-408056-0.00004-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124080560000047},
author = {William McKnight},
keywords = {data quality, referential integrity, system of origination, data profiling},
abstract = {We might as well not do any data storage if we are not storing and passing high quality data. This chapter defines data quality and a program to maintain high standards throughout the enterprise.}
}
@article{LIONO2019196,
title = {QDaS: Quality driven data summarisation for effective storage management in Internet of Things},
journal = {Journal of Parallel and Distributed Computing},
volume = {127},
pages = {196-208},
year = {2019},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S074373151830220X},
author = {Jonathan Liono and Prem Prakash Jayaraman and A.K. Qin and Thuong Nguyen and Flora D. Salim},
keywords = {Quality of data, Storage management, Internet of Things (IoT), Cloud computing, Quality of service, Data summarisation},
abstract = {The proliferation of Internet of Things (IoT) has led to the emergence of enabling many interesting applications within the realm of several domains including smart cities. However, the accumulation of data from smart IoT devices poses significant challenges for data storage while there are needs to deliver relevant and high quality services to consumers. In this paper, we propose QDaS, a novel domain agnostic framework as a solution for effective data storage and management of IoT applications. The framework incorporates a novel data summarisation mechanism that uses an innovative data quality estimation technique. This proposed data quality estimation technique computes the quality of data (based on their utility) without requiring any feedback from users of this IoT data or domain awareness of the data. We evaluate the effectiveness of the proposed QDaS framework using real world datasets.}
}
@article{FARROKHI2020257,
title = {Using artificial intelligence to detect crisis related to events: Decision making in B2B by artificial intelligence},
journal = {Industrial Marketing Management},
volume = {91},
pages = {257-273},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2020.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0019850120308464},
author = {Aydin Farrokhi and Farid Shirazi and Nick Hajli and Mina Tajvidi},
keywords = {Big data, Artificial intelligence, Machine learning, Data mining, Sentiment analytics},
abstract = {Artificial Intelligence (AI) could be an important foundation of competitive advantage in the market for firms. As such, firms use AI to achieve deep market engagement when the firm's data are employed to make informed decisions. This study examines the role of computer-mediated AI agents in detecting crises related to events in a firm. A crisis threatens organizational performance; therefore, a data-driven strategy will result in an efficient and timely reflection, which increases the success of crisis management. The study extends the situational crisis communication theory (SCCT) and Attribution theory frameworks built on big data and machine learning capabilities for early detection of crises in the market. This research proposes a structural model composed of a statistical and sentimental big data analytics approach. The findings of our empirical research suggest that knowledge extracted from day-to-day data communications such as email communications of a firm can lead to the sensing of critical events related to business activities. To test our model, we use a publicly available dataset containing 517,401 items belonging to 150 users, mostly senior managers of Enron during 1999 through the 2001 crisis. The findings suggest that the model is plausible in the early detection of Enron's critical events, which can support decision making in the market.}
}
@article{DEVI2022,
title = {Traffic management in smart cities using support vector machine for predicting the accuracy during peak traffic conditions},
journal = {Materials Today: Proceedings},
year = {2022},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.03.722},
url = {https://www.sciencedirect.com/science/article/pii/S2214785322022131},
author = {T. Devi and K. Alice and N. Deepa},
keywords = {Big data, Mobility, Linear and logistic regression, Support vector machine (SVM), Traffic management},
abstract = {Mobility is the main key for smart living, where navigation and automatic suggestions are also a strategy for a successful life in smart cities. Big Data analytics are behind urban changes in the mobility of smart cities to bring sustainable life. By the year 2025 all over Indian states can reach the expected lifestyle by providing high security and mobility which can grow the opportunities also high. As the population is rapidly increasing, the needs of people are also increasing such that necessitating real-time apps for daily needs, communication devices, and so on. We focus our idea on the benefits of traffic and safety measures which are becoming a huge challenge nowadays. Many are preferred with sophistication when traveling for short distances. In such a way the big data analytics tools R studio and weka are used on the dataset smart city from the Kaggle website for traffic patterns during the high traffic duration. Using the dataset, the data are classified using a Support vector machine (SVM) and applied with regression such as linear, logistic regression to find the accuracy of traffic peak situations. The proposed work aims to compare the efficiency of big data technologies which can be applied using various classification and regression that can be shown on various tools such as R, Weka, map-reduce which can produce accurate results to visualize the smart cities and their traffic analysis.}
}
@article{SONG201734,
title = {Data quality management for service-oriented manufacturing cyber-physical systems},
journal = {Computers & Electrical Engineering},
volume = {64},
pages = {34-44},
year = {2017},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2016.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0045790616302099},
author = {Zhiting Song and Yanming Sun and Jiafu Wan and Peipei Liang},
keywords = {Data quality, Cyber-physical systems, Service-oriented manufacturing, Workflow nets},
abstract = {Service-oriented manufacturing (SOM) is a new worldwide manufacturing paradigm, and a cyber-physical system (CPS) is accepted as a strategic choice of SOM enterprises looking to provide bundles of satisfying products and services to customers. The issue of data quality is common in any CPS and poses great challenges to its efficient operation. This paper focuses on defective data generated by the improper operation of physical and cyber components of a service-oriented manufacturing CPS (SMCPS), and develops effective managerial policies to deal with such data. First, formal semantics of workflow nets (WF-nets) are employed to construct process-oriented ontology for the SMCPS. Second, a two-stage optimization model together with algorithms is designed to find optimal policies that balance local and global management objectives. Finally, our model is illustrated through a case. Results show that the proposed control strategy outperforms one-stage control and random control in guaranteeing data quality and saving control costs.}
}
@article{ENCINAS2022109904,
title = {Downhole data correction for data-driven rate of penetration prediction modeling},
journal = {Journal of Petroleum Science and Engineering},
volume = {210},
pages = {109904},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109904},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521015217},
author = {Mauro A. Encinas and Andrzej T. Tunkiel and Dan Sui},
keywords = {Drilling, Machine learning, Rate of penetration, Drilling data quality improvement, Recurrent neural networks},
abstract = {In recent years, machine learning has been adopted in the Oil and Gas industry as a promising technology for solutions to the most demanding problems like downhole parameters estimations and incidents detection. A big amount of available data makes this technology an attractive option for solving a wide variety of drilling problems, as well as a reliable candidate for performing big-data analysis and interpretation. Nevertheless, this approach may cause, in some cases, that petroleum engineering concepts are disregarded in favor of more data-intensive approaches. This study aims to evaluate the impact of drilling data measurement correction on data-driven model performance. In our study, besides using the standard data processing technologies, like gap filling, outlier removal, noise reduction etc., the physics-based drilling models are also implemented for data quality improvement and data correction in consideration of the measurement physics, rarely mentioned in most of publications. In our case study, recurrent neural networks (RNN) that are able to capture temporal natures of a signal are employed for the rate of penetration (ROP) estimation with an adjustable predictive window. The results show that the RNN model produces the best results when using the drilling data recovered through analytical methods. Moreover, the comprehensive data-driven model evaluation and engineering interpretation are conducted to facilitate better understanding of the data-driven models and their applications.}
}
@article{VISSER2021623,
title = {Imprecision farming? Examining the (in)accuracy and risks of digital agriculture},
journal = {Journal of Rural Studies},
volume = {86},
pages = {623-632},
year = {2021},
issn = {0743-0167},
doi = {https://doi.org/10.1016/j.jrurstud.2021.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0743016721002217},
author = {Oane Visser and Sarah Ruth Sippel and Louis Thiemann},
keywords = {Digital agriculture, Smart farming, Precision agriculture, Accuracy, Big data},
abstract = {The myriad potential benefits of digital farming hinge on the promise of increased accuracy, which allows ‘doing more with less’ through precise, data-driven operations. Yet, precision farming's foundational claim of increased accuracy has hardly been the subject of comprehensive examination. Drawing on social science studies of big data, this article examines digital agriculture's (in)accuracies and their repercussions. Based on an examination of the daily functioning of the various components of yield mapping, it finds that digital farming is often ‘precisely inaccurate’, with the high volume and granularity of big data erroneously equated with high accuracy. The prevailing discourse of ‘ultra-precise’ digital technologies ignores farmers' essential efforts in making these technologies more accurate, via calibration, corroboration and interpretation. We suggest that there is the danger of a ‘precision trap’. Namely, an exaggerated belief in the precision of big data that over time leads to an erosion of checks and balances (analogue data, farmer observation et cetera) on farms. The danger of ‘precision traps’ increases with the opacity of algorithms, with shifts from real-time measurement and advice towards forecasting, and with farmers' increased remoteness from field operations. Furthermore, we identify an emerging ‘precision divide’: unequally distributed precision benefits resulting from the growing algorithmic divide between farmers focusing on staple crops, catered well by technological innovation on the one hand, and farmers cultivating other crops, who have to make do with much less advanced or applicable algorithms on the other. Consequently, for the latter farms digital farming may feel more like ‘imprecision farming’.}
}
@article{CLARKE2018467,
title = {Guidelines for the responsible application of data analytics},
journal = {Computer Law & Security Review},
volume = {34},
number = {3},
pages = {467-476},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917303643},
author = {Roger Clarke},
keywords = {Big data, Data science, Data quality, Decision quality, Regulation},
abstract = {The vague but vogue notion of ‘big data’ is enjoying a prolonged honeymoon. Well-funded, ambitious projects are reaching fruition, and inferences are being drawn from inadequate data processed by inadequately understood and often inappropriate data analytic techniques. As decisions are made and actions taken on the basis of those inferences, harm will arise to external stakeholders, and, over time, to internal stakeholders as well. A set of Guidelines is presented, whose purpose is to intercept ill-advised uses of data and analytical tools, prevent harm to important values, and assist organisations to extract the achievable benefits from data, rather than dreaming dangerous dreams.}
}
@article{SALVETAT2020101602,
title = {Data determinants of the activity of SMEs automobile dealers},
journal = {Journal of Engineering and Technology Management},
volume = {58},
pages = {101602},
year = {2020},
issn = {0923-4748},
doi = {https://doi.org/10.1016/j.jengtecman.2020.101602},
url = {https://www.sciencedirect.com/science/article/pii/S0923474820300503},
author = {David Salvetat and Jean-Sébastien Lacam},
keywords = {Big data, Smart data, Development, Automobile, SME},
abstract = {Many SMEs still seem reluctant to accept the management of large datasets, which still appear to be too complex for them. However, our study reveals that the majority of small French car dealers are developing Big data and Smart data policies to improve the quality of their offers, the dynamism of their sales and their access to new opportunities. However, not every policy has the same effects on the development of their business. Whereas Big data improves all the components of SME development in a global, short-term and operational way, Smart data presents itself as a more targeted, prospective and strategic approach.}
}
@article{SCHAEFER2021156,
title = {Framework of Data Analytics and Integrating Knowledge Management},
journal = {International Journal of Intelligent Networks},
volume = {2},
pages = {156-165},
year = {2021},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2021.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2666603021000208},
author = {Camilla Schaefer and Ana Makatsaria},
keywords = {Data analytics, Knowledge management, Big data, Business intelligence, Data discovery},
abstract = {Big data is significantly dependent on technologies such as cloud computing, machine learning and statistical models. However, its significance is becoming more dependent on human qualities e.g. judgment, value, intuition and experience. Therefore, the human knowledge presents a basis for knowledge management and big data, which are a major element of data analytics. This research contribution applies the process of Data, Information, Knowledge and Perception hierarchy as a structure to evaluate the end-users’ process. The framework in incorporating data analytics and display a conceptual data analytics process (with three phases) evaluated as knowledge management, including the creation, discovery and application of knowledge. Knowledge conversion theories are applicable in data analytics to emphasize on the typically overlooked organizational and human aspects, which are critical to the efficiency of data analytics. The synergy and alignment between knowledge management and data analytics is fundamental in fostering innovations and collaboration.}
}
@article{SHARMA2020538,
title = {MR-I MaxMin-scalable two-phase border based knowledge hiding technique using MapReduce},
journal = {Future Generation Computer Systems},
volume = {109},
pages = {538-550},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.05.063},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17322173},
author = {Shivani Sharma and Durga Toshniwal},
keywords = {Big data, Computational cost, Knowledge hiding techniques, MapReduce, Privacy preservation, Scalability},
abstract = {Border based Knowledge hiding techniques (BB-KHT) are widely adopted form of privacy preservation techniques of data mining. These approaches are used to hide sensitive knowledge (confidential information) present in a dataset before sharing or analyzing it. BB-KHT primarily rely on border theory and maximum criterion method for preserving privacy and perpetuating good data quality of sanitized dataset but costs high computational complexity. Further, due to sequential nature, these approaches are particularly felicitous for small datasets and become infeasible while dealing with large scale datasets. Therefore, to subjugate the identified challenges of infeasibility and high computational complexity, a scalable two-phase improved MaxMin BB-KHT using MapReduce framework (MR-I MaxMin) is proposed. The proposed scheme requires only two database scans throughout the hiding process and hence, is computationally inexpensive. Moreover, the scheme also commits to preserve good data quality of sanitized dataset. The MapReduce version of proposed approach helps in achieving the feasibility by processing large voluminous data in a parallel fashion. Quantitative experiments and evaluations have been performed over a number of real and synthetically generated large-scale datasets. It is shown that the proposed MR-I MaxMin technique outperforms the similar existing approaches and vanquishes the identified challenges along with much-needed privacy preservation.}
}
@article{XIONG2021386,
title = {Anti-collusion data auction mechanism based on smart contract},
journal = {Information Sciences},
volume = {555},
pages = {386-409},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.10.053},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520310458},
author = {Wei Xiong and Li Xiong},
keywords = {Data auction mechanism, Anti-collusion, Smart contract, Blockchain, Ethereum},
abstract = {Due to the uncertainty of the value of big data, it is difficult to directly give a reasonable price for big data. Auction is an effective method of distributing goods to the bidder with the highest valuation. Hence, the use of auction strategy can not only guarantee the interests of data sellers, but also conform to market principles. However, existing data auction mechanisms are centralized. It is hard to build trust among sellers, buyers and auctioneers. An open and anonymous online environment may cause entities involved in data auctions to collude to manipulate the results of data auctions. This will cause the price of auction data to fail to reach a fair and truthful level. Therefore, the first anti-collusion data auction mechanism based on smart contract is proposed. Through a well-designed anti-collusion data auction algorithm, mutual distrust and rational buyers and sellers safely participate in the data auction without a trusted third party. The data auction mechanism designed in the smart contract can effectively prevent collusion and realize the fairness and truthfulness of data auction. The webpack in the Truffle Boxes is used to implement the data auction mechanism, and the anti-collusion property of the mechanism has been verified. The source code of the smart contract has been uploaded to GitHub.}
}
@article{DIVAIO2022121201,
title = {Data intelligence and analytics: A bibliometric analysis of human–Artificial intelligence in public sector decision-making effectiveness},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121201},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121201},
url = {https://www.sciencedirect.com/science/article/pii/S004016252100634X},
author = {Assunta {Di Vaio} and Rohail Hassan and Claude Alavoine},
keywords = {Ambidexterity, Industry 4.0, Business intelligence, Big data, Intellectual capital, Human intellect, Accountability and performance},
abstract = {This study investigates the literary corpus of the role and potential of data intelligence and analytics through the lenses of artificial intelligence (AI), big data, and the human–AI interface to improve overall decision-making processes. It investigates how data intelligence and analytics improve decision-making processes in the public sector. A bibliometric analysis of a database containing 161 English-language articles published between 2017 and 2021 is performed, providing a map of the knowledge produced and disseminated in previous studies. It provides insights into key topics, citation patterns, publication activities, the status of collaborations between contributors over past studies, aggregated data intelligence, and analytics research contributions. The study provides a retrospective review of published content in the field of data intelligence and analytics. The findings indicate that field research has been concentrated mainly on emerging technologies' intelligence capabilities rather than on human–artificial intelligence in decision-making performance in the public sector. This study extends an ambidexterity theory in decision support, which enlightens how this ambidexterity can be encouraged and how it affects decision outcomes. The study emphasises the importance of the public sector adoption of data intelligence and analytics, as well as its efficiency. Furthermore, this study expands how researchers and practitioners interpret and understand data intelligence and analytics, AI, and big data for effective public sector decision-making.}
}
@incollection{TAT2021395,
title = {Chapter 17 - Ethical and legal challenges},
editor = {Subhi J. Al'Aref and Gurpreet Singh and Lohendran Baskaran and Dimitris Metaxas},
booktitle = {Machine Learning in Cardiovascular Medicine},
publisher = {Academic Press},
pages = {395-410},
year = {2021},
isbn = {978-0-12-820273-9},
doi = {https://doi.org/10.1016/B978-0-12-820273-9.00017-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202739000178},
author = {Emily Tat and Mark Rabbat},
keywords = {Artificial intelligence, Autonomy, Big data, Black box, Ethics, Informed consent, Liability, Privacy, Safety},
abstract = {As the technology of artificial intelligence (AI) grows in cardiovascular medicine, so do the ethical and legal challenges that come with it. Currently, the medical community is ill-informed of what these challenges entail, and policy and ethical guidelines are lacking. Physicians and policy makers should be informed of these issues to minimize harm and promote patient care. Three overarching themes relating to the data, the algorithms, and the results comprise the foundation of these challenges and will be discussed in this chapter. The introduction of big data raises concern for patient privacy and security, with issues of data quality and inconsistent medical records. There is also risk for biases in the algorithms that could worsen health disparities or skew results for financial gain. Finally, the archetypal “black box” algorithm, questions of legal liability, and what happens when humans and machine disagree are discussed in depth. Ultimately, a code of ethics in the coming integration of AI is needed to ensure the preservation of human rights.}
}
@article{XIA2021100055,
title = {Aiding pro-environmental behavior measurement by Internet of Things},
journal = {Current Research in Behavioral Sciences},
volume = {2},
pages = {100055},
year = {2021},
issn = {2666-5182},
doi = {https://doi.org/10.1016/j.crbeha.2021.100055},
url = {https://www.sciencedirect.com/science/article/pii/S2666518221000425},
author = {Ziqian Xia and Yurong Liu},
keywords = {Pro-environmental behavior, Internet of Things, Measurement, Big data, Environmental psychology},
abstract = {Promoting pro-environmental behavior is an effective means of reducing carbon emissions at the individual end, but the measurement of behavior has long been a problem for scholars. Especially in environmental psychology community, the complexity of social policies and habitat implies greater difficulty in measuring. Due to the limitations of traditional questionnaire, laboratory, and naturalistic observation methods, environmental psychologists need more realistic, accurate, and cost-effective ways to measure behavior. The rapid development of IoT technology lights up the hope for achieving this goal, and its large-scale popularization will bring great changes to the research community. This paper reviews the current methods and their limitations, proposes a framework for measuring behavior using IoT devices, and points out its future research directions.}
}
@article{EZERINS2022105569,
title = {Advancing safety analytics: A diagnostic framework for assessing system readiness within occupational safety and health},
journal = {Safety Science},
volume = {146},
pages = {105569},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105569},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521004112},
author = {Maira E. Ezerins and Timothy D. Ludwig and Tara O'Neil and Anne M. Foreman and Yalçın Açıkgöz},
keywords = {Safety analytics, Data analytics, Readiness assessment, Occupational health},
abstract = {Big data and analytics have shown promise in predicting safety incidents and identifying preventative measures directed towards specific risk variables. However, the safety industry is lagging in big data utilization due to various obstacles, which may include lack of data readiness (e.g., disparate databases, missing data, low validity) and personnel competencies. This paper provides a primer on the application of big data to safety. We then describe a safety analytics readiness assessment framework that highlights system requirements and the challenges that safety professionals may encounter in meeting these requirements. The proposed framework suggests that safety analytics readiness depends on (a) the quality of the data available, (b) organizational norms around data collection, scaling, and nomenclature, (c) foundational infrastructure, including technological platforms and skills required for data collection, storage, and analysis of health and safety metrics, and (d) measurement culture, or the emergent social patterns between employees, data acquisition, and analytic processes. A safety-analytics readiness assessment can assist organizations with understanding current capabilities so measurement systems can be matured to accommodate more advanced analytics for the ultimate purpose of improving decisions that mitigate injury and incidents.}
}
@incollection{BERMAN2018395,
title = {19 - Legalities},
editor = {Jules J. Berman},
booktitle = {Principles and Practice of Big Data (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {395-417},
year = {2018},
isbn = {978-0-12-815609-4},
doi = {https://doi.org/10.1016/B978-0-12-815609-4.00019-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128156094000194},
author = {Jules J. Berman},
keywords = {Data Quality Act, Freedom of Information Act, FOIA, Limited Data Use Agreements, and Madey v. Duke, Tort, Patents, Intellectual property, Informed consent, Data ownership, Copyright, Infringement, Fair use},
abstract = {Big Data projects always incur some legal risk. It is impossible to know all the data contained in a Big Data project, and it is impossible to know every purpose to which Big Data is used. Hence, the entities that produce Big Data may unknowingly contribute to a variety of illegal activities, chiefly: copyright and other intellectual property infringements, breaches of confidentiality, and privacy invasions. In addition, issues of data quality, data availability, and data documentation may contribute to the legal or regulatory disqualification of a Big Data resource. In this chapter, four issues will be discussed in detail: (1) responsibility for the accuracy of the contained data; (2) obtaining the rights to create, use, and share the data held in the resource; (3) intellectual property encumbrances incurred from the use of standards required for data representation and data exchange; and (4) protections for individuals whose personal information is used in the resource. Big Data managers contend with a wide assortment of legal issues, but these four issues will never go away.}
}
@incollection{VAIDYA2022409,
title = {Chapter 24 - Exploring performance and predictive analytics of agriculture data},
editor = {Ajith Abraham and Sujata Dash and Joel J.P.C. Rodrigues and Biswaranjan Acharya and Subhendu Kumar Pani},
booktitle = {AI, Edge and IoT-based Smart Agriculture},
publisher = {Academic Press},
pages = {409-436},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-823694-9},
doi = {https://doi.org/10.1016/B978-0-12-823694-9.00030-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012823694900030X},
author = {Madhavi Vaidya and Shweta Katkar},
keywords = {Agriculture, Big data, Weka, J48, Talend, Crops, Analytics, Fertilizers, Prediction},
abstract = {The exponential growth and ubiquity of both structured and unstructured data have led us into the big data era. Big data analytics is increasingly becoming a trending practice that many organizations are adopting to construct valuable information from big data. This field has substantially attracted academics, practitioners, and industries. But there are some challenges for big data processing and analytics that include integration of data, volume ofdata, the rate of transformation of data, and the veracity and validity of data. The history of griculture in India dates back to the Indus Valley civilization. Due to variations in climatic conditions, it has become challenging to achieve the desired results in crop yields. The use of technology in agriculture has increased in recent years and data analytics is one such trend that has penetrated the agriculture field. The main challenge in using big data in agriculture is identifying the effectiveness of big data analytics. Big data analysis can be processed and analyzed using parallel databases such as Talend or analytical paradigms like MapReduce on a Hadoop distributed file system. There are other mechanisms such as Weka and R, which are two of the most popular data analytical and statistical computing tools produced by the open source community, but there are certain challenges compared to the other techniques mentioned. In this chapter, the comparative studies of various mechanisms will be provided that will give an insight to process and analyze big data generated from farms and the grains obtained from it according to the seasons, the soil health, and the location. In addition, various case studies are shown for data processing in context with planting, agricultural growth, and smart farming. From the experimentation, the authors have shown which is the right fertilizer for a specific state and soil. In addition, the authors have worked on the analysis of crop production per state and per year.}
}
@article{SIRGO2018166,
title = {Validation of the ICU-DaMa tool for automatically extracting variables for minimum dataset and quality indicators: The importance of data quality assessment},
journal = {International Journal of Medical Informatics},
volume = {112},
pages = {166-172},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618300443},
author = {Gonzalo Sirgo and Federico Esteban and Josep Gómez and Gerard Moreno and Alejandro Rodríguez and Lluis Blanch and Juan José Guardiola and Rafael Gracia and Lluis {De Haro} and María Bodí},
keywords = {Electronic medical record, Quality indicators, Critical care, Information processing, Data quality, Verification},
abstract = {Background
Big data analytics promise insights into healthcare processes and management, improving outcomes while reducing costs. However, data quality is a major challenge for reliable results. Business process discovery techniques and an associated data model were used to develop data management tool, ICU-DaMa, for extracting variables essential for overseeing the quality of care in the intensive care unit (ICU).
Objective
To determine the feasibility of using ICU-DaMa to automatically extract variables for the minimum dataset and ICU quality indicators from the clinical information system (CIS).
Methods
The Wilcoxon signed-rank test and Fisher’s exact test were used to compare the values extracted from the CIS with ICU-DaMa for 25 variables from all patients attended in a polyvalent ICU during a two-month period against the gold standard of values manually extracted by two trained physicians. Discrepancies with the gold standard were classified into plausibility, conformance, and completeness errors.
Results
Data from 149 patients were included. Although there were no significant differences between the automatic method and the manual method, we detected differences in values for five variables, including one plausibility error and two conformance and completeness errors. Plausibility: 1) Sex, ICU-DaMa incorrectly classified one male patient as female (error generated by the Hospital’s Admissions Department). Conformance: 2) Reason for isolation, ICU-DaMa failed to detect a human error in which a professional misclassified a patient’s isolation. 3) Brain death, ICU-DaMa failed to detect another human error in which a professional likely entered two mutually exclusive values related to the death of the patient (brain death and controlled donation after circulatory death). Completeness: 4) Destination at ICU discharge, ICU-DaMa incorrectly classified two patients due to a professional failing to fill out the patient discharge form when thepatients died. 5) Length of continuous renal replacement therapy, data were missing for one patient because the CRRT device was not connected to the CIS.
Conclusions
Automatic generation of minimum dataset and ICU quality indicators using ICU-DaMa is feasible. The discrepancies were identified and can be corrected by improving CIS ergonomics, training healthcare professionals in the culture of the quality of information, and using tools for detecting and correcting data errors.}
}
@article{OLEARY2019113139,
title = {Technology life cycle and data quality: Action and triangulation},
journal = {Decision Support Systems},
volume = {126},
pages = {113139},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113139},
url = {https://www.sciencedirect.com/science/article/pii/S016792361930168X},
author = {Daniel E. O'Leary},
keywords = {Technology life cycle, Data quality, Non-stationary data, Hype cycle, Data biases, Data phase triangulation},
abstract = {Where a technology is its life cycle can make a difference in the data generated by or about adopting, using or implementing that technology. As a result, it is arguable that adoption, usage or implementation data generated early in a technology's life cycle is directly comparable to data generated later in the life cycle. In particular, comparisons of early and late data can result in a number of disparate results and limit replicability, because of biases in the data and non-stationary data. This paper suggests that it can be important for information systems researchers to disclose an estimate of the location in the technology's life cycle, as part of their analysis. The data life cycle discussion is then extended to the notion of “data phase triangulation,” which is contrasted with “methodology triangulation” and “data (collection) triangulation.” In addition, we discuss the importance of being able to use the findings from life cycle-based research to “push” a technology from one phase to another phase.}
}
@article{WANG2022643,
title = {Does city construction improve life quality?-evidence from POI data of China},
journal = {International Review of Economics & Finance},
volume = {80},
pages = {643-653},
year = {2022},
issn = {1059-0560},
doi = {https://doi.org/10.1016/j.iref.2022.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1059056022000041},
author = {Yang Wang and Hong Zhang and Libing Liu},
keywords = {Quality of life, Point of interest, Happiness},
abstract = {To explore the construction of a big data indicator system is conducive to a comprehensive, scientific, timely and accurate grasp of the quality of life of our residents and its evolutionary trends. This paper systematically sorts out the performance dimensions of the residents' quality of life, and integrates two types of methods of objective observation and subjective evaluation commercial POI(Point of Interest) data. From the aspects of life, entertainment, transportation, etc., preliminary development has been made including 8 first-level indicators, 16 second-level indicators, and 27 third-level indicators Big data indicator system, and measure the "clogging point" of the improvement of residents' quality of life, with a view to providing a scientific and feasible decision-making reference for "meeting the people's increasing needs for a better life".}
}
@incollection{KOLTAY201671,
title = {Chapter 5 - Digital Research Data: Where are we Now?},
editor = {David Baker and Wendy Evans},
booktitle = {Digital Information Strategies},
publisher = {Chandos Publishing},
pages = {71-84},
year = {2016},
isbn = {978-0-08-100251-3},
doi = {https://doi.org/10.1016/B978-0-08-100251-3.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002513000056},
author = {Tibor Koltay},
keywords = {data citation, data curation, data literacy, data management, data quality, data sharing, research data},
abstract = {The key topic of digital research data raises a multitude of issues: big data, data sharing, data quality, data management, data curation, data citation, data literacy. This chapter addresses questions related to the definition of these concepts, to the frameworks constructed for a better understanding and treatment of the different phenomena, as well as ethical considerations. The potential of libraries and information professionals in fulfilling data-related activities is outlined, together with the associated requirements of them.}
}
@article{DUPLESSIS2021100100,
title = {Necessity of making water smart for proactive informed decisive actions: A case study of the upper vaal catchment, South Africa},
journal = {Environmental Challenges},
volume = {4},
pages = {100100},
year = {2021},
issn = {2667-0100},
doi = {https://doi.org/10.1016/j.envc.2021.100100},
url = {https://www.sciencedirect.com/science/article/pii/S2667010021000792},
author = {Anja {du Plessis}},
keywords = {Data quality, Decisive action, Smart water management, Water quality, South Africa, Upper vaal catchment},
abstract = {The need for informed management of water resources has been continuously highlighted worldwide. Societies are increasingly faced with water quality challenges globally which directly translate into multifaceted challenges. South Africa has acknowledged that water is not receiving the attention and status it deserves. Wastage is rife and degradation widespread. The sustainability of South Africa's freshwater resources has reached a critical point and requires decisive action. Vast amounts of water quality data, varying in quality, is available however the seemingly lack of integrative data management has led to reactive planning and questionable decisions. The paper highlights the necessity for making water smart through a case study of the Upper Vaal catchment. The quality of available government data is mostly of an acceptable standard according to the evaluated data dimensions and elements. The practical application of determining hydrological responses to predict possible water quality changes towards land cover change in the Vaal river catchment emphasises that there is suitable data available and highlights the value of Smart Water Management (SWM). SWM can enable improved integrated water resource management by increasing sharing and effective use of real-time data of acceptable quality to promote proactive unambiguous strategies and decisions focused on overall improved water management and the evasion of a future water predicament.}
}
@article{MACHADO2022263,
title = {Data Mesh: Concepts and Principles of a Paradigm Shift in Data Architectures},
journal = {Procedia Computer Science},
volume = {196},
pages = {263-271},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022365},
author = {Inês Araújo Machado and Carlos Costa and Maribel Yasmina Santos},
keywords = {Big Data, Data Mesh, Data Architectures, Data Lake},
abstract = {Inherent to the growing use of the most varied forms of software (e.g., social applications), there is the creation and storage of data that, due to its characteristics (volume, variety, and velocity), make the concept of Big Data emerge. Big Data Warehouses and Data Lakes are concepts already well established and implemented by several organizations, to serve their decision-making needs. After analyzing the various problems demonstrated by those monolithic architectures, it is possible to conclude about the need for a paradigm shift that will make organizations truly data-oriented. In this new paradigm, data is seen as the main concern of the organization, and the pipelining tools and the Data Lake itself are seen as a secondary concern. Thus, the Data Mesh consists in the implementation of an architecture where data is intentionally distributed among several Mesh nodes, in such a way that there is no chaos or data silos, since there are centralized governance strategies and the guarantee that the core principles are shared throughout the Mesh nodes. This paper presents the motivation for the appearance of the Data Mesh paradigm, its features, and approaches for its implementation.}
}
@article{ZORRILLA2022103595,
title = {A reference framework for the implementation of data governance systems for industry 4.0},
journal = {Computer Standards & Interfaces},
volume = {81},
pages = {103595},
year = {2022},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2021.103595},
url = {https://www.sciencedirect.com/science/article/pii/S0920548921000908},
author = {Marta Zorrilla and Juan Yebenes},
keywords = {Data governance, Data-Centric architecture, Industry 4.0, Big data, IoT},
abstract = {The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This is mainly based on the digitalization of the industrial environment by means of the convergence of Information Technologies (IT) and operational Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT) and the use of data generated in real time for gaining insights and making decisions. Therefore data becomes a critical asset for Industry 4.0 and must be managed and governed like a strategic asset. We rely on Data Governance (DG) as a key instrument for carrying out this transformation. This paper presents the design of a specific governance framework for Industry 4.0. First, this contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, the cloud and edge computing, the artificial intelligence and the current regulations. Next, we formally define a reference framework for the implementation of Data Governance Systems for Industry 4.0 using international standards and providing several examples of architecture building blocks.}
}
@article{RAMSINGH2021107423,
title = {An integrated multi-node Hadoop framework to predict high-risk factors of Diabetes Mellitus using a Multilevel MapReduce based Fuzzy Classifier (MMR-FC) and Modified DBSCAN algorithm},
journal = {Applied Soft Computing},
volume = {108},
pages = {107423},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107423},
url = {https://www.sciencedirect.com/science/article/pii/S156849462100346X},
author = {J. Ramsingh and V. Bhuvaneswari},
keywords = {Fuzzy classifier, MDBSCAN, MapReduce, Hadoop, Diabetes mellitus},
abstract = {In the era of data deluge, the world is experiencing an intensive growth of Big data with complex structures. While processing of these data is a complex and labor-intensive process, a proper analysis of Big data leads to greater knowledge extraction. In this paper, Big data is used to predict high-risk factors of Diabetes Mellitus using a new integrated framework with four Hadoop clusters, which are developed to classify the data based on Multi-level MapReduce Fuzzy Classifier (MMR-FC) and MapReduce-Modified Density-Based Spatial Clustering of Applications with Noise (MR-MDBSCAN) algorithm. Big data concerning people’s food habits, physical activity are extracted from social media using the API’s provided. The MMR-FC takes place at three levels of index (Glycemic Index, Physical activity Index, Sleeping Pattern) values. The fuzzy rules are generated by the MMR-FC algorithm to predict the risk of Diabetes Mellitus using the data extracted. The result from MMR-FC is used as an input to the semantic location prediction framework to predict the high-risk zones of Diabetes Mellitus using the MR-MDBSCAN algorithm. The analysis shows that more than 55% of people are in a high-risk group with positive sentiments on the data extracted. More than 70% of food with a high Glycemic Index is usually consumed during Night and Early Evenings, which reveals that people consume food that has a high Glycemic Index during their sedentary slot and have irregular sleep practices. Around 70% of the unhealthiest dietary patterns are retrieved from urban hotspots such as Delhi, Cochin, Kolkata, and Chennai. From the results, it is evident that 55% of younger generations, users of social networking sites having high possibilities of Type II Diabetes Mellitus at large.}
}
@article{WANG2018139,
title = {Hyper-resolution monitoring of urban flooding with social media and crowdsourcing data},
journal = {Computers & Geosciences},
volume = {111},
pages = {139-147},
year = {2018},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S009830041730609X},
author = {Ruo-Qian Wang and Huina Mao and Yuan Wang and Chris Rae and Wesley Shaw},
abstract = {Hyper-resolution datasets for urban flooding are rare. This problem prevents detailed flooding risk analysis, urban flooding control, and the validation of hyper-resolution numerical models. We employed social media and crowdsourcing data to address this issue. Natural Language Processing and Computer Vision techniques are applied to the data collected from Twitter and MyCoast (a crowdsourcing app). We found these big data based flood monitoring approaches can complement the existing means of flood data collection. The extracted information is validated against precipitation data and road closure reports to examine the data quality. The two data collection approaches are compared and the two data mining methods are discussed. A series of suggestions is given to improve the data collection strategy.}
}
@article{MARTINEZ2021100183,
title = {Data Science Methodologies: Current Challenges and Future Approaches},
journal = {Big Data Research},
volume = {24},
pages = {100183},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2020.100183},
url = {https://www.sciencedirect.com/science/article/pii/S2214579620300514},
author = {Iñigo Martinez and Elisabeth Viles and Igor {G. Olaizola}},
keywords = {Data science, Big data, Data science methodology, Project life-cycle, Organizational impacts, Knowledge management},
abstract = {Data science has employed great research efforts in developing advanced analytics, improving data models and cultivating new algorithms. However, not many authors have come across the organizational and socio-technical challenges that arise when executing a data science project: lack of vision and clear objectives, a biased emphasis on technical issues, a low level of maturity for ad-hoc projects and the ambiguity of roles in data science are among these challenges. Few methodologies have been proposed on the literature that tackle these type of challenges, some of them date back to the mid-1990, and consequently they are not updated to the current paradigm and the latest developments in big data and machine learning technologies. In addition, fewer methodologies offer a complete guideline across team, project and data & information management. In this article we would like to explore the necessity of developing a more holistic approach for carrying out data science projects. We first review methodologies that have been presented on the literature to work on data science projects and classify them according to the their focus: project, team, data and information management. Finally, we propose a conceptual framework containing general characteristics that a methodology for managing data science projects with a holistic point of view should have. This framework can be used by other researchers as a roadmap for the design of new data science methodologies or the updating of existing ones.}
}
@incollection{REIS2020179,
title = {3.10 - Data Quality and Denoising: A Review☆},
editor = {Steven Brown and Romà Tauler and Beata Walczak},
booktitle = {Comprehensive Chemometrics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {179-204},
year = {2020},
isbn = {978-0-444-64166-3},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.14874-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472148747},
author = {M.S. Reis and P.M. Saraiva and B.R. Bakshi},
keywords = {Bayesian estimation, Data quality, Data rectification, Filtering, Fourier analysis, Gaussian and non-Gaussian noise, Kalman filtering, Model-based denoising, Multiscale analysis, Off-line and online denoising, Outliers, Smoothing, Wavelet analysis, Wavelet thresholding, Windowed Fourier analysis},
abstract = {This article introduces the methods of Fourier and wavelet analysis for enhancing data quality in typical chemometric and process analytics applications. Fourier analysis has been popular for many decades but is best suited for enhancing signals where most features are localized in frequency. In contrast, wavelet analysis is appropriate for signals that contain features localized in both time and frequency domains. It also retains the benefits of Fourier analysis such as orthonormality and computational efficiency. Practical algorithms for off-line and on-line denoising are described and compared via simple examples. These algorithms can be used for off-line or on-line applications in order to mitigate the impact of additive Gaussian as well as non-Gaussian noise.}
}
@article{SAEZ2019104954,
title = {Guest editorial: Special issue in biomedical data quality assessment methods},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104954},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0169260719309174},
author = {Carlos Sáez and Siaw-Teng Liaw and Eizen Kimura and Pascal Coorevits and Juan M Garcia-Gomez}
}
@incollection{CHUI2019111,
title = {Chapter 7 - Smart city is a safe city: information and communication technology–enhanced urban space monitoring and surveillance systems: the promise and limitations},
editor = {Anna Visvizi and Miltiadis D. Lytras},
booktitle = {Smart Cities: Issues and Challenges},
publisher = {Elsevier},
pages = {111-124},
year = {2019},
isbn = {978-0-12-816639-0},
doi = {https://doi.org/10.1016/B978-0-12-816639-0.00007-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166390000077},
author = {Kwok Tai Chui and Pandian Vasant and Ryan Wen Liu},
keywords = {Cyber security, Ethics, Policy-making, Security, Surveillance},
abstract = {Urban space monitoring and surveillance systems are present almost everywhere in various forms of sensing devices such as closed-circuit television, smartphone, and camera. This requires a robust and easy-to-manage information and communication technology (ICT) infrastructure that is generally comprises sensors, protocols, networks, and steps. Smart adoption of such systems could influence, manage, direct, and protect human beings and property. Nevertheless, it may create problems of government support, data quality, privacy, and security. Today's computational world allows implementation of artificial intelligence models for big data analytics to bring cities smart (with intelligence and optimal improvement). This chapter will discuss the applications of urban space monitoring and surveillance systems via ICT. The typical limitations of the current research are discussed in detail.}
}
@article{KAZMIERSKA202043,
title = {From multisource data to clinical decision aids in radiation oncology: The need for a clinical data science community},
journal = {Radiotherapy and Oncology},
volume = {153},
pages = {43-54},
year = {2020},
note = {Physics Special Issue: ESTRO Physics Research Workshops on Science in Development},
issn = {0167-8140},
doi = {https://doi.org/10.1016/j.radonc.2020.09.054},
url = {https://www.sciencedirect.com/science/article/pii/S016781402030829X},
author = {Joanna Kazmierska and Andrew Hope and Emiliano Spezi and Sam Beddar and William H. Nailon and Biche Osong and Anshu Ankolekar and Ananya Choudhury and Andre Dekker and Kathrine Røe Redalen and Alberto Traverso},
keywords = {Artificial intelligence, Big data, Data science, Personalized treatment, Radiotherapy, Shared decision making},
abstract = {Big data are no longer an obstacle; now, by using artificial intelligence (AI), previously undiscovered knowledge can be found in massive data collections. The radiation oncology clinic daily produces a large amount of multisource data and metadata during its routine clinical and research activities. These data involve multiple stakeholders and users. Because of a lack of interoperability, most of these data remain unused, and powerful insights that could improve patient care are lost. Changing the paradigm by introducing powerful AI analytics and a common vision for empowering big data in radiation oncology is imperative. However, this can only be achieved by creating a clinical data science community in radiation oncology. In this work, we present why such a community is needed to translate multisource data into clinical decision aids.}
}
@article{GANT2015S36,
title = {The importance of data quality to enhance the impact of omics sciences},
journal = {Toxicology Letters},
volume = {238},
number = {2, Supplement },
pages = {S36},
year = {2015},
note = {ABSTRACTS OF THE 51st Congress of the European Societies of Toxicology (EUROTOX)},
issn = {0378-4274},
doi = {https://doi.org/10.1016/j.toxlet.2015.08.097},
url = {https://www.sciencedirect.com/science/article/pii/S0378427415020378},
author = {T. Gant}
}
@article{STECKLER20151803,
title = {The preclinical data forum network: A new ECNP initiative to improve data quality and robustness for (preclinical) neuroscience},
journal = {European Neuropsychopharmacology},
volume = {25},
number = {10},
pages = {1803-1807},
year = {2015},
issn = {0924-977X},
doi = {https://doi.org/10.1016/j.euroneuro.2015.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0924977X15001674},
author = {Thomas Steckler and Katja Brose and Magali Haas and Martien J. Kas and Elena Koustova and Anton Bespalov},
keywords = {Reproducibility, Robustness, Relevance, Quality assurance, Neuroscience, Pre-clinical},
abstract = {Current limitations impeding on data reproducibility are often poor statistical design, underpowered studies, lack of robust data, lack of methodological detail, biased reporting and lack of open data sharing, coupled with wrong research incentives. To improve data reproducibility, robustness and quality for brain disease research, a Preclinical Data Forum Network was formed under the umbrella of the European College of Neuropsychopharmacology (ECNP). The goal of this network, members of which met for the first time in October 2014, is to establish a forum to collaborate in precompetitive space, to exchange and develop best practices, and to bring together the members from academia, pharmaceutical industry, publishers, journal editors, funding organizations, public/private partnerships and non-profit advocacy organizations. To address the most pertinent issues identified by the Network, it was decided to establish a data sharing platform that allows open exchange of information in the area of preclinical neuroscience and to develop an educational scientific program. It is also planned to reach out to other organizations to align initiatives to enhance efficiency, and to initiate activities to improve the clinical relevance of preclinical data. Those Network activities should contribute to scientific rigor and lead to robust and relevant translational data. Here we provide a synopsis of the proceedings from the inaugural meeting.}
}
@article{LIU2020101495,
title = {Discovering and merging related analytic datasets},
journal = {Information Systems},
volume = {91},
pages = {101495},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101495},
url = {https://www.sciencedirect.com/science/article/pii/S0306437920300065},
author = {Rutian Liu and Eric Simon and Bernd Amann and Stéphane Gançarski},
keywords = {Schema augmentation, Schema complement, Data quality, SAP HANA},
abstract = {The production of analytic datasets is a significant big data trend and has gone well beyond the scope of traditional IT-governed dataset development. Analytic datasets are now created by data scientists and data analysts using big data frameworks and agile data preparation tools. However, despite the profusion of available datasets, it remains quite difficult for a data analyst to start from a dataset at hand and customize it with additional attributes coming from other existing datasets. This article describes a model and algorithms that exploit automatically extracted and user-defined semantic relationships for extending analytic datasets with new atomic or aggregated attribute values. Our framework is implemented as a REST service in SAP HANA and includes a careful theoretical analysis and practical solutions for several complex data quality issues.}
}
@article{EICHSTADT2021100232,
title = {Metrology for the digital age},
journal = {Measurement: Sensors},
volume = {18},
pages = {100232},
year = {2021},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2021.100232},
url = {https://www.sciencedirect.com/science/article/pii/S2665917421001951},
author = {Sascha Eichstädt and Anke Keidel and Julia Tesch},
keywords = {Digital transformation, Digital certificates, Systemic metrology, Big data, Industry 4.0, Open science, FAIR},
abstract = {Based on digital technologies, big data, artificial intelligence and machine-readable information, the digital transformation rapidly changes society, industries, and economies. Metrology as a central element of international trade, for confidence in measurements and part of the quality infrastructure is facing several challenges and opportunities in these developments. In this contribution we discuss some of the key challenges and a potential future role of metrology in the digital age. We address metrological principles for confidence in data and Algorithms, cyber-physical systems, FAIR data and metrology, and the role of metrology in the digital transformation in the quality infrastructure.}
}
@incollection{SIMON201577,
title = {Chapter 8 - Considerations for the Big Data Era},
editor = {Alan Simon},
booktitle = {Enterprise Business Intelligence and Data Warehousing},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {77-82},
year = {2015},
isbn = {978-0-12-801540-7},
doi = {https://doi.org/10.1016/B978-0-12-801540-7.00008-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128015407000081},
author = {Alan Simon},
keywords = {Big Data, business intelligence, BI, analytics, predictive analytics, program manager, program management, project manager, project management, challenges, data warehouse, data warehousing, enterprise data warehouse, EDW},
abstract = {The Big Data era is creating seismic shifts in how we approach enterprise business intelligence and data warehousing. This final chapter discusses considerations related to technology and architecture, analytics-oriented requirements collection, and organizational structure.}
}
@article{CHEN201798,
title = {Data quality of electricity consumption data in a smart grid environment},
journal = {Renewable and Sustainable Energy Reviews},
volume = {75},
pages = {98-105},
year = {2017},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2016.10.054},
url = {https://www.sciencedirect.com/science/article/pii/S1364032116307109},
author = {Wen Chen and Kaile Zhou and Shanlin Yang and Cheng Wu},
keywords = {Electricity consumption data, Data quality, Outlier detection, Outlier data, Smart grid},
abstract = {With the increasing penetration of traditional and emerging information technologies in the electric power industry, together with the rapid development of electricity market reform, the electric power industry has accumulated a large amount of data. Data quality issues have become increasingly prominent, which affect the accuracy and effectiveness of electricity data mining and energy big data analytics. It is also closely related to the safety and reliability of the power system operation and management based on data-driven decision support. In this paper, we study the data quality of electricity consumption data in a smart grid environment. First, we analyze the significance of data quality. Also, the definition and classification of data quality issues are explained. Then we analyze the data quality of electricity consumption data and introduce the characteristics of electricity consumption data in a smart grid environment. The data quality issues of electricity consumption data are divided into three types, namely noise data, incomplete data and outlier data. We make a detailed discussion on these three types of data quality issues. In view of that outlier data is one of the most prominent issues in electricity consumption data, so we mainly focus on the outlier detection of electricity consumption data. This paper introduces the causes of electricity consumption outlier data and illustrates the significance of the electricity consumption outlier data from the negative and positive aspects respectively. Finally, the focus of this paper is to provide a review on the detection methods of electricity consumption outlier data. The methods are mainly divided into two categories, namely the data mining-based and the state estimation-based methods.}
}
@article{WANG2015782,
title = {Privacy trust crisis of personal data in China in the era of Big Data: The survey and countermeasures},
journal = {Computer Law & Security Review},
volume = {31},
number = {6},
pages = {782-792},
year = {2015},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2015.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0267364915001296},
author = {Zhong Wang and Qian Yu},
keywords = {Personal data, Privacy trust, Questionnaires, Interview, Big data},
abstract = {Privacy trust directly affects the personal willingness to share data and thus influences the quality and size of the data, thus affecting the development of big data technology and industry. As China is probably the largest personal data pool and vastest application market of big data, the situation of Chinese privacy trust plays a significant role. Based on the 17 most common data collection scenarios, the following aspects have been observed through 508 questionnaires and interviews of 20 samples. To start with, there is a severe privacy trust crisis in China, both in the field of enterprise services such as online shopping and social networks, etc. and in some public services like medical care and education, etc. Besides, there are also doubts about data collected by the government since individuals refuse to offer personal information or give false information as much as possible. Some people even buy two phone numbers, one is in use, while the other is not carried around or used by them, which is only bought to be offered to data collectors. Secondly, in terms of gender, females have lower trust in enterprises and social associations than males, especially in the fields of social networks and personal consumption. However, there is no obvious difference in fields of government and public services. Females possess stronger awareness but less skilled in precautions than males. Thirdly, people between the ages of 18 and 50 are more suspicious of data collected by enterprises, while age exerts little obvious influence on the credibility of data collected by the government, social associations and public services. Older people are less aware of precautions than people at other ages. In addition, from the perspective of education background, people with higher degrees possess stronger awareness of precautions and thus lower degree of trust. Therefore, it is suggested that more education on privacy consciousness should be given, and relative laws as well as regulations need improving. Besides, innovation in privacy protection technologies should be encouraged. What is more, we need to reinforce the management of the internet industry and strictly regulate personal data collection of the government.}
}
@article{KNEPPER20151504,
title = {Big Data on Ice: The Forward Observer System for In-flight Synthetic Aperture Radar Processing},
journal = {Procedia Computer Science},
volume = {51},
pages = {1504-1513},
year = {2015},
note = {International Conference On Computational Science, ICCS 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.05.340},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915011485},
author = {Richard Knepper and Matthew Standish and Matthew Link},
keywords = {Big Data, Network Filesystems, Synthetic Aperture Radar, Ice Sheet Data},
abstract = {We introduce the Forward Observer system, which is designed to provide data assurance in field data acquisition while receiving significant amounts (several terabytes per flight) of Synthetic Aperture Radar data during flights over the polar regions, which provide unique requirements for developing data collection and processing systems. Under polar conditions in the field and given the difficulty and expense of collecting data, data retention is absolutely critical. Our system provides a storage and analysis cluster with software that connects to field instruments via standard protocols, replicates data to multiple stores automatically as soon as it is written, and provides pre-processing of data so that initial visualizations are available immediately after collection, where they can provide feedback to researchers in the aircraft during the flight.}
}
@article{ZHAO20171085,
title = {An optimization model for green supply chain management by using a big data analytic approach},
journal = {Journal of Cleaner Production},
volume = {142},
pages = {1085-1097},
year = {2017},
note = {Special Volume on Improving natural resource management and human health to ensure sustainable societal development based upon insights gained from working within ‘Big Data Environments’},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616300579},
author = {Rui Zhao and Yiyun Liu and Ning Zhang and Tao Huang},
keywords = {Hazardous materials, Inherent risk, Carbon emissions, Multi-objective optimization, Green supply chain management, Big data analysis},
abstract = {This paper presents a multi-objective optimization model for a green supply chain management scheme that minimizes the inherent risk occurred by hazardous materials, associated carbon emission and economic cost. The model related parameters are capitalized on a big data analysis. Three scenarios are proposed to improve green supply chain management. The first scenario divides optimization into three options: the first involves minimizing risk and then dealing with carbon emissions (and thus economic cost); the second minimizes both risk and carbon emissions first, with the ultimate goal of minimizing overall cost; and the third option attempts to minimize risk, carbon emissions, and economic cost simultaneously. This paper provides a case study to verify the optimization model. Finally, the limitations of this research and approach are discussed to lay a foundation for further improvement.}
}
@incollection{KRISHNAN2013199,
title = {Chapter 10 - Integration of Big Data and Data Warehousing},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {199-217},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00010-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000106},
author = {Krish Krishnan},
keywords = {Big Data, Big Data appliances, Hadoop, NoSQL, RDBMS, data virtualization, semantic framework},
abstract = {The focus of this chapter is to discuss the integration of Big Data and the data warehouse, the possible techniques and pitfalls, and where we leverage a technology. How do we deal with complexity and heterogeneity of technologies? What are the performance and scalabilities of each technology, and how can we sustain performance for the new environment?}
}
@article{HAZEN2016592,
title = {Big data and predictive analytics for supply chain sustainability: A theory-driven research agenda},
journal = {Computers & Industrial Engineering},
volume = {101},
pages = {592-598},
year = {2016},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2016.06.030},
url = {https://www.sciencedirect.com/science/article/pii/S036083521630225X},
author = {Benjamin T. Hazen and Joseph B. Skipper and Jeremy D. Ezell and Christopher A. Boone},
keywords = {Big data, Predictive analytics, Supply chain management},
abstract = {Big data and predictive analytics (BDPA) tools and methodologies are leveraged by businesses in many ways to improve operational and strategic capabilities, and ultimately, to positively impact corporate financial performance. BDPA has become crucial for managing supply chain functions, where data intensive processes can be vastly improved through its effective use. BDPA has also become a competitive necessity for the management of supply chains, with practitioners and scholars focused almost entirely on how BDPA is used to increase economic measures of performance. There is limited understanding, however, as to how BDPA can impact other aspects of the triple bottom-line, namely environmental and social sustainability outcomes. Indeed, this area is in immediate need of attention from scholars in many fields including industrial engineering, supply chain management, information systems, business analytics, as well as other business and engineering disciplines. The purpose of this article is to motivate such research by proposing an agenda based in well-established theory. This article reviews eight theories that can be used by researchers to examine and clarify the nature of BDPA’s impact on supply chain sustainability, and presents research questions based upon this review. Scholars can leverage this article as the basis for future research activity, and practitioners can use this article as a means to understand how company-wide BDPA initiatives might impact measures of supply chain sustainability.}
}
@incollection{EBBELS2019329,
title = {Chapter 11 - Big Data and Databases for Metabolic Phenotyping},
editor = {John C. Lindon and Jeremy K. Nicholson and Elaine Holmes},
booktitle = {The Handbook of Metabolic Phenotyping},
publisher = {Elsevier},
pages = {329-367},
year = {2019},
isbn = {978-0-12-812293-8},
doi = {https://doi.org/10.1016/B978-0-12-812293-8.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128122938000116},
author = {Timothy M.D. Ebbels and Jake T.M. Pearce and Noureddin Sadawi and Jianliang Gao and Robert C. Glen},
keywords = {Metabolomics, Metabonomics, Metabolic phenotyping, Big data, Cloud computing, High-performance computing, Software tools, Databases, PhenoMeNal, Ethical, Legal, Social implications, ELSI},
abstract = {Metabolic phenotyping is entering the era of Big Data, leading to new opportunities and challenges. Cloud computing has been proposed as a novel paradigm, but as yet is not widely understood or used. In this chapter we introduce the concepts of Big Data and cloud computing, and discuss how they might change the landscape of metabolic phenotyping and analysis. We highlight some of the reasons for the increase in data size and explain advantages and disadvantages of large-scale computing in this context. We illustrate the area with a survey of software tools and databases currently available, and describe the newly developed cloud infrastructure “PhenoMeNal,” which will enable widespread use of these approaches. We conclude the chapter with a discussion of the important ethical, legal, and social implications (ELSI) of large-scale computing in this rapidly developing field.}
}
@article{SIVARAJAH2017263,
title = {Critical analysis of Big Data challenges and analytical methods},
journal = {Journal of Business Research},
volume = {70},
pages = {263-286},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S014829631630488X},
author = {Uthayasankar Sivarajah and Muhammad Mustafa Kamal and Zahir Irani and Vishanth Weerakkody},
keywords = {Big Data, Big Data Analytics, Challenges, Methods, Systematic literature review},
abstract = {Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized/proposed/employed by organizations to help others understand this landscape with the objective of making robust investment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 – What are the different types of BD challenges theorized/proposed/confronted by organizations? and Q2 – What are the different types of BDA methods theorized/proposed/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implications and potential further research avenues to support the academic community in exploring research themes/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.}
}
@incollection{HUGHES2016293,
title = {Chapter 13 - Surface Solutions Using Data Virtualization and Big Data},
editor = {Ralph Hughes},
booktitle = {Agile Data Warehousing for the Enterprise},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {293-327},
year = {2016},
isbn = {978-0-12-396464-9},
doi = {https://doi.org/10.1016/B978-0-12-396464-9.00013-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123964649000138},
author = {Ralph Hughes},
keywords = {Agile enterprise data warehousing, surface solutions, backfilling the architecture, shadow IT, data virtualization, big data, Hadoop, HDFS, Map/Reduce, Hive},
abstract = {Without investing in exotic data modeling techniques, EDW teams can achieve fast delivery using “surface solutions.” Surface solutions allow developers to first solve business problems with data taken from landing areas and then steadily “backfill” the DW/BI reference architecture to provide progressively more complete and robust solutions. Teams can create surface solutions by leveraging shadow IT, using data virtualization, and tapping a big data platform. When leveraging shadow IT, the EDW team delivers progressively richer data sets to departmental staff members, who build their own temporary BI solutions using that information. The data virtualization strategy relies on a “superoptimizer” that can create views across databases and data types, even including semistructured data as needed. The big data strategy employs a new category of products such as Hadoop’s HDFS, MapReduce, and Hive to provide access to new data, whether it be very large, poorly structured, and/or just unfamiliar to IT and the business users.}
}
@incollection{FIESCHI2018197,
title = {16 - Data for Epidemiology and Public Health, and Big Data11The questions posed by data processing for epidemiology and public health are often similar to those discussed in the chapters on clinical research (Chapter 18) and bioinformatics data (Chapter 17). For the sake of clarity, we address these questions in different chapters, although the problems are of the same nature and the solutions are isomorphic. In order to avoid too much repetition, the issue of big data is discussed here without going into the content of the other chapters.},
editor = {Marius Fieschi},
booktitle = {Health Data Processing},
publisher = {Elsevier},
pages = {197-212},
year = {2018},
isbn = {978-1-78548-287-8},
doi = {https://doi.org/10.1016/B978-1-78548-287-8.50016-X},
url = {https://www.sciencedirect.com/science/article/pii/B978178548287850016X},
author = {Marius Fieschi},
keywords = {Data processing, Data-sharing, e-health, Epidemiology, Health security, Monitoring systems, Preventive action, Public health, SurSaUD system},
abstract = {Abstract:
The approaches used by epidemiologists are diverse: they range from “field studies” for modeling and healthcare monitoring, to methods developed for researching and combating the emergence of diseases. Their analytical tools focus on the bio-statistics used as a tool to objectify phenomena studied in well-defined populations.}
}
@article{BARASH201510,
title = {Harnessing big data for precision medicine: A panel of experts elucidates the data challenges and proposes key strategic decisions points},
journal = {Applied & Translational Genomics},
volume = {4},
pages = {10-13},
year = {2015},
issn = {2212-0661},
doi = {https://doi.org/10.1016/j.atg.2015.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212066115000046},
author = {Carol Isaacson Barash and Keith O. Elliston and W. {Andrew Faucett} and Jonathan Hirsch and Gauri Naik and Alice Rathjen and Grant Wood},
abstract = {A group of disparate translational bioinformatics experts convened at the 6th Annual Precision Medicine Partnership Meeting, October 29–30, 2014 to discuss big data challenges and key strategic decisions needed to advance precision medicine, emerging solutions, and the anticipated path to success. This article reports the panel discussion.}
}
@article{BJORNSDOTTIR20181195,
title = {Exhibiting caution with use of big data: The case of amphetamine in Iceland's prescription registry},
journal = {Research in Social and Administrative Pharmacy},
volume = {14},
number = {12},
pages = {1195-1202},
year = {2018},
issn = {1551-7411},
doi = {https://doi.org/10.1016/j.sapharm.2018.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S155174111830127X},
author = {Ingunn Björnsdottir and Guri Birgitte Verne},
abstract = {Background
Data from large electronic databases are increasingly used in epidemiological research, but golden standards for database validation remain elusive. The Prescription Registry (IPR) and the National Health Service (NHS) databases in Iceland have not undergone formal validation, and gross errors have repeatedly been found in Icelandic statistics on pharmaceuticals. In 2015, new amphetamine tablets entered the Icelandic market, but were withdrawn half a year later due to being substandard. Return of unused stocks provided knowledge of the exact number of tablets used and hence a case where quality of the data could be assessed.
Objective
A case study of the quality of statistics in a national database on pharmaceuticals.
Methods
Data on the sales of the substandard amphetamine were obtained from the Prescription Registry and the pharmaceuticals statistics database. Upon the revelation of discrepancies, explanations were sought from the respective institutions, the producer, and dose dispensing companies.
Results
The substandard amphetamine was available from 1.9.2015 until 15.3.2016. According to NHS, 73990 tablets were sold to consumers in that period, whereas IPR initially stated 82860 tablets to have been sold, correcting to 74796 upon being notified about errors. The producer stated 72811 tablets to have been sold, and agreed with the dose dispensing companies on sales to those. The producer’s numbers were confirmed by the Medicines Agency.
Conclusion
Over-registration in the IPR was 13.8% before correction, 2.7% after correction, and 1.6% in the NHS. This case provided a unique opportunity for external validation of sales data for pharmaceuticals in Iceland, revealing enormous quality problems. The case has implications regarding database integrity beyond Iceland.}
}
@article{PONTORIERO2021106239,
title = {Automated Data Quality Control in FDOPA brain PET Imaging using Deep Learning},
journal = {Computer Methods and Programs in Biomedicine},
volume = {208},
pages = {106239},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106239},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721003138},
author = {Antonella D. Pontoriero and Giovanna Nordio and Rubaida Easmin and Alessio Giacomel and Barbara Santangelo and Sameer Jahuar and Ilaria Bonoldi and Maria Rogdaki and Federico Turkheimer and Oliver Howes and Mattia Veronese},
keywords = {FDOPA, PET, quality control, QC, convolutional neural networks},
abstract = {ABSTRACT
Introduction. With biomedical imaging research increasingly using large datasets, it becomes critical to find operator-free methods to quality control the data collected and the associated analysis. Attempts to use artificial intelligence (AI) to perform automated quality control (QC) for both single-site and multi-site datasets have been explored in some neuroimaging techniques (e.g. EEG or MRI), although these methods struggle to find replication in other domains. The aim of this study is to test the feasibility of an automated QC pipeline for brain [18F]-FDOPA PET imaging as a biomarker for the dopamine system. Methods. Two different Convolutional Neural Networks (CNNs) were used and combined to assess spatial misalignment to a standard template and the signal-to-noise ratio (SNR) relative to 200 static [18F]-FDOPA PET images that had been manually quality controlled from three different PET/CT scanners. The scans were combined with an additional 400 scans, in which misalignment (200 scans) and low SNR (200 scans) were simulated. A cross-validation was performed, where 80% of the data were used for training and 20% for validation. Two additional datasets of [18F]-FDOPA PET images (50 and 100 scans respectively with at least 80% of good quality images) were used for out-of-sample validation. Results. The CNN performance was excellent in the training dataset (accuracy for motion: 0.86 ± 0.01, accuracy for SNR: 0.69 ± 0.01), leading to 100% accurate QC classification when applied to the two out-of-sample datasets. Data dimensionality reduction affected the generalizability of the CNNs, especially when the classifiers were applied to the out-of-sample data from 3D to 1D datasets. Conclusions. This feasibility study shows that it is possible to perform automatic QC of [18F]-FDOPA PET imaging with CNNs. The approach has the potential to be extended to other PET tracers in both brain and non-brain applications, but it is dependent on the availability of large datasets necessary for the algorithm training.}
}
@article{ROMERO2015336,
title = {Tuning small analytics on Big Data: Data partitioning and secondary indexes in the Hadoop ecosystem},
journal = {Information Systems},
volume = {54},
pages = {336-356},
year = {2015},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2014.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306437914001458},
author = {Oscar Romero and Victor Herrero and Alberto Abelló and Jaume Ferrarons},
keywords = {Big Data, OLAP, Multidimensional model, Indexes, Partitioning, Cost estimation},
abstract = {In the recent years the problems of using generic storage (i.e., relational) techniques for very specific applications have been detected and outlined and, as a consequence, some alternatives to Relational DBMSs (e.g., HBase) have bloomed. Most of these alternatives sit on the cloud and benefit from cloud computing, which is nowadays a reality that helps us to save money by eliminating the hardware as well as software fixed costs and just pay per use. On top of this, specific querying frameworks to exploit the brute force in the cloud (e.g., MapReduce) have also been devised. The question arising next tries to clear out if this (rather naive) exploitation of the cloud is an alternative to tuning DBMSs or it still makes sense to consider other options when retrieving data from these settings. In this paper, we study the feasibility of solving OLAP queries with Hadoop (the Apache project implementing MapReduce) while benefiting from secondary indexes and partitioning in HBase. Our main contribution is the comparison of different access plans and the definition of criteria (i.e., cost estimation) to choose among them in terms of consumed resources (namely CPU, bandwidth and I/O).}
}
@article{MAIER201797,
title = {Big data in large-scale systemic mouse phenotyping},
journal = {Current Opinion in Systems Biology},
volume = {4},
pages = {97-104},
year = {2017},
note = {Big data acquisition and analysis • Pharmacology and drug discovery},
issn = {2452-3100},
doi = {https://doi.org/10.1016/j.coisb.2017.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S2452310017300525},
author = {Holger Maier and Stefanie Leuchtenberger and Helmut Fuchs and Valerie Gailus-Durner and Martin {Hrabe de Angelis}},
abstract = {Systemic phenotyping of mutant mice has been established at large scale in the last decade as a new tool to uncover the relations between genotype, phenotype and environment. Recent advances in that field led to the generation of a valuable open access data resource that can be used to better understanding the underlying causes for human diseases. From an ethical perspective, systemic phenotyping significantly contributes to the reduction of experimental animals and the refinement of animal experiments by enforcing standardisation efforts. There are particular logistical, experimental and analytical challenges of systemic large-scale mouse phenotyping. On all levels, IT solutions are critical to implement and efficiently support breeding, phenotyping and data analysis processes that lead to the generation of high-quality systemic phenotyping data accessible for the scientific community.}
}
@article{ABBASIAN201829,
title = {Improving early OSV design robustness by applying ‘Multivariate Big Data Analytics’ on a ship's life cycle},
journal = {Journal of Industrial Information Integration},
volume = {10},
pages = {29-38},
year = {2018},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X17300869},
author = {Niki Sadat Abbasian and Afshin Salajegheh and Henrique Gaspar and Per Olaf Brett},
keywords = {External data, Internal data, Abnormality, Missing data, Outliers, Randomness, Multivariate analysis, Data integration, Clustering},
abstract = {Typically, only a smaller portion of the monitorable operational data (e.g. from sensors and environment) from Offshore Support Vessels (OSVs) are used at present. Operational data, in addition to equipment performance data, design and construction data, creates large volumes of data with high veracity and variety. In most cases, such data richness is not well understood as to how to utilize it better during design and operation. It is, very often, too time consuming and resource demanding to estimate the final operational performance of vessel concept design solution in early design by applying simulations and model tests. This paper argues that there is a significant potential to integrate ship lifecycle data from different phases of its operation in large data repository for deliberate aims and evaluations. It is disputed discretely in the paper, evaluating performance of real similar type vessels during early stages of the design process, helps substantially improving and fine-tuning the performance criterion of the next generations of vessel design solutions. Producing learning from such a ship lifecycle data repository to find useful patterns and relationships among design parameters and existing fleet real performance data, requires the implementation of modern data mining techniques, such as big data and clustering concepts, which are introduced and applied in this paper. The analytics model introduced suggests and reviews all relevant steps of data knowledge discovery, including pre-processing (integration, feature selection and cleaning), processing (data analyzing) and post processing (evaluating and validating results) in this context.}
}
@article{DANIEL2019104804,
title = {Initializing a hospital-wide data quality program. The AP-HP experience.},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104804},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718306242},
author = {Christel Daniel and Patricia Serre and Nina Orlova and Stéphane Bréant and Nicolas Paris and Nicolas Griffon},
keywords = {Data accuracy, Data quality, Electronic health records, Data warehousing, Observational Studies as Topic},
abstract = {Background and Objectives
Data Quality (DQ) programs are recognized as a critical aspect of new-generation research platforms using electronic health record (EHR) data for building Learning Healthcare Systems. The AP-HP Clinical Data Repository aggregates EHR data from 37 hospitals to enable large-scale research and secondary data analysis. This paper describes the DQ program currently in place at AP-HP and the lessons learned from two DQ campaigns initiated in 2017.
Materials and Methods
As part of the AP-HP DQ program, two domains - patient identification (PI) and healthcare services (HS) - were selected for conducting DQ campaigns consisting of 5 phases: defining the scope, measuring, analyzing, improving and controlling DQ. Semi-automated DQ profiling was conducted in two data sets – the PI data set containing 8.8 M patients and the HS data set containing 13,099 consultation agendas and 2122 care units. Seventeen DQ measures were defined and DQ issues were classified using a unified DQ reporting framework. For each domain, actions plans were defined for improving and monitoring prioritized DQ issues.
Results
Eleven identified DQ issues (8 for the PI data set and 3 for the HS data set) were categorized into completeness (n = 6), conformance (n = 3) and plausibility (n = 2) DQ issues. DQ issues were caused by errors from data originators, ETL issues or limitations of the EHR data entry tool. The action plans included sixteen actions (9 for the PI domain and 7 for the HS domain). Though only partial implementation, the DQ campaigns already resulted in significant improvement of DQ measures.
Conclusion
DQ assessments of hospital information systems are largely unpublished. The preliminary results of two DQ campaigns conducted at AP-HP illustrate the benefit of the engagement into a DQ program. The adoption of a unified DQ reporting framework enables the communication of DQ findings in a well-defined manner with a shared vocabulary. Dedicated tooling is needed to automate and extend the scope of the generic DQ program. Specific DQ checks will be additionally defined on a per-study basis to evaluate whether EHR data fits for specific uses.}
}
@incollection{KRISHNAN2013219,
title = {Chapter 11 - Data-Driven Architecture for Big Data},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {219-240},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00011-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000118},
author = {Krish Krishnan},
keywords = {metadata, master data, machine learning, algorithms, semantic libraries, data governance},
abstract = {The goal of this chapter is to provide readers with data governance in the age of Big Data. We will discuss the goals of what managing data means with respect to the next generation of data warehousing and the role of metadata and master data in integrating Big Data into the data warehouse.}
}
@incollection{TALBURT20151,
title = {Chapter 1 - The Value Proposition for MDM and Big Data},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-16},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378000016},
author = {John R. Talburt and Yinle Zhou},
keywords = {Master data, master data management, MDM, Big Data, reference data management, RDM},
abstract = {This chapter gives a definition of master data management (MDM) and describes how it generates value for organizations. It also provides an overview of Big Data and the challenges it brings to MDM.}
}
@article{SEO201969,
title = {A pilot infrastructure for searching rainfall metadata and generating rainfall product using the big data of NEXRAD},
journal = {Environmental Modelling & Software},
volume = {117},
pages = {69-75},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815218307667},
author = {Bong-Chul Seo and Munsung Keem and Raymond Hammond and Ibrahim Demir and Witold F. Krajewski},
keywords = {NEXRAD, Rainfall, Cloud computing, Level II data, Hydrology},
abstract = {The Iowa Flood Center (IFC) developed a pilot infrastructure to explore rainfall metadata (descriptive statistics) and generate rainfall products over the Iowa domain based on the NEXRAD Level II data directly accessible through cloud storage (e.g., Amazon Web Services). Known as IFC-Cloud-NEXRAD, it resembles the Hydro-NEXRAD portal that provided researchers with ready access to NEXRAD radar data. Taking advantage of the cloud storage benefits (unlimited storage and instant access), IFC-Cloud-NEXRAD reduces the common challenges of most data exploration systems, which often lead to massive data acquisition/ingestion and rapid filling of limited system storage. Its map-based interface allows researchers to select a space-time domain of interest, retrieve and visualize pre-calculated rainfall metadata, and generate radar-derived rainfall products. Because the system provides generalized approaches to compute metadata and process data for rainfall estimation, the framework presented in this study would be readily transferrable to other geographic regions and larger scale applications.}
}
@article{ALVAREZSANCHEZ2019104824,
title = {TAQIH, a tool for tabular data quality assessment and improvement in the context of health data},
journal = {Computer Methods and Programs in Biomedicine},
volume = {181},
pages = {104824},
year = {2019},
note = {SI: Data Quality Assessment},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.12.029},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718304188},
author = {Roberto {Álvarez Sánchez} and Andoni {Beristain Iraola} and Gorka {Epelde Unanue} and Paul Carlin},
keywords = {Data quality, Exploratory data analysis, Data pre-processing},
abstract = {Background and Objectives
Data curation is a tedious task but of paramount relevance for data analytics and more specially in the health context where data-driven decisions must be extremely accurate. The ambition of TAQIH is to support non-technical users on 1) the exploratory data analysis (EDA) process of tabular health data, and 2) the assessment and improvement of its quality.
Methods
A web-based tool has been implemented with a simple yet powerful visual interface. First, it provides interfaces to understand the dataset, to gain the understanding of the content, structure and distribution. Then, it provides data visualization and improvement utilities for the data quality dimensions of completeness, accuracy, redundancy and readability.
Results
It has been applied in two different scenarios. (1) The Northern Ireland General Practitioners (GPs) Prescription Data, an open data set containing drug prescriptions. (2) A glucose monitoring tele health system dataset. Findings on (1) include: Features that had significant amount of missing values (e.g. AMP_NM variable 53.39%); instances that have high percentage of variable values missing (e.g. 0.21% of the instances with > 75% of missing values); highly correlated variables (e.g. Gross and Actual cost almost completely correlated (∼ + 1.0)). Findings on (2) include: Features that had significant amount of missing values (e.g. patient height, weight and body mass index (BMI) (> 70%), date of diagnosis 13%)); highly correlated variables (e.g. height, weight and BMI). Full detail of the testing and insights related to findings are reported.
Conclusions
TAQIH enables and supports users to carry out EDA on tabular health data and to assess and improve its quality. Having the layout of the application menu arranged sequentially as the conventional EDA pipeline helps following a consistent analysis process. The general description of the dataset and features section is very useful for the first overview of the dataset. The missing value heatmap is also very helpful in visually identifying correlations among missing values. The correlations section has proved to be supportive as a preliminary step before further data analysis pipelines, as well as the outliers section. Finally, the data quality section provides a quantitative value to the dataset improvements.}
}
@article{BENDLE2016115,
title = {Uncovering the message from the mess of big data},
journal = {Business Horizons},
volume = {59},
number = {1},
pages = {115-124},
year = {2016},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2015.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0007681315001408},
author = {Neil T. Bendle and Xin (Shane) Wang},
keywords = {Big data, User-Generated content, Latent Dirichlet Allocation, Topic modeling, Market research, Qualitative data},
abstract = {User-generated content, such as online product reviews, is a valuable source of consumer insight. Such unstructured big data is generated in real-time, is easily accessed, and contains messages consumers want managers to hear. Analyzing such data has potential to revolutionize market research and competitive analysis, but how can the messages be extracted? How can the vast amount of data be condensed into insights to help steer businesses’ strategy? We describe a non-proprietary technique that can be applied by anyone with statistical training. Latent Dirichlet Allocation (LDA) can analyze huge amounts of text and describe the content as focusing on unseen attributes in a specific weighting. For example, a review of a graphic novel might be analyzed to focus 70% on the storyline and 30% on the graphics. Aggregating the content from numerous consumers allows us to understand what is, collectively, on consumers’ minds, and from this we can infer what consumers care about. We can even highlight which attributes are seen positively or negatively. The value of this technique extends well beyond the CMO's office as LDA can map the relative strategic positions of competitors where they matter most: in the minds of consumers.}
}
@article{SCHULER2019191,
title = {Big Data Readiness in Radiation Oncology: An Efficient Approach for Relabeling Radiation Therapy Structures With Their TG-263 Standard Name in Real-World Data Sets},
journal = {Advances in Radiation Oncology},
volume = {4},
number = {1},
pages = {191-200},
year = {2019},
issn = {2452-1094},
doi = {https://doi.org/10.1016/j.adro.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S2452109418302240},
author = {Thilo Schuler and John Kipritidis and Thomas Eade and George Hruby and Andrew Kneebone and Mario Perez and Kylie Grimberg and Kylie Richardson and Sally Evill and Brooke Evans and Blanca Gallego},
abstract = {Purpose
To prepare for big data analyses on radiation therapy data, we developed Stature, a tool-supported approach for standardization of structure names in existing radiation therapy plans. We applied the widely endorsed nomenclature standard TG-263 as the mapping target and quantified the structure name inconsistency in 2 real-world data sets.
Methods and Materials
The clinically relevant structures in the radiation therapy plans were identified by reference to randomized controlled trials. The Stature approach was used by clinicians to identify the synonyms for each relevant structure, which was then mapped to the corresponding TG-263 name. We applied Stature to standardize the structure names for 654 patients with prostate cancer (PCa) and 224 patients with head and neck squamous cell carcinoma (HNSCC) who received curative radiation therapy at our institution between 2007 and 2017. The accuracy of the Stature process was manually validated in a random sample from each cohort. For the HNSCC cohort we measured the resource requirements for Stature, and for the PCa cohort we demonstrated its impact on an example clinical analytics scenario.
Results
All but 1 synonym group (“Hydrogel”) was mapped to the corresponding TG-263 name, resulting in a TG-263 relabel rate of 99% (8837 of 8925 structures). For the PCa cohort, Stature matched a total of 5969 structures. Of these, 5682 structures were exact matches (ie, following local naming convention), 284 were matched via a synonym, and 3 required manual matching. This original radiation therapy structure names therefore had a naming inconsistency rate of 4.81%. For the HNSCC cohort, Stature mapped a total of 2956 structures (2638 exact, 304 synonym, 14 manual; 10.76% inconsistency rate) and required 7.5 clinician hours. The clinician hours required were one-fifth of those that would be required for manual relabeling. The accuracy of Stature was 99.97% (PCa) and 99.61% (HNSCC).
Conclusions
The Stature approach was highly accurate and had significant resource efficiencies compared with manual curation.}
}
@article{LIU2018191,
title = {Steering data quality with visual analytics: The complexity challenge},
journal = {Visual Informatics},
volume = {2},
number = {4},
pages = {191-197},
year = {2018},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2018.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X18300573},
author = {Shixia Liu and Gennady Andrienko and Yingcai Wu and Nan Cao and Liu Jiang and Conglei Shi and Yu-Shuen Wang and Seokhee Hong},
keywords = {Data quality management, Visual analytics, Data cleansing},
abstract = {Data quality management, especially data cleansing, has been extensively studied for many years in the areas of data management and visual analytics. In the paper, we first review and explore the relevant work from the research areas of data management, visual analytics and human-computer interaction. Then for different types of data such as multimedia data, textual data, trajectory data, and graph data, we summarize the common methods for improving data quality by leveraging data cleansing techniques at different analysis stages. Based on a thorough analysis, we propose a general visual analytics framework for interactively cleansing data. Finally, the challenges and opportunities are analyzed and discussed in the context of data and humans.}
}
@incollection{MAYER201667,
title = {Chapter 5 - Big Data For Health Through Social Media},
editor = {Shabbir Syed-Abdul and Elia Gabarron and Annie Y.S. Lau},
booktitle = {Participatory Health Through Social Media},
publisher = {Academic Press},
pages = {67-82},
year = {2016},
isbn = {978-0-12-809269-9},
doi = {https://doi.org/10.1016/B978-0-12-809269-9.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128092699000050},
author = {M.A. Mayer and L. Fernández-Luque and A. Leis},
keywords = {Big Data, social media, data analysis, public health, Internet},
abstract = {Social Media (SM) can be a complementary channel of information to other official means for the health data collection such as the epidemiological surveillance activities and control carried out by health authorities. For this reason, more and more organizations, professionals, and scientific institutions are seeing the need to make the most of resources of health information based on SM platforms through the use of Big Data tools and analytics. Although there is a consensus on the potential benefits and opportunities that SM may provide when used for healthcare purposes, its use has brought unsuspected drawbacks and challenges related to the protection of personal data, it is essential to promote a wide reflection and that the authorities and governments establish, in collaboration with patients associations and professional institutions, specific ethical, legal guidelines, and use policies to the benefit of the current and future healthcare professional–patient relationship and general public.}
}
@article{DREWER2017298,
title = {The BIG DATA Challenge: Impact and opportunity of large quantities of information under the Europol Regulation},
journal = {Computer Law & Security Review},
volume = {33},
number = {3},
pages = {298-308},
year = {2017},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917300699},
author = {Daniel Drewer and Vesela Miladinova},
keywords = {Europol, Big data, Privacy, Data protection, Data protection impact assessment, Risk assessment, Privacy by design, Advanced technologies, Europol Regulation, Integrated Data Management Concept (IDMC)},
abstract = {In the digital age, the interaction between privacy, data protection and advanced technological developments such as big data analytics has become pertinent to Europol's effectiveness in providing accurate crime analyses. For the purposes of preventing and combating crime falling within the scope of its objectives, it is imperative for Europol to employ the fullest and most up-to-date information and technical capabilities possible whilst respecting fundamental human rights. The present article addresses precisely the “paradox” of on one side protecting fundamental human rights against external terrorist and/or cybercrime intrusions, and on the other providing a privacy-conscious approach to data collection and analytics, so that Europol can even more effectively support and strengthen action in protecting society against internal threats in a proportionate, responsible and legitimate manner. The advantage proposed in this very context of large quantities of data informing strategic analysis at Europol is a purpose-oriented data protection impact assessment. Namely, the evolution from traditional instruments in the fight against organised crime and terrorism to more technologically advanced ones equally requires an alteration of the conventional notions of privacy and investigative and information-sharing methods.}
}
@article{GUNTHER2019583,
title = {Data quality assessment for improved decision-making: a methodology for small and medium-sized enterprises},
journal = {Procedia Manufacturing},
volume = {29},
pages = {583-591},
year = {2019},
note = {“18th International Conference on Sheet Metal, SHEMET 2019”“New Trends and Developments in Sheet Metal Processing”},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.02.114},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919301477},
author = {Lisa C. Günther and Eduardo Colangelo and Hans-Hermann Wiendahl and Christian Bauer},
keywords = {Data quality assessment, Data quality control, Information quality, Benchmarking, Production planning, control},
abstract = {Industrial enterprises rely on prediction of market behavior, monitoring of performance measures, evaluation of production processes and other data analyses to support strategic and operational decisions. However, although an adequate data quality (DQ) is essential for any data analysis and several methodologies for DQ assessment exist, not all organizations consider DQ in decision-making processes. E.g., inaccurate and delayed data acquisition leads to imprecise master data and poor knowledge of machine utilization. While these aspects should influence production planning and control, current approaches to data evaluation are too complex to use them on a-day-to-day basis. In this paper, we propose a methodology that simplifies the execution of DQ evaluations and improves the understandability of its results. One of its main concerns is to make DQ assessment usable to small and medium-sized enterprises (SME). The approach takes selected, context related structured or semi-structured data as input and uses a set of generic test criteria applicable to different tasks and domains. It combines data and domain driven aspects and can be partly executed automated and without context specific domain knowledge. The results of the assessment can be summarized into quality dimensions and used for benchmarking. The methodology is validated using data from the enterprise resource planning (ERP) and manufacturing execution system (MES) of a sheet metal manufacturer covering a year of time. The particular application aims at calculating logistic key performance indicators. Based on these conditions, data requirements are defined and the available data is evaluated considering domain specific characteristics.}
}
@incollection{PHAN2017253,
title = {9 - Big Data and Monitoring the Grid},
editor = {Brian W. D’Andrade},
booktitle = {The Power Grid},
publisher = {Academic Press},
pages = {253-285},
year = {2017},
isbn = {978-0-12-805321-8},
doi = {https://doi.org/10.1016/B978-0-12-805321-8.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053218000094},
author = {Sonal K. Phan and Cathy Chen},
keywords = {Big data, power quality disturbance detection, intrusion detection, islanding detection, feature extraction, classification, data analytics, forecasting, visualization, smart meters, demand response},
abstract = {A traditional power grid, also known as the legacy grid, collects data at a few locations on the grid to monitor grid performance and forecast energy requirements on a macro level. A smart grid is the next generation of the electric power grid; it includes technologies for real-time data acquisition from various sections of the grid and provides a means for two-way communication between energy suppliers and consumers. Compared to a legacy grid, the smart grid generates large volumes of data that can be exploited for power quality event monitoring, intrusion detection, islanding detection, price forecasting, and energy forecasting at a much more granular level. These large volumes of data have to be analyzed in real-time and with high accuracy in order assist in decision making for power system operations and optimal power flow. This poses a big data challenge, which to be implemented successfully requires changes in infrastructure and data analysis methods. This chapter describes the smart grid and its associated big data and discusses methods for informative feature extraction from raw data, event monitoring, and energy consumption forecasting using these features and visualization methods to assist with data interpretation and decision making.}
}
@article{BIBAULT2016110,
title = {Big Data and machine learning in radiation oncology: State of the art and future prospects},
journal = {Cancer Letters},
volume = {382},
number = {1},
pages = {110-117},
year = {2016},
issn = {0304-3835},
doi = {https://doi.org/10.1016/j.canlet.2016.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0304383516303469},
author = {Jean-Emmanuel Bibault and Philippe Giraud and Anita Burgun},
keywords = {Radiation oncology, Big Data, Predictive model, Machine learning},
abstract = {Precision medicine relies on an increasing amount of heterogeneous data. Advances in radiation oncology, through the use of CT Scan, dosimetry and imaging performed before each fraction, have generated a considerable flow of data that needs to be integrated. In the same time, Electronic Health Records now provide phenotypic profiles of large cohorts of patients that could be correlated to this information. In this review, we describe methods that could be used to create integrative predictive models in radiation oncology. Potential uses of machine learning methods such as support vector machine, artificial neural networks, and deep learning are also discussed.}
}
@article{SONG2019288,
title = {Dynamic assessment of PM2.5 exposure and health risk using remote sensing and geo-spatial big data},
journal = {Environmental Pollution},
volume = {253},
pages = {288-296},
year = {2019},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2019.06.057},
url = {https://www.sciencedirect.com/science/article/pii/S026974911930418X},
author = {Yimeng Song and Bo Huang and Qingqing He and Bin Chen and Jing Wei and Rashed Mahmood},
keywords = {Human mobility, Spatiotemporal heterogeneity, Remote sensing, Big data, Environmental health},
abstract = {In the past few decades, extensive epidemiological studies have focused on exploring the adverse effects of PM2.5 (particulate matters with aerodynamic diameters less than 2.5 μm) on public health. However, most of them failed to consider the dynamic changes of population distribution adequately and were limited by the accuracy of PM2.5 estimations. Therefore, in this study, location-based service (LBS) data from social media and satellite-derived high-quality PM2.5 concentrations were collected to perform highly spatiotemporal exposure assessments for thirteen cities in the Beijing-Tianjin-Hebei (BTH) region, China. The city-scale exposure levels and the corresponding health outcomes were first estimated. Then the uncertainties in exposure risk assessments were quantified based on in-situ PM2.5 observations and static population data. The results showed that approximately half of the population living in the BTH region were exposed to monthly mean PM2.5 concentration greater than 80 μg/m3 in 2015, and the highest risk was observed in December. In terms of all-cause, cardiovascular, and respiratory disease, the premature deaths attributed to PM2.5 were estimated to be 138,150, 80,945, and 18,752, respectively. A comparative analysis between five different exposure models further illustrated that the dynamic population distribution and accurate PM2.5 estimations showed great influence on environmental exposure and health assessments and need be carefully considered. Otherwise, the results would be considerably over- or under-estimated.}
}
@article{MORANFERNANDEZ2022365,
title = {How important is data quality? Best classifiers vs best features},
journal = {Neurocomputing},
volume = {470},
pages = {365-375},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.05.107},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221011127},
author = {Laura Morán-Fernández and Verónica Bólon-Canedo and Amparo Alonso-Betanzos},
keywords = {Feature selection, Filters, Preprocessing, High dimensionality, Classification, Data analysis},
abstract = {The task of choosing the appropriate classifier for a given scenario is not an easy-to-solve question. First, there is an increasingly high number of algorithms available belonging to different families. And also there is a lack of methodologies that can help on recommending in advance a given family of algorithms for a certain type of datasets. Besides, most of these classification algorithms exhibit a degradation in the performance when faced with datasets containing irrelevant and/or redundant features. In this work we analyze the impact of feature selection in classification over several synthetic and real datasets. The experimental results obtained show that the significance of selecting a classifier decreases after applying an appropriate preprocessing step and, not only this alleviates the choice, but it also improves the results in almost all the datasets tested.}
}
@article{BABAR2018155,
title = {Energy-harvesting based on internet of things and big data analytics for smart health monitoring},
journal = {Sustainable Computing: Informatics and Systems},
volume = {20},
pages = {155-164},
year = {2018},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2017.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S2210537917302238},
author = {Muhammad Babar and Ataur Rahman and Fahim Arif and Gwanggil Jeon},
keywords = {Big data analytics, IoT, Energy harvesting},
abstract = {Current advancements and growth in the arena of the Internet of Things (IoT) is providing great potential in the novel epoch of healthcare. The future of healthcare is expansively promising, as it advances the excellence of life and health of humans, involving several health regulations. Continual increases of multifaceted IoT devices in healthcare is beset by challenges, such as powering IoT terminal nodes used for health monitoring, data processing, smart decisions, and event management. In this paper, we propose a healthcare architecture which is based on an analysis of energy harvesting for health monitoring sensors and the realization of Big Data analytics in healthcare. The rationale of the proposed architecture is two-fold: (1) comprehensive conceptual framework for energy harvesting for health monitoring sensors; and (2) data processing and decision management for healthcare. The proposed architecture is a three-layered architecture that comprises: (1) energy harvesting and data generation; (2) data pre-processing; and (3) data processing and application. The proposed scheme highlights the effectiveness of energy-harvesting based IoT in healthcare. In addition, it also proposes a solution for smart health monitoring and planning. We also utilized consistent datasets on the Hadoop server to validate the proposed architecture based on threshold limit values (TLVs). The study demonstrates that the proposed architecture offers substantial and immediate value to the field of smart health.}
}
@article{RAJAN2019193,
title = {Towards a content agnostic computable knowledge repository for data quality assessment},
journal = {Computer Methods and Programs in Biomedicine},
volume = {177},
pages = {193-201},
year = {2019},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718306254},
author = {Naresh Sundar Rajan and Ramkiran Gouripeddi and Peter Mo and Randy K. Madsen and Julio C. Facelli},
keywords = {Data Quality Metadata Repository, Knowledge representation, Data quality assessment, Data quality dimensions, Data quality framework},
abstract = {Background and objective
In recent years, several data quality conceptual frameworks have been proposed across the Data Quality and Information Quality domains towards assessment of quality of data. These frameworks are diverse, varying from simple lists of concepts to complex ontological and taxonomical representations of data quality concepts. The goal of this study is to design, develop and implement a platform agnostic computable data quality knowledge repository for data quality assessments.
Methods
We identified computable data quality concepts by performing a comprehensive literature review of articles indexed in three major bibliographic data sources. From this corpus, we extracted data quality concepts, their definitions, applicable measures, their computability and identified conceptual relationships. We used these relationships to design and develop a data quality meta-model and implemented it in a quality knowledge repository.
Results
We identified three primitives for programmatically performing data quality assessments: data quality concept, its definition, its measure or rule for data quality assessment, and their associations. We modeled a computable data quality meta-data repository and extended this framework to adapt, store, retrieve and automate assessment of other existing data quality assessment models.
Conclusion
We identified research gaps in data quality literature towards automating data quality assessments methods. In this process, we designed, developed and implemented a computable data quality knowledge repository for assessing quality and characterizing data in health data repositories. We leverage this knowledge repository in a service-oriented architecture to perform scalable and reproducible framework for data quality assessments in disparate biomedical data sources.}
}
@article{SARAN2017713,
title = {The China Kidney Disease Network (CK-NET): “Big Data—Big Dreams”},
journal = {American Journal of Kidney Diseases},
volume = {69},
number = {6},
pages = {713-716},
year = {2017},
issn = {0272-6386},
doi = {https://doi.org/10.1053/j.ajkd.2017.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0272638617306340},
author = {Rajiv Saran and Diane Steffick and Jennifer Bragg-Gresham}
}
@article{CHALVATZIS2019381,
title = {Sustainable resource allocation for power generation: The role of big data in enabling interindustry architectural innovation},
journal = {Technological Forecasting and Social Change},
volume = {144},
pages = {381-393},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.04.031},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517315147},
author = {Konstantinos J. Chalvatzis and Hanif Malekpoor and Nishikant Mishra and Fiona Lettice and Sonal Choudhary},
keywords = {Energy innovation, Interindustry architectural innovation, Sustainable energy, Fuel mix, Grey TOPSIS, grey linear programming},
abstract = {Economic, social and environmental requirements make planning for a sustainable electricity generation mix a demanding endeavour. Technological innovation offers a range of renewable generation and energy management options which require fine tuning and accurate control to be successful, which calls for the use of large-scale, detailed datasets. In this paper, we focus on the UK and use Multi-Criteria Decision Making (MCDM) to evaluate electricity generation options against technical, environmental and social criteria. Data incompleteness and redundancy, usual in large-scale datasets, as well as expert opinion ambiguity are dealt with using a comprehensive grey TOPSIS model. We used evaluation scores to develop a multi-objective optimization model to maximize the technical, environmental and social utility of the electricity generation mix and to enable a larger role for innovative technologies. Demand uncertainty was handled with an interval range and we developed our problem with multi-objective grey linear programming (MOGLP). Solving the mathematical model provided us with the electricity generation mix for every 5 min of the period under study. Our results indicate that nuclear and renewable energy options, specifically wind, solar, and hydro, but not biomass energy, perform better against all criteria indicating that interindustry architectural innovation in the power generation mix is key to sustainable UK electricity production and supply.}
}
@incollection{GUDIVADA2016169,
title = {Chapter 5 - Cognitive Analytics: Going Beyond Big Data Analytics and Machine Learning},
editor = {Venkat N. Gudivada and Vijay V. Raghavan and Venu Govindaraju and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {35},
pages = {169-205},
year = {2016},
booktitle = {Cognitive Computing: Theory and Applications},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2016.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169716116300517},
author = {V.N. Gudivada and M.T. Irfan and E. Fathi and D.L. Rao},
keywords = {Cognitive analytics, Text analytics, Learning analytics, Educational data mining, Cognitive systems, Cognitive computing, Personalized learning, Data science, Machine learning, Big data analytics, Business analytics},
abstract = {This chapter defines analytics and traces its evolution from its origin in 1988 to its current stage—cognitive analytics. We discuss types of learning and describe classes of machine learning algorithms. Given this backdrop, we propose a reference architecture for cognitive analytics and indicate ways to implement the architecture. A few cognitive analytics applications are briefly described. The chapter concludes by indicating current trends and future research direction.}
}
@article{CALYAM20163,
title = {Synchronous Big Data analytics for personalized and remote physical therapy},
journal = {Pervasive and Mobile Computing},
volume = {28},
pages = {3-20},
year = {2016},
note = {Special Issue on Big Data for Healthcare; Guest Editors: Sriram Chellappan, Nirmalya Roy, Sajal K. Das and Special Issue on Security and Privacy in Mobile Clouds Guest; Editors: Sherman S.M. Chow, Urs Hengartner, Joseph K. Liu, Kui Ren},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2015.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1574119215001704},
author = {Prasad Calyam and Anup Mishra and Ronny Bazan Antequera and Dmitrii Chemodanov and Alex Berryman and Kunpeng Zhu and Carmen Abbott and Marjorie Skubic},
keywords = {Smart health care, Personalized remote physical therapy, Synchronous Big Data, Gigabit networking app},
abstract = {With gigabit networking becoming economically feasible and widely installed at homes, there are new opportunities to revisit in-home, personalized telehealth services. In this paper, we describe a novel telehealth eldercare service that we developed viz., “PhysicalTherapy-as-a-Service” (PTaaS) that connects a remote physical therapist at a clinic to a senior at home. The service leverages a high-speed, low-latency network connection through an interactive interface built on top of Microsoft Kinect motion sensing capabilities. The interface that is built using user-centered design principles for wellness coaching exercises is essentially a ‘Synchronous Big Data’ application due to its: (i) high data-in-motion velocity (i.e., peak data rate is ≈400 Mbps), (ii) considerable variety (i.e., measurements include 3D sensing, network health, user opinion surveys and video clips of RGB, skeletal and depth data), and (iii) large volume (i.e., several GB of measurement data for a simple exercise activity). The successful PTaaS delivery through this interface is dependent on the veracity analytics needed for correlation of the real-time Big Data streams within a session, in order to assess exercise balance of the senior without any bias due to network quality effects. Our experiments with PTaaS in an actual testbed involving senior homes in Kansas City with Google Fiber connections and our university clinic demonstrate the network configuration and time synchronization related challenges in order to perform online analytics. Our findings provide insights on how to: (a) enable suitable resource calibration and perform network troubleshooting for high user experience for both the therapist and the senior, and (b) realize a Big Data architecture for PTaaS and other similar personalized healthcare services to be remotely delivered at a large-scale in a reliable, secure and cost-effective manner.}
}
@incollection{ANYA201699,
title = {Chapter 5 - Leveraging Big Data Analytics for Personalized Elderly Care: Opportunities and Challenges},
editor = {Dhiya Al-Jumeily and Abir Hussain and Conor Mallucci and Carol Oliver},
booktitle = {Applied Computing in Medicine and Health},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {99-124},
year = {2016},
series = {Emerging Topics in Computer Science and Applied Computing},
isbn = {978-0-12-803468-2},
doi = {https://doi.org/10.1016/B978-0-12-803468-2.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128034682000059},
author = {Obinna Anya and Hissam Tawfik},
keywords = {Elderly care, personalized care, independent living, ACTVAGE, Big Data analytics, CAPIM, lifestyle-oriented, context-awareness, framework},
abstract = {Owing to the growing increase in the world’s ageing population, research has focused on developing information and communication technology (ICT)–based services for personalized care, improved health, and quality social life for the elderly. Recent efforts explore Big Data in order to build mathematical models of personal behavior and lifestyle for analytics. Leveraging Big Data analytics holds enormous potential for solving some of the biggest and most intractable challenges in personalized elderly care through quantified modeling of a person’s lifestyle in a way that takes cognizance of their beliefs, values, and preferences, and connects to a history of events, things, and places around which they have progressively built their lives. However, the idea of discovering patterns to personalize care and inform critical health care decisions for the elderly is challenged as data grow exponentially in volume, become faster and increasingly unstructured, and are generated from sociodigital engagements that often may not accurately reflect the real-world entities and contexts they represent. As a result, the idea raises issues along several dimensions, including social, technical, and context-aware challenges. In this chapter, we present an overview of the state of the art in personalized elderly care, and explore the opportunities and inherent sociotechnical challenges in leveraging Big Data analytics to support elderly care and independent living. Based on this discussion, and arguing that analytics need to take account of the contexts that shape the generation and use of data, ACTVAGE, a context-aware lifestyle-oriented framework for personalized elderly care and independent living is proposed.}
}
@article{GUETA2016139,
title = {Quantifying the value of user-level data cleaning for big data: A case study using mammal distribution models},
journal = {Ecological Informatics},
volume = {34},
pages = {139-145},
year = {2016},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2016.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1574954116300577},
author = {Tomer Gueta and Yohay Carmel},
keywords = {Biodiversity informatics, Data-cleaning, SDM performance, MaxEnt, Australian mammals, Big-data},
abstract = {The recent availability of species occurrence data from numerous sources, standardized and connected within a single portal, has the potential to answer fundamental ecological questions. These aggregated big biodiversity databases are prone to numerous data errors and biases. The data-user is responsible for identifying these errors and assessing if the data are suitable for a given purpose. Complex technical skills are increasingly required for handling and cleaning biodiversity data, while biodiversity scientists possessing these skills are rare. Here, we estimate the effect of user-level data cleaning on species distribution model (SDM) performance. We implement several simple and easy-to-execute data cleaning procedures, and evaluate the change in SDM performance. Additionally, we examine if a certain group of species is more sensitive to the use of erroneous or unsuitable data. The cleaning procedures used in this research improved SDM performance significantly, across all scales and for all performance measures. The largest improvement in distribution models following data cleaning was for small mammals (1g–100g). Data cleaning at the user level is crucial when using aggregated occurrence data, and facilitating its implementation is a key factor in order to advance data-intensive biodiversity studies. Adopting a more comprehensive approach for incorporating data cleaning as part of data analysis, will not only improve the quality of biodiversity data, but will also impose a more appropriate usage of such data.}
}
@incollection{KRISHNAN2013101,
title = {Chapter 5 - Big Data Driving Business Value},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {101-123},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000052},
author = {Krish Krishnan},
keywords = {sensor data, machine data, social media, compliance, safety},
abstract = {The first four chapters provided you an introduction to Big Data, the complexities associated with Big Data, and the processing techniques and technologies for Big Data. This chapter will focus on use cases of Big Data and how real-world companies are implementing Big Data.}
}
@article{PFEIFFER2015213,
title = {Spatial and temporal epidemiological analysis in the Big Data era},
journal = {Preventive Veterinary Medicine},
volume = {122},
number = {1},
pages = {213-220},
year = {2015},
issn = {0167-5877},
doi = {https://doi.org/10.1016/j.prevetmed.2015.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167587715002111},
author = {Dirk U. Pfeiffer and Kim B. Stevens},
keywords = {Data science, Exploratory analysis, Internet of Things, Modelling, Multi-criteria decision analysis, Spatial analysis, Visualisation},
abstract = {Concurrent with global economic development in the last 50 years, the opportunities for the spread of existing diseases and emergence of new infectious pathogens, have increased substantially. The activities associated with the enormously intensified global connectivity have resulted in large amounts of data being generated, which in turn provides opportunities for generating knowledge that will allow more effective management of animal and human health risks. This so-called Big Data has, more recently, been accompanied by the Internet of Things which highlights the increasing presence of a wide range of sensors, interconnected via the Internet. Analysis of this data needs to exploit its complexity, accommodate variation in data quality and should take advantage of its spatial and temporal dimensions, where available. Apart from the development of hardware technologies and networking/communication infrastructure, it is necessary to develop appropriate data management tools that make this data accessible for analysis. This includes relational databases, geographical information systems and most recently, cloud-based data storage such as Hadoop distributed file systems. While the development in analytical methodologies has not quite caught up with the data deluge, important advances have been made in a number of areas, including spatial and temporal data analysis where the spectrum of analytical methods ranges from visualisation and exploratory analysis, to modelling. While there used to be a primary focus on statistical science in terms of methodological development for data analysis, the newly emerged discipline of data science is a reflection of the challenges presented by the need to integrate diverse data sources and exploit them using novel data- and knowledge-driven modelling methods while simultaneously recognising the value of quantitative as well as qualitative analytical approaches. Machine learning regression methods, which are more robust and can handle large datasets faster than classical regression approaches, are now also used to analyse spatial and spatio-temporal data. Multi-criteria decision analysis methods have gained greater acceptance, due in part, to the need to increasingly combine data from diverse sources including published scientific information and expert opinion in an attempt to fill important knowledge gaps. The opportunities for more effective prevention, detection and control of animal health threats arising from these developments are immense, but not without risks given the different types, and much higher frequency, of biases associated with these data.}
}
@article{SALEM2021,
title = {LODQuMa: A Free-ontology process for Linked (Open) Data quality management},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001348},
author = {Samah Salem and Fouzia Benchikha},
keywords = {Linked Open Data, Quality assessment, Quality improvement, Synonym predicates, Profiling statistics, DBpedia},
abstract = {For many years, data quality is among the most commonly discussed issue in Linked Open Data (LOD) due to the huge volume of integrated datasets that are usually heterogeneous. Several ontology-based approaches dealing with quality problems have been proposed. However, when datasets lack a well-defined schema, these approaches become ineffective because of the lack of metadata. Moreover, the detection of quality problems based on an analysis between RDF (Resource Description Framework) triples without requiring ontology statistical and semantical information is not addressed. Keeping in mind that ontologies are not always available and they may be incomplete or misused. In this paper, a novel free-ontology process called LODQuMa is proposed to assess and improve the quality of LOD. It is mainly based on profiling statistics, synonym relationships between predicates, QVCs (Quality Verification Cases), and SPARQL (SPARQL Protocol and RDF Query Language) query templates. Experiments on the DBpedia dataset demonstrate that the proposed process is effective for increasing the intrinsic quality dimensions, resulting in correct and compact datasets.}
}
@article{DEBAUCHE2018112,
title = {Cloud Platform using Big Data and HPC Technologies for Distributed and Parallels Treatments},
journal = {Procedia Computer Science},
volume = {141},
pages = {112-118},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318064},
author = {Olivier Debauche and Sidi Ahmed Mahmoudi and Saïd Mahmoudi and Pierre Manneback},
keywords = {GPU, FPGA, MIC, CPU, TPU, Cloud, Big Data, parallel, distributed processing, heterogeneous cloud architecture},
abstract = {Smart agriculture is one of the most diverse research. In addition, the quantity of data to be stored and the choice of the most efficient algorithms to process are significant elements in this field. The storage of collecting data from Internet of Things (IoT), existing on distributed, local databases and open data need a particular infrastructure to federate all these data to make complex treatments. The storage of this wide range of data that comes at high frequency and variable throughput is particularly difficult. In this paper, we propose the use of distributed databases and high-performance computing architecture in order to exploit multiple re-configurable computing and application specific processing such as CPUs, GPUs, TPUs and FPGAs efficiently. This exploitation allows an accurate training for an application to machine learning, deep learning and unsupervised modeling algorithms. The last ones are used for training supervised algorithms on images when it labels a set of images and unsupervised algorithms on IoT data which are unlabeled with variable qualities. The processing of data is based on Hadoop 3.1 MapReduce to achieve parallel processing and use containerization technologies to distribute treatments on Multi GPU, MIC and FPGA. This architecture allows efficient treatments of data coming from several sources with a cloud high-performance heterogeneous architecture. The proposed 4 layers infrastructure can also implement FPGA and MIC which are now natively supported by recent version of Hadoop. Moreover, with the advent of new technologies like Intel® MovidiusTM; it is now possible to deploy CNN at the Fog level in the IoT network and to make inference with the cloud and therefore limit significantly the network traffic that result in reducing the move of large amounts of data to the cloud.}
}
@article{CHOI2019139,
title = {Data quality challenges for sustainable fashion supply chain operations in emerging markets: Roles of blockchain, government sponsors and environment taxes},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {131},
pages = {139-152},
year = {2019},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2019.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S1366554519311494},
author = {Tsan-Ming Choi and Suyuan Luo},
keywords = {Fashion business operations, Supply chain centralization, Emerging markets, Sustainable operations, Social welfare},
abstract = {In emerging markets, there are data quality problems. In this paper, we establish theoretical models to explore how data quality problems affect sustainable fashion supply chain operations. We start with the decentralized supply chain and find that poor data quality lowers supply chain profit and social welfare. We consider the implementation of blockchain to help and identify the situation in which blockchain helps enhance social welfare but brings harm to supply chain profitability. We propose a government sponsor scheme as well as an environment taxation waiving scheme to help. We further extend the study to the centralized supply chain setting.}
}
@article{FUMEO2015437,
title = {Condition Based Maintenance in Railway Transportation Systems Based on Big Data Streaming Analysis},
journal = {Procedia Computer Science},
volume = {53},
pages = {437-446},
year = {2015},
note = {INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.321},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915018244},
author = {Emanuele Fumeo and Luca Oneto and Davide Anguita},
keywords = {Big Data Streams, Data Analytics, Condition Based Maintenance, Intelligent Transporta- tion Systems, Online Learning, Model Selection},
abstract = {Streaming Data Analysis (SDA) of Big Data Streams (BDS) for Condition Based Maintenance (CBM) in the context of Rail Transportation Systems (RTS) is a state-of-the-art field of re- search. SDA of BDS is the problem of analyzing, modeling and extracting information from huge amounts of data that continuously come from several sources in real time through com- putational aware solutions. Among others, CBM for Rail Transportation is one of the most challenging SDA problems, consisting of the implementation of a predictive maintenance system for evaluating the future status of the monitored assets in order to reduce risks related to failures and to avoid service disruptions. The challenge is to collect and analyze all the data streams that come from the numerous on-board sensors monitoring the assets. This paper deals with the problem of CBM applied to the condition monitoring and predictive maintenance of train axle bearings based on sensors data collection, with the purpose of maximizing their Remaining Useful Life (RUL). In particular we propose a novel algorithm for CBM based on SDA that takes advantage of the Online Support Vector Regression (OL-SVR) for predicting the RUL. The novelty of our proposal is the heuristic approach for optimizing the trade-off between the accuracy of the OL-SVR models and the computational time and resources needed in order to build them. Results from tests on a real-world dataset show the actual benefits brought by the proposed methodology.}
}
@article{BIBRI2017449,
title = {ICT of the new wave of computing for sustainable urban forms: Their big data and context-aware augmented typologies and design concepts},
journal = {Sustainable Cities and Society},
volume = {32},
pages = {449-474},
year = {2017},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S2210670716302475},
author = {Simon Elias Bibri and John Krogstie},
keywords = {Sustainable urban forms, Smart sustainable cities, Big data analytics, Context-aware computing, Typologies and design concepts, Technologies and applications, ICT of the new wave of computing},
abstract = {Undoubtedly, sustainable development has inspired a generation of scholars and practitioners in different disciplines into a quest for the immense opportunities created by the development of sustainable urban forms for human settlements that will enable built environments to function in a more constructive and efficient way. However, there are still significant challenges that need to be addressed and overcome. The issue of such forms has been problematic and difficult to deal with, particularly in relation to the evaluation and improvement of their contribution to the goals of sustainable development. As it is an urban world where the informational and physical landscapes are increasingly being merged, sustainable urban forms need to embrace and leverage what current and future ICT has to offer as innovative solutions and sophisticated methods so as to thrive—i.e. advance their contribution to sustainability. The need for ICT of the new wave of computing to be embedded in such forms is underpinned by the recognition that urban sustainability applications are deemed of high relevance to the contemporary research agenda of computing and ICT. To unlock and exploit the underlying potential, the field of sustainable urban planning is required to extend its boundaries and broaden its horizons beyond the ambit of the built form of cities to include technological innovation opportunities. This paper explores and substantiates the real potential of ICT of the new wave of computing to evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. This entails merging big data and context-aware technologies and their applications with the typologies and design concepts of sustainable urban forms to achieve multiple hitherto unrealized goals. In doing so, this paper identifies models of smart sustainable city and their technologies and applications and models of sustainable urban form and their design concepts and typologies. In addition, it addresses the question of how these technologies and applications can be amalgamated with these design concepts and typologies in ways that ultimately evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. The overall aim of this paper suits a mix of three methodologies: literature review, thematic analysis, and secondary (qualitative) data analysis to achieve different but related objectives. The study identifies four technologies and two classes of applications pertaining to models of smart sustainable city as well as three design concepts and four typologies related to models of sustainable urban form. Finally, this paper proposes a Matrix to help scholars and planners in understanding and analyzing how and to what extent the contribution of sustainable urban forms to sustainability can be improved through ICT of the new wave of computing as to the underlying novel technologies and their applications, as well as a data-centric approach into investigating and evaluating this contribution and a simulation method for strategically optimizing it.}
}
@article{SINGH2018652,
title = {Real world big data for clinical research and drug development},
journal = {Drug Discovery Today},
volume = {23},
number = {3},
pages = {652-660},
year = {2018},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S1359644617305950},
author = {Gurparkash Singh and Duane Schulthess and Nigel Hughes and Bart Vannieuwenhuyse and Dipak Kalra},
abstract = {The objective of this paper is to identify the extent to which real world data (RWD) is being utilized, or could be utilized, at scale in drug development. Through screening peer-reviewed literature, we have cited specific examples where RWD can be used for biomarker discovery or validation, gaining a new understanding of a disease or disease associations, discovering new markers for patient stratification and targeted therapies, new markers for identifying persons with a disease, and pharmacovigilance. None of the papers meeting our criteria was specifically geared toward novel targets or indications in the biopharmaceutical sector; the majority were focused on the area of public health, often sponsored by universities, insurance providers or in combination with public health bodies such as national insurers. The field is still in an early phase of practical application, and is being harnessed broadly where it serves the most direct need in public health applications in early, rare and novel disease incidents. However, these exemplars provide a valuable contribution to insights on the use of RWD to create novel, faster and less invasive approaches to advance disease understanding and biomarker discovery. We believe that pharma needs to invest in making better use of Electronic Health Records and the need for more precompetitive collaboration to grow the scale of this ‘big denominator’ capability, especially given the needs of precision medicine research.}
}
@incollection{LOSHIN2013105,
title = {Chapter 11 - Developing the Big Data Roadmap},
editor = {David Loshin},
booktitle = {Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {105-120},
year = {2013},
isbn = {978-0-12-417319-4},
doi = {https://doi.org/10.1016/B978-0-12-417319-4.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000119},
author = {David Loshin},
keywords = {Need for big data, organizational buy-in, team building, big data evangelist, application architect, data integration, platform architect, data scientist, proof of concept, big data pilot, technology evaluation, technology selection, data management, appliance, application development, YARN, MapReduce, SDLC, training, project scoping, platform scoping, problem data size, computational complexity, storage configuration, integration plan, maintenance, management, assessment},
abstract = {This final chapter reviews best practices for incrementally adopting big data into the enterprise. The chapter revisits assessing the need and value of big data, organizational buy-in, building the big data team, scoping and piloting a proof of concept, technology evaluation and selection, application development, testing, and implementation, platform and project scoping, the big data integration plan, management and maintenance, assessment of success criteria, and overall summary and considerations.}
}
@incollection{TONG2020107,
title = {Chapter 5 - Machine learning for spatiotemporal big data in air pollution},
editor = {Lixin Li and Xiaolu Zhou and Weitian Tong},
booktitle = {Spatiotemporal Analysis of Air Pollution and Its Application in Public Health},
publisher = {Elsevier},
pages = {107-134},
year = {2020},
isbn = {978-0-12-815822-7},
doi = {https://doi.org/10.1016/B978-0-12-815822-7.00005-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128158227000054},
author = {Weitian Tong},
keywords = {Air pollution, Fine particulate matter, Spatiotemporal interpolation, Machine learning, Deep learning},
abstract = {An accurate understanding of air pollutants in a continuous space-time domain is critical for meaningful assessment of the quantitative relationship between the adverse health effects and the concentrations of air pollutants. Traditional interpolation methods, including various statistic and nonstatistic regression models, typically involve restrictive assumptions regarding independence of observations and distributions of outcomes. Moreover, a set of relationships among variables need to be defined strictly in advance. Machine learning opens a new door to understand the air pollution data based on the exposing data-driven relationships and predicting outcomes without empirical models. In this chapter, the state-of-the-art machine learning methods will be introduced to unlock the full potential of the air pollutant data, that is, to estimate the PM2.5 concentration more accurately in the spatiotemporal domain. The methods can be extended to the other air pollutants.}
}
@article{MAYO2016260,
title = {The big data effort in radiation oncology: Data mining or data farming?},
journal = {Advances in Radiation Oncology},
volume = {1},
number = {4},
pages = {260-271},
year = {2016},
issn = {2452-1094},
doi = {https://doi.org/10.1016/j.adro.2016.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2452109416300550},
author = {Charles S. Mayo and Marc L. Kessler and Avraham Eisbruch and Grant Weyburne and Mary Feng and James A. Hayman and Shruti Jolly and Issam {El Naqa} and Jean M. Moran and Martha M. Matuszak and Carlos J. Anderson and Lynn P. Holevinski and Daniel L. McShan and Sue M. Merkel and Sherry L. Machnak and Theodore S. Lawrence and Randall K. {Ten Haken}},
abstract = {Although large volumes of information are entered into our electronic health care records, radiation oncology information systems and treatment planning systems on a daily basis, the goal of extracting and using this big data has been slow to emerge. Development of strategies to meet this goal is aided by examining issues with a data farming instead of a data mining conceptualization. Using this model, a vision of key data elements, clinical process changes, technology issues and solutions, and role for professional societies is presented. With a better view of technology, process and standardization factors, definition and prioritization of efforts can be more effectively directed.}
}
@article{RAMOS20151031,
title = {Primary Education Evaluation in Brazil Using Big Data and Cluster Analysis},
journal = {Procedia Computer Science},
volume = {55},
pages = {1031-1039},
year = {2015},
note = {3rd International Conference on Information Technology and Quantitative Management, ITQM 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915015367},
author = {Thiago Graca Ramos and Jean Cristian Ferreira Machado and Bruna Principe Vieira Cordeiro},
keywords = {Big Data, Data Warehouse, Cluster, Education, IDEB},
abstract = {This study aims to understand the assessment of basic education in the perspective of the State Reviewer as a mechanism that generates information regarding the positivity and weaknesses of a school or an educational system to provide improvements. For this reason, a Data Warehouse was created and later some analysis of the indicators were performed through clustering.}
}
@incollection{TALBURT2015161,
title = {Chapter 10 - CSRUD for Big Data},
editor = {John R. Talburt and Yinle Zhou},
booktitle = {Entity Information Life Cycle for Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {161-190},
year = {2015},
isbn = {978-0-12-800537-8},
doi = {https://doi.org/10.1016/B978-0-12-800537-8.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128005378000107},
author = {John R. Talburt and Yinle Zhou},
keywords = {Big Data, Hadoop Map/Reduce, Transitive Closure, Graph Component},
abstract = {This chapter describes how a distributed processing environment such as Hadoop Map/Reduce can be used to support the CSRUD Life Cycle for Big Data. The examples shown in this chapter use the match key blocking described in Chapter 9 as a data partitioning strategy to perform ER on large datasets. The chapter includes an algorithm for finding the transitive closure of multiple match keys in a distributed processing environment using an iterative algorithm that minimizes the amount of local memory required for each processor. It also outlines a structure for an identity knowledge base in a distributed key-value data store, and describes strategies and distributed processing workflows for capture and update phases of the CSRUD life cycle using both record-based and attribute-based cluster-to-cluster structure projections.}
}
@article{CORIZZO201918,
title = {Anomaly Detection and Repair for Accurate Predictions in Geo-distributed Big Data},
journal = {Big Data Research},
volume = {16},
pages = {18-35},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214579618302119},
author = {Roberto Corizzo and Michelangelo Ceci and Nathalie Japkowicz},
keywords = {Anomaly detection, Data repair, Geo-distributed big data, Spatial autocorrelation, Neural networks, Gradient-boosting},
abstract = {The increasing presence of geo-distributed sensor networks implies the generation of huge volumes of data from multiple geographical locations at an increasing rate. This raises important issues which become more challenging when the final goal is that of the analysis of the data for forecasting purposes or, more generally, for predictive tasks. This paper proposes a framework which supports predictive modeling tasks from streaming data coming from multiple geo-referenced sensors. In particular, we propose a distance-based anomaly detection strategy which considers objects described by embedding features learned via a stacked auto-encoder. We then devise a repair strategy which repairs the data detected as anomalous exploiting non-anomalous data measured by sensors in nearby spatial locations. Subsequently, we adopt Gradient Boosted Trees (GBTs) to predict/forecast values assumed by a target variable of interest for the repaired newly arriving (unlabeled) data, using the original feature representation or the embedding feature representation learned via the stacked auto-encoder. The workflow is implemented with distributed Apache Spark programming primitives and tested on a cluster environment. We perform experiments to assess the performance of each module, separately and in a combined manner, considering the predictive modeling of one-day-ahead energy production, for multiple renewable energy sites. Accuracy results show that the proposed framework allows reducing the error up to 13.56%. Moreover, scalability results demonstrate the efficiency of the proposed framework in terms of speedup, scaleup and execution time under a stress test.}
}
@incollection{SAMPSON2015229,
title = {Chapter 15 - The Legal Challenges of Big Data Application in Law Enforcement},
editor = {Babak Akhgar and Gregory B. Saathoff and Hamid R. Arabnia and Richard Hill and Andrew Staniforth and Petra Saskia Bayerl},
booktitle = {Application of Big Data for National Security},
publisher = {Butterworth-Heinemann},
pages = {229-237},
year = {2015},
isbn = {978-0-12-801967-2},
doi = {https://doi.org/10.1016/B978-0-12-801967-2.00015-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012801967200015X},
author = {Fraser Sampson},
keywords = {Dilemma, Human rights, Jurisdiction, Law enforcement, Privacy, Purpose limitation},
abstract = {This chapter considers the specific issues that Big Data presents for law enforcement agencies (LEAs). In particular, it looks at the dilemmas created for LEAs seeking to use the advantages Big Data gives them while remaining compliant with the developing legal framework governing privacy and the protection of personal data, and how those very advantages can present challenges in law enforcement.}
}
@incollection{SAHOO2019227,
title = {Chapter 9 - Intelligence-Based Health Recommendation System Using Big Data Analytics},
editor = {Nilanjan Dey and Himansu Das and Bighnaraj Naik and Himansu Sekhar Behera},
booktitle = {Big Data Analytics for Intelligent Healthcare Management},
publisher = {Academic Press},
pages = {227-246},
year = {2019},
series = {Advances in ubiquitous sensing applications for healthcare},
isbn = {978-0-12-818146-1},
doi = {https://doi.org/10.1016/B978-0-12-818146-1.00009-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818146100009X},
author = {Abhaya Kumar Sahoo and Sitikantha Mallik and Chittaranjan Pradhan and Bhabani Shankar Prasad Mishra and Rabindra Kumar Barik and Himansu Das},
keywords = {Big data analytics, Classification, Healthcare, Privacy preservation, Recommendation system},
abstract = {In today's digital world, healthcare is one of the core areas in the medical domain. A healthcare system is required to analyze a large amount of patient data, which helps to derive insights and predictions of disease. This system should be intelligent and able to predict the patient's health condition by analyzing the patient's lifestyle, physical health records, and social activities. The health recommendation system (HRS) is becoming an important platform for healthcare services. In this context, health intelligent systems have become indispensable tools in decision-making processes in the healthcare sector. The main objective is to ensure the availability of valuable information at the right time by ensuring information quality, trustworthiness, authentication, and privacy. As people use social networks to learn about their health condition, so the HRS is very important to derive outcomes such as recommending diagnosis, health insurance, clinical pathway-based treatment methods, and alternative medicines based on the patient's health profile. In this chapter, we discuss recent research that targeted utilization of large volumes of medical data while combining multimodal data from disparate sources, which reduces the workload and cost in healthcare. In the healthcare sector, big data analytics using a recommendation system has an important role in terms of decision-making processes regarding the patient's health. This chapter presents a proposed intelligent HRS that provides an insight into how to use big data analytics for implementing an effective health recommendation engine and shows how to transform the healthcare industry from the traditional scenario to more personalized paradigm in a tele-health environment. Our proposed intelligent HRS resulted in lower MAE value when compared to existing approaches.}
}
@article{YE201965,
title = {A hybrid IT framework for identifying high-quality physicians using big data analytics},
journal = {International Journal of Information Management},
volume = {47},
pages = {65-75},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S026840121830834X},
author = {Yan Ye and Yang Zhao and Jennifer Shang and Liyi Zhang},
keywords = {Online healthcare communities, Physician identifying, Signaling theory, Machine learning, Topic modeling, Multi-criterion analysis},
abstract = {Patients face difficulties identifying appropriate doctors owing to the sizeable quantity and uneven quality of information in online healthcare communities. In studying physician searches, researchers often focus on expertise similarity matches and sentiment analyses of reviews. However, the quality is often ignored. To address patients' information needs holistically, we propose a four-dimensional IT framework based on signaling theory. The model takes expertise knowledge, online reviews, profile descriptions (e.g., hospital reputation, number of patients, city) and service quality (e.g., response speed, interaction frequency, cost) as signals that distinguish high-quality physicians. It uses machine learning approaches to derive similarity matches and sentiment analysis. It also measures the relative importance of the signals by multi-criterion analysis and derives the physician rankings through the aggregated scores. Our study revealed that the proposed approach performs better compared with the other two recommend techniques. This research expands the boundary of signaling theory to healthcare management and enriches the literature on IT use and inter-organizational systems. The proposed IT model may improve patient care, alleviate the physician-patient relationship and reduce lawsuits against hospitals; it also has practical implications for healthcare management.}
}
@incollection{KRISHNAN2013257,
title = {Chapter 14 - Implementing the Big Data – Data Warehouse – Real-Life Situations},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {257-265},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000143},
author = {Krish Krishnan},
keywords = {Hadoop, RDBMS, NoSQL, transformation, architecture},
abstract = {This chapter discusses the real-life implementation of the next-generation platform by three different companies and the direction they each have chosen from a technology and architecture perspective.}
}
@article{ALGHAMDI2021462,
title = {Data quality-aware task offloading in Mobile Edge Computing: An Optimal Stopping Theory approach},
journal = {Future Generation Computer Systems},
volume = {117},
pages = {462-479},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2033079X},
author = {Ibrahim Alghamdi and Christos Anagnostopoulos and Dimitrios P. Pezaros},
keywords = {Mobile edge computing, Tasks offloading, Data quality, Optimal stopping theory, Sequential decision making},
abstract = {An important use case of the Mobile Edge Computing (MEC) paradigm is task and data offloading. Computational offloading is beneficial for a wide variety of mobile applications on different platforms including autonomous vehicles and smartphones. With the envision deployment of MEC servers along the roads and while mobile nodes are moving and having certain tasks (or data) to be offloaded to edge servers, choosing an appropriate time and an ideally suited MEC server to guarantee the Quality of Service (QoS) is challenging. We tackle the data quality-aware offloading sequential decision making problem by adopting the principles of Optimal Stopping Theory (OST) to minimize the expected processing time. A variety of OST stochastic models and their applications to the offloading decision making problem are investigated and assessed. A performance evaluation is provided using simulation approach and real world data sets together with the assessment of baseline deterministic and stochastic offloading models. The results show that the proposed OST models can significantly minimize the expected processing time for analytics task execution and can be implemented in the mobile nodes efficiently.}
}
@article{SU201722,
title = {A geo-big data approach to intra-urban food deserts: Transit-varying accessibility, social inequalities, and implications for urban planning},
journal = {Habitat International},
volume = {64},
pages = {22-40},
year = {2017},
issn = {0197-3975},
doi = {https://doi.org/10.1016/j.habitatint.2017.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0197397517300498},
author = {Shiliang Su and Zekun Li and Mengya Xu and Zhongliang Cai and Min Weng},
keywords = {Food geography, Healthy food access, Accessibility, Social inequalities, Transport mode, Multilevel regression},
abstract = {Urban studies attempt to identify the geographic areas with restricted access to healthy and affordable foods (defined as food deserts in the literature). While prior publications have reported the socioeconomic disparities in healthy food accessibility, little evidence has been released from developing countries, especially in China. This paper proposes a geo-big data approach to measuring transit-varying healthy food accessibility and applies it to identify the food deserts within Shenzhen, China. In particular, we develop a crawling tool to harvest the daily travel time from each community (8117) to each healthy food store (102) from the Baidu Map under four transport modes (walking, public transit, private car, and bicycle) during 17:30–20:30 in June 2016. Based on the travel time calculations, we develop four travel time indicators to measure the healthy food accessibility: the minimum, the maximum, the weighted average, and the standard deviation. Results show that the four accessibility indicators generate different estimations and the nearest service (minimum time) alone fails to reflect the multidimensional nature of healthy food accessibility. The communities within Shenzhen present quite different typology with respect to healthy food accessibility under different transport modes. Multilevel additive regression is further applied to examine the associations between healthy food accessibility and nested socioeconomic characteristics at two geographic levels (community and district). We discover that the associations are divergent with transport modes and with geographic levels. More specifically, significant social equalities in healthy food accessibility are identified via walking, public transit, and bicycle in Shenzhen. Based on the associations, we finally map the food deserts and propose corresponding planning strategies. The methods demonstrated in this study should offer deeper spatial insights into intra-urban foodscapes and provide more nuanced understanding of food deserts in urban settings of developing countries.}
}
@incollection{REEVE2013141,
title = {Chapter 21 - Big Data Integration},
editor = {April Reeve},
booktitle = {Managing Data in Motion},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {141-156},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-397167-8},
doi = {https://doi.org/10.1016/B978-0-12-397167-8.00021-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123971678000212},
author = {April Reeve}
}
@article{JESSE2016275,
title = {Internet of Things and Big Data – The Disruption of the Value Chain and the Rise of New Software Ecosystems},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {29},
pages = {275-282},
year = {2016},
note = {17th IFAC Conference on International Stability, Technology and Culture TECIS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.11.079},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316325174},
author = {Norbert Jesse},
keywords = {Internet of Things, Smart Factories, Big Data, Software Platforms, Data Science},
abstract = {Abstract:
IoT connects devices, humans, places, and even abstract items like events. Driven by smart sensors, powerful embedded microelectronics, high-speed connectivity and the standards of the internet, IoT is on the brink of disrupting today's value chains. Big Data, characterized by high volume, high velocity and a high variety of formats, is a result of and also a driving force for IoT. The datafication of business presents completely new opportunities and risks. To hedge the technical risks posed by the interaction between “everything”, IoT requires comprehensive modelling tools. Furthermore, new IT platforms and architectures are necessary to process and store the unprecedented flow of structured and unstructured, repetitive and non-repetitive data in real-time. In the end, only powerful analytics tools are able to extract “sense” from the exponentially growing amount of data and, as a consequence, data science becomes a strategic asset. The era of IoT relies heavily on standards for technologies which guarantee the interoperability of everything. This paper outlines some fundamental standardization activities. Big Data approaches for real-time processing are outlined and tools for analytics are addressed. As consequence, IoT is a (fast) evolutionary process whose success in penetrating all dimensions of life heavily depends on close cooperation between standardization organizations, open source communities and IT experts.}
}
@article{ANDREASEN201926,
title = {Term Structure Analysis with Big Data: One-Step Estimation Using Bond Prices},
journal = {Journal of Econometrics},
volume = {212},
number = {1},
pages = {26-46},
year = {2019},
note = {Big Data in Dynamic Predictive Econometric Modeling},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2019.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0304407619300740},
author = {Martin M. Andreasen and Jens H.E. Christensen and Glenn D. Rudebusch},
keywords = {Extended Kalman filter, Fixed-coupon bond prices, Arbitrage-free Nelson–Siegel model},
abstract = {Nearly all studies that analyze the term structure of interest rates take a two-step approach. First, actual bond prices are summarized by interpolated synthetic zero-coupon yields, and second, some of these yields are used as the source data for further empirical examination. In contrast, we consider the advantages of a one-step approach that directly analyzes the universe of bond prices. To illustrate the feasibility and desirability of the one-step approach, we compare arbitrage-free dynamic term structure models estimated using both approaches. We also provide a simulation study showing that a one-step approach can extract the information in large panels of bond prices and avoid any arbitrary noise introduced from a first-stage interpolation of yields.}
}
@incollection{BROWN2018277,
title = {Chapter Five - Big Data in Drug Discovery},
editor = {David R. Witty and Brian Cox},
series = {Progress in Medicinal Chemistry},
publisher = {Elsevier},
volume = {57},
pages = {277-356},
year = {2018},
issn = {0079-6468},
doi = {https://doi.org/10.1016/bs.pmch.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0079646817300243},
author = {Nathan Brown and Jean Cambruzzi and Peter J. Cox and Mark Davies and James Dunbar and Dean Plumbley and Matthew A. Sellwood and Aaron Sim and Bryn I. Williams-Jones and Magdalena Zwierzyna and David W. Sheppard},
keywords = {Big Data, Artificial intelligence, Drug discovery, Biology, Chemistry, Clinical trials},
abstract = {Interpretation of Big Data in the drug discovery community should enhance project timelines and reduce clinical attrition through improved early decision making. The issues we encounter start with the sheer volume of data and how we first ingest it before building an infrastructure to house it to make use of the data in an efficient and productive way. There are many problems associated with the data itself including general reproducibility, but often, it is the context surrounding an experiment that is critical to success. Help, in the form of artificial intelligence (AI), is required to understand and translate the context. On the back of natural language processing pipelines, AI is also used to prospectively generate new hypotheses by linking data together. We explain Big Data from the context of biology, chemistry and clinical trials, showcasing some of the impressive public domain sources and initiatives now available for interrogation.}
}
@incollection{DHAESE2018137,
title = {Chapter 13 - Big Data and Deep Brain Stimulation},
editor = {Elliot S. Krames and P. Hunter Peckham and Ali R. Rezai},
booktitle = {Neuromodulation (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {137-145},
year = {2018},
isbn = {978-0-12-805353-9},
doi = {https://doi.org/10.1016/B978-0-12-805353-9.00013-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053539000139},
author = {Pierre-Francois D’Haese and Peter E. Konrad and Benoit M. Dawant},
keywords = {Atlas, Big data, Collaborative, CranialCloud, DBS, Normalization},
abstract = {Surgeons, neurologists, researchers, and patients have lacked the technology-based tools to facilitate sharing the tremendously valuable data about patients’ treatment and research in regard to what is working and what is not. Today, only 9% of patients who could benefit from complex therapies to address neurologic conditions actually receive them, and the medical information for each patient who does is hidden away in disconnected databases. To optimize and accelerate our understanding of the brain, we need to gather intelligence around every case, every research subject, every study while connecting that information through a unified, Health Insurance Portability and Accountability Act of 1996 (HIPAA)-compliant network that leverages technology and harnesses the Internet to drive advancements and better connect patients to their care teams. In this chapter, we highlight the key aspects needed to fulfill the requirements of a robust, HIPAA-compliant archive for brain data and highlight the impact of normalization on the accuracy of statistical analyses.}
}
@article{SOUIBGUI2019676,
title = {Data quality in ETL process: A preliminary study},
journal = {Procedia Computer Science},
volume = {159},
pages = {676-687},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.223},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314097},
author = {Manel Souibgui and Faten Atigui and Saloua Zammali and Samira Cherfi and Sadok Ben Yahia},
keywords = {Business Intelligence & Analytics, ETL quality, Data, process quality, Talend Data Integration, Talend Data Quality},
abstract = {The accuracy and relevance of Business Intelligence & Analytics (BI&A) rely on the ability to bring high data quality to the data warehouse from both internal and external sources using the ETL process. The latter is complex and time-consuming as it manages data with heterogeneous content and diverse quality problems. Ensuring data quality requires tracking quality defects along the ETL process. In this paper, we present the main ETL quality characteristics. We provide an overview of the existing ETL process data quality approaches. We also present a comparative study of some commercial ETL tools to show how much these tools consider data quality dimensions. To illustrate our study, we carry out experiments using an ETL dedicated solution (Talend Data Integration) and a data quality dedicated solution (Talend Data Quality). Based on our study, we identify and discuss quality challenges to be addressed in our future research.}
}
@article{MATHEW201585,
title = {Big-data for building energy performance: Lessons from assembling a very large national database of building energy use},
journal = {Applied Energy},
volume = {140},
pages = {85-93},
year = {2015},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2014.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S0306261914012112},
author = {Paul A. Mathew and Laurel N. Dunn and Michael D. Sohn and Andrea Mercado and Claudine Custudio and Travis Walter},
keywords = {Buildings Performance Database, Building performance, Big data, Building data collection, Data-driven decision support},
abstract = {Building energy data has been used for decades to understand energy flows in buildings and plan for future energy demand. Recent market, technology and policy drivers have resulted in widespread data collection by stakeholders across the buildings industry. Consolidation of independently collected and maintained datasets presents a cost-effective opportunity to build a database of unprecedented size. Applications of the data include peer group analysis to evaluate building performance, and data-driven algorithms that use empirical data to estimate energy savings associated with building retrofits. This paper discusses technical considerations in compiling such a database using the DOE Buildings Performance Database (BPD) as a case study. We gathered data on over 750,000 residential and commercial buildings. We describe the process and challenges of mapping and cleansing data from disparate sources. We analyze the distributions of buildings in the BPD relative to the Commercial Building Energy Consumption Survey (CBECS) and Residential Energy Consumption Survey (RECS), evaluating peer groups of buildings that are well or poorly represented, and discussing how differences in the distributions of the three datasets impact use-cases of the data. Finally, we discuss the usefulness and limitations of the current dataset and the outlook for increasing its size and applications.}
}
@article{RANJAN2017495,
title = {A note on exploration of IoT generated big data using semantics},
journal = {Future Generation Computer Systems},
volume = {76},
pages = {495-498},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.06.032},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17313912},
author = {Rajiv Ranjan and Dhavalkumar Thakker and Armin Haller and Rajkumar Buyya},
abstract = {Welcome to this special issue of the Future Generation Computer Systems (FGCS) journal. The special issue compiles seven technical contributions that significantly advance the state-of-the-art in exploration of Internet of Things (IoT) generated big data using semantic web techniques and technologies.}
}
@article{DABEK2015265,
title = {Leveraging Big Data to Model the Likelihood of Developing Psychological Conditions After a Concussion},
journal = {Procedia Computer Science},
volume = {53},
pages = {265-273},
year = {2015},
note = {INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.303},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915018062},
author = {Filip Dabek and Jesus J. Caban},
keywords = {Big Data, Machine Learning, Concussion, Informatics mild Traumatic Brain Injury},
abstract = {A concussion is an invisible and poorly understood mild traumatic brain injury (mTBI) that can alter the way the brain functions. Patients who have screened positive for mTBI are at an increased risk of depression, post-traumatic stress disorder (PTSD), headaches, sleep disorders, and other neurological and psychological problems. Early detection of psychological conditions such as PTSD following a concussion might improve the overall outcome of a patient and could potentially reduce the cost associated with intense interventions often required when conditions go untreated for a long time. Statistical and predictive models that leverage large-scale clinical repositories and use pre-existing conditions to determine the probability of a patient developing psychological conditions following a concussion have not been widely studied. This paper presents an SVM-based model that has been trained with a longitudinal dataset of over 5.3 million clinical encounters of 89,840 service members that have sustained a concussion. The model has been tested and validated with over 16,045 patients that developed PTSD and it has shown an accuracy of over 85% (AUC of 86.52%) at predicting the condition within the first year following the injury.}
}
@incollection{LEVIN2016317,
title = {Chapter 11 - From Databases to Big Data},
editor = {Elaine Holmes and Jeremy K. Nicholson and Ara W. Darzi and John C. Lindon},
booktitle = {Metabolic Phenotyping in Personalized and Public Healthcare},
publisher = {Academic Press},
address = {Boston},
pages = {317-331},
year = {2016},
isbn = {978-0-12-800344-2},
doi = {https://doi.org/10.1016/B978-0-12-800344-2.00011-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128003442000112},
author = {Nadine Levin and Reza M. Salek and Christoph Steinbeck},
keywords = {Databases, big data, metabolomics, metabolic phenotyping, data exchange, data warehousing},
abstract = {This chapter explains the concept of big data and shows that the analytical data and related meta-data of metabolic phenotyping experiments fall into this category. The various databases that can be used to aid interpretation of such data, comprising general chemical and biochemical data, biochemical pathway specific data, analytical chemistry data to aid metabolite identification, and finally metabolic results, are discussed. The concept of data warehousing is explored in the context of metabolic data sets. Finally, the challenges for successful data storage, data exchange, and data interpretation are discussed.}
}
@article{ENRIQUEZ201714,
title = {Entity reconciliation in big data sources: A systematic mapping study},
journal = {Expert Systems with Applications},
volume = {80},
pages = {14-27},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417301550},
author = {J.G. Enríquez and F.J. Domínguez-Mayo and M.J. Escalona and M. Ross and G. Staples},
keywords = {Systematic mapping study, Entity reconciliation, Heterogeneous databases, Big data},
abstract = {The entity reconciliation (ER) problem aroused much interest as a research topic in today's Big Data era, full of big and open heterogeneous data sources. This problem poses when relevant information on a topic needs to be obtained using methods based on: (i) identifying records that represent the same real world entity, and (ii) identifying those records that are similar but do not correspond to the same real-world entity. ER is an operational intelligence process, whereby organizations can unify different and heterogeneous data sources in order to relate possible matches of non-obvious entities. Besides, the complexity that the heterogeneity of data sources involves, the large number of records and differences among languages, for instance, must be added. This paper describes a Systematic Mapping Study (SMS) of journal articles, conferences and workshops published from 2010 to 2017 to solve the problem described before, first trying to understand the state-of-the-art, and then identifying any gaps in current research. Eleven digital libraries were analyzed following a systematic, semiautomatic and rigorous process that has resulted in 61 primary studies. They represent a great variety of intelligent proposals that aim to solve ER. The conclusion obtained is that most of the research is based on the operational phase as opposed to the design phase, and most studies have been tested on real-world data sources, where a lot of them are heterogeneous, but just a few apply to industry. There is a clear trend in research techniques based on clustering/blocking and graphs, although the level of automation of the proposals is hardly ever mentioned in the research work.}
}
@article{MARKOWETZ2014405,
title = {Psycho-Informatics: Big Data shaping modern psychometrics},
journal = {Medical Hypotheses},
volume = {82},
number = {4},
pages = {405-411},
year = {2014},
issn = {0306-9877},
doi = {https://doi.org/10.1016/j.mehy.2013.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S0306987713005598},
author = {Alexander Markowetz and Konrad Błaszkiewicz and Christian Montag and Christina Switala and Thomas E. Schlaepfer},
abstract = {For the first time in history, it is possible to study human behavior on great scale and in fine detail simultaneously. Online services and ubiquitous computational devices, such as smartphones and modern cars, record our everyday activity. The resulting Big Data offers unprecedented opportunities for tracking and analyzing behavior. This paper hypothesizes the applicability and impact of Big Data technologies in the context of psychometrics both for research and clinical applications. It first outlines the state of the art, including the severe shortcomings with respect to quality and quantity of the resulting data. It then presents a technological vision, comprised of (i) numerous data sources such as mobile devices and sensors, (ii) a central data store, and (iii) an analytical platform, employing techniques from data mining and machine learning. To further illustrate the dramatic benefits of the proposed methodologies, the paper then outlines two current projects, logging and analyzing smartphone usage. One such study attempts to thereby quantify severity of major depression dynamically; the other investigates (mobile) Internet Addiction. Finally, the paper addresses some of the ethical issues inherent to Big Data technologies. In summary, the proposed approach is about to induce the single biggest methodological shift since the beginning of psychology or psychiatry. The resulting range of applications will dramatically shape the daily routines of researches and medical practitioners alike. Indeed, transferring techniques from computer science to psychiatry and psychology is about to establish Psycho-Informatics, an entire research direction of its own.}
}
@article{NIMMAGADDA20171871,
title = {Big Data Guided Design Science Information System (DSIS) Development for Sustainability Management and Accounting},
journal = {Procedia Computer Science},
volume = {112},
pages = {1871-1880},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.233},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917316381},
author = {Shastri L Nimmagadda and Torsten Reiners and and {Gary Burke}},
keywords = {Design Science, Digital Ecosystem, Sustainability, Multidimensional Artefacts, Data Interpretation},
abstract = {Sustainability is a dynamic, complex and composite data relationship among geographically distributed human and environment ecosystems. The ecosystems may have strong interactions among their elements and processes, but with dynamic implicit boundaries. Multi-scalable and multidimensional ecosystems have significance based on a commonality of basic structural units and domains. We intend to develop a holistic information system for managing different ecosystems within a sustainability framework/context, using an empirical qualitative and quantitative interpretation and analysis of the measured observations. Design Science Research (DSR) approach is aimed at developing an information system using the volumes of unstructured Big Data observations. Collaborating multiple domains, interpreting and evaluating the commonality, uncovering the connectivity among multiple systems are key aspects of the study. The Design Science Information System (DSIS), evolved from DSR approach is used in solving the ecosystem issues associated with multiple domains, in which the sustainability challenges manifest. In this context, we propose a human-environment-economic ecosystem (HEES) framework consisting of human, environment and economic elements and processes. In broad terms, human, environment and economic domains are conceptualized as different players/agents that operate within a range of sustainability scenarios. This approach recognizes the existing constraints of the systems as well as the emerging knowledge of the boundaries of ecosystems and their connectivity. The connectivity and interaction among the systems are analyzed by data mining, visualization and interpretation artefacts within a sustainability policy framework.}
}
@article{MACKIE2015189,
title = {Big data! Big deal?},
journal = {Public Health},
volume = {129},
number = {3},
pages = {189-190},
year = {2015},
issn = {0033-3506},
doi = {https://doi.org/10.1016/j.puhe.2015.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0033350615000621},
author = {P. Mackie and F. Sim and C. Johnman}
}
@article{DUVIER2018196,
title = {Data quality challenges in the UK social housing sector},
journal = {International Journal of Information Management},
volume = {38},
number = {1},
pages = {196-200},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216308222},
author = {Caroline Duvier and Daniel Neagu and Crina Oltean-Dumbrava and Dave Dickens},
keywords = {Social housing, Data quality},
abstract = {The social housing sector has yet to realise the potential of high data quality. While other businesses, mainly in the private sector, reap the benefits of data quality, the social housing sector seems paralysed, as it is still struggling with recent government regulations and steep revenue reduction. This paper offers a succinct review of relevant literature on data quality and how it relates to social housing. The Housing and Development Board in Singapore offers a great example on how to integrate data quality initiatives in the social housing sector. Taking this example, the research presented in this paper is extrapolating cross-disciplinarily recommendations on how to implement data quality initiatives in social housing providers in the UK.}
}
@article{LIN2014532,
title = {A New Idea of Study on the Influence Factors of Companies’ Debt Costs in the Big Data Era},
journal = {Procedia Computer Science},
volume = {31},
pages = {532-541},
year = {2014},
note = {2nd International Conference on Information Technology and Quantitative Management, ITQM 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.299},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914004761},
author = {Li Lin and Wang Shuang and Liu Yifang and Wang Shouyang},
keywords = {debt cost, big data, quality of accounting information, corporate governance, LASSO method},
abstract = {Under the background of big data era today, once been widely used method – multiple linear regressions can not satisfy people's need to handle big data any more because of its bad characteristics such as multicollinearity, instability, subjectivity in model chosen etc. Contrary to MLR, LASSO method has many good natures. it is stable and can handle multicollinearity and successfully select the best model and do estimation in the same time. LASSO method is an effective improvement of multiple linear regressions. It is a natural change and innovation to introduce LASSO method into the accounting field and use it to deal with the debt costs problems. It helps us join the statistic field and accounting field together step by step. What's more, in order to proof the applicability of LASSO method in dealing with debt costs problems, we take 2301 companies’ data from Shanghai and Shenzhen A-share market in 2012 as samples, and chose 18 indexes to verify that the results of LASSO method is scientific, reasonable and accurate. In the end, we compare LASSO method with traditional multiple linear regressions and ridge regression, finding out that LASSO method can not only offer the most accurate prediction but also simplify the model.}
}
@incollection{HOLLIN201514,
title = {Chapter 2 - Drilling into the Big Data Gold Mine: Data Fusion and High-Performance Analytics for Intelligence Professionals},
editor = {Babak Akhgar and Gregory B. Saathoff and Hamid R. Arabnia and Richard Hill and Andrew Staniforth and Petra Saskia Bayerl},
booktitle = {Application of Big Data for National Security},
publisher = {Butterworth-Heinemann},
pages = {14-20},
year = {2015},
isbn = {978-0-12-801967-2},
doi = {https://doi.org/10.1016/B978-0-12-801967-2.00002-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128019672000021},
author = {Rupert Hollin},
keywords = {Big Data, Fusion, High-performance analytics, Visualization},
abstract = {Threats to local, national, and global public security are continually evolving, and for those tasked with preventing and responding to these threats, the amount of potentially useful data can often seem overwhelming. What compounds this Big Data issue is the fact that we are living in a time of global economic austerity in which national security and law enforcement agencies need to become better at exploiting information while managing the demands of ever-shrinking budgets. This chapter looks at how, by using the latest software tools and techniques for data fusion and high-performance analytics, agencies can automate traditional labor-intensive tasks, gain a holistic view of information that originates from multiple sources, and extract valuable intelligence in a timely and more efficient manner.}
}
@article{WONG201944,
title = {Artificial Intelligence for infectious disease Big Data Analytics},
journal = {Infection, Disease & Health},
volume = {24},
number = {1},
pages = {44-48},
year = {2019},
issn = {2468-0451},
doi = {https://doi.org/10.1016/j.idh.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2468045118301445},
author = {Zoie S.Y. Wong and Jiaqi Zhou and Qingpeng Zhang},
keywords = {Infectious diseases modelling, Emergency response, Artificial Intelligence, Machine learning},
abstract = {Background
Since the beginning of the 21st century, the amount of data obtained from public health surveillance has increased dramatically due to the advancement of information and communications technology and the data collection systems now in place.
Methods
This paper aims to highlight the opportunities gained through the use of Artificial Intelligence (AI) methods to enable reliable disease-oriented monitoring and projection in this information age.
Results and Conclusion
It is foreseeable that together with reliable data management platforms AI methods will enable analysis of massive infectious disease and surveillance data effectively to support government agencies, healthcare service providers, and medical professionals to response to disease in the future.}
}
@incollection{SHEIKH2013185,
title = {Chapter 11 - Big Data, Hadoop, and Cloud Computing},
editor = {Nauman Sheikh},
booktitle = {Implementing Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {185-197},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-401696-5},
doi = {https://doi.org/10.1016/B978-0-12-401696-5.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124016965000116},
author = {Nauman Sheikh},
keywords = {Hadoop, Big Data, cloud computing},
abstract = {When the idea for this book was originally conceived, Big Data and Hadoop were not the most popular themes on the tech circuit, although cloud computing was somewhat more prominent. Some of the reviewer feedback suggested that these topics should be addressed in the context of the conceptual layout of analytics solutions. In this chapter their use in an overall analytics solution will be explained using the previous chapters as a foundation. Big Data, Hadoop, and cloud computing are presented as standalone material, each tying back into the overall analytics solution implementations presented in preceding chapters.}
}
@article{AZEROUAL201850,
title = {Analyzing data quality issues in research information systems via data profiling},
journal = {International Journal of Information Management},
volume = {41},
pages = {50-56},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218300975},
author = {Otmane Azeroual and Gunter Saake and Eike Schallehn},
keywords = {Current research information systems, CRIS, Research information systems, RIS, Research information, Data sources, Data quality, Extraction transformation load, ETL, Data analysis, Data profiling, Science system, Standardization},
abstract = {The success or failure of a RIS in a scientific institution is largely related to the quality of the data available as a basis for the RIS applications. The most beautiful Business Intelligence (BI) tools (reporting, etc.) are worthless when displaying incorrect, incomplete, or inconsistent data. An integral part of every RIS is thus the integration of data from the operative systems. Before starting the integration process (ETL) of a source system, a rich analysis of source data is required. With the support of a data quality check, causes of quality problems can usually be detected. Corresponding analyzes are performed with data profiling to provide a good picture of the state of the data. In this paper, methods of data profiling are presented in order to gain an overview of the quality of the data in the source systems before their integration into the RIS. With the help of data profiling, the scientific institutions can not only evaluate their research information and provide information about their quality, but also examine the dependencies and redundancies between data fields and better correct them within their RIS.}
}
@article{ELKASSAR2019483,
title = {Green innovation and organizational performance: The influence of big data and the moderating role of management commitment and HR practices},
journal = {Technological Forecasting and Social Change},
volume = {144},
pages = {483-498},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517315226},
author = {Abdul-Nasser El-Kassar and Sanjay Kumar Singh},
keywords = {Green innovation, Corporate environmental ethics, Large scale data, Human resource practices, Management commitment, Environmental and economic performance},
abstract = {Faced with internal and external pressure to adapt and implement environmental friendly business activities, it is becoming crucial for firms to identify practices that enhance their competitive advantage, economic, and environmental performance. Green innovation, green technologies, and the implementation of green supply chain management are examples of such practices. Green innovation and the adoption of the combination of green product innovation and green process innovation involve reduction in consumption of energy and pollution emission, recycling of wastes, sustainable utilization of resources, and green product designs. Although the extent research in this area is substantial, research on the importance of considering corporate environmental ethics, stakeholders view of green product, and demand for green products as drivers of green innovation must be conducted. Moreover, the role of large scale data, management commitment, and human resource practices play to overcome the technological challenges, achieve competitive advantage, and enhance the economic and environmental performance have yet to be addressed. This paper develops and tests a holistic model that depicts and examines the relationships among green innovation, its drivers, as well as factors that help overcome the technological challenges and influence the performance and competitive advantage of the firm. This paper is among the first works to deal with such a complex framework which considers the interrelationships among numerous constructs and their effects on competitive advantage as well as overall organizational performance. A questionnaire was designed to measure the influence of green innovation adoption/implementation and its drivers on performance and competitive advantage while taking into consideration the impact of management commitment and HR practices, as well as the use of large data on these relationships. Data collected from a sample of 215 respondents working in Middle East and North Africa (MENA) region and Golf-Cooperation Countries (GCC) were used to test the proposed relationships. The proposed model proved to be fit. The hypotheses were supported, and implications were discussed.}
}
@article{201616,
title = {Clinical research and big data},
journal = {Dental Abstracts},
volume = {61},
number = {1},
pages = {16-17},
year = {2016},
issn = {0011-8486},
doi = {https://doi.org/10.1016/j.denabs.2015.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0011848615010043}
}
@article{MCDERMOTT2015303,
title = {What are the implications of the big data paradigm shift for disability and health?},
journal = {Disability and Health Journal},
volume = {8},
number = {3},
pages = {303-304},
year = {2015},
issn = {1936-6574},
doi = {https://doi.org/10.1016/j.dhjo.2015.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1936657415000515},
author = {Suzanne McDermott and Margaret A. Turk}
}
@article{YU20171,
title = {Data pricing strategy based on data quality},
journal = {Computers & Industrial Engineering},
volume = {112},
pages = {1-10},
year = {2017},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217303509},
author = {Haifei Yu and Mengxiao Zhang},
keywords = {Big data, Data marketplace, Data pricing, Production management, Bi-level programming model},
abstract = {This paper presents a bi-level mathematical programming model for the data-pricing problem that considers both data quality and data versioning strategies. Data products and data-related services differ from information products or services in terms of quality assessment methods. For this problem, we consider two aspects of data quality: (1) its multidimensionality and (2) the interaction between the dimensions. We designed a multi-version data strategy and propose a data-pricing bi-level programming model based on the data quality to maximize the profit by the owner of the data platform and the utility to consumers. A genetic algorithm was used to solve the model. The numerical solutions for the data-pricing model indicate that the multi-version strategy achieves a better market segmentation and is more profitable and feasible when the multiple dimensions of data quality are considered. These results also provide managerial guidance on data provision and data pricing for platform owners.}
}
@article{JEFFREYKUO2018120,
title = {Analyze the energy consumption characteristics and affecting factors of Taiwan's convenience stores-using the big data mining approach},
journal = {Energy and Buildings},
volume = {168},
pages = {120-136},
year = {2018},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2018.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0378778817334345},
author = {Chung-Feng {Jeffrey Kuo} and Chieh-Hung Lin and Ming-Hao Lee},
keywords = {Convenience store, Data mining, Machine learning, Energy consumption characteristics, Energy consumption affecting factor},
abstract = {This study applies big data mining, machine learning analysis technique and uses the Waikato Environment for Knowledge Analysis (WEKA) as a tool to discuss the convenience stores energy consumption performance in Taiwan which consists of (a). Influential factors of architectural space environment and geographical conditions; (b). Influential factors of management type; (c). Influential factors of business equipment; (d). Influential factors of local climatic conditions; (e). Influential factors of service area socioeconomic conditions. The survey data of 1,052 chain convenience stores belong to 7-Eleven, Family Mart and Hi-Life groups by Taiwan Architecture and Building Center (TABC) in 2014. The implicit knowledge will be explored in order to improve the traditional analysis technique which is unlikely to build a model for complex, inexact and uncertain dynamic energy consumption system for convenience stores. The analysis process comprises of (a). Problem definition and objective setting; (b). Data source selection; (c). Data collection; (d). Data preprocessing/preparation; (e). Data attributes selection; (f). Data mining and model construction; (g). Results analysis and evaluation; (h). Knowledge discovery and dissemination. The key factors influencing the convenience stores energy consumption and the influence intensity order can be explored by data attributes selection. The numerical prediction model for energy consumption is built by applying regression analysis and classification techniques. The optimization thresholds of various influential factors are obtained. The different cluster data are compared by using clustering analysis to verify the correlation between the factors influencing the convenience stores energy consumption characteristic. The implicit knowledge of energy consumption characteristic obtained by the aforesaid analysis can be used to (a). Provide the owners with accurate predicted energy consumption performance to optimize architectural space, business equipment and operations management mode; (b). The design planners can obtain the optimum design proposal of Cost Performance Ratio (C/P) by planning the thresholds of various key factors and the validation of prediction model; (c). Provide decision support for government energy and environment departments, to make energy saving and carbon emission reduction policies, in order to estimate and set the energy consumption scenarios of convenience store industry.}
}
@article{SUN20171,
title = {Special Issue on Scalable Computing Systems for Big Data Applications},
journal = {Journal of Parallel and Distributed Computing},
volume = {108},
pages = {1-2},
year = {2017},
note = {Special Issue on Scalable Computing Systems for Big Data Applications},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517301776},
author = {Xian-He Sun and Marc Frincu and Charalampos Chelmis}
}
@article{MARSDEN2018A1,
title = {Numerical data quality in IS research and the implications for replication},
journal = {Decision Support Systems},
volume = {115},
pages = {A1-A7},
year = {2018},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167923618301647},
author = {James R. Marsden and David E. Pingry},
abstract = {We argue that there are major, persistent numerical data quality issues in IS academic research. These issues undermine the ability to replicate our research – a critical element of scientific investigation and analysis. In IS empirical and analytics research articles, the amount of space devoted to the details of data collection, validation, and/or quality pales in comparison to the space devoted to the evaluation and selection of relatively sophisticated model form(s) and estimation technique(s). Yet erudite modeling and estimation can yield no immediate value or be meaningfully replicated without high quality data inputs. The purpose of this paper is: 1) to detail potential quality issues with data types currently used in IS research, and 2) to start a wider and deeper discussion of data quality in IS research. No data type is inherently of low quality and no data type guarantees high quality. As researchers, our empirical research must always address data quality issues and provide the information necessary to determine What, When, Where, How, Who, and Which.}
}
@article{VITOLO2015185,
title = {Web technologies for environmental Big Data},
journal = {Environmental Modelling & Software},
volume = {63},
pages = {185-198},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214002965},
author = {Claudia Vitolo and Yehia Elkhatib and Dominik Reusser and Christopher J.A. Macleod and Wouter Buytaert},
keywords = {Web-based modelling, Big Data, Web services, OGC standards},
abstract = {Recent evolutions in computing science and web technology provide the environmental community with continuously expanding resources for data collection and analysis that pose unprecedented challenges to the design of analysis methods, workflows, and interaction with data sets. In the light of the recent UK Research Council funded Environmental Virtual Observatory pilot project, this paper gives an overview of currently available implementations related to web-based technologies for processing large and heterogeneous datasets and discuss their relevance within the context of environmental data processing, simulation and prediction. We found that, the processing of the simple datasets used in the pilot proved to be relatively straightforward using a combination of R, RPy2, PyWPS and PostgreSQL. However, the use of NoSQL databases and more versatile frameworks such as OGC standard based implementations may provide a wider and more flexible set of features that particularly facilitate working with larger volumes and more heterogeneous data sources.}
}
@article{SCHUH201943,
title = {Data quality program management for digital shadows of products},
journal = {Procedia CIRP},
volume = {86},
pages = {43-48},
year = {2019},
note = {7th CIRP Global Web Conference – Towards shifted production value stream patterns through inference of data, models, and technology (CIRPe 2019)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.01.027},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120300366},
author = {Günther Schuh and Eric Rebentisch and Michael Riesener and Thorben Ipers and Christian Tönnes and Merle-Hendrikje Jank},
keywords = {data quality program, digital shadow, data quality management},
abstract = {Nowadays, companies are facing challenges due to increasingly dynamic market environments, a growing internal and external complexity, as well as globally intensifying competition. To keep pace, companies need to establish extensive knowledge about their business and its surroundings based on insights generated through the analysis of data. The digital shadow is a novel information system concept that integrates data of heterogeneous sources to provide product-related information to stakeholders across the company. The concept aims at improving the results of decision making, enabling advanced data analyses, and increasing information handling efficiency. As insufficient information quality has immediate effects on the utility of the information and induces significant costs, managing the quality of the digital shadow data basis is crucial. However, there are currently no comprehensive methodologies for the assessment and improvement of the data quality of digital shadows. Therefore, this paper introduces a methodology that supports the derivation of data quality projects aimed at optimizing the digital shadow data basis. The proposed methodology comprises four steps: First, digital shadow use cases along the product lifecycle are described. Next, the use cases are prioritized with regard to the expected benefits of applying the digital shadow. Third, quality deficiencies in the digital shadow data basis are assessed with respect to use case specific requirements. Finally, the prioritized use cases in relation with the identified quality deficits allow deriving needs for action, which are addressed by data quality projects. Together, the data quality projects constitute a data quality program. The methodology is applied in an industry case to prove the practical effectivity and efficiency.}
}
@article{EYOB201927,
title = {Ensuring safe surgical care across resource settings via surgical outcomes data & quality improvement initiatives},
journal = {International Journal of Surgery},
volume = {72},
pages = {27-32},
year = {2019},
note = {Endoscopic Surgery},
issn = {1743-9191},
doi = {https://doi.org/10.1016/j.ijsu.2019.07.036},
url = {https://www.sciencedirect.com/science/article/pii/S174391911930189X},
author = {Belain Eyob and Marissa A. Boeck and Patrick FaSiOen and Shamir Cawich and Michael D. Kluger},
keywords = {Surgical outcomes, Developing countries, Caribbean, Safe surgery, Quality improvement, Big data},
abstract = {Staggering statistics regarding the global burden of disease due to lack of surgical care worldwide has been gaining attention in the global health literature over the last 10 years. The Lancet Commission on Global Surgery reported that 16.9 million lives were lost due to an absence of surgical care in 2010, equivalent to 33% of all deaths worldwide. Although data from low- and middle-income countries (LMICs) are limited, recent investigations, such as the African Surgical Outcomes Study, highlight that despite operating on low risk patients, there is increased postoperative mortality in LMICs versus higher-resource settings, a majority of which occur secondary to seemingly preventable complications like surgical site infections. We propose that implementing creative, low-cost surgical outcomes monitoring and select quality improvement systems proven effective in high-income countries, such as surgical infection prevention programs and safety checklists, can enhance the delivery of safe surgical care in existing LMIC surgical systems. While efforts to initiate and expand surgical access and capacity continues to deserve attention in the global health community, here we advocate for creative modifications to current service structures, such as promoting a culture of safety, employing technology and mobile health (mHealth) for patient data collection and follow-up, and harnessing partnerships for information sharing, to create a framework for improving morbidity and mortality in responsible, scalable, and sustainable ways.}
}
@article{GIL201696,
title = {Modeling and Management of Big Data: Challenges and opportunities},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {96-99},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15002514},
author = {David Gil and Il-Yeol Song},
keywords = {Conceptual modeling Big Data, Ecosystem, Integrate & analyze & visualize},
abstract = {The term Big Data denotes huge-volume, complex, rapid growing datasets with numerous, autonomous and independent sources. In these new circumstances Big Data bring many attractive opportunities; however, good opportunities are always followed by challenges, such as modelling, new paradigms, novel architectures that require original approaches to address data complexities. The purpose of this special issue on Modeling and Management of Big Data is to discuss research and experience in modelling and to develop as well as deploy systems and techniques to deal with Big Data. A summary of the selected papers is presented, followed by a conceptual modelling proposal for Big Data. Big Data creates new requirements based on complexities in data capture, data storage, data analysis and data visualization. These concerns are discussed in detail in this study and proposals are recommended for specific areas of future research.}
}
@incollection{KRISHNAN2013241,
title = {Chapter 12 - Information Management and Life Cycle for Big Data},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {241-250},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00012-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012405891000012X},
author = {Krish Krishnan},
keywords = {information life-cycle management, governance, program governance, data governance, data quality},
abstract = {This chapter deals with how to implement information life-cycle management principles to Big Data and create a sustainable process that will ensure that business continuity is not interrupted and data is available on demand.}
}
@incollection{KRISHNAN20133,
title = {Chapter 1 - Introduction to Big Data},
editor = {Krish Krishnan},
booktitle = {Data Warehousing in the Age of Big Data},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {3-14},
year = {2013},
series = {MK Series on Business Intelligence},
isbn = {978-0-12-405891-0},
doi = {https://doi.org/10.1016/B978-0-12-405891-0.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124058910000015},
author = {Krish Krishnan},
keywords = {Big Data, data warehousing, sentiments, social media, machine data},
abstract = {Why this book? Why now? The goal of this book is to provide readers with a concise perspective into the biggest buzz in the industry—Big Data—and, more importantly, its impact on data processing, management, decision support, and data warehousing. At the time of this writing, there is a lot of interest to adopt a Big Data solution, but the profound confusion is what is the future of data warehousing and many investments that have been made over the years into building the decision support platform. This book addresses those areas of concern and provides readers an introduction to the next-generation of data management and data warehousing. This chapter provides you a concise and example driven introduction to what is Big Data, and how any organization needs to understand the value of Big Data.}
}
@article{OBRIEN2015442,
title = {‘Accounting’ for Data Quality in Enterprise Systems},
journal = {Procedia Computer Science},
volume = {64},
pages = {442-449},
year = {2015},
note = {Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.539},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915026745},
author = {Tony O’Brien},
keywords = {Data Quality, Enterprise Systems, Accounting Information Systems, ERP, SCM, CRM, Big Data},
abstract = {Organisations are facing ever more diverse challenges in managing their enterprise systems as emerging technologies bring both added complexities as well as opportunities to the way they conduct their business. Underpinning this ever-increasing volatility is the importance of having quality data to provide information to make those important enterprise-wide decisions. Numerous studies suggest that many organisations are not paying enough attention to their data and that a major cause of this is their failure to measure its quality and value and/or evaluate the costs of having poor data. This study proposes an integrated framework that organisations can adopt as part of their financial and management control processes to provide a mechanism for quantifying data problems, costing potential solutions and monitoring the on-going costs and benefits, to assist them in improving and then sustaining the quality of their data.}
}
@article{DEMIRKAN2013412,
title = {Leveraging the capabilities of service-oriented decision support systems: Putting analytics and big data in cloud},
journal = {Decision Support Systems},
volume = {55},
number = {1},
pages = {412-421},
year = {2013},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2012.05.048},
url = {https://www.sciencedirect.com/science/article/pii/S0167923612001595},
author = {Haluk Demirkan and Dursun Delen},
keywords = {Cloud computing, Service orientation, Service science, Data-as-a-service, Information-as-a-service, Analytics-as-a-service, Big data},
abstract = {Using service-oriented decision support systems (DSS in cloud) is one of the major trends for many organizations in hopes of becoming more agile. In this paper, after defining a list of requirements for service-oriented DSS, we propose a conceptual framework for DSS in cloud, and discus about research directions. A unique contribution of this paper is its perspective on how to servitize the product oriented DSS environment, and demonstrate the opportunities and challenges of engineering service oriented DSS in cloud. When we define data, information and analytics as services, we see that traditional measurement mechanisms, which are mainly time and cost driven, do not work well. Organizations need to consider value of service level and quality in addition to the cost and duration of delivered services. DSS in CLOUD enables scale, scope and speed economies. This article contributes new knowledge in service science by tying the information technology strategy perspectives to the database and design science perspectives for a broader audience.}
}
@article{CHANG201656,
title = {A model to compare cloud and non-cloud storage of Big Data},
journal = {Future Generation Computer Systems},
volume = {57},
pages = {56-76},
year = {2016},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003167},
author = {Victor Chang and Gary Wills},
keywords = {Organizational sustainability modeling (OSM), Comparison between Cloud and non-Cloud storage platforms, Real Cloud case studies, Data analysis and visualization},
abstract = {When comparing Cloud and non-Cloud Storage it can be difficult to ensure that the comparison is fair. In this paper we examine the process of setting up such a comparison and the metric used. Performance comparisons on Cloud and non-Cloud systems, deployed for biomedical scientists, have been conducted to identify improvements of efficiency and performance. Prior to the experiments, network latency, file size and job failures were identified as factors which degrade performance and experiments were conducted to understand their impacts. Organizational Sustainability Modeling (OSM) is used before, during and after the experiments to ensure fair comparisons are achieved. OSM defines the actual and expected execution time, risk control rates and is used to understand key outputs related to both Cloud and non-Cloud experiments. Forty experiments on both Cloud and non-Cloud systems were undertaken with two case studies. The first case study was focused on transferring and backing up 10,000 files of 1 GB each and the second case study was focused on transferring and backing up 1000 files 10 GB each. Results showed that first, the actual and expected execution time on the Cloud was lower than on the non-Cloud system. Second, there was more than 99% consistency between the actual and expected execution time on the Cloud while no comparable consistency was found on the non-Cloud system. Third, the improvement in efficiency was higher on the Cloud than the non-Cloud. OSM is the metric used to analyze the collected data and provided synthesis and insights to the data analysis and visualization of the two case studies.}
}
@article{XIAO2014594,
title = {Knowledge diffusion path analysis of data quality literature: A main path analysis},
journal = {Journal of Informetrics},
volume = {8},
number = {3},
pages = {594-605},
year = {2014},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2014.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1751157714000492},
author = {Yu Xiao and Louis Y.Y. Lu and John S. Liu and Zhili Zhou},
keywords = {Data quality, Main path analysis, Knowledge diffusion, Citation analysis, Social network analysis, Big data},
abstract = {This study presents a unique approach in investigating the knowledge diffusion structure for the field of data quality through an analysis of the main paths. We study a dataset of 1880 papers to explore the knowledge diffusion path, using citation data to build the citation network. The main paths are then investigated and visualized via social network analysis. This paper takes three different main path analyses, namely local, global, and key-route, to depict the knowledge diffusion path and additionally implements the g-index and h-index to evaluate the most important journals and researchers in the data quality domain.}
}
@incollection{LOSHIN201329,
title = {Chapter 4 - Developing a Strategy for Integrating Big Data Analytics into the Enterprise},
editor = {David Loshin},
booktitle = {Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {29-37},
year = {2013},
isbn = {978-0-12-417319-4},
doi = {https://doi.org/10.1016/B978-0-12-417319-4.00004-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000041},
author = {David Loshin},
keywords = {Strategic plan, business requirements, technology adoption, massive scalability, data reuse, data repurposing, oversight, governance, mainstreaming technology, enterprise integration},
abstract = {This chapter expands on the previous one by looking at some key issues that often plague new technology adoption and show that the key issues are not new ones, and that there is likely to be organizational knowledge that can help in fleshing out a reasonable strategic plan. We look at the typical hype cycle, and how its flaws can be mitigated by instituting good practices for defining expectations and continuing to measure performance. We help define the acceptability criteria for evaluating the result of a big data pilot that can be used to make a go/no-go decision. We then pose some thoughts about preparing the organization for massive scalability, data reuse, and the need for oversight and governance. The objective is to provide a pathway for mainstreaming big data into the technology infrastructure that is integrated with the existing investment.}
}
@incollection{CELKO2014119,
title = {Chapter 9 - Big Data and Cloud Computing},
editor = {Joe Celko},
booktitle = {Joe Celko’s Complete Guide to NoSQL},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {119-128},
year = {2014},
isbn = {978-0-12-407192-6},
doi = {https://doi.org/10.1016/B978-0-12-407192-6.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124071926000091},
author = {Joe Celko},
keywords = {Forrester Research, V-list, cloud computing, Big Data, data mining},
abstract = {Big Data is largely a buzzword in IT right now. It was coined by Forrester Research to put a wrapper around existing database mining, data management, and other extensions of existing technology to the current hardware. The goal is to use mixed tools with larger volumes of several different forms of data being brought together under one roof. Along with this approach to data, we are also concerned with cloud computing, which is a public or private Internet network that replaces the tradition hardwired landlines within a company.}
}
@article{DEKHTIAR2018227,
title = {Deep learning for big data applications in CAD and PLM – Research review, opportunities and case study},
journal = {Computers in Industry},
volume = {100},
pages = {227-243},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517305560},
author = {Jonathan Dekhtiar and Alexandre Durupt and Matthieu Bricogne and Benoit Eynard and Harvey Rowson and Dimitris Kiritsis},
keywords = {Deep learning, Machine learning, Computer vision, Product Lifecycle Management, Digital mock-up, Shape retrieval},
abstract = {With the increasing amount of available data, computing power and network speed for a decreasing cost, the manufacturing industry is facing an unprecedented amount of data to process, understand and exploit. Phenomena such as Big Data, the Internet-of-Things, Closed-Loop Product Lifecycle Management, and the advances of Smart Factories tend to produce humanly unmanageable quantities of data. The paper approaches the aforesaid context by assuming that any data processing automation is not only desirable but rather necessary in order to prevent prohibitive data analytics costs. This study focuses on highlighting the major specificities of engineering data and the data-processing difficulties which are inherent to data coming from the manufacturing industry. The artificial intelligence field of research is able to provide methods and tools to address some of the identified issues. A special attention was paid to provide a literature review of the most recent (in 2017) applications, that could present a high potential for the manufacturing industry, in the fields of machine learning and deep learning. In order to illustrate the proposed work, a case study was conducted on the challenging research question of object recognition in heterogeneous formats (3D models, photos and videos) with deep learning techniques. The DICE project – DMU Imagery Comparison Engine – is presented and has been completely open-sourced in order to encourage reuse and improvements of the proposed case-study. This project also leads to the development of an open-source research dataset of 2000 CAD Models, called DMU-Net available at: https://www.dmu-net.org.}
}
@article{FOSSOWAMBA2015234,
title = {How ‘big data’ can make big impact: Findings from a systematic review and a longitudinal case study},
journal = {International Journal of Production Economics},
volume = {165},
pages = {234-246},
year = {2015},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314004253},
author = {Samuel {Fosso Wamba} and Shahriar Akter and Andrew Edwards and Geoffrey Chopin and Denis Gnanzou},
keywords = {‘Big data’, Analytics, Business value, Issues, Case study, Emergency services, Literature review},
abstract = {Big data has the potential to revolutionize the art of management. Despite the high operational and strategic impacts, there is a paucity of empirical research to assess the business value of big data. Drawing on a systematic review and case study findings, this paper presents an interpretive framework that analyzes the definitional perspectives and the applications of big data. The paper also provides a general taxonomy that helps broaden the understanding of big data and its role in capturing business value. The synthesis of the diverse concepts within the literature on big data provides deeper insights into achieving value through big data strategy and implementation.}
}
@article{LAU2019209,
title = {Pitfalls in big data analysis: next-generation technologies, last-generation data},
journal = {Diagnostic Microbiology and Infectious Disease},
volume = {94},
number = {2},
pages = {209-210},
year = {2019},
issn = {0732-8893},
doi = {https://doi.org/10.1016/j.diagmicrobio.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0732889318306710},
author = {Susanna K.P. Lau and Patrick C.Y. Woo}
}
@article{GENENDERFELTHEIMER2018112,
title = {Visualizing High Dimensional and Big Data},
journal = {Procedia Computer Science},
volume = {140},
pages = {112-121},
year = {2018},
note = {Cyber Physical Systems and Deep Learning Chicago, Illinois November 5-7, 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.308},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918319811},
author = {Amy Genender-Feltheimer},
keywords = {Data Visualization, Dimensionality Reduction, Parallel Processing, Hadoop, Machine Learning, Big Data},
abstract = {The amount of data created by people, machines and corporations around the world is growing every year. Thanks to innovations such as the Internet of Things, this trend will continue, giving rise to the creation of Big Data. Data visualization leverages principles of visual psychology to help stakeholders identify patterns, trends and correlations that might go undetected in text-based or spreadsheet data. The return on investment (ROI) of big data visualization is well-documented in numerous studies and use cases. However, to achieve ROI from analytics investments, key insights must be uncovered, understood and communicated. Synthesizing huge quantities of data into key insights grows more challenging as data volumes and varieties increase. To address visualization challenges posed by big and high-dimensional data, this paper explores algorithms and techniques that compress the amount of data and/or reduce the number of attributes to be analyzed and visualized. Specifically, this paper examines applying dimensionality reduction and data compression algorithms to reduce attributes, tuples and data points returned to the visualization. By reducing data returned to the visualization, trends, patterns and correlations are easier to view and visualization tool performance is optimized.}
}
@article{CHENG20181,
title = {Data and knowledge mining with big data towards smart production},
journal = {Journal of Industrial Information Integration},
volume = {9},
pages = {1-13},
year = {2018},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2017.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X17300584},
author = {Ying Cheng and Ken Chen and Hemeng Sun and Yongping Zhang and Fei Tao},
keywords = {Big data, Data mining techniques (DMTs), Production management, Smart manufacturing, Statistical analysis, Knowledge discovery},
abstract = {Driven by the innovative improvement of information and communication technologies (ICTs) and their applications into manufacturing industry, the big data era in manufacturing is correspondingly arising, and the developing data mining techniques (DMTs) pave the way for pursuing the aims of smart production with the real-time, dynamic, self-adaptive and precise control. However, lots of factors in the ever-changing environment of manufacturing industry, such as, various of complex production processes, larger scale and uncertainties, more complicated constrains, coupling of operational performance, and so on, make production management face with more and more big challenges. The dynamic inflow of a large number of raw data which is collected from the physical manufacturing sites or generated in various related information systems, caused the heavy information overload problems. Indeed, most of traditional DMTs are not yet sufficient to process such big data for smart production management. Therefore, this paper reviews the development of DMTs in the big data era, and makes discussion on the applications of DMTs in production management, by selecting and analyzing the relevant papers since 2010. In the meantime, we point out limitations and put forward some suggestions about the smartness and further applications of DMTs used in production management.}
}
@article{PAPANAGNOU2018343,
title = {Coping with demand volatility in retail pharmacies with the aid of big data exploration},
journal = {Computers & Operations Research},
volume = {98},
pages = {343-354},
year = {2018},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2017.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0305054817302162},
author = {Christos I. Papanagnou and Omeiza Matthews-Amune},
keywords = {Retail pharmacy, Data mining, Time series, Forecasting, Big data, Demand uncertainty},
abstract = {Data management tools and analytics have provided managers with the opportunity to contemplate inventory performance as an ongoing activity by no longer examining only data agglomerated from ERP systems, but also, considering internet information derived from customers’ online buying behaviour. The realisation of this complex relationship has increased interest in business intelligence through data and text mining of structured, semi-structured and unstructured data, commonly referred to as “big data” to uncover underlying patterns which might explain customer behaviour and improve the response to demand volatility. This paper explores how sales structured data can be used in conjunction with non-structured customer data to improve inventory management either in terms of forecasting or treating some inventory as “top-selling” based on specific customer tendency to acquire more information through the internet. A medical condition is considered - namely pain - by examining 129 weeks of sales data regarding analgesics and information seeking data by customers through Google, online newspapers and YouTube. In order to facilitate our study we consider a VARX model with non-structured data as exogenous to obtain the best estimation and we perform tests against several univariate models in terms of best fit performance and forecasting.}
}
@article{PHILIPCHEN2014314,
title = {Data-intensive applications, challenges, techniques and technologies: A survey on Big Data},
journal = {Information Sciences},
volume = {275},
pages = {314-347},
year = {2014},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2014.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0020025514000346},
author = {C.L. {Philip Chen} and Chun-Yang Zhang},
keywords = {Big Data, Data-intensive computing, e-Science, Parallel and distributed computing, Cloud computing},
abstract = {It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore’s Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing.}
}
@incollection{CARNICERO2019121,
title = {Chapter 8 - Healthcare Decision-Making Support Based on the Application of Big Data to Electronic Medical Records: A Knowledge Management Cycle},
editor = {Firas Kobeissy and Ali Alawieh and Fadi A. Zaraket and Kevin Wang},
booktitle = {Leveraging Biomedical and Healthcare Data},
publisher = {Academic Press},
pages = {121-131},
year = {2019},
isbn = {978-0-12-809556-0},
doi = {https://doi.org/10.1016/B978-0-12-809556-0.00008-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095560000083},
author = {Javier Carnicero and David Rojas},
keywords = {Big Data, Electronic medical record, Practice-based medicine, Learning health system, Semantic interoperability},
abstract = {Any given health system needs to increase efficiency and effectiveness up to the point of requiring a transformation of their current model to ensure their sustainability and continuity. The electronic medical record (EMR) is the main source of knowledge to improve the quality of healthcare, clinical research, epidemiological surveillance, patient empowerment, personalized medicine, and clinical decision-making support systems. There is also a huge amount of available information related to diseases and other medical conditions, such as drugs and therapies, omics data (genetic and proteomic), social networks, and wearable devices. Big Data technologies allow the processing of this data to reach the final goal, which is a learning health system. The great diversity of data, sources, structures, and uses requires a data linkage procedure to integrate and harmonize these data. This generation of knowledge allows the transition from evidence-based medicine, which still prevails, to practice-based medicine. The key points for any Big Data project based on EMRs and other medical information sources are semantic interoperability, data structure and granularity, information quality, patient privacy, legal framework, and bioethics.}
}
@article{LIN2018220,
title = {An on-demand coverage based self-deployment algorithm for big data perception in mobile sensing networks},
journal = {Future Generation Computer Systems},
volume = {82},
pages = {220-234},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17313262},
author = {Yaguang Lin and Xiaoming Wang and Fei Hao and Liang Wang and Lichen Zhang and Ruonan Zhao},
keywords = {Mobile sensing network, High performance sensing, Big data perception, Node self-deployment, On-demand coverage, Mobile cellular learning automata},
abstract = {Mobile Sensing Networks have been widely applied to many fields for big data perception such as intelligent transportation, medical health and environment sensing. However, in some complex environments and unreachable regions of inconvenience for human, the establishment of the mobile sensing networks, the layout of the nodes and the control of the network topology to achieve high performance sensing of big data are increasingly becoming a main issue in the applications of the mobile sensing networks. To deal with this problem, we propose a novel on-demand coverage based self-deployment algorithm for big data perception based on mobile sensing networks in this paper. Firstly, by considering characteristics of mobile sensing nodes, we extend the cellular automata model and propose a new mobile cellular automata model for effectively characterizing the spatial–temporal evolutionary process of nodes. Secondly, based on the learning automata theory and the historical information of node movement, we further explore a new mobile cellular learning automata model, in which nodes can self-adaptively and intelligently decide the best direction of movement with low energy consumption. Finally, we propose a new optimization algorithm which can quickly solve the node self-adaptive deployment problem, thus, we derive the best deployment scheme of nodes in a short time. The extensive simulation results show that the proposed algorithm in this paper outperforms the existing algorithms by as much as 40% in terms of the degree of satisfaction of network coverage, the iterations of the algorithm, the average moving steps of nodes and the energy consumption of nodes. Hence, we believe that our work will make contributions to large-scale adaptive deployment and high performance sensing scenarios of the mobile sensing networks.}
}
@article{CANO201736,
title = {Perspectives on Big Data applications of health information},
journal = {Current Opinion in Systems Biology},
volume = {3},
pages = {36-42},
year = {2017},
note = {• Mathematical modelling • Mathematical modelling, Dynamics of brain activity at the systems level • Clinical and translational systems biology},
issn = {2452-3100},
doi = {https://doi.org/10.1016/j.coisb.2017.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S2452310017300409},
author = {Isaac Cano and Akos Tenyi and Emili Vela and Felip Miralles and Josep Roca},
keywords = {Digital health, Secondary use of data, Health analytics, Predictive modeling, Health forecasting},
abstract = {Recent advances on prospective monitoring and retrospective analysis of health information at national or regional level are generating high expectations for the application of Big Data technologies that aim to analyze at real time high-volumes and/or complex of data from healthcare delivery (e.g., electronic health records, laboratory and radiology information, electronic prescriptions, etc.) and citizens' lifestyles (e.g., personal health records, personal monitoring devices, social networks, etc.). Along these same lines, advances in the field of genomics are revolutionizing biomedical research, both in terms of data volume and prospects, as well as in terms of the social impact it entails. The potential of Big Data applications that consider all of the above levels of health information lies in the possibility of combining and integrating de-identified health information to allow secondary uses of data. This is the use and re-use of various sources of health information for purposes in addition to the direct clinical care of specific patients or the direct investigation of specific biomedical research hypotheses. Current applications include: epidemiological and pharmacovigilance studies, facilitating recruitment to randomized controlled trials, carrying out audits and benchmarking studies, financial and service planning, and ultimately supporting the generation of novel biomedical research outcomes.}
}
@article{BIVAND201887,
title = {Big data sampling and spatial analysis: “which of the two ladles, of fig-wood or gold, is appropriate to the soup and the pot?”},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {87-91},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300579},
author = {Roger Bivand and Konstantin Krivoruchko},
keywords = {Change of support, Sampling design, Data transformation, Prediction standard error},
abstract = {Following from Krivoruchko and Bivand (2009), we consider some general points related to challenges to the usefulness of big data in spatial statistical applications when data collection is compromised or one or more model assumptions are violated. We look further at the desirability of comparison of new methods intended to handle large spatial and spatio-temporal datasets.}
}
@article{DALLAVALLE201876,
title = {Social media big data integration: A new approach based on calibration},
journal = {Expert Systems with Applications},
volume = {111},
pages = {76-90},
year = {2018},
note = {Big Data Analytics for Business Intelligence},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417308667},
author = {Luciana {Dalla Valle} and Ron Kenett},
keywords = {Bayesian networks, Calibration, Data integration, Social media, Information quality (InfoQ), Resampling techniques},
abstract = {In recent years, the growing availability of huge amounts of information, generated in every sector at high speed and in a wide variety of forms and formats, is unprecedented. The ability to harness big data is an opportunity to obtain more accurate analyses and to improve decision-making in industry, government and many other organizations. However, handling big data may be challenging and proper data integration is a key dimension in achieving high information quality. In this paper, we propose a novel approach to data integration that calibrates online generated big data with interview based customer survey data. A common issue of customer surveys is that responses are often overly positive, making it difficult to identify areas of weaknesses in organizations. On the other hand, online reviews are often overly negative, hampering an accurate evaluation of areas of excellence. The proposed methodology calibrates the levels of unbalanced responses in different data sources via resampling and performs data integration using Bayesian Networks to propagate the new re-balanced information. In this paper we show, with a case study example, how the novel data integration approach allows businesses and organizations to get a bias corrected appraisal of the level of satisfaction of their customers. The application is based on the integration of online data of review blogs and customer satisfaction surveys from the San Francisco airport. We illustrate how this integration enhances the information quality of the data analytic work in four of InfoQ dimensions, namely, Data Structure, Data Integration, Temporal Relevance and Chronology of Data and Goal.}
}
@article{THADURI2015457,
title = {Railway Assets: A Potential Domain for Big Data Analytics},
journal = {Procedia Computer Science},
volume = {53},
pages = {457-467},
year = {2015},
note = {INNS Conference on Big Data 2015 Program San Francisco, CA, USA 8-10 August 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.323},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915018268},
author = {Adithya Thaduri and Diego Galar and Uday Kumar},
keywords = {Big Data, Railways, Maintenance, Transportation},
abstract = {Two concepts currently at the leading edge of todays information technology revolution are Analytics and Big Data. The public transportation industry has been at the forefront in utilizing and implementing Analytics and Big Data, from ridership forecasting to transit operations Rail transit systems have been especially involved with these IT concepts, and tend to be especially amenable to the advantages of Analytics and Big Data because they are generally closed systems that involve sophisticated processing of large volumes of data. The more that public transportation professionals and decision makers understand the role of Analytics and Big Data in their industry in perspective, the more effectively they will be able to utilize its promise. This paper gives an overview of Big Data technologies in context of transportation with specific to Railways. This paper also gives an insight on how the existing data modules from the transport authority combines Big Data and how can be incorporated in providing maintenance decision making.}
}
@article{VETRO2021101619,
title = {A data quality approach to the identification of discrimination risk in automated decision making systems},
journal = {Government Information Quarterly},
volume = {38},
number = {4},
pages = {101619},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101619},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000551},
author = {Antonio Vetrò and Marco Torchiano and Mariachiara Mecati},
keywords = {Automated decision making, Data ethics, Data quality, Data bias, Algorithm fairness, Digital policy, Digital governance},
abstract = {Automated decision-making (ADM) systems may affect multiple aspects of our lives. In particular, they can result in systematic discrimination of specific population groups, in violation of the EU Charter of Fundamental Rights. One of the potential causes of discriminative behavior, i.e., unfairness, lies in the quality of the data used to train such ADM systems. Using a data quality measurement approach combined with risk management, both defined in ISO standards, we focus on balance characteristics and we aim to understand how balance indexes (Gini, Simpson, Shannon, Imbalance Ratio) identify discrimination risk in six large datasets containing the classification output of ADM systems. The best result is achieved using the Imbalance Ratio index. Gini and Shannon indexes tend to assume high values and for this reason they have modest results in both aspects: further experimentation with different thresholds is needed. In terms of policies, the risk-based approach is a core element of the EU approach to regulate algorithmic systems: in this context, balance measures can be easily assumed as risk indicators of propagation – or even amplification – of bias in the input data of ADM systems.}
}
@article{BALA2017114,
title = {A Fine‐Grained Distribution Approach for ETL Processes in Big Data Environments},
journal = {Data & Knowledge Engineering},
volume = {111},
pages = {114-136},
year = {2017},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X16300611},
author = {Mahfoud Bala and Omar Boussaid and Zaia Alimazighi},
keywords = {Data Warehousing, ETL, Parallel and Distributed Processing, Big Data, MapReduce},
abstract = {Among the so-called “4Vs” (volume, velocity, variety, and veracity) that characterize the complexity of Big Data, this paper focuses on the issue of “Volume” in order to ensure good performance for Extracting-Transforming-Loading (ETL) processes. In this study, we propose a new fine-grained parallelization/distribution approach for populating the Data Warehouse (DW). Unlike prior approaches that distribute the ETL only at coarse-grained level of processing, our approach provides different ways of parallelization/distribution both at process, functionality and elementary functions levels. In our approach, an ETL process is described in terms of its core functionalities which can run on a cluster of computers according to the MapReduce (MR) paradigm. The novel approach allows thereby the distribution of the ETL process at three levels: the “process” level for coarse-grained distribution and the “functionality” and “elementary functions” levels for fine-grained distribution. Our performance analysis reveals that employing 25 to 38 parallel tasks enables the novel approach to speed up the ETL process by up to 33% with the improvement rate being linear.}
}
@article{KUM2015127,
title = {Using big data for evidence based governance in child welfare},
journal = {Children and Youth Services Review},
volume = {58},
pages = {127-136},
year = {2015},
issn = {0190-7409},
doi = {https://doi.org/10.1016/j.childyouth.2015.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0190740915300591},
author = {Hye-Chung Kum and C. {Joy Stewart} and Roderick A. Rose and Dean F. Duncan},
keywords = {Big data, Evidence based governance, Knowledge discovery and data mining (KDD), Data science, Population informatics, Policy informatics, Academic government partnership, Administrative data},
abstract = {Numerous approaches are available for improving governance of the child welfare system, all of which require longitudinal data reporting on child welfare clients. A substantial amount of agency administrative information – big data – can be transformed into knowledge for policy and management actions through a rigorous information generation process. Important properties of the information generation process are that it must generate accurate, timely information while protecting the confidentiality of the clients. In addition, it must be extensible to serve an ever-changing policy and technology environment. Knowledge discovery and data mining (KDD), aka data science, is a method developed in the private sector to mine consumer data and can be used in public settings to support evidence based governance. KDD consists of a rigorous 5-step process that includes a Web-based end-user interface. The relationship between KDD and governance is a continuous feedback cycle that enables ongoing development of new information and knowledge as stakeholders identify emerging needs. In this paper, we synthesis the different frameworks for utilizing big data for public governance, introduce the KDD process, describe the nature of big data in child welfare, and then present an updated KDD architecture that can support these frameworks to utilize big data for governance. We also demonstrate the role KDD plays in child welfare management through 2 case studies. We conclude with a discussion on implications for agency–university partnerships and research-to-practice.}
}
@article{PERRONS2015117,
title = {Data as an asset: What the oil and gas sector can learn from other industries about “Big Data”},
journal = {Energy Policy},
volume = {81},
pages = {117-121},
year = {2015},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2015.02.020},
url = {https://www.sciencedirect.com/science/article/pii/S0301421515000932},
author = {Robert K. Perrons and Jesse W. Jensen},
keywords = {Big data, Oil and gas, Information technologies, Data},
abstract = {The upstream oil and gas industry has been contending with massive data sets and monolithic files for many years, but “Big Data” is a relatively new concept that has the potential to significantly re-shape the industry. Despite the impressive amount of value that is being realized by Big Data technologies in other parts of the marketplace, however, much of the data collected within the oil and gas sector tends to be discarded, ignored, or analyzed in a very cursory way. This viewpoint examines existing data management practices in the upstream oil and gas industry, and compares them to practices and philosophies that have emerged in organizations that are leading the way in Big Data. The comparison shows that, in companies that are widely considered to be leaders in Big Data analytics, data is regarded as a valuable asset—but this is usually not true within the oil and gas industry insofar as data is frequently regarded there as descriptive information about a physical asset rather than something that is valuable in and of itself. The paper then discusses how the industry could potentially extract more value from data, and concludes with a series of policy-related questions to this end.}
}
@article{YANG2019755,
title = {How big data enriches maritime research – a critical review of Automatic Identification System (AIS) data applications},
journal = {Transport Reviews},
volume = {39},
number = {6},
pages = {755-773},
year = {2019},
issn = {0144-1647},
doi = {https://doi.org/10.1080/01441647.2019.1649315},
url = {https://www.sciencedirect.com/science/article/pii/S0144164722001568},
author = {Dong Yang and Lingxiao Wu and Shuaian Wang and Haiying Jia and Kevin X. Li},
keywords = {AIS data, data mining, navigation safety, ship behaviour analysis, environmental evaluation, advanced applications of AIS data},
abstract = {ABSTRACT
The information-rich vessel movement data provided by the Automatic Identification System (AIS) has gained much popularity over the past decade, during which the employment of satellite-based receivers has enabled wide coverage and improved data quality. The application of AIS data has developed from simply navigation-oriented research to now include trade flow estimation, emission accounting, and vessel performance monitoring. The AIS now provides high frequency, real-time positioning and sailing patterns for almost the whole world's commercial fleet, and therefore, in combination with supplementary databases and analyses, AIS data has arguably kickstarted the era of digitisation in the shipping industry. In this study, we conduct a comprehensive review of the literature regarding AIS applications by dividing it into three development stages, namely, basic application, extended application, and advanced application. Each stage contains two to three application fields, and in total we identified seven application fields, including (1) AIS data mining, (2) navigation safety, (3) ship behaviour analysis, (4) environmental evaluation, (5) trade analysis, (6) ship and port performance, and (7) Arctic shipping. We found that the original application of AIS data to navigation safety has, with the improvement of data accessibility, evolved into diverse applications in various directions. Moreover, we summarised the major methodologies in the literature into four categories, these being (1) data processing and mining, (2) index measurement, (3) causality analysis, and (4) operational research. Undoubtedly, the applications of AIS data will be further expanded in the foreseeable future. This will not only provide a more comprehensive understanding of voyage performance and allow researchers to examine shipping market dynamics from the micro level, but also the abundance of AIS data may also open up the rather opaque aspect of how shipping companies release information to external authorities, including the International Maritime Organization, port states, scientists and researchers. It is expected that more multi-disciplinary AIS studies will emerge in the coming years. We believe that this study will shed further light on the future development of AIS studies.}
}
@article{SHIN2015311,
title = {Ecological views of big data: Perspectives and issues},
journal = {Telematics and Informatics},
volume = {32},
number = {2},
pages = {311-320},
year = {2015},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2014.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0736585314000665},
author = {Dong-Hee Shin and Min Jae Choi},
keywords = {Big data, Data ecosystem, Ecology, South Korea, Socio-technical perspective, Big data policy, Big data for development},
abstract = {From the viewpoint of big data as a socio-technical phenomenon, this study examines the associated assumptions and biases critically and contextually. The research analyzes the big data phenomenon from a socio-technical systems theory perspective: cultural, technological, and scholarly phenomena that rest on the interplay of technology, analysis, and mythology provoking extensive utopian and dystopian rhetoric. It examines the development of big data by reviewing this theory, identifying key components of the big data ecosystem, and explaining how these components are likely to evolve over time. Despite extensive investment and proactive drive, uncertainty exists concerning the evolution of big data and the impact on the new information milieu. Significant concerns recently addressed are in the areas of privacy, data quality, access, curation, preservation, and use. This study provides insight into these challenges and opportunities through the lens of a socio-technical analysis of big data development, which includes social dynamics, political discourse, and technological choices inherent in the design and development of next-generation ICT ecology. The policy implications of big data are addressed using Korean information initiatives to highlight key considerations as the country progresses in this new ecology era.}
}
@article{MEHMOOD20151107,
title = {Big Data Logistics: A health-care Transport Capacity Sharing Model},
journal = {Procedia Computer Science},
volume = {64},
pages = {1107-1114},
year = {2015},
note = {Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.566},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915027015},
author = {Rashid Mehmood and Gary Graham},
keywords = {future city, Big data, transport operation management, healthcare informationsystems, integrated systems, shared resources},
abstract = {The growth of cities in the 21st century has put more pressure on resources and conditions of urban life. There are several reasons why the health-care industry is the focus of this investigation. For instance, in the UK various studies point to the lack of failure of basic quality control procedures and misalignment between customer needs and provider services and duplication of logistics practices. The development of smart cities and big data present unprecedented challenges and opportunities for operations managers; they need to develop new tools and techniques for network planning and control. Our paper aims to make a contribution to big data and city operations theory by exploring how big data can lead to improvements in transport capacity sharing. We explore using Markov models the integration of big data with future city (health-care) transport sharing. A mathematical model was designed to illustrate how sharing transport load (and capacity) in a smart city can improve efficiencies in meeting demand for city services. The results from our analysis of 13 different sharing/demand scenarios are presented. A key finding is that the probability for system failure and performance variance tends to be highest in a scenario of high demand/zero sharing.}
}
@article{RAIKOV2016147,
title = {Big Data Refining on the Base of Cognitive Modeling},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {32},
pages = {147-152},
year = {2016},
note = {Cyber-Physical & Human-Systems CPHS 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.12.205},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316328774},
author = {Alexander N. Raikov and Z. Avdeeva and A. Ermakov},
keywords = {data refining, cognitive modeling, Big Data, intellectual agents, networked expertise},
abstract = {Abstract:
In conditions of rapid external changes the requirement to quality of control of purposeful development of complex system (states, regions, corporations etc.) dramatically increases. Automation support of the key stages of decision making process is one of the ways to cope with the challenges. This paper focuses on the approach based on the Big Data Refining during cognitive modeling that proves the correctness of modeling and decision-making. The approach uses the requests to the Big Data for cognitive model components verification. The requests are created by intelligent agents with feedback from decision makers. Some practical results confirm the adequacy of the proposed approach.}
}
@incollection{2022vii,
title = {In praise of Meeting the Challenges of Data Quality Management},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {vii-ix},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00016-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821737500016X}
}
@article{HUI2019S90,
title = {PCN181 THE NATIONAL CANCER BIG DATA PLATFORM OF CHINA: VISION AND STATUS},
journal = {Value in Health},
volume = {22},
pages = {S90},
year = {2019},
note = {ISPOR 2019: Rapid. Disruptive. Innovative: A New Era in HEOR},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2019.04.303},
url = {https://www.sciencedirect.com/science/article/pii/S1098301519304954},
author = {Z.G. Hui and Q. Guo and W.Z. Shi and M.C. Gong and C. Liu and H. Xu and H. Li}
}
@article{RICHTER201537,
title = {Medicinal chemistry in the era of big data},
journal = {Drug Discovery Today: Technologies},
volume = {14},
pages = {37-41},
year = {2015},
note = {From Chemistry to Biology Database Curation},
issn = {1740-6749},
doi = {https://doi.org/10.1016/j.ddtec.2015.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1740674915000141},
author = {Lars Richter and Gerhard F. Ecker},
abstract = {In the era of big data medicinal chemists are exposed to an enormous amount of bioactivity data. Numerous public data sources allow for querying across medium to large data sets mostly compiled from literature. However, the data available are still quite incomplete and of mixed quality. This mini review will focus on how medicinal chemists might use such resources and how valuable the current data sources are for guiding drug discovery.}
}
@article{HUANG2018165,
title = {A technology delivery system for characterizing the supply side of technology emergence: Illustrated for Big Data & Analytics},
journal = {Technological Forecasting and Social Change},
volume = {130},
pages = {165-176},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517311927},
author = {Ying Huang and Alan L. Porter and Scott W. Cunningham and Douglas K.R. Robinson and Jianhua Liu and Donghua Zhu},
keywords = {Technology delivery system, Tech mining, Emerging technology, Big Data, Technology assessment, Impact assessment},
abstract = {While there is a general recognition that breakthrough innovation is non-linear and requires an alignment between producers (supply) and users (demand), there is still a need for strategic intelligence about the emerging supply chains of new technological innovations. This technology delivery system (TDS) is an updated form of the TDS model and provides a promising chain-link approach to the supply side of innovation. Building on early research into supply-side TDS studies, we present a systematic approach to building a TDS model that includes four phases: (1) identifying the macroeconomic and policy environment, including market competition, financial investment, and industrial policy; (2) specifying the key public and private institutions; (3) addressing the core technical complements and their owners, then tracing their interactions through information linkages and technology transfers; and (4) depicting the market prospects and evaluating the potential profound influences on technological change and social developments. Our TDS methodology is illustrated using the field of Big Data & Analytics (“BDA”).}
}
@incollection{KRESS201911,
title = {Big Data for Ecological Models},
editor = {Brian Fath},
booktitle = {Encyclopedia of Ecology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {11-20},
year = {2019},
isbn = {978-0-444-64130-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.10557-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489105573},
author = {Marin M. Kress},
keywords = {Big data, Crowdsourcing or crowdsourced, Data discovery, Data discovery, Data science, Database, Dryad, Environmental health, Interdisciplinary, Machine readable, Metadata, Remote sensing, Social media},
abstract = {The use of data repositories for parameterizing ecological models and storing model runs is becoming more common, yet often these data archives do not contain the appropriate metadata, nor are they maintained for others to use. Data archiving and sharing are additional steps in the scientific process that add value to a researcher׳s work, and more importantly, facilitate transparency and repeatability of a researcher׳s work. Historically, peer-reviewed publications did not allow for the full presentation of underlying datasets, which were only shared through personal contact with a scientist. However, with the expanding use of “supporting online material” (SOM) files that accompany digital publication there is an increased expectation that even large datasets can be made accessible to readers. Thus, researchers are faced with the additional task of becoming their own archivist and depositing data in a repository where it can be used by others. This article introduces basic concepts in data archiving and sharing, including major digital repositories for life science data, commonly used digital file formats, and why metadata is an essential element to successful data sharing when machine-readable data is increasingly used in large-scale studies.}
}
@article{KOZJEK2018209,
title = {Big data analytics for operations management in engineer-to-order manufacturing},
journal = {Procedia CIRP},
volume = {72},
pages = {209-214},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.098},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118302531},
author = {Dominik Kozjek and Rok Vrabič and Borut Rihtaršič and Peter Butala},
keywords = {Engineer-to-order manufacturing, Operations management, Data analytics, Industrial data, Data mining, Big data},
abstract = {Manufacturing data offers big potential for improving management of manufacturing operations. The paper addresses an approach to data analytics in engineer-to-order (ETO) manufacturing systems where the product quality and due-date reliability play a key role in management decisionmaking. The objective of the research is to investigate manufacturing data which are collected by a manufacturing execution system (MES) during operations in an ETO enterprise and to develop tools for supporting scheduling of operations. The developed tools can be used for simulation of production and forecasting of potential resource overloads.}
}
@article{CHEN2016184,
title = {On Big Data and Hydroinformatics},
journal = {Procedia Engineering},
volume = {154},
pages = {184-191},
year = {2016},
note = {12th International Conference on Hydroinformatics (HIC 2016) - Smart Water for the Future},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.07.443},
url = {https://www.sciencedirect.com/science/article/pii/S187770581631832X},
author = {Yiheng Chen and Dawei Han},
keywords = {Big data, Hydroinformatics},
abstract = {Big data is an increasingly hot concept in the past five years in the area of computer science, e-commence, and bioinformatics, because more and more data has been collected by the internet, remote sensing network, wearable devices and the Internet of Things. The big data technology provides techniques and analytical tools to handle large datasets, so that creative ideas and new values can be extracted from them. However, the hydroinformatics research community are not so familiar with big data. This paper provides readers who are embracing the data-rich era with a timely review on big data and its relevant technology, and then points out the relevance with hydroinformatics in three aspects.}
}
@article{ZAMAN2017537,
title = {Challenges and Opportunities of Big Data Analytics for Upcoming Regulations and Future Transformation of the Shipping Industry},
journal = {Procedia Engineering},
volume = {194},
pages = {537-544},
year = {2017},
note = {10th International Conference on Marine Technology, MARTEC 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.08.182},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817333386},
author = {Ibna Zaman and Kayvan Pazouki and Rose Norman and Shervin Younessi and Shirley Coleman},
keywords = {Carbon emission, data-oriented, MRV, big data},
abstract = {Shipping is a heavily regulated industry and responsible for around 3% of global carbon emissions. Global trade is highly dependent on shipping which covers around 90% of commercial demand. Now the industry is expected to navigate through many twists and turns of different situations like upcoming regulations, climate change, energy shortages and technological revolutions. Technological development is apparent across all marine sectors due to the rapid development of sensor technology, IT, automation and robotics. The industry must continue to develop at a rapid pace over the next decade in order to be able to adapt to upcoming regulations and market pressure. Ship intelligence will be the driving force shaping the future of the industry. Ships generate a large volume of data from different sources and in different formats. So big data has become the talk of the industry nowadays. Big data analysis discovers correlations between different measurable or unmeasurable parameters to determine hidden patterns and trends. This analysis will have a significant impact on vessel performance monitoring and provide performance prediction, real-time transparency, and decision-making support to the ship operator. Big data will also bring new opportunities and challenges for the maritime industry. It will increase the capability of performance monitoring, remove human error and increase interdependencies of components. However, the industry will have to face many challenges such as data processing, reliability, and data security. Many regulations rely on ship data including the new EU MRV (Monitoring, Reporting and Verification) regulation to quantify the CO2 emissions for ships above 5000 gross tonnage. As a result, ship operators will have to monitor and report the verified amount of CO2 emitted by their vessels on voyages to, from and between EU ports and will also be required to provide information on energy efficiency parameters. The MRV is a data-oriented regulation requiring ship operators to capture and monitor the ship emissions and other related data and although it is a regional regulation at the moment there is scope for the International Maritime Organisation (IMO) to implement it globally in the near future.}
}
@article{REHMAN2016917,
title = {Big data reduction framework for value creation in sustainable enterprises},
journal = {International Journal of Information Management},
volume = {36},
number = {6, Part A},
pages = {917-928},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216303097},
author = {Muhammad Habib ur Rehman and Victor Chang and Aisha Batool and Teh Ying Wah},
keywords = {Sustainable enterprises, Value creation, Big data analytics, Data reduction, Business model},
abstract = {Value creation is a major sustainability factor for enterprises, in addition to profit maximization and revenue generation. Modern enterprises collect big data from various inbound and outbound data sources. The inbound data sources handle data generated from the results of business operations, such as manufacturing, supply chain management, marketing, and human resource management, among others. Outbound data sources handle customer-generated data which are acquired directly or indirectly from customers, market analysis, surveys, product reviews, and transactional histories. However, cloud service utilization costs increase because of big data analytics and value creation activities for enterprises and customers. This article presents a novel concept of big data reduction at the customer end in which early data reduction operations are performed to achieve multiple objectives, such as (a) lowering the service utilization cost, (b) enhancing the trust between customers and enterprises, (c) preserving privacy of customers, (d) enabling secure data sharing, and (e) delegating data sharing control to customers. We also propose a framework for early data reduction at customer end and present a business model for end-to-end data reduction in enterprise applications. The article further presents a business model canvas and maps the future application areas with its nine components. Finally, the article discusses the technology adoption challenges for value creation through big data reduction in enterprise applications.}
}
@article{DAMIANI20181,
title = {Large databases (Big Data) and evidence-based medicine},
journal = {European Journal of Internal Medicine},
volume = {53},
pages = {1-2},
year = {2018},
issn = {0953-6205},
doi = {https://doi.org/10.1016/j.ejim.2018.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0953620518301961},
author = {Andrea Damiani and Graziano Onder and Vincenzo Valentini}
}
@article{KOBUSINSKA20181321,
title = {Big Data fingerprinting information analytics for sustainability},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {1321-1337},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.12.061},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17329965},
author = {Anna Kobusińska and Kamil Pawluczuk and Jerzy Brzeziński},
keywords = {Big Data, Fingerprinting, Web tracking, Security, Analytics},
abstract = {Web-based device fingerprinting is the process of collecting security information through the browser to perform stateless device identification. Fingerprints may then be used to identify and track computing devices in the web. There are various reasons why device-related information may be needed. Among the others, this technique could help to efficiently analyze security information for sustainability. In this paper we introduce a fingerprinting analytics tool that discovers the most appropriate device fingerprints and their corresponding optimal implementations. The fingerprints selected in the result of the performed analysis are used to enrich and improve an open-source fingerprinting analytics tool Fingerprintjs2, daily consumed by hundreds of websites. As a result, the paper provides a noticeable progress in analytics of dozens of values of device fingerprints, and enhances analysis of fingerprints security information.}
}
@article{LIU201797,
title = {Practical-oriented protocols for privacy-preserving outsourced big data analysis: Challenges and future research directions},
journal = {Computers & Security},
volume = {69},
pages = {97-113},
year = {2017},
note = {Security Data Science and Cyber Threat Management},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2016.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167404816301778},
author = {Zhe Liu and Kim-Kwang Raymond Choo and Minghao Zhao},
keywords = {Big data analysis, Privacy-preserving, Outsourced big data, Oblivious RAM, Security, Practical-oriented, Secure query},
abstract = {With the significant increase in the volume, variety, velocity and veracity of data generated, collected and transmitted through computing and networking systems, it is of little surprise that big data analysis and processing is the subject of focus from enterprise, academia and government. Outsourcing is one popular solution considered in big data processing, although security and privacy are two key concerns often attributed to the underutilization of outsourcing and other promising big data analysis and processing technologies. In this paper, we survey the state-of-the-art literature on cryptographic solutions designed to ensure the security and/or privacy in big data outsourcing. For example, we provide concrete examples to explain how these cryptographic solutions can be deployed. We summarize the existing state-of-play before discussing research opportunities.}
}
@article{OLMEDILLA201679,
title = {Harvesting Big Data in social science: A methodological approach for collecting online user-generated content},
journal = {Computer Standards & Interfaces},
volume = {46},
pages = {79-87},
year = {2016},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2016.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0920548916300034},
author = {M. Olmedilla and M.R. Martínez-Torres and S.L. Toral},
keywords = {Big Data, User-generated content, e-Social science, Computing, Data gathering},
abstract = {Online user-generated content is playing a progressively important role as information source for social scientists seeking for digging out value. Advances procedures and technologies to enable the capture, storage, management, and analysis of the data make possible to exploit increasing amounts of data generated directly by users. In that regard, Big Data is gaining meaning into social science from quantitative datasets side, which differs from traditional social science where collecting data has always been hard, time consuming, and resource intensive. Hence, the emergent field of computational social science is broadening researchers' perspectives. However, it also requires a multidisciplinary approach involving several and different knowledge areas. This paper outlines an architectural framework and methodology to collect Big Data from an electronic Word-of-Mouth (eWOM) website containing user-generated content. Although the paper is written from the social science perspective, it must be also considered together with other complementary disciplines such as data accessing and computing.}
}
@article{JANKE2016227,
title = {Exploring the Potential of Predictive Analytics and Big Data in Emergency Care},
journal = {Annals of Emergency Medicine},
volume = {67},
number = {2},
pages = {227-236},
year = {2016},
issn = {0196-0644},
doi = {https://doi.org/10.1016/j.annemergmed.2015.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S0196064415005302},
author = {Alexander T. Janke and Daniel L. Overbeek and Keith E. Kocher and Phillip D. Levy},
abstract = {Clinical research often focuses on resource-intensive causal inference, whereas the potential of predictive analytics with constantly increasing big data sources remains largely unexplored. Basic prediction, divorced from causal inference, is much easier with big data. Emergency care may benefit from this simpler application of big data. Historically, predictive analytics have played an important role in emergency care as simple heuristics for risk stratification. These tools generally follow a standard approach: parsimonious criteria, easy computability, and independent validation with distinct populations. Simplicity in a prediction tool is valuable, but technological advances make it no longer a necessity. Emergency care could benefit from clinical predictions built using data science tools with abundant potential input variables available in electronic medical records. Patients’ risks could be stratified more precisely with large pools of data and lower resource requirements for comparing each clinical encounter to those that came before it, benefiting clinical decisionmaking and health systems operations. The largest value of predictive analytics comes early in the clinical encounter, in which diagnostic and prognostic uncertainty are high and resource-committing decisions need to be made. We propose an agenda for widening the application of predictive analytics in emergency care. Throughout, we express cautious optimism because there are myriad challenges related to database infrastructure, practitioner uptake, and patient acceptance. The quality of routinely compiled clinical data will remain an important limitation. Complementing big data sources with prospective data may be necessary if predictive analytics are to achieve their full potential to improve care quality in the emergency department.}
}
@article{OPRESNIK2015174,
title = {The value of Big Data in servitization},
journal = {International Journal of Production Economics},
volume = {165},
pages = {174-184},
year = {2015},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.12.036},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314004307},
author = {David Opresnik and Marco Taisch},
keywords = {Servitization, Big Data, Manufacturing, Competitive advantage, Value, Information},
abstract = {Servitization has become a pervasive business strategy among manufacturers, enabling them to undergird their competitive advantage. However, it has at least one weakness. While it is used worldwide also in economies with lower production costs, services in manufacturing are slowly becoming commoditized and will become a necessary, though not sufficient, condition for reaching an above average competitive advantage. Consequently, in this article we propose a new basis for competitive advantage for manufacturing enterprises called a Big Data Strategy in servitization. We scrutinize how manufacturers can exploit the opportunity arising from combined Big Data and servitization. Therefore, the concept of a Big Data Strategy framework in servitization is proposed. The findings are benchmarked against established frameworks in the Big Data and servitization literature. Its impact on competitive advantage is assessed through three theoretical perspectives that increase the validity of the results. The main finding is that, through the proposed strategy, new revenue streams can be created, while opening the possibility to decrease prices for product–services. Through the proposed strategy manufacturers can differentiate themselves from the ones that are already servitizing. This article introduces the possibility of influencing the most important of the five “Vs” in Big Data–Value, in addition to the other four “Vs”—Volume, Variety, Velocity and Verification. As in regards to servitization, the article adds a third layer of added value— “information”, beside the two existing ones: product and service. The results have strategic implications for managers.}
}
@article{SIMPAO2015350,
title = {Big data and visual analytics in anaesthesia and health care†},
journal = {British Journal of Anaesthesia},
volume = {115},
number = {3},
pages = {350-356},
year = {2015},
issn = {0007-0912},
doi = {https://doi.org/10.1093/bja/aeu552},
url = {https://www.sciencedirect.com/science/article/pii/S0007091217311479},
author = {A.F. Simpao and L.M. Ahumada and M.A. Rehman},
keywords = {decision support systems, clinical, electronic health records, integrated advanced information management systems, medical informatics},
abstract = {Advances in computer technology, patient monitoring systems, and electronic health record systems have enabled rapid accumulation of patient data in electronic form (i.e. big data). Organizations such as the Anesthesia Quality Institute and Multicenter Perioperative Outcomes Group have spearheaded large-scale efforts to collect anaesthesia big data for outcomes research and quality improvement. Analytics—the systematic use of data combined with quantitative and qualitative analysis to make decisions—can be applied to big data for quality and performance improvements, such as predictive risk assessment, clinical decision support, and resource management. Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces, and it can facilitate performance of cognitive activities involving big data. Ongoing integration of big data and analytics within anaesthesia and health care will increase demand for anaesthesia professionals who are well versed in both the medical and the information sciences.}
}
@article{KACFAHEMANI201570,
title = {Understandable Big Data: A survey},
journal = {Computer Science Review},
volume = {17},
pages = {70-81},
year = {2015},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2015.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1574013715000064},
author = {Cheikh {Kacfah Emani} and Nadine Cullot and Christophe Nicolle},
keywords = {Big data, Hadoop, Reasoning, Coreference resolution, Entity linking, Information extraction, Ontology alignment},
abstract = {This survey presents the concept of Big Data. Firstly, a definition and the features of Big Data are given. Secondly, the different steps for Big Data data processing and the main problems encountered in big data management are described. Next, a general overview of an architecture for handling it is depicted. Then, the problem of merging Big Data architecture in an already existing information system is discussed. Finally this survey tackles semantics (reasoning, coreference resolution, entity linking, information extraction, consolidation, paraphrase resolution, ontology alignment) in the Big Data context.}
}
@incollection{KRISHNAN2020175,
title = {10 - Building the big data application},
editor = {Krish Krishnan},
booktitle = {Building Big Data Applications},
publisher = {Academic Press},
pages = {175-197},
year = {2020},
isbn = {978-0-12-815746-6},
doi = {https://doi.org/10.1016/B978-0-12-815746-6.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157466000107},
author = {Krish Krishnan},
keywords = {Big data, Business continuity, Research project, Software, Storyboard, User interface},
abstract = {This chapter will discuss the building and delivery of the application. We will look at all aspects of what needs to be done to complete the process from requirements to outcomes, program management, testing, methodology, and all risks and pitfalls. We will discuss KANBAN, budgets and finances, governance, timeline, increase of efficiency, maintenance, support, and application implementation.}
}
@article{ENGLEBRIGHT2016280,
title = {The Role of the Chief Nurse Executive in the Big Data Revolution},
journal = {Nurse Leader},
volume = {14},
number = {4},
pages = {280-284},
year = {2016},
issn = {1541-4612},
doi = {https://doi.org/10.1016/j.mnl.2016.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1541461215300094},
author = {Jane Englebright and Barbara Caspers}
}
@incollection{AGOSTON201953,
title = {Chapter 4 - Big Data, Artificial Intelligence, and Machine Learning in Neurotrauma},
editor = {Firas Kobeissy and Ali Alawieh and Fadi A. Zaraket and Kevin Wang},
booktitle = {Leveraging Biomedical and Healthcare Data},
publisher = {Academic Press},
pages = {53-75},
year = {2019},
isbn = {978-0-12-809556-0},
doi = {https://doi.org/10.1016/B978-0-12-809556-0.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128095560000046},
author = {Denes V. Agoston},
keywords = {Big Data, Artificial intelligence and machine learning in neurotrauma},
abstract = {Rapid advances in the collection, storage, and analysis of large volumes of data—Big Data—offer the much-needed help to identify and treat the various pathological conditions triggered by traumatic brain injury (TBI). Big Data (BD) is defined as extremely large, complex, and mostly unstructured data that cannot be analyzed using traditional approaches. BD can be only analyzed by using text mining (TM), artificial intelligence (AI), or machine learning (ML). These approaches can reveal patterns, trends, and associations, critical for understanding the “most complex disease of the most complex organ.” While powerful and successfully tested computational tools are available, using BD approaches in TBI is currently hampered by the limited availability of legacy and/or primary data, by incompatible data formats and standards. This chapter introduces Big Data and Big Data approaches such as text mining, artificial intelligence, and machine learning; outlines the benefits of using BD approaches; and suggests potential solutions that can help using the full potential of BD in TBI. It also identifies necessary changes of how researchers can help ushering in a new era of preclinical and clinical TBI research by recording and storing ALL the data generated and making ALL the data available for BD approaches—text mining, artificial intelligence, and machine learning so new correlations, relationships, and trends can be identified. In turn, these new information will help to develop novel diagnostics, evidence-based treatments, and improve outcomes.}
}
@article{BELLOORGAZ201645,
title = {Social big data: Recent achievements and new challenges},
journal = {Information Fusion},
volume = {28},
pages = {45-59},
year = {2016},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2015.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1566253515000780},
author = {Gema Bello-Orgaz and Jason J. Jung and David Camacho},
keywords = {Big data, Data mining, Social media, Social networks, Social-based frameworks and applications},
abstract = {Big data has become an important issue for a large number of research areas such as data mining, machine learning, computational intelligence, information fusion, the semantic Web, and social networks. The rise of different big data frameworks such as Apache Hadoop and, more recently, Spark, for massive data processing based on the MapReduce paradigm has allowed for the efficient utilisation of data mining methods and machine learning algorithms in different domains. A number of libraries such as Mahout and SparkMLib have been designed to develop new efficient applications based on machine learning algorithms. The combination of big data technologies and traditional machine learning algorithms has generated new and interesting challenges in other areas as social media and social networks. These new challenges are focused mainly on problems such as data processing, data storage, data representation, and how data can be used for pattern mining, analysing user behaviours, and visualizing and tracking data, among others. In this paper, we present a revision of the new methodologies that is designed to allow for efficient data mining and information fusion from social media and of the new applications and frameworks that are currently appearing under the “umbrella” of the social networks, social media and big data paradigms.}
}
@article{KORTESNIEMI201890,
title = {The European Federation of Organisations for Medical Physics (EFOMP) White Paper: Big data and deep learning in medical imaging and in relation to medical physics profession},
journal = {Physica Medica},
volume = {56},
pages = {90-93},
year = {2018},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1120179718313152},
author = {Mika Kortesniemi and Virginia Tsapaki and Annalisa Trianni and Paolo Russo and Ad Maas and Hans-Erik Källman and Marco Brambilla and John Damilakis},
abstract = {Big data and deep learning will profoundly change various areas of professions and research in the future. This will also happen in medicine and medical imaging in particular. As medical physicists, we should pursue beyond the concept of technical quality to extend our methodology and competence towards measuring and optimising the diagnostic value in terms of how it is connected to care outcome. Functional implementation of such methodology requires data processing utilities starting from data collection and management and culminating in the data analysis methods. Data quality control and validation are prerequisites for the deep learning application in order to provide reliable further analysis, classification, interpretation, probabilistic and predictive modelling from the vast heterogeneous big data. Challenges in practical data analytics relate to both horizontal and longitudinal analysis aspects. Quantitative aspects of data validation, quality control, physically meaningful measures, parameter connections and system modelling for the future artificial intelligence (AI) methods are positioned firmly in the field of Medical Physics profession. It is our interest to ensure that our professional education, continuous training and competence will follow this significant global development.}
}
@article{TAN2015223,
title = {Harvesting big data to enhance supply chain innovation capabilities: An analytic infrastructure based on deduction graph},
journal = {International Journal of Production Economics},
volume = {165},
pages = {223-233},
year = {2015},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.12.034},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314004289},
author = {Kim Hua Tan and YuanZhu Zhan and Guojun Ji and Fei Ye and Chingter Chang},
keywords = {Big data, Analytic infrastructure, Competence set, Deduction graph, Supply chain innovation},
abstract = {Today, firms can access to big data (tweets, videos, click streams, and other unstructured sources) to extract new ideas or understanding about their products, customers, and markets. Thus, managers increasingly view data as an important driver of innovation and a significant source of value creation and competitive advantage. To get the most out of the big data (in combination with a firm׳s existing data), a more sophisticated way of handling, managing, analysing and interpreting data is necessary. However, there is a lack of data analytics techniques to assist firms to capture the potential of innovation afforded by data and to gain competitive advantage. This research aims to address this gap by developing and testing an analytic infrastructure based on the deduction graph technique. The proposed approach provides an analytic infrastructure for firms to incorporate their own competence sets with other firms. Case studies results indicate that the proposed data analytic approach enable firms to utilise big data to gain competitive advantage by enhancing their supply chain innovation capabilities.}
}
@article{JAN2019275,
title = {Deep learning in big data Analytics: A comparative study},
journal = {Computers & Electrical Engineering},
volume = {75},
pages = {275-287},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2017.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617315835},
author = {Bilal Jan and Haleem Farman and Murad Khan and Muhammad Imran and Ihtesham Ul Islam and Awais Ahmad and Shaukat Ali and Gwanggil Jeon},
keywords = {Big data, Deep learning, Deep belief networks, Convolutional Neural Networks},
abstract = {Deep learning methods are extensively applied to various fields of science and engineering such as speech recognition, image classifications, and learning methods in language processing. Similarly, traditional data processing techniques have several limitations of processing large amount of data. In addition, Big Data analytics requires new and sophisticated algorithms based on machine and deep learning techniques to process data in real-time with high accuracy and efficiency. However, recently, research incorporated various deep learning techniques with hybrid learning and training mechanisms of processing data with high speed. Most of these techniques are specific to scenarios and based on vector space thus, shows poor performance in generic scenarios and learning features in big data. In addition, one of the reason of such failure is high involvement of humans to design sophisticated and optimized algorithms based on machine and deep learning techniques. In this article, we bring forward an approach of comparing various deep learning techniques for processing huge amount of data with different number of neurons and hidden layers. The comparative study shows that deep learning techniques can be built by introducing a number of methods in combination with supervised and unsupervised training techniques.}
}
@article{HECKMAN202019,
title = {The Role of Physicians in the Era of Big Data},
journal = {Canadian Journal of Cardiology},
volume = {36},
number = {1},
pages = {19-21},
year = {2020},
issn = {0828-282X},
doi = {https://doi.org/10.1016/j.cjca.2019.09.018},
url = {https://www.sciencedirect.com/science/article/pii/S0828282X19312826},
author = {George A. Heckman and John P. Hirdes and Robert S. McKelvie}
}
@article{GOVINDAN2018343,
title = {Big data analytics and application for logistics and supply chain management},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {114},
pages = {343-349},
year = {2018},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2018.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S1366554518302606},
author = {Kannan Govindan and T.C.E. Cheng and Nishikant Mishra and Nagesh Shukla},
keywords = {Big data analytics, Supply chain management, Logistics},
abstract = {This special issue explores big data analytics and applications for logistics and supply chain management by examining novel methods, practices, and opportunities. The articles present and analyse a variety of opportunities to improve big data analytics and applications for logistics and supply chain management, such as those through exploring technology-driven tracking strategies, financial performance relations with data driven supply chains, and implementation issues and supply chain capability maturity with big data. This editorial note summarizes the discussions on the big data attributes, on effective practices for implementation, and on evaluation and implementation methods.}
}
@article{MANTELERO2017584,
title = {Regulating big data. The guidelines of the Council of Europe in the context of the European data protection framework},
journal = {Computer Law & Security Review},
volume = {33},
number = {5},
pages = {584-602},
year = {2017},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917301644},
author = {Alessandro Mantelero},
keywords = {Big data, Data protection, Council of Europe, Risk assessment, Data protection by design, Consent, Data anonymization, Open data, Algorithms},
abstract = {In January 2017 the Consultative Committee of Convention 108 adopted its Guidelines on the Protection of Individuals with Regard to the Processing of Personal Data in a World of Big Data. These are the first guidelines on data protection provided by an international body which specifically address the issues surrounding big data applications. This article examines the main provisions of these Guidelines and highlights the approach adopted by the Consultative Committee, which contextualises the traditional principles of data protection in the big data scenario and also takes into account the challenges of the big data paradigm. The analysis of the different provisions adopted focuses primarily on the core of the Guidelines namely the risk assessment procedure. Moreover, the article discusses the novel solutions provided by the Guidelines with regard to the data subject's informed consent, the by-design approach, anonymization, and the role of the human factor in big data-supported decisions. This critical analysis of the Guidelines introduces a broader reflection on the divergent approaches of the Council of Europe and the European Union to regulating data processing. Where the principle-based model of the Council of Europe differs from the approach adopted by the EU legislator in the detailed Regulation (EU) 2016/679. In the light of this, the provisions of the Guidelines and their attempt to address the major challenges of the new big data paradigm set the stage for concluding remarks about the most suitable regulatory model to deal with the different issues posed by the development of technology.}
}
@article{REKHA2015295,
title = {Survey on Software Project Risks and Big Data Analytics},
journal = {Procedia Computer Science},
volume = {50},
pages = {295-300},
year = {2015},
note = {Big Data, Cloud and Computing Challenges},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.04.045},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915005463},
author = {J.H. Rekha and R. Parvathi},
keywords = {software project, big data analytics, anlytics tools.},
abstract = {Software project is collaborative enterprise of making a desired software for the client. Each software is unique and is delivered by following the process. The process includes understanding the requirement, planning, designing the software and implementation. Risk occurs in the software project which need attention by the managers and workers to make the project efficient. Big data analytics is commonly used in all fields. Big data deals with huge data which are unstructured. Using analytics tools, it can be chunked down and analyzed to provide valuable solutions. In this paper, a review of risk in software project and big data analytics are briefed out.}
}
@article{BERNASCONI2021100009,
title = {Data quality-aware genomic data integration},
journal = {Computer Methods and Programs in Biomedicine Update},
volume = {1},
pages = {100009},
year = {2021},
issn = {2666-9900},
doi = {https://doi.org/10.1016/j.cmpbup.2021.100009},
url = {https://www.sciencedirect.com/science/article/pii/S2666990021000082},
author = {Anna Bernasconi},
keywords = {Data quality, Data integration, Data curation, Genomic datasets, Metadata, Interoperability},
abstract = {Genomic data are growing at unprecedented pace, along with new protocols, update polices, formats and guidelines, terminologies and ontologies, which are made available every day by data providers. In this continuously evolving universe, enforcing quality on data and metadata is increasingly critical. While many aspects of data quality are addressed at each individual source, we focus on the need for a systematic approach when data from several sources are integrated, as such integration is an essential aspect for modern genomic data analysis. Data quality must be assessed from many perspectives, including accessibility, currency, representational consistency, specificity, and reliability. In this article we review relevant literature and, based on the analysis of many datasets and platforms, we report on methods used for guaranteeing data quality while integrating heterogeneous data sources. We explore several real-world cases that are exemplary of more general underlying data quality problems and we illustrate how they can be resolved with a structured method, sensibly applicable also to other biomedical domains. The overviewed methods are implemented in a large framework for the integration of processed genomic data, which is made available to the research community for supporting tertiary data analysis over Next Generation Sequencing datasets, continuously loaded from many open data sources, bringing considerable added value to biological knowledge discovery.}
}
@article{COLEMAN20151091,
title = {How Big Data Informs Us About Cataract Surgery: The LXXII Edward Jackson Memorial Lecture},
journal = {American Journal of Ophthalmology},
volume = {160},
number = {6},
pages = {1091-1103.e3},
year = {2015},
issn = {0002-9394},
doi = {https://doi.org/10.1016/j.ajo.2015.09.028},
url = {https://www.sciencedirect.com/science/article/pii/S000293941500598X},
author = {Anne Louise Coleman},
abstract = {Purpose
To characterize the role of Big Data in evaluating quality of care in ophthalmology, to highlight opportunities for studying quality improvement using data available in the American Academy of Ophthalmology Intelligent Research in Sight (IRIS) Registry, and to show how Big Data informs us about rare events such as endophthalmitis after cataract surgery.
Design
Review of published studies, analysis of public-use Medicare claims files from 2010 to 2013, and analysis of IRIS Registry from 2013 to 2014.
Methods
Statistical analysis of observational data.
Results
The overall rate of endophthalmitis after cataract surgery was 0.14% in 216 703 individuals in the Medicare database. In the IRIS Registry the endophthalmitis rate after cataract surgery was 0.08% among 511 182 individuals. Endophthalmitis rates tended to be higher in eyes with combined cataract surgery and anterior vitrectomy (P = .051), although only 0.08% of eyes had this combined procedure. Visual acuity (VA) in the IRIS Registry in eyes with and without postoperative endophthalmitis measured 1–7 days postoperatively were logMAR 0.58 (standard deviation [SD]: 0.84) (approximately Snellen acuity of 20/80) and logMAR 0.31 (SD: 0.34) (approximately Snellen acuity of 20/40), respectively. In 33 547 eyes with postoperative VA after cataract surgery, 18.3% had 1-month-postoperative VA worse than 20/40.
Conclusions
Big Data drawing on Medicare claims and IRIS Registry records can help identify additional areas for quality improvement, such as in the 18.3% of eyes in the IRIS Registry having 1-month-postoperative VA worse than 20/40. The ability to track patient outcomes in Big Data sets provides opportunities for further research on rare complications such as postoperative endophthalmitis and outcomes from uncommon procedures such as cataract surgery combined with anterior vitrectomy. But privacy and data-security concerns associated with Big Data should not be taken lightly.}
}
@article{SEMANJSKI201738,
title = {Spatial context mining approach for transport mode recognition from mobile sensed big data},
journal = {Computers, Environment and Urban Systems},
volume = {66},
pages = {38-52},
year = {2017},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2017.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0198971516304367},
author = {Ivana Semanjski and Sidharta Gautama and Rein Ahas and Frank Witlox},
keywords = {Transport mode recognition, Mobile sensed big data, Spatial awareness, Geographic information systems, Smart city, Support vector machines, Context mining, Urban data},
abstract = {Knowledge about what transport mode people use is important information of any mobility or travel behaviour research. With ubiquitous presence of smartphones, and its sensing possibilities, new opportunities to infer transport mode from movement data are appearing. In this paper we investigate the role of spatial context of human movements in inferring transport mode from mobile sensed data. For this we use data collected from more than 8000 participants over a period of four months, in combination with freely available geographical information. We develop a support vectors machines-based model to infer five transport modes and achieve success rate of 94%. The developed model is applicable across different mobile sensed data, as it is independent on the integration of additional sensors in the device itself. Furthermore, suggested approach is robust, as it strongly relies on pre-processed data, which makes it applicable for big data implementations in (smart) cities and other data-driven mobility platforms.}
}
@article{AKTER2016113,
title = {How to improve firm performance using big data analytics capability and business strategy alignment?},
journal = {International Journal of Production Economics},
volume = {182},
pages = {113-131},
year = {2016},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2016.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0925527316302110},
author = {Shahriar Akter and Samuel Fosso Wamba and Angappa Gunasekaran and Rameshwar Dubey and Stephen J. Childe},
keywords = {Capabilities, Entanglement view, Big data analytics, Hierarchical modeling},
abstract = {The recent interest in big data has led many companies to develop big data analytics capability (BDAC) in order to enhance firm performance (FPER). However, BDAC pays off for some companies but not for others. It appears that very few have achieved a big impact through big data. To address this challenge, this study proposes a BDAC model drawing on the resource-based theory (RBT) and the entanglement view of sociomaterialism. The findings show BDAC as a hierarchical model, which consists of three primary dimensions (i.e., management, technology, and talent capability) and 11 subdimensions (i.e., planning, investment, coordination, control, connectivity, compatibility, modularity, technology management knowledge, technical knowledge, business knowledge and relational knowledge). The findings from two Delphi studies and 152 online surveys of business analysts in the U.S. confirm the value of the entanglement conceptualization of the higher-order BDAC model and its impact on FPER. The results also illuminate the significant moderating impact of analytics capability–business strategy alignment on the BDAC–FPER relationship.}
}
@article{LIU2020113381,
title = {Minimizing the data quality problem of information systems: A process-based method},
journal = {Decision Support Systems},
volume = {137},
pages = {113381},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113381},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620301366},
author = {Qi Liu and Gengzhong Feng and Xi Zhao and Wenlong Wang},
keywords = {Data quality, Information system, Petri net, Optimization model, Process model},
abstract = {The low quality of data in information systems poses enormous risks to business operations and decision making. In this paper, a single-period resource allocation problem for controlling the information system's data quality problem is considered. We develop a Data-Quality-Petri net to capture the process through which data quality problem generates, propagates, and accumulates in the information system. The net considers not only the factors leading to the production of the data quality problem by the data operation nodes and the data flow structure, but also the data transfer ratio of the nodes. Then, we propose a nonlinear programming optimization model with control resource constraints. The result of the model provides an optimal strategy to allocate resources for minimizing the expected data quality problem of an information system. Further, we examine the impact of the data flow structure on optimal resource allocation. The result shows that the optimal resource input level for a data operation node is proportional to its potential for downstream propagation. A warehouse management system of an e-commerce company is utilized to illustrate the model. Our study provides a method for data managers to control the information system's data quality problem by employing a process perspective.}
}
@article{KUMAR2018428,
title = {A big data driven sustainable manufacturing framework for condition-based maintenance prediction},
journal = {Journal of Computational Science},
volume = {27},
pages = {428-439},
year = {2018},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877750316305129},
author = {Ajay Kumar and Ravi Shankar and Lakshman S. Thakur},
keywords = {data driven sustainable enterprise, fuzzy unordered induction algo, big data analytics, condition-based maintenance, machine learning techniques, backward feature elimination},
abstract = {Smart manufacturing refers to a future-state of manufacturing and it can lead to remarkable changes in all aspects of operations through minimizing energy and material usage while simultaneously maximizing sustainability enabling a futuristic more digitalized scenario of manufacturing. This research develops a big data analytics framework that optimizes the maintenance schedule through condition-based maintenance (CBM) optimization and also improves the prediction accuracy to quantify the remaining life prediction uncertainty. Through effective utilization of condition monitoring and prediction information, CBM would enhance equipment reliability leading to reduction in maintenance cost. The proposed framework uses a CBM optimization method that utilizes a new linguistic interval-valued fuzzy reasoning method for predicting the information. The proposed big data analytics framework in our study for estimating the uncertainty based on backward feature elimination and fuzzy unordered rule induction algorithm prediction errors, is an innovative contribution to the remaining life prediction field. Our paper elaborates on the basic underlying structure of CBM system that is defined by transaction matrix and the threshold value of failure probability. We developed this framework for analysing the CBM policy cost more accurately and to find the probabilistic threshold values of covariate that corresponds to the lowest price of predictive maintenance cost. The experimental results are performed on a big dataset which is generated from a sophisticated simulator of a gas turbine propulsion plant. A comparative analysis confirms that the method used in the proposed framework outpaces the classical methods in terms of classification accuracy and other statistical performance evaluation metrics.}
}
@article{ZAIN2018140,
title = {Big Data Analytics based on PANFIS MapReduce},
journal = {Procedia Computer Science},
volume = {144},
pages = {140-152},
year = {2018},
note = {INNS Conference on Big Data and Deep Learning},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.514},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918322233},
author = {Choiru Za’in and Mahardhika Pratama and Edwin Lughofer and Meftahul Ferdaus and Qing Cai and Mukesh Prasad},
keywords = {Big data stream analytic, Distributed evolving algorithm, Scalable real-time data mining, Parallel learning, Rule merging strategy},
abstract = {In this paper, a big data analytic framework is introduced for processing high-frequency data stream. This framework architecture is developed by combining an advanced evolving learning algorithm namely Parsimonious Network Fuzzy Inference System (PANFIS) with MapReduce parallel computation, where PANFIS has the capability of processing data stream in large volume. Big datasets are learnt chunk by chunk by processors in MapReduce environment and the results are fused by rule merging method, that reduces the complexity of the rules. The performance measurement has been conducted, and the results are showing that the MapReduce framework along with PANFIS evolving system helps to reduce the processing time around 22 percent in average in comparison with the PANFIS algorithm without reducing performance in accuracy.}
}
@article{RAMBUR2018176,
title = {A plea to nurse educators: Incorporate big data use as a foundational skill for undergraduate and graduate nurses},
journal = {Journal of Professional Nursing},
volume = {34},
number = {3},
pages = {176-181},
year = {2018},
issn = {8755-7223},
doi = {https://doi.org/10.1016/j.profnurs.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S8755722316303283},
author = {Betty Rambur and Therese Fitzpatrick}
}
@article{KHENNOU201860,
title = {Improving the Use of Big Data Analytics within Electronic Health Records: A Case Study based OpenEHR},
journal = {Procedia Computer Science},
volume = {127},
pages = {60-68},
year = {2018},
note = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918301091},
author = {Fadoua Khennou and Youness Idrissi Khamlichi and Nour El Houda Chaoui},
keywords = {Electronic Health Records, EHRs, Analytic tools, Big Data, Health Practitioners},
abstract = {Recently there has been an increasing adoption of electronic health records (EHRs) in different countries. Thanks to these systems, multiple health bodies can now store, manage and process their data effectively. However, the existence of such powerful and meticulous entities raise new challenges and issues for health practitioners. In fact, while the main objective of EHRs is to gain actionable big data insights from the health workflow, very few physicians exploit widely analytic tools, this is mainly due to the fact of having to deal with multiple systems and steps, which completely discourage them from engaging more and more. In this paper, we shed light and explore precisely the proper adaptation of analytical tools to EHRs in order to upgrade their use by health practitioners. For that, we present a case study of the implementation process of an EHR based OpenEHR and investigate health analytics adoption in each step of the methodology.}
}
@article{ZHANG2015606,
title = {A System for Tender Price Evaluation of Construction Project Based on Big Data},
journal = {Procedia Engineering},
volume = {123},
pages = {606-614},
year = {2015},
note = {Selected papers from Creative Construction Conference 2015},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.10.114},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815032154},
author = {Yongcheng Zhang and Hanbin Luo and Yi He},
keywords = {System, Bid price evaluation, Construction project, Big data},
abstract = {Tender price evaluation of construction project is one of the most important works for the clients to control project cost in the bidding stage. However,the previously underutilization of project cost data made the tender price evaluation of new projects lack of effective evaluation criterion, which brings challenge to cost control. With the improvement of companies’ information technology application and the advent of big data era, the project cost-related data can be completely and systematically recorded in real time, as well as fully utilized to support decision-making for construction project cost management. In this paper, a system for tender price evaluation of construction project based on big data is presented, aiming to use related technique of big data to analysis project cost data to give a reasonable cost range, which contributes to obtaining the evaluation criterion to support the tender price controls. The paper introduced the data sources, data extraction, data storage and data analysis of the system respectively. A case study is conducted in a metro station project to evaluate the system. The results show that the system based on big data is significant for tender price evaluation in construction project.}
}
@article{JI2017187,
title = {Big data analytics based fault prediction for shop floor scheduling},
journal = {Journal of Manufacturing Systems},
volume = {43},
pages = {187-194},
year = {2017},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2017.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0278612517300389},
author = {Wei Ji and Lihui Wang},
keywords = {Big data analytics, Fault prediction, Shop floor, Scheduling},
abstract = {The current task scheduling mainly concerns the availability of machining resources, rather than the potential errors after scheduling. To minimise such errors in advance, this paper presents a big data analytics based fault prediction approach for shop floor scheduling. Within the context, machining tasks, machining resources, and machining processes are represented by data attributes. Based on the available data on the shop floor, the potential fault/error patterns, referring to machining errors, machine faults and maintenance states, are mined for unsuitable scheduling arrangements before machining as well as upcoming errors during machining. Comparing the data-represented tasks with the mined error patterns, their similarities or differences are calculated. Based on the calculated similarities, the fault probabilities of the scheduled tasks or the current machining tasks can be obtained, and they provide a reference of decision making for scheduling and rescheduling the tasks. By rescheduling high-risk tasks carefully, the potential errors can be avoided. In this paper, the architecture of the approach consisting of three steps in three levels is proposed. Furthermore, big data are considered in three levels, i.e. local data, local network data and cloud data. In order to implement this idea, several key techniques are illustrated in detail, e.g. data attribute, data cleansing, data integration of databases in different levels, and big data analytic algorithms. Finally, a simplified case study is described to show the prediction process of the proposed method.}
}
@article{LEFEVRE20181,
title = {Big data in forensic science and medicine},
journal = {Journal of Forensic and Legal Medicine},
volume = {57},
pages = {1-6},
year = {2018},
note = {Thematic section: Big dataGuest editor: Thomas LefèvreThematic section: Health issues in police custodyGuest editors: Patrick Chariot and Steffen Heide},
issn = {1752-928X},
doi = {https://doi.org/10.1016/j.jflm.2017.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1752928X17301154},
author = {Thomas Lefèvre},
keywords = {Forensic science, Big data, Personalized medicine, Predictive medicine, Machine learning, Dimensionality},
abstract = {In less than a decade, big data in medicine has become quite a phenomenon and many biomedical disciplines got their own tribune on the topic. Perspectives and debates are flourishing while there is a lack for a consensual definition for big data. The 3Vs paradigm is frequently evoked to define the big data principles and stands for Volume, Variety and Velocity. Even according to this paradigm, genuine big data studies are still scarce in medicine and may not meet all expectations. On one hand, techniques usually presented as specific to the big data such as machine learning techniques are supposed to support the ambition of personalized, predictive and preventive medicines. These techniques are mostly far from been new and are more than 50 years old for the most ancient. On the other hand, several issues closely related to the properties of big data and inherited from other scientific fields such as artificial intelligence are often underestimated if not ignored. Besides, a few papers temper the almost unanimous big data enthusiasm and are worth attention since they delineate what is at stakes. In this context, forensic science is still awaiting for its position papers as well as for a comprehensive outline of what kind of contribution big data could bring to the field. The present situation calls for definitions and actions to rationally guide research and practice in big data. It is an opportunity for grounding a true interdisciplinary approach in forensic science and medicine that is mainly based on evidence.}
}
@article{YAHIA20181,
title = {Preface: Special Issue on Big Data},
journal = {Fuzzy Sets and Systems},
volume = {348},
pages = {1-3},
year = {2018},
note = {SI: Fuzzy Approaches to Big Data},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2018.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S0165011418302987},
author = {Sadok Ben Yahia and Anne Laurent and Gabriella Pasi}
}
@article{NATIVI20151,
title = {Big Data challenges in building the Global Earth Observation System of Systems},
journal = {Environmental Modelling & Software},
volume = {68},
pages = {1-26},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215000481},
author = {Stefano Nativi and Paolo Mazzetti and Mattia Santoro and Fabrizio Papeschi and Max Craglia and Osamu Ochiai},
keywords = {GEOSS, Big Data, Multidisciplinary systems, Earth System Science, Research infrastructures, Interoperability, Cloud systems},
abstract = {There are many expectations and concerns about Big Data in the sector of Earth Observation. It is necessary to understand whether Big Data is a radical shift or an incremental change for the existing digital infrastructures. This manuscript explores the impact of Big Data dimensionalities (commonly known as ‘V’ axes: volume, variety, velocity, veracity, visualization) on the Global Earth Observation System of Systems (GEOSS) and particularly its common digital infrastructure (i.e. the GEOSS Common Infrastructure). GEOSS is a global and flexible network of content providers allowing decision makers to access an extraordinary range of data and information. GEOSS is a pioneering framework for global and multidisciplinary data sharing in the EO realm. The manuscript introduces and discusses the general GEOSS strategies to address Big Data challenges, focusing on the cloud-based discovery and access solutions. A final section reports the results of the scalability and flexibility performance tests.}
}
@article{PURANIK2019838,
title = {The perils and pitfalls of big data analysis in medicine},
journal = {The Ocular Surface},
volume = {17},
number = {4},
pages = {838-839},
year = {2019},
issn = {1542-0124},
doi = {https://doi.org/10.1016/j.jtos.2019.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1542012419301740},
author = {C.J. Puranik and Sreenivasa Rao and S. Chennamaneni}
}
@article{MAUDSLEY2018961,
title = {Intelligent and effective informatic deconvolution of “Big Data” and its future impact on the quantitative nature of neurodegenerative disease therapy},
journal = {Alzheimer's & Dementia},
volume = {14},
number = {7},
pages = {961-975},
year = {2018},
issn = {1552-5260},
doi = {https://doi.org/10.1016/j.jalz.2018.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S1552526018300402},
author = {Stuart Maudsley and Viswanath Devanarayan and Bronwen Martin and Hugo Geerts},
keywords = {Big data, Informatics, High-dimensionality, Alzheimer's disease, Aging, Molecular signature, Transcriptomics, Metabolomics, Proteomics, Genomics},
abstract = {Biomedical data sets are becoming increasingly larger and a plethora of high-dimensionality data sets (“Big Data”) are now freely accessible for neurodegenerative diseases, such as Alzheimer's disease. It is thus important that new informatic analysis platforms are developed that allow the organization and interrogation of Big Data resources into a rational and actionable mechanism for advanced therapeutic development. This will entail the generation of systems and tools that allow the cross-platform correlation between data sets of distinct types, for example, transcriptomic, proteomic, and metabolomic. Here, we provide a comprehensive overview of the latest strategies, including latent semantic analytics, topological data investigation, and deep learning techniques that will drive the future development of diagnostic and therapeutic applications for Alzheimer's disease. We contend that diverse informatic “Big Data” platforms should be synergistically designed with more advanced chemical/drug and cellular/tissue-based phenotypic analytical predictive models to assist in either de novo drug design or effective drug repurposing.}
}
@article{NUNEZREIZ201952,
title = {Big data and machine learning in critical care: Opportunities for collaborative research},
journal = {Medicina Intensiva},
volume = {43},
number = {1},
pages = {52-57},
year = {2019},
issn = {0210-5691},
doi = {https://doi.org/10.1016/j.medin.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0210569118301827},
author = {Antonio {Núñez Reiz} and Fernando {Martínez Sagasti} and Manuel {Álvarez González} and Antonio {Blesa Malpica} and Juan Carlos {Martín Benítez} and Mercedes {Nieto Cabrera} and Ángela {del Pino Ramírez} and José Miguel {Gil Perdomo} and Jesús {Prada Alonso} and Leo Anthony Celi and Miguel Ángel {Armengol de la Hoz} and Rodrigo Deliberato and Kenneth Paik and Tom Pollard and Jesse Raffa and Felipe Torres and Julio Mayol and Joan Chafer and Arturo {González Ferrer} and Ángel Rey and Henar {González Luengo} and Giuseppe Fico and Ivana Lombroni and Liss Hernandez and Laura López and Beatriz Merino and María Fernanda Cabrera and María Teresa Arredondo and María Bodí and Josep Gómez and Alejandro Rodríguez and Miguel {Sánchez García}},
keywords = {Big data, Machine learning, Artificial intelligence, Clinical databases, MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases de datos clínicos, MIMIC III, Datathon, Trabajo colaborativo},
abstract = {The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.
Resumen
La aparición de los sistemas de información clínica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos clínicos en formato electrónico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas clínicas, para su uso en la gestión de recursos o para sugerir líneas de investigación que luego pueden ser explotadas mediante ensayos clínicos aleatorizados. Sin embargo, los médicos clínicos carecen de la formación necesaria para la explotación de grandes bases de datos, lo que supone un obstáculo para aprovechar esta oportunidad. Además, existen cuestiones de índole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estadísticos, epidemiólogos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigación clínica o análisis de resultados (benchmarking). Se describe la reunión multidisciplinar (Critical Care Datathon) realizada en Madrid los días 1, 2 y 3 de diciembre de 2017. Esta reunión, celebrada bajo los auspicios de la Sociedad Española de Medicina Intensiva, Crítica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovación y el Servicio de Medicina Intensiva del Hospital Clínico San Carlos, así como el grupo de investigación «Life Supporting Technologies» de la Universidad Politécnica de Madrid. Tras unas ponencias de formación sobre big data, seguridad y calidad de los datos, y su aplicación al entorno de la medicina intensiva, un grupo de clínicos, analistas de datos, estadísticos, expertos en seguridad informática de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas clínicas establecidas previamente a la reunión. El trabajo colaborativo permitió establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias líneas de investigación clínica a desarrollar en el futuro. Además, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en España, y se estableció un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparación de resultados entre unidades o la elaboración de nuevas líneas de investigación clínica.}
}
@article{GAO2016952,
title = {A review of control loop monitoring and diagnosis: Prospects of controller maintenance in big data era},
journal = {Chinese Journal of Chemical Engineering},
volume = {24},
number = {8},
pages = {952-962},
year = {2016},
issn = {1004-9541},
doi = {https://doi.org/10.1016/j.cjche.2016.05.039},
url = {https://www.sciencedirect.com/science/article/pii/S1004954116305134},
author = {Xinqing Gao and Fan Yang and Chao Shang and Dexian Huang},
keywords = {Control loop performance assessment, Industrial alarm system, Process knowledge, Root cause diagnosis, Big data},
abstract = {Owing to wide applications of automatic control systems in the process industries, the impacts of controller performance on industrial processes are becoming increasingly significant. Consequently, controller maintenance is critical to guarantee routine operations of industrial processes. The workflow of controller maintenance generally involves the following steps: monitor operating controller performance and detect performance degradation, diagnose probable root causes of control system malfunctions, and take specific actions to resolve associated problems. In this article, a comprehensive overview of the mainstream of control loop monitoring and diagnosis is provided, and some existing problems are also analyzed and discussed. From the viewpoint of synthesizing abundant information in the context of big data, some prospective ideas and promising methods are outlined to potentially solve problems in industrial applications.}
}
@article{DUNCAN2019127,
title = {Big data sharing and analysis to advance research in post-traumatic epilepsy},
journal = {Neurobiology of Disease},
volume = {123},
pages = {127-136},
year = {2019},
note = {Antiepileptogenesis following Traumatic Brain Injury},
issn = {0969-9961},
doi = {https://doi.org/10.1016/j.nbd.2018.05.026},
url = {https://www.sciencedirect.com/science/article/pii/S0969996118301700},
author = {Dominique Duncan and Paul Vespa and Asla Pitkänen and Adebayo Braimah and Niina Lapinlampi and Arthur W. Toga},
keywords = {Biomarkers, EEG, Epilepsy, Epileptogenesis, Informatics, MRI, Neuroimaging, TBI},
abstract = {We describe the infrastructure and functionality for a centralized preclinical and clinical data repository and analytic platform to support importing heterogeneous multi-modal data, automatically and manually linking data across modalities and sites, and searching content. We have developed and applied innovative image and electrophysiology processing methods to identify candidate biomarkers from MRI, EEG, and multi-modal data. Based on heterogeneous biomarkers, we present novel analytic tools designed to study epileptogenesis in animal model and human with the goal of tracking the probability of developing epilepsy over time.}
}
@article{GARG2016940,
title = {Challenges and Techniques for Testing of Big Data},
journal = {Procedia Computer Science},
volume = {85},
pages = {940-948},
year = {2016},
note = {International Conference on Computational Modelling and Security (CMS 2016)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.285},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916306354},
author = {Naveen Garg and Sanjay Singla and Surender Jangra},
keywords = {Big Data, Testing, Verasity, Hadoop},
abstract = {Big Data, the new buzz word in the industry, is data that exceeds the processing and analytic capacity of conventional database systems within the time necessary to make them useful. With multiple data stores in abundant formats, billions of rows of data with hundreds of millions of data combinations and the urgent need of making best possible decisions, the challenge is big and the solution bigger, Big Data. Comes with it, new advances in computing technology together with its high performance analytics for simpler and faster processing of only relevant data to enable timely and accurate insights using data mining and predictive analytics, text mining, forecasting and optimization on complex data to continuously drive innovation and make the best possible decisions. While Big Data provides solutions to complex business problems like analyzing larger volumes of data than was previously possible to drive more precise answers, analyzing data in motion to capture opportunities that were previously lost, it poses bigger challenges in testing these scenarios. Testing such highly volatile data, which is more often than not unstructured generated from myriad sources such as web logs, radio frequency Id (RFID), sensors embedded in devices, GPS systems etc. and mostly clustered data for its accuracy, high availability, security requires specialization. One of the most challenging things for a tester is to keep pace with changing dynamics of the industry. While on most aspects of testing, the tester need not know the technical details behind the scene however this is where testing Big Data Technology is so different. A tester not only needs to be strong on testing fundamentals but also has to be equally aware of minute details in the architecture of the database designs to analyze several performance bottlenecks and other issues. Like in the example quoted above on In-Memory databases, a tester would need to know how the operating systems allocate and de-allocate memory and understand how much memory is being used at any given time. So, concluding, as the data- analytics Industry evolves further we would see the IT Testing Services getting closely aligned with the Database Engineering and the industry would need more skilled testing professional in this domain to grab the new opportunities.}
}
@article{TAGLANG201617,
title = {Use of “big data” in drug discovery and clinical trials},
journal = {Gynecologic Oncology},
volume = {141},
number = {1},
pages = {17-23},
year = {2016},
issn = {0090-8258},
doi = {https://doi.org/10.1016/j.ygyno.2016.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S0090825816300464},
author = {Guillaume Taglang and David B. Jackson},
keywords = {Big data, Drug discovery, Clinical trials, Precision medicine, Biomarkers},
abstract = {Oncology is undergoing a data-driven metamorphosis. Armed with new and ever more efficient molecular and information technologies, we have entered an era where data is helping us spearhead the fight against cancer. This technology driven data explosion, often referred to as “big data”, is not only expediting biomedical discovery, but it is also rapidly transforming the practice of oncology into an information science. This evolution is critical, as results to-date have revealed the immense complexity and genetic heterogeneity of patients and their tumors, a sobering reminder of the challenge facing every patient and their oncologist. This can only be addressed through development of clinico-molecular data analytics that provide a deeper understanding of the mechanisms controlling the biological and clinical response to available therapeutic options. Beyond the exciting implications for improved patient care, such advancements in predictive and evidence-based analytics stand to profoundly affect the processes of cancer drug discovery and associated clinical trials.}
}
@incollection{LEI202029,
title = {2 - Fundamentals of big data in radio astronomy},
editor = {Linghe Kong and Tian Huang and Yongxin Zhu and Shenghua Yu},
booktitle = {Big Data in Astronomy},
publisher = {Elsevier},
pages = {29-58},
year = {2020},
isbn = {978-0-12-819084-5},
doi = {https://doi.org/10.1016/B978-0-12-819084-5.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128190845000109},
author = {Jiale Lei and Linghe Kong},
keywords = {Big data, Astronomy, Statistical challenges, Astronomical data analysis, Platforms for big data process},
abstract = {Large digital sky surveys are becoming the dominant source of data in astronomy. There are more than 100 terabytes of data in major archives, and that amount is growing rapidly. A typical sky survey archive has approximately 10 terabytes of image data and a billion detected sources (stars, galaxies, quasars, etc.), with hundreds of measured attributes per source. These surveys span the full range of wavelengths, radio through gamma ray, yet they are just a taste of the much larger datasets to come. Yearly advances in electronics bring new instruments that double the amount of data collected each year and lead to the exponential growth of information in astronomy. Thus, datasets that are orders of magnitude larger, more complex, and more homogeneous than in the past are on the horizon. In comparison, the size of the human genome is about 1 gigabyte and that of the Library of Congress is about 20 terabytes. Truly, astronomy has come to the big data era.}
}
@incollection{LOPES2017167,
title = {Chapter 10 - Big Data: A Practitioners Perspective},
editor = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
booktitle = {Software Architecture for Big Data and the Cloud},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {167-179},
year = {2017},
isbn = {978-0-12-805467-3},
doi = {https://doi.org/10.1016/B978-0-12-805467-3.00010-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128054673000107},
author = {Darshan Lopes and Kevin Palmer and Fiona O'Sullivan},
keywords = {Pitfalls, Considerations, Implementation, Migration pattern, Practitioner's perspective, Open source, Data warehouse},
abstract = {Big data solutions represent a significant challenge for some organizations. There are a huge variety of software products, deployment patterns and solution options that need to be considered to ensure a successful outcome for an organization trying to implement a big data solution. With that in mind, the chapter “Big Data: a practitioner's perspective” will focus on four key areas associated with big data that require consideration from a practical and implementation perspective: (i) Big Data is a new Paradigm – Differences with Traditional Data Warehouse, Pitfalls and Considerations; (ii) Product considerations for Big Data – Use of Open Source products for Big Data, Pitfalls and Considerations; (iii) Use of Cloud for hosting Big Data – Why use Cloud, Pitfalls and Considerations; and (iv) Big Data Implementation – Architecture definition, processing framework and migration patterns from Data Warehouse to Big Data.}
}
@article{KIM2022100256,
title = {Organizational process maturity model for IoT data quality management},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100256},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100256},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000480},
author = {Sunho Kim and Ricardo Pérez-Castillo and Ismael Caballero and Downgwoo Lee},
keywords = {Data quality, Data quality management, IoT, ISO 8000, Process-centric, Process reference model, Maturity, Process maturity, process attribute},
abstract = {Data quality management (DQM) is one of the most critical aspects to ensure successful applications of the Internet of Things (IoT). So far, most of the approaches for assuring data quality are typically data-centric, i.e., mainly focus on fixing data issues for specific values. However, organizations can also benefit from improving their capabilities of their DQM processes by developing organizational best DQM practices. In this regard, our investigation addresses how well organizations perform their DQM processes in the IoT domain. The main contribution of this study is to establish a framework for IoT DQM maturity. This framework is compliant with ISO 8000-61 (DQM: process reference model) and ISO 8000-62 (DQM: organizational process maturity assessment) and can be used to assess and improve the capabilities of the DQM processes for IoT data. The framework is composed of two elements. First, a process reference model (PRM) for IoT DQM is proposed by extending the PRM for DQM defined in ISO 8000-61, tailoring some existing processes and adding new ones. Second, a maturity model suitable for IoT data is proposed based on the PRM for IoT DQM. The maturity model, named IoT DQM3, is proposed by extending the maturity model defined in ISO 8000-62. However, in order to increase the usability of IoT DQM3, we consider adequate the proposition of a simplification of the IoT DQM3, by introducing a lightweight version to reduce assessment indicators and facilitate its industrial adoption. A simplified method to measure the capability of a process is also suggested considering the relationship of process attributes with the measurement stack defined in ISO 8000-63. The empirical validation of the maturity model is twofold. First, the appropriateness of the two models is surveyed with data quality experts who are currently working in various organizations around the world. Second, in order to demonstrate the feasibility of the proposal, the light-weight version is applied to a manufacturing company as a case study.}
}
@article{BROTHERS201884,
title = {Integrity, standards, and QC-related issues with big data in pre-clinical drug discovery},
journal = {Biochemical Pharmacology},
volume = {152},
pages = {84-93},
year = {2018},
issn = {0006-2952},
doi = {https://doi.org/10.1016/j.bcp.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0006295218301199},
author = {John F. Brothers and Matthew Ung and Renan Escalante-Chong and Jermaine Ross and Jenny Zhang and Yoonjeong Cha and Andrew Lysaght and Jason Funt and Rebecca Kusko},
keywords = {Big data, Genomics, Transcriptomics, RNA-seq, Microarray, Exome},
abstract = {The tremendous expansion of data analytics and public and private big datasets presents an important opportunity for pre-clinical drug discovery and development. In the field of life sciences, the growth of genetic, genomic, transcriptomic and proteomic data is partly driven by a rapid decline in experimental costs as biotechnology improves throughput, scalability, and speed. Yet far too many researchers tend to underestimate the challenges and consequences involving data integrity and quality standards. Given the effect of data integrity on scientific interpretation, these issues have significant implications during preclinical drug development. We describe standardized approaches for maximizing the utility of publicly available or privately generated biological data and address some of the common pitfalls. We also discuss the increasing interest to integrate and interpret cross-platform data. Principles outlined here should serve as a useful broad guide for existing analytical practices and pipelines and as a tool for developing additional insights into therapeutics using big data.}
}
@article{MENDESSAMPAIO20158304,
title = {DQ2S – A framework for data quality-aware information management},
journal = {Expert Systems with Applications},
volume = {42},
number = {21},
pages = {8304-8326},
year = {2015},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2015.06.050},
url = {https://www.sciencedirect.com/science/article/pii/S0957417415004522},
author = {Sandra de F. {Mendes Sampaio} and Chao Dong and Pedro Sampaio},
keywords = {Information management, Data quality, Query language extensions, Data profiling, Decision support systems, Big data},
abstract = {This paper describes the design and implementation of the Data Quality Query System (DQ2S), a query processing framework and tool incorporating data quality profiling functionality in the processing of queries involving quality-aware query language extensions. DQ2S supports the combination of performance and quality-oriented query optimizations, and a query processing platform that enables advanced data profiling queries to be formulated based on well established query language constructs, often used to interact with relational database management systems. DQ2S encompasses a declarative query language and a data model that provides users with the capability to express constraints on the quality of query results as well as query quality-related information; a set of algebraic operators for manipulating data quality-related information, and optimization heuristics. The proposed query language and algebra represent seamless extensions to SQL and relational database engines, respectively. The constructs of the proposed data model are implemented at the user’s view level and are internally mapped into relational model constructs. The quality-aware extensions and features are extremely useful when users need to assess the quality of relational data sets and define quality constraints for acceptable data prior to using candidate data sources in decision support systems and conducting big data analytical tasks.}
}
@article{PIRRACCHIO2019377,
title = {Big data and targeted machine learning in action to assist medical decision in the ICU},
journal = {Anaesthesia Critical Care & Pain Medicine},
volume = {38},
number = {4},
pages = {377-384},
year = {2019},
issn = {2352-5568},
doi = {https://doi.org/10.1016/j.accpm.2018.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S2352556818302169},
author = {Romain Pirracchio and Mitchell J Cohen and Ivana Malenica and Jonathan Cohen and Antoine Chambaz and Maxime Cannesson and Christine Lee and Matthieu Resche-Rigon and Alan Hubbard},
abstract = {Historically, personalised medicine has been synonymous with pharmacogenomics and oncology. We argue for a new framework for personalised medicine analytics that capitalises on more detailed patient-level data and leverages recent advances in causal inference and machine learning tailored towards decision support applicable to critically ill patients. We discuss how advances in data technology and statistics are providing new opportunities for asking more targeted questions regarding patient treatment, and how this can be applied in the intensive care unit to better predict patient-centred outcomes, help in the discovery of new treatment regimens associated with improved outcomes, and ultimately how these rules can be learned in real-time for the patient.}
}
@article{ALLES201644,
title = {Incorporating big data in audits: Identifying inhibitors and a research agenda to address those inhibitors},
journal = {International Journal of Accounting Information Systems},
volume = {22},
pages = {44-59},
year = {2016},
note = {2015 Research Symposium on Information Integrity & Information Systems Assurance},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2016.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1467089516300811},
author = {Michael Alles and Glen L. Gray},
keywords = {Big Data, Auditing, Accounting information systems},
abstract = {With corporate investment in Big Data of $34 billion in 2013 growing to $232 billion through 2016 (Gartner 2012), the Big 4 accounting firms are aiming to be at the forefront of Big Data implementations. Notably, they see Big Data as an increasingly essential part of their assurance practice. We argue that while there is a place for Big Data in auditing, its application to auditing is less clear than it is in the other fields, such as marketing and medical research. The objectives of this paper are to: (1) provide a discussion of both the inhibitors of incorporating Big Data into financial statement audits; and (3) present a research agenda to identify approaches to ameliorate those inhibitors.}
}
@article{LIU2021257,
title = {Massive-scale carbon pollution control and biological fusion under big data context},
journal = {Future Generation Computer Systems},
volume = {118},
pages = {257-262},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21000042},
author = {Yi Liu and Jie Xu and Weijie Yi},
keywords = {Internet-scale network, Dense subgraph mining, Low carbon, Multiple features, Biologically-aware},
abstract = {In the modern society, there are a rich number of low-carbon enterprises that the explicitly/implicitly collaborated. Effectively understanding the mechanism of their complex cooperative relationships is becoming an urgent and significant problem in information processing and management. Traditionally, these cooperation behavior are analyzed in a holistic and non-quantitative way, where the complicated relationships among various enterprises cannot be well represented. In this work, we propose to understand the low-carbon entrepreneurs’ cooperation by leveraging a massive-scale dense subgraph mining technique, based on which an evolutionary graphical model is built to dynamically represent such complex relationships. More specifically, given million-scale low-carbon enterprises, we first extract multiple biologically-aware features (e.g., production value and carbon emission) to represent each of them. Based on this, a massive-scale affinity network is constructed to characterize the relationships among these enterprises. Based on this, an efficient subgraph mining algorithm (called graph shift) is deployed to discover the neighbors for each enterprise. Finally, based on the discovered neighbors of each enterprise, we can build a graphical model to represent the relationships among explicitly/implicitly-connected enterprises. The flows of multiple attributes (benefit exchange and resources swap) can be modeled effectively. To demonstrate the usefulness of our method, we manually label the attributes of 20,000 enterprises, based on which a classification model is trained by encoding the neighboring attributes of each enterprise. Comparative results have clearly demonstrated the competitiveness of our method. Moreover, visualization results can reveal the effectiveness of our method in uncovering the intrinsic distributions/correlations of million-scale enterprises.}
}
@article{SARALADEVI2015596,
title = {Big Data and Hadoop-a Study in Security Perspective},
journal = {Procedia Computer Science},
volume = {50},
pages = {596-601},
year = {2015},
note = {Big Data, Cloud and Computing Challenges},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.04.091},
url = {https://www.sciencedirect.com/science/article/pii/S187705091500592X},
author = {B. Saraladevi and N. Pazhaniraja and P. Victer Paul and M.S. Saleem Basha and P. Dhavachelvan},
keywords = {Big data ;Hadoop ;HDFS ;Security},
abstract = {Big data is the collection and analysis of large set of data which holds many intelligence and raw information based on user data, Sensor data, Medical and Enterprise data. The Hadoop platform is used to Store, Manage, and Distribute Big data across several server nodes. This paper shows the Big data issues and focused more on security issue arises in Hadoop Architecture base layer called Hadoop Distributed File System (HDFS). The HDFS security is enhanced by using three approaches like Kerberos, Algorithm and Name node.}
}
@article{YU201621,
title = {Single-cell Transcriptome Study as Big Data},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {14},
number = {1},
pages = {21-30},
year = {2016},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2016.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1672022916000437},
author = {Pingjian Yu and Wei Lin},
keywords = {Single cell, RNA-seq, Big data, Transcriptional heterogeneity, Signal normalization},
abstract = {The rapid growth of single-cell RNA-seq studies (scRNA-seq) demands efficient data storage, processing, and analysis. Big-data technology provides a framework that facilitates the comprehensive discovery of biological signals from inter-institutional scRNA-seq datasets. The strategies to solve the stochastic and heterogeneous single-cell transcriptome signal are discussed in this article. After extensively reviewing the available big-data applications of next-generation sequencing (NGS)-based studies, we propose a workflow that accounts for the unique characteristics of scRNA-seq data and primary objectives of single-cell studies.}
}
@article{KOZIEL2021116057,
title = {Investments in data quality: Evaluating impacts of faulty data on asset management in power systems},
journal = {Applied Energy},
volume = {281},
pages = {116057},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.116057},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920314896},
author = {Sylvie Koziel and Patrik Hilber and Per Westerlund and Ebrahim Shayesteh},
keywords = {Asset management, Component replacement, Data quality costs, Electric power distribution, Optimization, Trade-off},
abstract = {Data play an essential role in asset management decisions. The amount of data is increasing through accumulating historical data records, new measuring devices, and communication technology, notably with the evolution toward smart grids. Consequently, the management of data quantity and quality is becoming even more relevant for asset managers to meet efficiency and reliability requirements for power grids. In this work, we propose an innovative data quality management framework enabling asset managers (i) to quantify the impact of poor data quality, and (ii) to determine the conditions under which an investment in data quality improvement is required. To this end, an algorithm is used to determine the optimal year for component replacement based on three scenarios, a Reference scenario, an Imperfect information scenario, and an Investment in higher data quality scenario. Our results indicate that (i) the impact on the optimal year of replacement is the highest for middle-aged components; (ii) the profitability of investments in data quality improvement depends on various factors, including data quality, and the cost of investment in data quality improvement. Finally, we discuss the implementation of the proposed models to control data quality in practice, while taking into account real-world technological and economic limitations.}
}
@article{ASSUNCAO20153,
title = {Big Data computing and clouds: Trends and future directions},
journal = {Journal of Parallel and Distributed Computing},
volume = {79-80},
pages = {3-15},
year = {2015},
note = {Special Issue on Scalable Systems for Big Data Management and Analytics},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2014.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731514001452},
author = {Marcos D. Assunção and Rodrigo N. Calheiros and Silvia Bianchi and Marco A.S. Netto and Rajkumar Buyya},
keywords = {Big Data, Cloud computing, Analytics, Data management},
abstract = {This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions.}
}
@article{WOLFERT201769,
title = {Big Data in Smart Farming – A review},
journal = {Agricultural Systems},
volume = {153},
pages = {69-80},
year = {2017},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2017.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X16303754},
author = {Sjaak Wolfert and Lan Ge and Cor Verdouw and Marc-Jeroen Bogaardt},
keywords = {Agriculture, Data, Information and communication technology, Data infrastructure, Governance, Business modelling},
abstract = {Smart Farming is a development that emphasizes the use of information and communication technology in the cyber-physical farm management cycle. New technologies such as the Internet of Things and Cloud Computing are expected to leverage this development and introduce more robots and artificial intelligence in farming. This is encompassed by the phenomenon of Big Data, massive volumes of data with a wide variety that can be captured, analysed and used for decision-making. This review aims to gain insight into the state-of-the-art of Big Data applications in Smart Farming and identify the related socio-economic challenges to be addressed. Following a structured approach, a conceptual framework for analysis was developed that can also be used for future studies on this topic. The review shows that the scope of Big Data applications in Smart Farming goes beyond primary production; it is influencing the entire food supply chain. Big data are being used to provide predictive insights in farming operations, drive real-time operational decisions, and redesign business processes for game-changing business models. Several authors therefore suggest that Big Data will cause major shifts in roles and power relations among different players in current food supply chain networks. The landscape of stakeholders exhibits an interesting game between powerful tech companies, venture capitalists and often small start-ups and new entrants. At the same time there are several public institutions that publish open data, under the condition that the privacy of persons must be guaranteed. The future of Smart Farming may unravel in a continuum of two extreme scenarios: 1) closed, proprietary systems in which the farmer is part of a highly integrated food supply chain or 2) open, collaborative systems in which the farmer and every other stakeholder in the chain network is flexible in choosing business partners as well for the technology as for the food production side. The further development of data and application infrastructures (platforms and standards) and their institutional embedment will play a crucial role in the battle between these scenarios. From a socio-economic perspective, the authors propose to give research priority to organizational issues concerning governance issues and suitable business models for data sharing in different supply chain scenarios.}
}
@article{HUANG20181413,
title = {Improving Quality of Experience in multimedia Internet of Things leveraging machine learning on big data},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {1413-1423},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.02.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17324500},
author = {Xiaohong Huang and Kun Xie and Supeng Leng and Tingting Yuan and Maode Ma},
keywords = {Data fusion, Multimedia Internet of Things, Big data, Quality of Experience, Machine learning, Neural network},
abstract = {With rapid evolution of the Internet of Things (IoT) applications on multimedia, there is an urgent need to enhance the satisfaction level of Multimedia IoT (MIoT) network users. An important and unsolved problem is automatic optimization of Quality of Experience (QoE) through collecting/managing/processing various data from MIoT network. In this paper, we propose an MIoT QoE optimization mechanism leveraging data fusion technology, called QoE optimization via Data Fusion (QoEDF). QoEDF consists of two steps. Firstly, a multimodal data fusion approach is proposed to build a QoE mapping between the uncontrollable user data with the controllable network-related system data. Secondly, an automatic QoE optimization model is built taking fused results, which is different from the traditional way. QoEDF is able to adjust network-related system data automatically so as to achieve optimized user satisfaction. Simulation results show that QoEDF will lead to significant improvements in QoE level as well as be adaptable to dynamic network changes.}
}
@article{ZHU2018107,
title = {Review and big data perspectives on robust data mining approaches for industrial process modeling with outliers and missing data},
journal = {Annual Reviews in Control},
volume = {46},
pages = {107-133},
year = {2018},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2018.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1367578818301056},
author = {Jinlin Zhu and Zhiqiang Ge and Zhihuan Song and Furong Gao},
keywords = {Data mining, Robustness, Process modeling, Statistical process monitoring, Big data analytics},
abstract = {Industrial process data are usually mixed with missing data and outliers which can greatly affect the statistical explanation abilities for traditional data-driven modeling methods. In this sense, more attention should be paid on robust data mining methods so as to investigate those stable and reliable modeling prototypes for decision-making. This paper gives a systematic review of various state-of-the-art data preprocessing tricks as well as robust principal component analysis methods for process understanding and monitoring applications. Afterwards, comprehensive robust techniques have been discussed for various circumstances with diverse process characteristics. Finally, big data perspectives on potential challenges and opportunities have been highlighted for future explorations in the community.}
}
@article{HASHEM2016748,
title = {The role of big data in smart city},
journal = {International Journal of Information Management},
volume = {36},
number = {5},
pages = {748-758},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216302778},
author = {Ibrahim Abaker Targio Hashem and Victor Chang and Nor Badrul Anuar and Kayode Adewole and Ibrar Yaqoob and Abdullah Gani and Ejaz Ahmed and Haruna Chiroma},
keywords = {Smart city, Big data, Internet of things, Smart environments, Cloud computing, Distributed computing},
abstract = {The expansion of big data and the evolution of Internet of Things (IoT) technologies have played an important role in the feasibility of smart city initiatives. Big data offer the potential for cities to obtain valuable insights from a large amount of data collected through various sources, and the IoT allows the integration of sensors, radio-frequency identification, and Bluetooth in the real-world environment using highly networked services. The combination of the IoT and big data is an unexplored research area that has brought new and interesting challenges for achieving the goal of future smart cities. These new challenges focus primarily on problems related to business and technology that enable cities to actualize the vision, principles, and requirements of the applications of smart cities by realizing the main smart environment characteristics. In this paper, we describe the state-of-the-art communication technologies and smart-based applications used within the context of smart cities. The visions of big data analytics to support smart cities are discussed by focusing on how big data can fundamentally change urban populations at different levels. Moreover, a future business model of big data for smart cities is proposed, and the business and technological research challenges are identified. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.}
}
@article{GIL201761,
title = {Big Data. New approaches of modelling and management},
journal = {Computer Standards & Interfaces},
volume = {54},
pages = {61-63},
year = {2017},
note = {SI: New modeling in Big Data},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917301022},
author = {David Gil and Il-Yeol Song and José F. Aldana and Juan Trujillo},
abstract = {Nowadays, there are a huge number of autonomous and diverse information sources providing heterogeneous data. Sensors, social media data, data on the Web, open data, just to name a few, resulting in a major confluence of Big Data. In this survey, we discuss these diverse data sources and detail the way in which data are acquired, stored, processed and analysed. Although some of the opportunities in this new state are mentioned, the main objective of this analysis is to present the challenges for Big Data. To accomplish this goal, we examine the new proposals and approaches presented in this special issue with the aim of establishing new models for improving the management of the volume, velocity, and variety, of Big Data. Some of these schemes establish the use of Ontologies, Semantic Processing, Cloud Computing and Data Management and could be seen as intelligent services integrated as context-aware services.}
}
@article{BUDHIRAJA2016241,
title = {The Role of Big Data in the Management of Sleep-Disordered Breathing},
journal = {Sleep Medicine Clinics},
volume = {11},
number = {2},
pages = {241-255},
year = {2016},
note = {Novel Approaches to the Management of Sleep-Disordered Breathing},
issn = {1556-407X},
doi = {https://doi.org/10.1016/j.jsmc.2016.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1556407X1630008X},
author = {Rohit Budhiraja and Robert Thomas and Matthew Kim and Susan Redline},
keywords = {Sleep-disordered breathing, Big data, Management, Sleep apnea}
}
@article{PAPADOPOULOS20171108,
title = {The role of Big Data in explaining disaster resilience in supply chains for sustainability},
journal = {Journal of Cleaner Production},
volume = {142},
pages = {1108-1118},
year = {2017},
note = {Special Volume on Improving natural resource management and human health to ensure sustainable societal development based upon insights gained from working within ‘Big Data Environments’},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.03.059},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616301275},
author = {Thanos Papadopoulos and Angappa Gunasekaran and Rameshwar Dubey and Nezih Altay and Stephen J. Childe and Samuel Fosso-Wamba},
keywords = {Resilience, Big Data, Sustainability, Disaster, Exploratory factor analysis, Confirmatory factor analysis},
abstract = {The purpose of this paper is to propose and test a theoretical framework to explain resilience in supply chain networks for sustainability using unstructured Big Data, based upon 36,422 items gathered in the form of tweets, news, Facebook, WordPress, Instagram, Google+, and YouTube, and structured data, via responses from 205 managers involved in disaster relief activities in the aftermath of Nepal earthquake in 2015. The paper uses Big Data analysis, followed by a survey which was analyzed using content analysis and confirmatory factor analysis (CFA). The results of the analysis suggest that swift trust, information sharing and public–private partnership are critical enablers of resilience in supply chain networks. The current study used cross-sectional data. However the hypotheses of the study can be tested using longitudinal data to attempt to establish causality. The article advances the literature on resilience in disaster supply chain networks for sustainability in that (i) it suggests the use of Big Data analysis to propose and test particular frameworks in the context of resilient supply chains that enable sustainability; (ii) it argues that swift trust, public private partnerships, and quality information sharing link to resilience in supply chain networks; and (iii) it uses the context of Nepal, at the moment of the disaster relief activities to provide contemporaneous perceptions of the phenomenon as it takes place.}
}
@article{FANG2022104070,
title = {BIM-integrated portfolio-based strategic asset data quality management},
journal = {Automation in Construction},
volume = {134},
pages = {104070},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104070},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521005215},
author = {Zigeng Fang and Yan Liu and Qiuchen Lu and Michael Pitt and Sean Hanna and Zhichao Tian},
keywords = {Strategic asset management (SAM), Building information modeling (BIM), Portfolio management, Data quality management},
abstract = {A building's strategic asset management (SAM) capability has traditionally been limited by its site-based management. With the emergence of needs from clients about delivering a long-term portfolio-based building asset management plan that minimizes the asset risk and optimizes the value of their asset portfolios, SAM Units have emerged as a new business form to provide various SAM services to their clients. However, the quality of their current data model is still hindered by many issues, such as missing important attributes and the lack of customized information flow guidance. In addition, there is a gap in integrating their existing data collection with various data sources and Building Information Modeling (BIM) to enhance their data quality. By evaluating a SAM Unit's portfolio case study, this paper identifies the factors limiting the quality of SAM Units' data model and develops a guide to integrating various data sources better. We develop a BIM-integrated portfolio-based SAM information flow framework and a detailed hierarchical portfolio-based non-geometric data structure. The proposed framework and data structure will help SAM professionals, building asset owners, and other facilities management professionals embrace the benefits of managing the portfolio-based SAM data.}
}
@article{HE201835,
title = {Statistical process monitoring as a big data analytics tool for smart manufacturing},
journal = {Journal of Process Control},
volume = {67},
pages = {35-43},
year = {2018},
note = {Big Data: Data Science for Process Control and Operations},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2017.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0959152417301257},
author = {Q. Peter He and Jin Wang},
keywords = {Statistical process monitoring, Big data, Smart manufacturing, Feature extraction, Internet of things},
abstract = {With ever-accelerating advancement of information, communication, sensing and characterization technologies, such as industrial Internet of Things (IoT) and high-throughput instruments, it is expected that the data generated from manufacturing will grow exponentially, generating so called ‘big data’. One of the focuses of smart manufacturing is to create manufacturing intelligence from real-time data to support accurate and timely decision-making. Therefore, big data analytics is expected to contribute significantly to the advancement of smart manufacturing. In this work, a roadmap of statistical process monitoring (SPM) is presented. Most recent developments in SPM are briefly reviewed and summarized. Specific challenges and potential solutions in handling manufacturing big data are discussed. We suggest that process characteristics or feature based SPM, instead of process variable based SPM, is a promising route for next generation SPM and could play a significant role in smart manufacturing. The advantages of feature based SPM are discussed to support the suggestion and future directions in SPM are discussed in the context of smart manufacturing.}
}
@article{HAMMER2017715,
title = {Profit Per Hour as a Target Process Control Parameter for Manufacturing Systems Enabled by Big Data Analytics and Industry 4.0 Infrastructure},
journal = {Procedia CIRP},
volume = {63},
pages = {715-720},
year = {2017},
note = {Manufacturing Systems 4.0 – Proceedings of the 50th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.03.094},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117302408},
author = {Markus Hammer and Ken Somers and Hugo Karre and Christian Ramsauer},
keywords = {Operations Management, Manufacturing Systems 4.0, Profit per Hour, Advanced Process Control, Big Data Analytics, Agile Manufacturing},
abstract = {The rise of Industry 4.0 and in particular Big Data analytics of production parameters offers exciting new ways for optimization. The majority of factories in process industries currently aim for example, either for output maximization, yield increase, or cost reduction. The availability of real-time data and online processing capability with advanced algorithms enables a profit per hour operational management approach. Profit per hour as a target control metric allows running factories at the optimal available operating point taking all revenue and cost drivers into account. This paper describes the suitability of profit per hour as a target process control parameter for production in process industries. The authors explain how this management approach helps to make better operational decisions, trading off yield, energy, throughput, among other factors, and the resulting cumulative benefits. They also lay out how Big Data and advanced algorithms are the key enabler to this new approach, as well as a standardized methodology for implementation. With profit per hour an agile control approach is presented which aims to optimize the performance of industrial manufacturing systems in a world of ever increasing volatility.}
}
@incollection{CHESSELL201733,
title = {Chapter 3 - Architecting to Deliver Value From a Big Data and Hybrid Cloud Architecture},
editor = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
booktitle = {Software Architecture for Big Data and the Cloud},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {33-48},
year = {2017},
isbn = {978-0-12-805467-3},
doi = {https://doi.org/10.1016/B978-0-12-805467-3.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012805467300003X},
author = {Mandy Chessell and Dan Wolfson and Tim Vincent},
keywords = {Enterprise architecture, Self-service data, Systems of insight, Data-driven security, Business-driven governance, Trust and confidence, Hybrid-cloud, Information supply chains},
abstract = {Big data and analytics, particularly when combined with the use of cloud-based deployments, can transform the operation of an organization – increasing innovation, improving time to value and decision-making. However, an organization only derives value from data and analytics when (1) the collection of big data is organized, systematic and automated and (2) the use of data and analytic insight is embedded in the organization's day-to-day operation. Often the ambition of a big data and analytics solution requires data to flow freely across an organization. This can be in direct conflict with the organization's political and process silos that exist to partition the work of the organization into manageable chunks of function and responsibility. Thus the architecture of a big data solution must accommodate the realities within the organization to ensure sufficient value is realized by all of the stakeholders that are needed to enable this data interchange. Through examples of architectures for big data and analytics solutions, we explain how the scope of a big data solution can affect its architecture and the additional components necessary when a big data solution needs to span multiple organization silos.}
}
@incollection{GONCALVESPINHO2021155,
title = {Chapter 8 - The use of Big Data in Psychiatry—The role of administrative databases},
editor = {Ahmed A. Moustafa},
booktitle = {Big Data in Psychiatry #x0026; Neurology},
publisher = {Academic Press},
pages = {155-165},
year = {2021},
isbn = {978-0-12-822884-5},
doi = {https://doi.org/10.1016/B978-0-12-822884-5.00009-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012822884500009X},
author = {Manuel Gonçalves-Pinho and Alberto Freitas},
keywords = {Administrative database, Mental Health, Secondary data, Psychiatry, Research design},
abstract = {Administrative databases (AD) are repositories of administrative and clinical data related to patient contact episodes with all sorts of health facilities (primary care, hospitals, pharmacies, etc.). The use of AD data is increasing in Mental Health research as the advantages of using AD surpass some of the difficulties Mental health researchers find when using data from other sources (clinical trials, cohort studies, etc.). The large number of patients/contact episodes available, the systematic and broad register, and the fact that AD provides real-world data are some of the pros in using AD data. There are some methodological aspects that must be addressed when using this type of databases in order to provide solid and valid results. The possibility of clinical and administrative errors in an AD is a reality when using secondary data in Mental Health Research, and diagnostic code validation studies may be performed to estimate clinical and administrative accuracy. This chapter described in detail the pros and cons of using secondary data in mental health research and specifies the methodological steps a researcher must follow in order to find valid conclusions in AD from a clinical point of view.}
}
@article{XU2018309,
title = {A Platform for Fault Diagnosis of High-Speed Train based on Big Data⁎⁎Project supported by the National Natural Science Foundation, China(61490704, 61440015) and the National High-Tech. R&D Program, China (No. 2015AA043802).},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {18},
pages = {309-314},
year = {2018},
note = {10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.318},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318320007},
author = {Quan Xu and Peng Zhang and Wenqin Liu and Qiang Liu and Changxin Liu and Liangyong Wang and Anthony Toprac and S. {Joe Qin}},
keywords = {Fault Diagnosis, High-Speed Train, Big Data, Cloud Computing, Edge Computing},
abstract = {High-speed trains are very fast (e.g. 350km/h) and operate at high traffic density, so once a fault has occurred, the consequences are disastrous. In order to better control the train operational status by timely and rapid detection of faults, we need new methods to handle and analyze the huge volumes of high-speed railway data. In this paper, we propose a novel framework and platform for high-speed train fault diagnosis based on big data technologies. The framework aims to allow researchers to focus on fault detection algorithm development and on-line application, with all the complexities of big data import, storage, management, and realtime use handled transparently by the framework. The framework uses a combination of cloud computing and edge computing and a two-level architecture that handles the massive data of train operations. The platform uses Hadoop as its basic framework and combines HDFS, HBase, Redis and MySQL database as the data storage framework. A lossless data compression method is presented to reduce the data storage space and improve data storage efficiency. In order to support various types of data analysis tasks for fault diagnosis and prognosis, the framework integrates online computation, off-line computation, stream computation, real-time computation and so on. Moreover, the platform provides fault diagnosis and prognosis as services to users and a simple case study is given to further illustrate how the basic functions of the platform are implemented.}
}
@incollection{SEBASTIANCOLEMAN202231,
title = {Chapter 2 - Organizational Data and the Five Challenges of Managing Data Quality},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {31-45},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821737500002X},
author = {Laura Sebastian-Coleman},
keywords = {Data, history of data, data quality, data management, data governance, data stewardship, data and technology, process improvement, technology strategy, culture/organization, data literacy},
abstract = {This chapter describes the five challenges in data quality management (data, process, technology, people, and culture/organization) and proposes that organizations that want to get more value and insight from their data should take a strategic approach to data quality management. This is because quality is not an accident, and it cannot be an afterthought, especially in today’s complex organizations. This chapter provides the context for Section 2 and introduces critical terminology used throughout the book.}
}
@article{LIU2019242,
title = {How Physical Exercise Level Affects Sleep Quality? Analyzing Big Data Collected from Wearables},
journal = {Procedia Computer Science},
volume = {155},
pages = {242-249},
year = {2019},
note = {The 16th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2019),The 14th International Conference on Future Networks and Communications (FNC-2019),The 9th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919309494},
author = {Xiaoli Liu and Satu Tamminen and Topi Korhonen and Juha Röning},
keywords = {Data analytics, wearables, sleep quality, statistical methods},
abstract = {Physical exercise and sleep have independent, yet synergistic, impacts on the health. However, the effects of acute exercise level on sleep quality have not been well investigated. We utilize statistical methods to investigate the differences of exercise level between the good and bad sleep nights. Our results present a complex interrelation between physical exercise and sleep quality with analyzing large personal data sets collected from wearables. As far as we know, this is the first study to investigate insights of interrelation of physical exercise and sleep quality based on a big volume of data collected from wearable devices of real users.}
}
@article{CLARK2016443,
title = {Big data and ophthalmic research},
journal = {Survey of Ophthalmology},
volume = {61},
number = {4},
pages = {443-465},
year = {2016},
issn = {0039-6257},
doi = {https://doi.org/10.1016/j.survophthal.2016.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0039625716000023},
author = {Antony Clark and Jonathon Q. Ng and Nigel Morlet and James B. Semmens},
keywords = {data linkage, clinical registry, health services research, ophthalmic epidemiology, big data},
abstract = {Large population-based health administrative databases, clinical registries, and data linkage systems are a rapidly expanding resource for health research. Ophthalmic research has benefited from the use of these databases in expanding the breadth of knowledge in areas such as disease surveillance, disease etiology, health services utilization, and health outcomes. Furthermore, the quantity of data available for research has increased exponentially in recent times, particularly as e-health initiatives come online in health systems across the globe. We review some big data concepts, the databases and data linkage systems used in eye research—including their advantages and limitations, the types of studies previously undertaken, and the future direction for big data in eye research.}
}
@article{ZERBINO2018818,
title = {Big Data-enabled Customer Relationship Management: A holistic approach},
journal = {Information Processing & Management},
volume = {54},
number = {5},
pages = {818-846},
year = {2018},
note = {In (Big) Data we trust: Value creation in knowledge organizations},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306457317300067},
author = {Pierluigi Zerbino and Davide Aloini and Riccardo Dulmin and Valeria Mininno},
keywords = {Big Data, CRM, Literature review, Critical Success Factors (CSFs), Word tree},
abstract = {This paper aims to figure out the potential impact of Big Data (BD) on Critical Success Factors (CSFs) of Customer Relationship Management (CRM). In fact, while some authors have posited a relationship between BD and CRM, literature lacks works that go into the heart of the matter. Through an extensive up-to-date in-depth literature review about CRM, twenty (20) CSFs were singled out from 104 selected papers, and organized within an ad-hoc classification framework. The consistency of the classification was checked by means of a content analysis. Evidences were discussed and linked to the BD literature, and five propositions about how BD could affect CRM CSFs were formalized. Our results suggest that BD-enabled CRM initiatives could require several changes in the pertinent CSFs. In order to get rid of the hype effect surrounding BD, we suggest to adopt an explorative approach towards them by defining a mandatory business direction through sound business cases and pilot tests. From a general standpoint, BD could be framed as an enabling factor of well-known projects, like CRM initiatives, in order to reap the benefits from the new technologies by addressing the efforts through already acknowledged management paths.}
}
@incollection{JOINER201895,
title = {Chapter 5 - Information Seeking With Big Data: Not Just the Facts},
editor = {Ida Arlene Joiner},
booktitle = {Emerging Library Technologies},
publisher = {Chandos Publishing},
pages = {95-110},
year = {2018},
series = {Chandos Information Professional Series},
isbn = {978-0-08-102253-5},
doi = {https://doi.org/10.1016/B978-0-08-102253-5.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022535000058},
author = {Ida Arlene Joiner},
keywords = {Big data, libraries, security, privacy, infrastructure, Hadoop},
abstract = {As experts at searching, retrieving, analyzing, and managing information, librarians are uniquely suited to work with big data. This chapter provides an overview of the popular big data technology. We examine what big data is, challenges and opportunities, and how it is currently being used in many industries and libraries. The chapter concludes with additional resources, some technologies for managing big data, big data terminology, and questions for further discussion.}
}
@article{NGUYEN2018254,
title = {Big data analytics in supply chain management: A state-of-the-art literature review},
journal = {Computers & Operations Research},
volume = {98},
pages = {254-264},
year = {2018},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2017.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0305054817301685},
author = {Truong Nguyen and Li ZHOU and Virginia Spiegler and Petros Ieromonachou and Yong Lin},
keywords = {Literature review, Big data, Big data analytics, Supply chain management, Research directions},
abstract = {The rapidly growing interest from both academics and practitioners in the application of big data analytics (BDA) in supply chain management (SCM) has urged the need for review of up-to-date research development in order to develop a new agenda. This review responds to the call by proposing a novel classification framework that provides a full picture of current literature on where and how BDA has been applied within the SCM context. The classification framework is structurally based on the content analysis method of Mayring (2008), addressing four research questions: (1) in what areas of SCM is BDA being applied? (2) At what level of analytics is BDA used in these SCM areas? (3) What types of BDA models are used in SCM? (4) What BDA techniques are employed to develop these models? The discussion tackling these four questions reveals a number of research gaps, which leads to future research directions.}
}
@article{ZHANG2018146,
title = {A survey on deep learning for big data},
journal = {Information Fusion},
volume = {42},
pages = {146-157},
year = {2018},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2017.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1566253517305328},
author = {Qingchen Zhang and Laurence T. Yang and Zhikui Chen and Peng Li},
keywords = {Deep learning, Big data, Stacked auto-encoders, Deep belief networks, Convolutional neural networks, Recurrent neural networks},
abstract = {Deep learning, as one of the most currently remarkable machine learning techniques, has achieved great success in many applications such as image analysis, speech recognition and text understanding. It uses supervised and unsupervised strategies to learn multi-level representations and features in hierarchical architectures for the tasks of classification and pattern recognition. Recent development in sensor networks and communication technologies has enabled the collection of big data. Although big data provides great opportunities for a broad of areas including e-commerce, industrial control and smart medical, it poses many challenging issues on data mining and information processing due to its characteristics of large volume, large variety, large velocity and large veracity. In the past few years, deep learning has played an important role in big data analytic solutions. In this paper, we review the emerging researches of deep learning models for big data feature learning. Furthermore, we point out the remaining challenges of big data deep learning and discuss the future topics.}
}
@article{WANG2016747,
title = {Towards felicitous decision making: An overview on challenges and trends of Big Data},
journal = {Information Sciences},
volume = {367-368},
pages = {747-765},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516304868},
author = {Hai Wang and Zeshui Xu and Hamido Fujita and Shousheng Liu},
keywords = {Big Data, Data deluge, Decision making, Data analysis, Data-intensive applications, Computational social science},
abstract = {The era of Big Data has arrived along with large volume, complex and growing data generated by many distinct sources. Nowadays, nearly every aspect of the modern society is impacted by Big Data, involving medical, health care, business, management and government. It has been receiving growing attention of researches from many disciplines including natural sciences, life sciences, engineering and even art & humanities. It also leads to new research paradigms and ways of thinking on the path of development. Lots of developed and under-developing tools improve our ability to make more felicitous decisions than what we have made ever before. This paper presents an overview on Big Data including four issues, namely: (i) concepts, characteristics and processing paradigms of Big Data; (ii) the state-of-the-art techniques for decision making in Big Data; (iii) felicitous decision making applications of Big Data in social science; and (iv) the current challenges of Big Data as well as possible future directions.}
}
@article{ADDOTENKORANG2016528,
title = {Big data applications in operations/supply-chain management: A literature review},
journal = {Computers & Industrial Engineering},
volume = {101},
pages = {528-543},
year = {2016},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2016.09.023},
url = {https://www.sciencedirect.com/science/article/pii/S0360835216303631},
author = {Richard Addo-Tenkorang and Petri T. Helo},
keywords = {Big data – applications and analysis, Internet of Things (IoT), Cloud computing, Master database management, Operations/supply-chain management},
abstract = {Purpose
Big data is increasingly becoming a major organizational enterprise force to reckon with in this global era for all sizes of industries. It is a trending new enterprise system or platform which seemingly offers more features for acquiring, storing and analysing voluminous generated data from various sources to obtain value-additions. However, current research reveals that there is limited agreement regarding the performance of “big data.” Therefore, this paper attempts to thoroughly investigate “big data,” its application and analysis in operations or supply-chain management, as well as the trends and perspectives in this research area. This paper is organized in the form of a literature review, discussing the main issues of “big data” and its extension into “big data II”/IoT–value-adding perspectives by proposing a value-adding framework.
Methodology/research approach
The research approach employed is a comprehensive literature review. About 100 or more peer-reviewed journal articles/conference proceedings as well as industrial white papers are reviewed. Harzing Publish or Perish software was employed to investigate and critically analyse the trends and perspectives of “big data” applications between 2010 and 2015.
Findings/results
The four main attributes or factors identified with “big data” include – big data development sources (Variety – V1), big data acquisition (Velocity – V2), big data storage (Volume – V3), and finally big data analysis (Veracity – V4). However, the study of “big data” has evolved and expanded a lot based on its application and implementation processes in specific industries in order to create value (Value-adding – V5) – “Big Data cloud computing perspective/Internet of Things (IoT)”. Hence, the four Vs of “big data” is now expanded into five Vs.
Originality/value of research
This paper presents original literature review research discussing “big data” issues, trends and perspectives in operations/supply-chain management in order to propose “Big data II” (IoT – Value-adding) framework. This proposed framework is supposed or assumed to be an extension of “big data” in a value-adding perspective, thus proposing that “big data” be explored thoroughly in order to enable industrial managers and businesses executives to make pre-informed strategic operational and management decisions for increased return-on-investment (ROI). It could also empower organizations with a value-adding stream of information to have a competitive edge over their competitors.}
}
@article{SHAHA2016725,
title = {Big Data, Big Problems: Incorporating Mission, Values, and Culture in Provider Affiliations},
journal = {Orthopedic Clinics of North America},
volume = {47},
number = {4},
pages = {725-732},
year = {2016},
note = {Sports-Related Injuries},
issn = {0030-5898},
doi = {https://doi.org/10.1016/j.ocl.2016.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0030589816300554},
author = {Steven H. Shaha and Zain Sayeed and Afshin A. Anoushiravani and Mouhanad M. El-Othmani and Khaled J. Saleh},
keywords = {Big data, Comparative effectiveness, Orthopedics, Total joint arthroplasty, Administrative database, Clinical database}
}
@article{MIKALEF2020103237,
title = {Big data and business analytics: A research agenda for realizing business value},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103237},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103237},
url = {https://www.sciencedirect.com/science/article/pii/S0378720619310687},
author = {Patrick Mikalef and Ilias O. Pappas and John Krogstie and Paul A. Pavlou}
}
@article{SAIF2018118,
title = {Performance Analysis of Big Data and Cloud Computing Techniques: A Survey},
journal = {Procedia Computer Science},
volume = {132},
pages = {118-127},
year = {2018},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.05.172},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918309062},
author = {Subia Saif and Samar Wazir},
keywords = {Big Data, Big Data Analytics (BDA), Cloud Computing, Cloud based Big Data Enterprise Solutions, Big Data Storage, Big Data Warehouse, Streaming Data, Amazon Web Services (AWS), Google Cloud Platform (GCP), IBM Cloud, Microsoft Azure},
abstract = {A cloud framework refers to the aggregation of components like development tools, middleware and database services, needed for cloud computing, which aids in developing, deploying and managing cloud based applications strenuously, consequently making it an efficacious paradigm for massive scaling of dynamically allocated resources and their complex computing. Big Data Analytics (BDA) delivers data management solutions in the cloud architecture for storing, analysing and processing a huge volume of data. This paper presents a survey for performance based comparative analysis of cloud-based big data frameworks from leading enterprises like Amazon, Google, IBM, and Microsoft, which will assist researcher, IT analysts, reader and business user in picking the framework best suited for their work ensuring success in terms of favourable outcomes.}
}
@incollection{SHARMA2019189,
title = {Chapter 8 - Why Big Data and What Is It? Basic to Advanced Big Data Journey for the Medical Industry},
editor = {Valentina E. Balas and Le Hoang Son and Sudan Jha and Manju Khari and Raghvendra Kumar},
booktitle = {Internet of Things in Biomedical Engineering},
publisher = {Academic Press},
pages = {189-212},
year = {2019},
isbn = {978-0-12-817356-5},
doi = {https://doi.org/10.1016/B978-0-12-817356-5.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128173565000103},
author = {Neha Sharma and Malini M. Patil and Madhavi Shamkuwar},
keywords = {Big data, Medical big data, Healthcare data, Medical big data analytics, Healthcare data analytics, Data analytics, Pharmacology data analytics},
abstract = {The idea of big data is mainly reflected in its dimensions, which are popularly known as the Big Vs, which stands for Volume, Variety, Velocity, and Veracity. However, the concept goes beyond the Big Vs and testing of hypotheses, to focus on data analysis, hypothesis generation, and ascertaining the progressive strength of association. Preliminary study reveals that big data analytics adopts many data mining methods, such as descriptive, diagnostic, predictive, and prescriptive analytics. This evolving technology has tremendous application in healthcare, such as surveillance of safety or disease, predictive modeling, public health, pharma data analytics, clinical data analytics, healthcare analytics, and research. Moreover, the journey of big data in the medical domain is proving to be one of the important research thrusts of recent times. Study reveals that medical data is very specific and heterogeneous due to varied data sources such as scanned images, CT scan reports, doctor prescriptions, electronic health records (EHRs), etc. Medical data analytics faces some bottlenecks due to missing data, high dimensions, bias, and limitations of the study of patients through observation. Therefore, special big data techniques are required to handle them. Besides, many ethical, legal, social, clinical, and utility challenges are also a part of the data-handling process, which makes the role of big data in the medical field very challenging. Nevertheless, big data analytics is a fuel to the healthcare system that will provide a healthier life to patients; the issues and bottlenecks when removed from the system will be a boon for the entire human race. The chapter focuses on understanding the big data characteristics in medical big data, medical big data analytics, and its various applications in the interest of society.}
}
@incollection{VALEEV2021209,
title = {Chapter 6 - Big data analytics and process safety},
editor = {Sagit Valeev and Natalya Kondratyeva},
booktitle = {Process Safety and Big Data},
publisher = {Elsevier},
pages = {209-270},
year = {2021},
isbn = {978-0-12-822066-5},
doi = {https://doi.org/10.1016/B978-0-12-822066-5.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220665000017},
author = {Sagit Valeev and Natalya Kondratyeva},
keywords = {Analytics, Machine learning, Prediction, Clustering, Classification, Regression, Time series, Text analysis, Image analysis},
abstract = {The chapter discusses the basics of big data analytics and the features of using analytical models in the field of process safety and risk management. The definition and basic principles of data analytics are necessary to understand the analytical techniques. The requirements for input data and the properties of analytical models are important for effective analytics. The concept, basic components, and varieties of machine learning are discussed. We consider such basic machine learning algorithms as clustering, classification, and regression. As advanced methods of data analytics, time series analysis methods, text analysis, and image analysis are proposed. Examples of the application of data analytics for risk management in the framework of process safety are considered.}
}
@article{BIKAKIS2019100123,
title = {Big Data Exploration, Visualization and Analytics},
journal = {Big Data Research},
volume = {18},
pages = {100123},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.100123},
url = {https://www.sciencedirect.com/science/article/pii/S2214579619302254},
author = {Nikos Bikakis and George Papastefanatos and Olga Papaemmanouil}
}
@article{BLAZQUEZ201899,
title = {Big Data sources and methods for social and economic analyses},
journal = {Technological Forecasting and Social Change},
volume = {130},
pages = {99-113},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517310946},
author = {Desamparados Blazquez and Josep Domenech},
keywords = {Big Data architecture, Forecasting, Nowcasting, Data lifecycle, Socio-economic data, Non-traditional data sources, Non-traditional analysis methods},
abstract = {The Data Big Bang that the development of the ICTs has raised is providing us with a stream of fresh and digitized data related to how people, companies and other organizations interact. To turn these data into knowledge about the underlying behavior of the social and economic agents, organizations and researchers must deal with such amount of unstructured and heterogeneous data. Succeeding in this task requires to carefully plan and organize the whole process of data analysis taking into account the particularities of the social and economic analyses, which include the wide variety of heterogeneous sources of information and a strict governance policy. Grounded on the data lifecycle approach, this paper develops a Big Data architecture that properly integrates most of the non-traditional information sources and data analysis methods in order to provide a specifically designed system for forecasting social and economic behaviors, trends and changes.}
}
@article{STOREY201750,
title = {Big data technologies and Management: What conceptual modeling can do},
journal = {Data & Knowledge Engineering},
volume = {108},
pages = {50-67},
year = {2017},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2017.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300277},
author = {Veda C. Storey and Il-Yeol Song},
abstract = {The era of big data has resulted in the development and applications of technologies and methods aimed at effectively using massive amounts of data to support decision-making and knowledge discovery activities. In this paper, the five Vs of big data, volume, velocity, variety, veracity, and value, are reviewed, as well as new technologies, including NoSQL databases that have emerged to accommodate the needs of big data initiatives. The role of conceptual modeling for big data is then analyzed and suggestions made for effective conceptual modeling efforts with respect to big data.}
}
@article{RIVERA20191,
title = {Is Big Data About to Retire Expert Knowledge? A Predictive Maintenance Study},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {24},
pages = {1-6},
year = {2019},
note = {5th IFAC Symposium on Telematics Applications TA 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.364},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319322645},
author = {Domingo Llorente Rivera and Markus R. Scholz and Christoph Bühl and Markus Krauss and Klaus Schilling},
keywords = {industrial analytics, anomaly detection, predictive maintenance, hydraulic pump, stochastic modeling},
abstract = {In this contribution, a data-driven approach towards the prediction of maintenance for the critical component of an injection molding machine is presented. We present our path from exploring and cleaning the data towards the implementation of a prediction algorithm based on kernel density estimation. We give first analytical evidence of the algorithms potential. Moreover, we compare the approach described here with our previous work where we went a model-based approach and present advantages and disadvantages of the two approaches. We try to contribute to a non-comprehensive guide on the implementation of predictive maintenance systems for industrial mass production facilities.}
}
@article{BALACHANDRAN20171112,
title = {Challenges and Benefits of Deploying Big Data Analytics in the Cloud for Business Intelligence},
journal = {Procedia Computer Science},
volume = {112},
pages = {1112-1122},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.138},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917314953},
author = {Bala M. Balachandran and Shivika Prasad},
keywords = {Cloud Computing, Big Data Analytics, Cloud Analytics, Security, Privacy, Business Intelligence, MapReduce, AaaS, CLaaS},
abstract = {Cloud computing and big data analytics are, without a doubt, two of the most important technologies to enter the mainstream IT industry in recent years. Surprisingly, the two technologies are coming together to deliver powerful results and benefits for businesses. Cloud computing is already changing the way IT services are provided by so called cloud companies and how businesses and users interact with IT resources. Big Data is a data analysis methodology enables by recent advances in information and communications technology. However, big data analysis requires a huge amount of computing resources making adoption costs of big data technology is not affordable for many small to medium enterprises. In this paper, we outline the the benefits and challenges involved in deploying big data analytics through cloud computing. We argue that cloud computing can support the storage and computing requirements of big data analytics. We discuss how the consolidation of these two dominant technologies can enhance the process of big data mining enabling businesses to improve decision-making processes. We also highlight the issues and risks that should be addressed when using a so called CLaaS, cloud-based service model.}
}
@incollection{QING2021181,
title = {Chapter 9 - Global Practice of AI and Big Data in Oil and Gas Industry},
editor = {Patrick Bangert},
booktitle = {Machine Learning and Data Science in the Oil and Gas Industry},
publisher = {Gulf Professional Publishing},
pages = {181-210},
year = {2021},
isbn = {978-0-12-820714-7},
doi = {https://doi.org/10.1016/B978-0-12-820714-7.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207147000091},
author = {Wu Qing},
keywords = {artificial intelligence, big data, digital core, digital rock physics, multiphase flow physics information, oil recovery, molecule advanced planning and scheduling, system (MAPS), crude oil selection, crude oil property prediction, process optimization, CCR unit, FCC unit, ethylene cracking unit, correlation analysis, the anomaly detection, parameters optimization analysis, prediction analysis, equipment preventive maintenance, distributed equipment health monitoring system, time series, early warning system for equipment fault monitoring, residual life prediction for equipment},
abstract = {This chapter introduces typical cases of artificial intelligence and big data application in oil and gas industry. In the upstream field, it introduces how to combine digital rock physics with big data and AI to optimize recovery efficiency. Digital core (also known as digital petrophysics—DRP) technology enables more reliable physical information about pore-scale multiphase flows to determine the cause of low recovery and provides new ways for different injection solutions to improve reservoir performance. Combined with digital rock technology and AI, we can integrate the characteristics of digital rock databases into logging and well data, and use a variety of advanced classification techniques to identify the remaining oil and gas potential. Through the multi-phase flow simulation, the multi-scale model can predict the best injection method for maximum recovery under different conditions and propose possible solutions to optimize crude oil production. In the downstream field, the application of AI and big data analysis in planning and scheduling systems, process unit optimization, preventive maintenance of equipment, and other aspects is introduced. Among them, the molecular-level advanced planning and scheduling system (MAPS) can realize the cost performance measurement under different production schemes for potential types of processable crude oil, which is conducive to more accurate selection of crude oil and prediction of crude oil properties. In addition, the whole process simulation can be used to understand the product quality changes under different crude oil blending schemes and different unit operating conditions, which is conducive to timely adjusting the product blending schemes according to economic benefits or ex-factory demands. The operation conditions of secondary units and even the properties of mixed crude oil can be deduced according to different product quality requirements. In the process of optimization, the Continuous Catalytic Reforming (CCR) unit in the refinery process, for example, introduces the application of large data analysis, including correlation analysis, single index detection, multidimensional data anomaly detection, and the parameters of the single objective optimization, a multi-objective parameter optimization analysis, unstructured data analysis, and forecast analysis based on material properties. Good practices in CCR units have also been extended to other oil refining and chemical units, such as Fluid Catalytic Cracking (FCC) and ethylene cracking. In terms of equipment preventive maintenance, it introduced how to integrated application of Internet of things, deep machine learning, knowledge map and other technology to build real-time and on-line distributed equipment health monitoring and early warning system, for early detection of equipment hidden danger, early warning, early treatment of providing effective means, guarantee equipment run healthy and stable for a long period of time, to reduce unplanned downtime losses. In particular, the establishment of equipment prediction model based on time series and AI can realize effective monitoring and early warning of equipment faults such as shaft displacement, shaft fracture, shell cracking, power overload, and prediction of equipment remaining life.}
}
@article{GUNTHER2017191,
title = {Debating big data: A literature review on realizing value from big data},
journal = {The Journal of Strategic Information Systems},
volume = {26},
number = {3},
pages = {191-209},
year = {2017},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2017.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0963868717302615},
author = {Wendy Arianne Günther and Mohammad H. {Rezazade Mehrizi} and Marleen Huysman and Frans Feldberg},
keywords = {Big data, Analytics, Literature review, Value realization, Portability, Interconnectivity},
abstract = {Big data has been considered to be a breakthrough technological development over recent years. Notwithstanding, we have as yet limited understanding of how organizations translate its potential into actual social and economic value. We conduct an in-depth systematic review of IS literature on the topic and identify six debates central to how organizations realize value from big data, at different levels of analysis. Based on this review, we identify two socio-technical features of big data that influence value realization: portability and interconnectivity. We argue that, in practice, organizations need to continuously realign work practices, organizational models, and stakeholder interests in order to reap the benefits from big data. We synthesize the findings by means of an integrated model.}
}
@article{YANG20141563,
title = {A spatiotemporal compression based approach for efficient big data processing on Cloud},
journal = {Journal of Computer and System Sciences},
volume = {80},
number = {8},
pages = {1563-1583},
year = {2014},
note = {Special Issue on Theory and Applications in Parallel and Distributed Computing Systems},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2014.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S002200001400066X},
author = {Chi Yang and Xuyun Zhang and Changmin Zhong and Chang Liu and Jian Pei and Kotagiri Ramamohanarao and Jinjun Chen},
keywords = {Big data, Graph data, Spatiotemporal compression, Cloud computing, Scheduling},
abstract = {It is well known that processing big graph data can be costly on Cloud. Processing big graph data introduces complex and multiple iterations that raise challenges such as parallel memory bottlenecks, deadlocks, and inefficiency. To tackle the challenges, we propose a novel technique for effectively processing big graph data on Cloud. Specifically, the big data will be compressed with its spatiotemporal features on Cloud. By exploring spatial data correlation, we partition a graph data set into clusters. In a cluster, the workload can be shared by the inference based on time series similarity. By exploiting temporal correlation, in each time series or a single graph edge, temporal data compression is conducted. A novel data driven scheduling is also developed for data processing optimisation. The experiment results demonstrate that the spatiotemporal compression and scheduling achieve significant performance gains in terms of data size and data fidelity loss.}
}
@article{TIWARI2018319,
title = {Big data analytics in supply chain management between 2010 and 2016: Insights to industries},
journal = {Computers & Industrial Engineering},
volume = {115},
pages = {319-330},
year = {2018},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217305508},
author = {Sunil Tiwari and H.M. Wee and Yosef Daryanto},
keywords = {Big data analytics, Supply chain management, Big data application},
abstract = {This paper investigates big data analytics research and application in supply chain management between 2010 and 2016 and provides insights to industries. In recent years, the amount of data produced from end-to-end supply chain management practices has increased exponentially. Moreover, in current competitive environment supply chain professionals are struggling in handling the huge data. They are surveying new techniques to investigate how data are produced, captured, organized and analyzed to give valuable insights to industries. Big Data analytics is one of the best techniques which can help them in overcoming their problem. Realizing the promising benefits of big data analytics in the supply chain has motivated us to write a review on the importance/impact of big data analytics and its application in supply chain management. First, we discuss big data analytics individually, and then we discuss the role of big data analytics in supply chain management (supply chain analytics). Current research and application are also explored. Finally, we outline the insights to industries. Observations and insights from this paper could provide the guideline for academia and practitioners in implementing big data analytics in different aspects of supply chain management.}
}
@article{WANG20183,
title = {Big data analytics: Understanding its capabilities and potential benefits for healthcare organizations},
journal = {Technological Forecasting and Social Change},
volume = {126},
pages = {3-13},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2015.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516000500},
author = {Yichuan Wang and LeeAnn Kung and Terry Anthony Byrd},
keywords = {Big data analytics, Big data analytics architecture, Big data analytics capabilities, Business value of information technology (IT), Health care},
abstract = {To date, health care industry has not fully grasped the potential benefits to be gained from big data analytics. While the constantly growing body of academic research on big data analytics is mostly technology oriented, a better understanding of the strategic implications of big data is urgently needed. To address this lack, this study examines the historical development, architectural design and component functionalities of big data analytics. From content analysis of 26 big data implementation cases in healthcare, we were able to identify five big data analytics capabilities: analytical capability for patterns of care, unstructured data analytical capability, decision support capability, predictive capability, and traceability. We also mapped the benefits driven by big data analytics in terms of information technology (IT) infrastructure, operational, organizational, managerial and strategic areas. In addition, we recommend five strategies for healthcare organizations that are considering to adopt big data analytics technologies. Our findings will help healthcare organizations understand the big data analytics capabilities and potential benefits and support them seeking to formulate more effective data-driven analytics strategies.}
}
@article{SHUKLA20191015,
title = {Next generation smart sustainable auditing systems using Big Data Analytics: Understanding the interaction of critical barriers},
journal = {Computers & Industrial Engineering},
volume = {128},
pages = {1015-1026},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.04.055},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218301992},
author = {Manish Shukla and Lana Mattar},
keywords = {Big Data Analytics, Sustainable auditing systems, Barriers, RSPO, Interpretive Structural Modelling},
abstract = {In the current scenario, sustainable auditing, for example roundtable of sustainable palm oil (RSPO), requires a huge amount of data to be manually collected and entered into paper forms by farmers. Such systems are inherently inefficient, time-consuming, and, prone to errors. Researchers have proposed Big Data Analytics (BDA) based framework for next-generation smart sustainable auditing systems. Though theoretically feasible, real-life implementation of such frameworks is extremely difficult. Thus, this paper aims to identify the critical barriers that hinder the application of BDA based smart sustainable auditing system. It also aims to explore the dynamic interrelations among the barriers. We applied Interpretive Structural Modelling (ISM) approach to develop the model that extrapolates BDA adoption barriers and their relationships. The proposed model illustrates how barriers are spread over various levels and how specific barriers impact other barriers through direct and/or transitive links. This study provides practitioners with a roadmap to prioritise the interventions to facilitate the adoption of BDA in the sustainable auditing systems. Insights of this study could be used by academics to enhance understanding of the barriers to BDA applications.}
}
@article{ZUO2018839,
title = {Using big data from air quality monitors to evaluate indoor PM2.5 exposure in buildings: Case study in Beijing},
journal = {Environmental Pollution},
volume = {240},
pages = {839-847},
year = {2018},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2018.05.030},
url = {https://www.sciencedirect.com/science/article/pii/S0269749118307681},
author = {JinXing Zuo and Wei Ji and YuJie Ben and Muhammad Azher Hassan and WenHong Fan and Liam Bates and ZhaoMin Dong},
keywords = {Indoor PM, Infiltration factor, Indoor/outdoor ratio, Beijing},
abstract = {Due to time- and expense- consuming of conventional indoor PM2.5 (particulate matter with aerodynamic diameter of less than 2.5 μm) sampling, the sample size in previous studies was generally small, which leaded to high heterogeneity in indoor PM2.5 exposure assessment. Based on 4403 indoor air monitors in Beijing, this study evaluated indoor PM2.5 exposure from 15th March 2016 to 14th March 2017. Indoor PM2.5 concentration in Beijing was estimated to be 38.6 ± 18.4 μg/m3. Specifically, the concentration in non-heating season was 34.9 ± 15.8 μg/m3, which was 24% lower than that in heating season (46.1 ± 21.2 μg/m3). A significant correlation between indoor and ambient PM2.5 (p < 0.05) was evident with an infiltration factor of 0.21, and the ambient PM2.5 contributed approximately 52% and 42% to indoor PM2.5 for non-heating and heating seasons, respectively. Meanwhile, the mean indoor/outdoor (I/O) ratio was estimated to be 0.73 ± 0.54. Finally, the adjusted PM2.5 exposure level integrating the indoor and outdoor impact was calculated to be 46.8 ± 27.4 μg/m3, which was approximately 42% lower than estimation only relied on ambient PM2.5 concentration. This study is the first attempt to employ big data from commercial air monitors to evaluate indoor PM2.5 exposure and risk in Beijing, which may be instrumental to indoor PM2.5 pollution control.}
}
@article{ELRAGAL2014242,
title = {ERP and Big Data: The Inept Couple},
journal = {Procedia Technology},
volume = {16},
pages = {242-249},
year = {2014},
note = {CENTERIS 2014 - Conference on ENTERprise Information Systems / ProjMAN 2014 - International Conference on Project MANagement / HCIST 2014 - International Conference on Health and Social Care Information Systems and Technologies},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.10.089},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314003168},
author = {Ahmed Elragal},
keywords = {ERP, Big Data, Research Agenda},
abstract = {The world is witnessing an unprecedented interest in big data. Big data is data that is big in size (volume), big in variety (structured; semi-structured; unstructured), and big in speed of change (velocity). It was reported that almost 90% of the data worldwide was just created in the past 2 years. Therefore, this paper is an attempt to align ERP systems with big data. The objective is to suggest a future research agenda to bring together big data and ERP. While almost everyone is talking about big data at the product or tool level, relationship with social media, relationship with Internet of things, etc. no one has tried to integrate big data and ERP. A research agenda is discussed and introduced in this paper.}
}
@article{BONNEL20181,
title = {Transport survey methods - in the era of big data facing new and old challenges},
journal = {Transportation Research Procedia},
volume = {32},
pages = {1-15},
year = {2018},
note = {Transport Survey Methods in the era of big data:facing the challenges},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352146518301522},
author = {Patrick Bonnel and Marcela A. Munizaga},
keywords = {ISCTSC, transport, survey methodology, big data},
abstract = {This document presents an introduction to the ISCTSC Special Issue of Transport Research Procedia. It synthesizes the discussions held at the 11th International Conference on Transport Survey Methods, and describes the contents of the selected contributions. This conference has been held in different countries from all over the world, involving an increasing group of enthusiastic and generous specialists, willing to share their knowledge. This 11th conference was an opportunity to discuss the state of the art on transport survey methods, but also to question the way transport surveys are conducted in the era of big data. We took the opportunity to identify the main challenges, and the most important questions.}
}
@article{SIDDIQA2016151,
title = {A survey of big data management: Taxonomy and state-of-the-art},
journal = {Journal of Network and Computer Applications},
volume = {71},
pages = {151-166},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1084804516300583},
author = {Aisha Siddiqa and Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Mohsen Marjani and Shahabuddin Shamshirband and Abdullah Gani and Fariza Nasaruddin},
keywords = {Big data management, Storage, Big data, Processing, Security},
abstract = {The rapid growth of emerging applications and the evolution of cloud computing technologies have significantly enhanced the capability to generate vast amounts of data. Thus, it has become a great challenge in this big data era to manage such voluminous amount of data. The recent advancements in big data techniques and technologies have enabled many enterprises to handle big data efficiently. However, these advances in techniques and technologies have not yet been studied in detail and a comprehensive survey of this domain is still lacking. With focus on big data management, this survey aims to investigate feasible techniques of managing big data by emphasizing on storage, pre-processing, processing and security. Moreover, the critical aspects of these techniques are analyzed by devising a taxonomy in order to identify the problems and proposals made to alleviate these problems. Furthermore, big data management techniques are also summarized. Finally, several future research directions are presented.}
}
@article{JIA20191652,
title = {Opportunities and challenges of using big data for global health},
journal = {Science Bulletin},
volume = {64},
number = {22},
pages = {1652-1654},
year = {2019},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2019.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S2095927319305523},
author = {Peng Jia and Hong Xue and Shiyong Liu and Hao Wang and Lijian Yang and Therese Hesketh and Lu Ma and Hongwei Cai and Xin Liu and Yaogang Wang and Youfa Wang}
}
@article{REY201837,
title = {Causes of deaths data, linkages and big data perspectives},
journal = {Journal of Forensic and Legal Medicine},
volume = {57},
pages = {37-40},
year = {2018},
note = {Thematic section: Big dataGuest editor: Thomas LefèvreThematic section: Health issues in police custodyGuest editors: Patrick Chariot and Steffen Heide},
issn = {1752-928X},
doi = {https://doi.org/10.1016/j.jflm.2016.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1752928X16301652},
author = {Grégoire Rey and Karim Bounebache and Claire Rondet},
keywords = {Causes of death data, Data linkages, Big data},
abstract = {The study of cause-specific mortality data is one of the main sources of information for public health monitoring. In most industrialized countries, when a death occurs, it is a legal requirement that a medical certificate based on the international form recommended by World Health Organization's (WHO) is filled in by a physician. The physician reports the causes of death that directly led or contributed to the death on the death certificate. The death certificate is then forwarded to a coding office, where each cause is coded, and one underlying cause is defined, using the rules of the International Classification of Diseases and Related Health Problems, now in its 10th Revision (ICD-10). Recently, a growing number of countries have adopted, or have decided to adopt, the coding software Iris, developed and maintained by an international consortium1. This whole standardized production process results in a high and constantly increasing international comparability of cause-specific mortality data. While these data could be used for international comparisons and benchmarking of global burden of diseases, quality of care and prevention policies, there are also many other ways and methods to explore their richness, especially when they are linked with other data sources. Some of these methods are potentially referring to the so-called “big data” field. These methods could be applied both to the production of the data, to the statistical processing of the data, and even more to process these data linked to other databases. In the present note, we depict the main domains in which this new field of methods could be applied. We focus specifically on the context of France, a 65 million inhabitants country with a centralized health data system. Finally we will insist on the importance of data quality, and the specific problematics related to death certification in the forensic medicine domain.}
}
@article{SHIN2016837,
title = {Demystifying big data: Anatomy of big data developmental process},
journal = {Telecommunications Policy},
volume = {40},
number = {9},
pages = {837-854},
year = {2016},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2015.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0308596115000567},
author = {Dong-Hee Shin},
keywords = {Big data, Data ecosystem, Normalization, Normalization process theory, Big data user, Big data user experience},
abstract = {This study seeks to understand big data ecology, how it is perceived by different stakeholders, the potential value and challenges, and the implications for the private sector and public organizations, as well as for policy makers. With Normalization Process Theory in place, this study conducts socio-technical evaluation on the big data phenomenon to understand the developmental processes through which new practices of thinking and enacting are implemented, embedded, and integrated in South Korea. It also undertakes empirical analyses of user modeling to explore the factors influencing users׳ adoption of big data by integrating cognitive motivations as well as user values as the primary determining factors. Based on the qualitative and quantitative findings, this study concludes that big data should be developed with user-centered ideas and that users should be the focus of big data design.}
}
@article{AHMAD2016439,
title = {An efficient divide-and-conquer approach for big data analytics in machine-to-machine communication},
journal = {Neurocomputing},
volume = {174},
pages = {439-453},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.04.109},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215012369},
author = {Awais Ahmad and Anand Paul and M. Mazhar Rathore},
keywords = {M2M, Big Data, Divide-and-conquer, Data fusion domain},
abstract = {Machine-to-Machine (M2M) communication relies on the physical objects (e.g., satellites, sensors, and so forth) interconnected with each other, creating mesh of machines producing massive volume of data about large geographical area (e.g., living and non-living environment). Thus, the M2M is an ideal example of Big Data. On the contrary, the M2M platforms that handle Big Data might perform poorly or not according to the goals of their operator (in term of cost, database utilization, data quality, processing and computational efficiency, analysis and feature extraction applications). Therefore, to address the aforementioned needs, we propose a new effective, memory and processing efficient system architecture for Big Data in M2M, which, unlike other previous proposals, does not require whole set of data to be processed (including raw data sets), and to be kept in the main memory. Our designed system architecture exploits divide-and-conquer approach and data block-wise vertical representation of the database follows a particular petitionary strategy, which formalizes the problem of feature extraction applications. The architecture goes from physical objects to the processing servers, where Big Data set is first transformed into a several data blocks that can be quickly processed, then it classifies and reorganizes these data blocks from the same source. In addition, the data blocks are aggregated in a sequential manner based on a machine ID, and equally partitions the data using fusion algorithm. Finally, the results are stored in a server that helps the users in making decision. The feasibility and efficiency of the proposed system architecture are implemented on Hadoop single node setup on UBUNTU 14.04 LTS core™i5 machine with 3.2GHz processor and 4GB memory. The results show that the proposed system architecture efficiently extract various features (such as River) from the massive volume of satellite data.}
}
@article{HONG20191387,
title = {Energy forecasting in the big data world},
journal = {International Journal of Forecasting},
volume = {35},
number = {4},
pages = {1387-1388},
year = {2019},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019301062},
author = {Tao Hong and Pierre Pinson}
}
@article{GU201722,
title = {Visualizing the knowledge structure and evolution of big data research in healthcare informatics},
journal = {International Journal of Medical Informatics},
volume = {98},
pages = {22-32},
year = {2017},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2016.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1386505616302556},
author = {Dongxiao Gu and Jingjing Li and Xingguo Li and Changyong Liang},
keywords = {Big data, Healthcare informatics, Bibliometrics, Knowledge structure, Knowledge management},
abstract = {Background
In recent years, the literature associated with healthcare big data has grown rapidly, but few studies have used bibliometrics and a visualization approach to conduct deep mining and reveal a panorama of the healthcare big data field.
Methods
To explore the foundational knowledge and research hotspots of big data research in the field of healthcare informatics, this study conducted a series of bibliometric analyses on the related literature, including papers’ production trends in the field and the trend of each paper’s co-author number, the distribution of core institutions and countries, the core literature distribution, the related information of prolific authors and innovation paths in the field, a keyword co-occurrence analysis, and research hotspots and trends for the future.
Results
By conducting a literature content analysis and structure analysis, we found the following: (a) In the early stage, researchers from the United States, the People’s Republic of China, the United Kingdom, and Germany made the most contributions to the literature associated with healthcare big data research and the innovation path in this field. (b) The innovation path in healthcare big data consists of three stages: the disease early detection, diagnosis, treatment, and prognosis phase, the life and health promotion phase, and the nursing phase. (c) Research hotspots are mainly concentrated in three dimensions: the disease dimension (e.g., epidemiology, breast cancer, obesity, and diabetes), the technical dimension (e.g., data mining and machine learning), and the health service dimension (e.g., customized service and elderly nursing).
Conclusion
This study will provide scholars in the healthcare informatics community with panoramic knowledge of healthcare big data research, as well as research hotspots and future research directions.}
}
@article{HERRMANN2022194,
title = {An ERP Data Quality Assessment Framework for the Implementation of an APS system using Bayesian Networks},
journal = {Procedia Computer Science},
volume = {200},
pages = {194-204},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.218},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002277},
author = {Jan-Phillip Herrmann and Sven Tackenberg and Elio Padoano and Jörg Hartlief and Jens Rautenstengel and Christine Loeser and Jörg Böhme},
keywords = {Data Quality Assessment, Advanced Planning, Scheduling, Bayesian Network, Enterprise Resource Planning},
abstract = {In today’s manufacturing industry, enterprise-resource-planning (ERP) systems reach their limit when planning and scheduling production subject to multiple objectives and constraints. Advanced planning and scheduling (APS) systems provide these capabilities and are an extension for ERP systems. However, when integrating an APS and ERP system, the ERP data frequently lacks quality, hindering the APS system from working as required. This paper introduces a data quality (DQ) assessment framework that employs a Bayesian Network (BN) to perform quick DQ assessments based on expert interviews and DQ measurements with actual ERP data. We explain the BN’s functionality, design, and validation and show how using the perceived DQ of experts and a semi-supervised learning algorithm improves the BN’s predictions over time. We discuss applying our framework in an APS system implementation project involving an APS system provider and a medium-sized manufacturer of hydraulic cylinders. Despite considering the DQ assessment framework in such a specific context, it is not restricted to a particular domain. We close by discussing the framework’s limits, particularly the BN as a DQ assessment methodology and future works to improve its performance.}
}
@article{SANTOS2017750,
title = {A Big Data system supporting Bosch Braga Industry 4.0 strategy},
journal = {International Journal of Information Management},
volume = {37},
number = {6},
pages = {750-760},
year = {2017},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2017.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217306023},
author = {Maribel Yasmina Santos and Jorge {Oliveira e Sá} and Carina Andrade and Francisca {Vale Lima} and Eduarda Costa and Carlos Costa and Bruno Martinho and João Galvão},
keywords = {Big Data, Industry 4.0, Big Data analytics, Big Data architecture, Bosch},
abstract = {People, devices, infrastructures and sensors can constantly communicate exchanging data and generating new data that trace many of these exchanges. This leads to vast volumes of data collected at ever increasing velocities and of different variety, a phenomenon currently known as Big Data. In particular, recent developments in Information and Communications Technologies are pushing the fourth industrial revolution, Industry 4.0, being data generated by several sources like machine controllers, sensors, manufacturing systems, among others. Joining volume, variety and velocity of data, with Industry 4.0, makes the opportunity to enhance sustainable innovation in the Factories of the Future. In this, the collection, integration, storage, processing and analysis of data is a key challenge, being Big Data systems needed to link all the entities and data needs of the factory. Thereby, this paper addresses this key challenge, proposing and implementing a Big Data Analytics architecture, using a multinational organisation (Bosch Car Multimedia – Braga) as a case study. In this work, all the data lifecycle, from collection to analysis, is handled, taking into consideration the different data processing speeds that can exist in the real environment of a factory (batch or stream).}
}
@article{DESILVA2021104305,
title = {Clinical notes as prognostic markers of mortality associated with diabetes mellitus following critical care: A retrospective cohort analysis using machine learning and unstructured big data},
journal = {Computers in Biology and Medicine},
volume = {132},
pages = {104305},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104305},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521000998},
author = {Kushan {De Silva} and Noel Mathews and Helena Teede and Andrew Forbes and Daniel Jönsson and Ryan T. Demmer and Joanne Enticott},
keywords = {Critical care, Diabetes, Electronic health records, LASSO, Machine learning, Mortality, Natural language processing, Prognosis, Text mining},
abstract = {Background
Clinical notes are ubiquitous resources offering potential value in optimizing critical care via data mining technologies.
Objective
To determine the predictive value of clinical notes as prognostic markers of 1-year all-cause mortality among people with diabetes following critical care.
Materials and methods
Mortality of diabetes patients were predicted using three cohorts of clinical text in a critical care database, written by physicians (n = 45253), nurses (159027), and both (n = 204280). Natural language processing was used to pre-process text documents and LASSO-regularized logistic regression models were trained and tested. Confusion matrix metrics of each model were calculated and AUROC estimates between models were compared. All predictive words and corresponding coefficients were extracted. Outcome probability associated with each text document was estimated.
Results
Models built on clinical text of physicians, nurses, and the combined cohort predicted mortality with AUROC of 0.996, 0.893, and 0.922, respectively. Predictive performance of the models significantly differed from one another whereas inter-rater reliability ranged from substantial to almost perfect across them. Number of predictive words with non-zero coefficients were 3994, 8159, and 10579, respectively, in the models of physicians, nurses, and the combined cohort. Physicians’ and nursing notes, both individually and when combined, strongly predicted 1-year all-cause mortality among people with diabetes following critical care.
Conclusion
Clinical notes of physicians and nurses are strong and novel prognostic markers of diabetes-associated mortality in critical care, offering potentially generalizable and scalable applications. Clinical text-derived personalized risk estimates of prognostic outcomes such as mortality could be used to optimize patient care.}
}
@incollection{APPEL2021193,
title = {Chapter 6 - Psychological targeting in the age of Big Data},
editor = {Dustin Wood and Stephen J. Read and P.D. Harms and Andrew Slaughter},
booktitle = {Measuring and Modeling Persons and Situations},
publisher = {Academic Press},
pages = {193-222},
year = {2021},
isbn = {978-0-12-819200-9},
doi = {https://doi.org/10.1016/B978-0-12-819200-9.00015-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128192009000156},
author = {Ruth E. Appel and Sandra C. Matz},
keywords = {Psychological targeting, Psychological profiling, Psychologically-informed interventions, Big Data, Digital footprints, Methods, Ethics, Privacy, Contextual integrity},
abstract = {Advances in the collection, storage, and processing of large amounts of user data have given rise to psychological targeting, which we define as the process of extracting individuals’ psychological characteristics from their digital footprints in order to target them with psychologically-informed interventions at scale. In this chapter, we introduce a two-stage framework of psychological targeting consisting of (1) psychological profiling and (2) psychologically-informed interventions. We summarize the most important research findings in relation to the two stages and discuss important methodological opportunities and pitfalls. To help researchers make the most of the opportunities, we also provide practical advice on how to deal with some of the potential pitfalls. Finally, we highlight ethical opportunities and challenges and offer some suggestions for addressing these challenges. If done right, psychological targeting has the potential to advance our scientific understanding of human nature and to enhance the well-being of individuals and society at large.}
}
@article{KARIM2020942,
title = {Big data management in participatory sensing: Issues, trends and future directions},
journal = {Future Generation Computer Systems},
volume = {107},
pages = {942-955},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17311627},
author = {Ahmad Karim and Aisha Siddiqa and Zanab Safdar and Maham Razzaq and Syeda Anum Gillani and Huma Tahir and Sana Kiran and Ejaz Ahmed and Muhammad Imran},
keywords = {Participatory sensing, Big data, Big data management, Big data analytics, Mobile cloud computing},
abstract = {Participatory sensing has become an emerging technology of this era owing to its low cost in big sensor data collection. Prior to participatory sensing, large-scale deployment complexities were found in wireless sensor networks when collecting data from widespread resources. Participatory sensing systems employ handheld devices as sensors to collect data from communities and transmit to the cloud, where data are further analyzed by expert systems. The processes involved in participatory sensing, such as data collection, transmission, analysis, and visualization, exhibit certain management issues. This study aims to identify big data management issues that must be addressed at the cloud side during data processing and storing and at the participant side during data collection and visualization. It then proposes a framework for big data management in participatory sensing to resolve the contemporary big data management issues on the basis of suggested principles. Moreover, this work presents case studies to elaborate the existence of the highlighted issues. Finally, the limitations, recommendations, and future research directions for academia and industry in the domain of participatory sensing are discussed.}
}
@article{SANCHEZMARTINEZ2016236,
title = {Workshop 5 report: Harnessing big data},
journal = {Research in Transportation Economics},
volume = {59},
pages = {236-241},
year = {2016},
note = {Competition and Ownership in Land Passenger Transport (selected papers from the Thredbo 14 conference)},
issn = {0739-8859},
doi = {https://doi.org/10.1016/j.retrec.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0739885916301494},
author = {Gabriel E. Sánchez-Martínez and Marcela Munizaga},
keywords = {Big data, Measurement, Implementation challenges, Analysis tools, Transit best practices},
abstract = {A group of researchers, consultants, software developers, and transit agencies convened in Santiago, Chile over 3 days as part of the Thredbo workshop titled “Harnessing Big Data”, to present their recent research and discuss the state of practice, state of the art, and future directions of big data in public transportation. This report documents their discussion. The key conclusion of the workshop is that, although much progress has been made in utilizing big data to improve transportation planning and operations, much remains to be done, both in terms of developing further analysis tools and use cases of big data, and of disseminating best practices so that they are adopted across the industry.}
}
@article{VANDENBROEK2018330,
title = {Governance of big data collaborations: How to balance regulatory compliance and disruptive innovation},
journal = {Technological Forecasting and Social Change},
volume = {129},
pages = {330-338},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.09.040},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517314695},
author = {Tijs {van den Broek} and Anne Fleur {van Veenstra}},
keywords = {Disruptive innovation, Data protection regulation, Big data, Governance, Inter-organizational collaboration},
abstract = {Big data is an important driver of disruptive innovation that may increase organizations' competitive advantage. To create innovative data combinations and decrease investments, big data is often shared among organizations, crossing organizational boundaries. However, these big data collaborations need to balance disruptive innovation and compliance to a strict data protection regime in the EU. This paper investigates how inter-organizational big data collaborations arrange and govern their activities in the context of this dilemma. We conceptualize big data as inter-organizational systems and build on IS and Organization Theory literature to develop four archetypical governance arrangements: Market, Hierarchy, Bazaar and Network. Subsequently, these arrangements are investigated in four big data collaboration use cases. The contributions of this study to literature are threefold. First, we conceptualize the organization behind big data collaborations as IOS governance. Second, we show that the choice for an inter-organizational governance arrangement highly depends on the institutional pressure from regulation and the type of data that is shared. In this way, we contribute to the limited body of research on the antecedents of IOS governance. Last, we highlight with four use cases how the principles of big data, specifically data maximization, clash with the principles of EU data protection regulation. Practically, our study provides guidelines for IT and innovation managers how to arrange and govern the sharing of data among multiple organizations.}
}
@article{PALANISAMY2019415,
title = {Implications of big data analytics in developing healthcare frameworks – A review},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {31},
number = {4},
pages = {415-425},
year = {2019},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2017.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1319157817302938},
author = {Venketesh Palanisamy and Ramkumar Thirunavukarasu},
keywords = {Big data, Healthcare, Framework, Infrastructure, Analytics, Patterns, Tools},
abstract = {The domain of healthcare acquired its influence by the impact of big data since the data sources involved in the healthcare organizations are well-known for their volume, heterogeneous complexity and high dynamism. Though the role of big data analytical techniques, platforms, tools are realized among various domains, their impact on healthcare organization for implementing and delivering novel use-cases for potential healthcare applications shows promising research directions. In the context of big data, the success of healthcare applications solely depends on the underlying architecture and utilization of appropriate tools as evidenced in pioneering research attempts. Novel research works have been carried out for deriving application specific healthcare frameworks that offer diversified data analytical capabilities for handling sources of data ranging from electronic health records to medical images. In this paper, we have presented various analytical avenues that exist in the patient-centric healthcare system from the perspective of various stakeholders. We have also reviewed various big data frameworks with respect to underlying data sources, analytical capability and application areas. In addition, the implication of big data tools in developing healthcare eco system is also presented.}
}
@article{RICHTER201929,
title = {Efficient learning from big data for cancer risk modeling: A case study with melanoma},
journal = {Computers in Biology and Medicine},
volume = {110},
pages = {29-39},
year = {2019},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2019.04.039},
url = {https://www.sciencedirect.com/science/article/pii/S0010482519301477},
author = {Aaron N. Richter and Taghi M. Khoshgoftaar},
keywords = {Big data, Cloud computing, Machine learning, Electronic health records, Early detection of cancer},
abstract = {Background
Building cancer risk models from real-world data requires overcoming challenges in data preprocessing, efficient representation, and computational performance. We present a case study of a cloud-based approach to learning from de-identified electronic health record data and demonstrate its effectiveness for melanoma risk prediction.
Methods
We used a hybrid distributed and non-distributed approach to computing in the cloud: distributed processing with Apache Spark for data preprocessing and labeling, and non-distributed processing for machine learning model training with scikit-learn. Moreover, we explored the effects of sampling the training dataset to improve computational performance. Risk factors were evaluated using regression weights as well as tree SHAP values.
Results
Among 4,061,172 patients who did not have melanoma through the 2016 calendar year, 10,129 were diagnosed with melanoma within one year. A gradient-boosted classifier achieved the best predictive performance with cross-validation (AUC = 0.799, Sensitivity = 0.753, Specificity = 0.688). Compared to a model built on the original data, a dataset two orders of magnitude smaller could achieve statistically similar or better performance with less than 1% of the training time and cost.
Conclusions
We produced a model that can effectively predict melanoma risk for a diverse dermatology population in the U.S. by using hybrid computing infrastructure and data sampling. For this de-identified clinical dataset, sampling approaches significantly shortened the time for model building while retaining predictive accuracy, allowing for more rapid machine learning model experimentation on familiar computing machinery. A large number of risk factors (>300) were required to produce the best model.}
}
@article{ARUNACHALAM2018416,
title = {Understanding big data analytics capabilities in supply chain management: Unravelling the issues, challenges and implications for practice},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {114},
pages = {416-436},
year = {2018},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1366554516303799},
author = {Deepak Arunachalam and Niraj Kumar and John Paul Kawalek},
keywords = {Supply chain management, Big data analytics, Capabilities, Maturity model},
abstract = {In the era of Big Data, many organisations have successfully leveraged Big Data Analytics (BDA) capabilities to improve their performance. However, past literature on BDA have put limited focus on understanding the capabilities required to extract value from big data. In this context, this paper aims to provide a systematic literature review of BDA capabilities in supply chain and develop the capabilities maturity model. The paper presents the bibliometric and thematic analysis of research papers from 2008 to 2016. This paper contributes in theorizing BDA capabilities in context of supply chain, and provides future direction of research in this field.}
}
@article{YAQOOB20161231,
title = {Big data: From beginning to future},
journal = {International Journal of Information Management},
volume = {36},
number = {6, Part B},
pages = {1231-1247},
year = {2016},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2016.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0268401216304753},
author = {Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Abdullah Gani and Salimah Mokhtar and Ejaz Ahmed and Nor Badrul Anuar and Athanasios V. Vasilakos},
keywords = {Big data, Parallel and distributed computing, Cloud computing, Internet of things, Social media, Analytics},
abstract = {Big data is a potential research area receiving considerable attention from academia and IT communities. In the digital world, the amounts of data generated and stored have expanded within a short period of time. Consequently, this fast growing rate of data has created many challenges. In this paper, we use structuralism and functionalism paradigms to analyze the origins of big data applications and its current trends. This paper presents a comprehensive discussion on state-of-the-art big data technologies based on batch and stream data processing. Moreover, strengths and weaknesses of these technologies are analyzed. This study also discusses big data analytics techniques, processing methods, some reported case studies from different vendors, several open research challenges, and the opportunities brought about by big data. The similarities and differences of these techniques and technologies based on important parameters are also investigated. Emerging technologies are recommended as a solution for big data problems.}
}
@incollection{MISHRA20201,
title = {Chapter 1 - Analysis of the role and scope of big data analytics with IoT in health care domain},
editor = {Valentina Emilia Balas and Vijender Kumar Solanki and Raghvendra Kumar and Manju Khari},
booktitle = {Handbook of Data Science Approaches for Biomedical Engineering},
publisher = {Academic Press},
pages = {1-23},
year = {2020},
isbn = {978-0-12-818318-2},
doi = {https://doi.org/10.1016/B978-0-12-818318-2.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128183182000015},
author = {Sushruta Mishra and Brojo Kishore Mishra and Hrudaya Kumar Tripathy and Arijit Dutta},
keywords = {Big data analytics, Cloud, Health care domain, Internet of Things (IoT), Sensors},
abstract = {Data analytics play an active role in medical applications to extract relevant information from heaps of data samples. Internet of things (IoT) technology has slowly captured the market and is entering the health care sector too. With the help of big data analytics, various IoT-based devices can auto-monitor the health conditions of patients and can send the status to concerned physicians and family members. Thus, the integration of big data analytics with IoT technology forms a favorable combination in the health care domain. In this chapter, we discuss the two latest trends that include big data analytics and IoT with respect to its relevance in medical fields. We also analyze a health care monitoring system, which is an IoT-based model integrated with big data analytics. The system integrates patient specific information over the cloud. In the more developed model, the implementation was made to monitor the health status of the patients. The developed model was found to be faster and thus it can be easily implemented into a real time patient health monitoring and status management system. Medical experts can take advantage of this system model, thereby providing appropriate information to appropriate patient and doctors at appropriate time.}
}
@article{BILAL2016500,
title = {Big Data in the construction industry: A review of present status, opportunities, and future trends},
journal = {Advanced Engineering Informatics},
volume = {30},
number = {3},
pages = {500-521},
year = {2016},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2016.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034616301938},
author = {Muhammad Bilal and Lukumon O. Oyedele and Junaid Qadir and Kamran Munir and Saheed O. Ajayi and Olugbenga O. Akinade and Hakeem A. Owolabi and Hafiz A. Alaka and Maruf Pasha},
keywords = {Big Data Engineering, Big Data Analytics, Construction industry, Machine learning},
abstract = {The ability to process large amounts of data and to extract useful insights from data has revolutionised society. This phenomenon—dubbed as Big Data—has applications for a wide assortment of industries, including the construction industry. The construction industry already deals with large volumes of heterogeneous data; which is expected to increase exponentially as technologies such as sensor networks and the Internet of Things are commoditised. In this paper, we present a detailed survey of the literature, investigating the application of Big Data techniques in the construction industry. We reviewed related works published in the databases of American Association of Civil Engineers (ASCE), Institute of Electrical and Electronics Engineers (IEEE), Association of Computing Machinery (ACM), and Elsevier Science Direct Digital Library. While the application of data analytics in the construction industry is not new, the adoption of Big Data technologies in this industry remains at a nascent stage and lags the broad uptake of these technologies in other fields. To the best of our knowledge, there is currently no comprehensive survey of Big Data techniques in the context of the construction industry. This paper fills the void and presents a wide-ranging interdisciplinary review of literature of fields such as statistics, data mining and warehousing, machine learning, and Big Data Analytics in the context of the construction industry. We discuss the current state of adoption of Big Data in the construction industry and discuss the future potential of such technologies across the multiple domain-specific sub-areas of the construction industry. We also propose open issues and directions for future work along with potential pitfalls associated with Big Data adoption in the industry.}
}
@article{LABRIE201845,
title = {Big data analytics sentiment: US-China reaction to data collection by business and government},
journal = {Technological Forecasting and Social Change},
volume = {130},
pages = {45-55},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.06.029},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517308612},
author = {Ryan C. LaBrie and Gerhard H. Steinke and Xiangmin Li and Joseph A. Cazier},
keywords = {Big data ethics, Business data usage, Corporate data collection, Government data usage, Technology ethics, US-China similarities, US-China differences},
abstract = {As society continues its rapid change to a digitized individual, corporate, and government environment it is prudent for researchers to investigate the zeitgeist of the global citizenry. The technological changes brought about by big data analytics are changing the way we gather and view data. This big data analytics sentiment research examines how Chinese and American respondents may view big data collection and analytics differently. The paper follows with an analysis of reported attitudes toward possible viewpoints from each country on various big data analytics topics ranging from individual to business and governmental foci. Hofstede's cultural dimensions are used to inform and frame our research hypotheses. Findings suggest that Chinese and American perspectives differ on individual data values, with the Chinese being more open to data collection and analytic techniques targeted toward individuals. Furthermore, support is found that US respondents have a more favorable view of businesses' use of data analytics. Finally, there is a strong difference in the attitudes toward governmental use of data, where US respondents do not favor governmental big data analytics usage and the Chinese respondents indicated a greater acceptance of governmental data usage. These findings are helpful in better understanding appropriate technological change and adoption from a societal perspective. Specifically, this research provides insights for corporate business and government entities suggesting how they might adjust their approach to big data collection and management in order to better support and sustain their organization's services and products.}
}
@article{SOHRABI2018280,
title = {Systematic method for finding emergence research areas as data quality},
journal = {Technological Forecasting and Social Change},
volume = {137},
pages = {280-287},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517318140},
author = {Babak Sohrabi and Ahmad Khalilijafarabad},
keywords = {Data quality, Text mining, Science mapping, Data mining, Trend analysis},
abstract = {The analysis of the transformation and changes in scientific disciplines has always been a critical path for policymakers and researchers. The current study examines the changes in the research areas of data and information quality (DIQ). The aim of this study was to detect different types of changes occurring in the scientific areas including birth, death, growth, decline, merge, and splitting. A model has been developed for this data mining. To test the model, all DIQ articles published in online scientific citation indexing service or Web of Science (WOS) between 1970 and 2016 were extracted and analyzed using the given model. The study is related to the Big Data as well as the integration methods in Big Data which is the most important area in DIQ. It is demonstrated that the first and second emerging research areas are sub-disciplines of entity resolution and record linkage. Accordingly, linkage and privacy are the first emerging research area and the entity resolution using ontology is the second in DIQ. This is followed by the social media issues and genetic related DIQ issues.}
}
@article{NORORI2021100347,
title = {Addressing bias in big data and AI for health care: A call for open science},
journal = {Patterns},
volume = {2},
number = {10},
pages = {100347},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100347},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921002026},
author = {Natalia Norori and Qiyang Hu and Florence Marcelle Aellen and Francesca Dalia Faraci and Athina Tzovara},
keywords = {artificial intelligence, deep learning, health care, bias, open science, participatory science, data standards},
abstract = {Summary
Artificial intelligence (AI) has an astonishing potential in assisting clinical decision making and revolutionizing the field of health care. A major open challenge that AI will need to address before its integration in the clinical routine is that of algorithmic bias. Most AI algorithms need big datasets to learn from, but several groups of the human population have a long history of being absent or misrepresented in existing biomedical datasets. If the training data is misrepresentative of the population variability, AI is prone to reinforcing bias, which can lead to fatal outcomes, misdiagnoses, and lack of generalization. Here, we describe the challenges in rendering AI algorithms fairer, and we propose concrete steps for addressing bias using tools from the field of open science.}
}
@article{ZHU2019229,
title = {The application of big data and the development of nursing science: A discussion paper},
journal = {International Journal of Nursing Sciences},
volume = {6},
number = {2},
pages = {229-234},
year = {2019},
issn = {2352-0132},
doi = {https://doi.org/10.1016/j.ijnss.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352013218305507},
author = {Ruifang Zhu and Shifan Han and Yanbing Su and Chichen Zhang and Qi Yu and Zhiguang Duan},
keywords = {Artificial intelligence, Data mining, Knowledge bases, Nursing},
abstract = {Based on the concept and research status of big data, we analyze and examine the importance of constructing the knowledge system of nursing science for the development of the nursing discipline in the context of big data and propose that it is necessary to establish big data centers for nursing science to share resources, unify language standards, improve professional nursing databases, and establish a knowledge system structure.}
}
@article{RIGGINS201723,
title = {Data governance case at KrauseMcMahon LLP in an era of self-service BI and Big Data},
journal = {Journal of Accounting Education},
volume = {38},
pages = {23-36},
year = {2017},
note = {Special Issue on Big Data},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2016.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0748575116301208},
author = {Frederick J. Riggins and Bonnie K. Klamm},
keywords = {Big Data, Data governance, Self-service business intelligence},
abstract = {This case increases your understanding of data governance in an era of sophisticated analytics and Big Data where corporate data integrity and data quality may be at risk. KrauseMcMahon, a large certified public accounting and business consulting firm, faces a tradeoff of increasing control of the company’s data assets versus unleashing end user innovation due to the proliferation of self-service business intelligence tools. You are required to analyze the issues in the case from organizational, financial, and technical perspectives to propose alternatives the organization should consider and make specific recommendations on how the company should proceed. By completing this case, you will demonstrate cross-disciplinary abilities related to foundational business, accounting, and broad management competencies. By addressing such competencies, the case requires your use of accounting, MIS, and upper-level business skills.}
}
@incollection{SEBASTIANCOLEMAN2022229,
title = {Chapter 10 - Dimensions of Data Quality},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {229-256},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000109},
author = {Laura Sebastian-Coleman},
keywords = {Data quality dimensions, data completeness, data integrity, validity, data currency, metadata management, reference data management, data modeling},
abstract = {This chapter provides an in-depth discussion about a core concept in data quality management: data quality dimensions. Dimensions provide a framework through which we can understand the core capabilities. As the foundation for data quality rules and requirements, they play a critical role in helping answer the fundamental questions about data quality: “What do we mean by high-quality data?” “How do we detect low-quality data?” and “What action will we take when data does not meet quality standards?” This chapter will review a comprehensive set of dimensions (i.e., completeness, correctness, uniqueness, consistency, currency, validity, integrity, reasonability, precision, clarity, accessibility, timeliness, relevance, usability, trustworthiness) in the context of challenges associated with data structure and meaning, the processes for creating data, the influence of technology on quality, and the perceptions of data consumers.}
}
@incollection{VIDHYALAKSHMI20201,
title = {Chapter 1 - Medical big data mining and processing in e-health care},
editor = {Valentina Emilia Balas and Vijender Kumar Solanki and Raghvendra Kumar},
booktitle = {An Industrial IoT Approach for Pharmaceutical Industry Growth},
publisher = {Academic Press},
pages = {1-30},
year = {2020},
isbn = {978-0-12-821326-1},
doi = {https://doi.org/10.1016/B978-0-12-821326-1.00001-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128213261000012},
author = {A. Vidhyalakshmi and C. Priya},
keywords = {Big data, IoT, health care, telemedicine, WHO, image processing},
abstract = {Health care is thought to be one of the business fields with the largest big data potential. Based on the prevailing definition, big data has a large amount of data which can be processed easily and can be modified or updated easily. These data can be quickly stored, processed, and transformed into valuable information using older technologies. At present, many new trends regarding new data resources and innovative data analysis are followed in medicine and health care. In practice, electronic health-care records, free open-source data, and the “quantified self” provide new approaches for analyzing data. Some of these advancements have been made in information extraction from the text data based on analytics, which is useful in data unlocking for analytics purposes from clinical documentation. Choosing big data approaches in the medicine and health-care fields has been lagging. This has led to the rise specific problems regarding data complexity and organizational, legal, and ethical challenges. With the growth of the uptake of big data in general, and medicine and health care in specific, innovative ideas and solutions are expected. Telemedicine is a new opportunity for the Internet of Things (IoT). This enables the specialist to consult a patient despite them being in different places. Medical image segmentation is needed for the analysis, storage, and protection of medical images in telemedicine. Telemedicine is defined by the World Health Organization (WHO) as “the practice of medical care using interactive audiovisual and data communications. This includes the delivery of medical care services, diagnosis, consultation, treatment, as well as health education and the transfer of medical data.” IoT-based applications mainly include remote patient monitoring and clinical monitoring. In addition, preventive measures-based applications are also part of smart health care. These applications require image processing-based technologies which could be integrated into medical health-care systems. Various types of input taken from cameras and processing of CT and MRI images could be integrated into IoT-based medical applications.}
}
@article{MCKINNEY201763,
title = {The need for ‘skeptical’ accountants in the era of Big Data},
journal = {Journal of Accounting Education},
volume = {38},
pages = {63-80},
year = {2017},
note = {Special Issue on Big Data},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2016.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0748575116301051},
author = {Earl McKinney and Charles J. Yoos and Ken Snead},
keywords = {Big Data, Analysis, Informed skepticism, Questioning},
abstract = {Big Data is now readily available for analysis, and analysts trained to conduct effective analysis in this area are in high demand. However, there is a dearth of discussion in the literature related to identifying the important cognitive skills required for accountants to conduct effective Big Data analysis. Here we argue that accountants need to approach Big Data analysis as informed skeptics, being ever ready to challenge the analysis by asking good questions in appropriate topical areas. These areas include understanding the limits of measurement and representation, the subjectiveness of insight, the challenges of statistics and integrating data sets, and the effects of underdetermination and inductive reasoning. Accordingly, we develop a framework and an illustrative example to facilitate the training of accounting students to become informed skeptics in the era of Big Data by explaining the conceptual relevance of each of the topical areas to Big Data analysis. In addition, example questions are identified that accountants conducting Big Data analysis should be asking regarding each topic. Further, for each topic, references to additional resources are provided that students can access to learn more about effectively conducting Big Data analysis.}
}
@article{WANG2017287,
title = {Exploring the path to big data analytics success in healthcare},
journal = {Journal of Business Research},
volume = {70},
pages = {287-299},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316304891},
author = {Yichuan Wang and Nick Hajli},
keywords = {Big data analytics, Business value, Capability building view, Resource-based theory, Information technology source management, Health care industries},
abstract = {Although big data analytics have tremendous benefits for healthcare organizations, extant research has paid insufficient attention to the exploration of its business value. In order to bridge this knowledge gap, this study proposes a big data analytics-enabled business value model in which we use the resource-based theory (RBT) and capability building view to explain how big data analytics capabilities can be developed and what potential benefits can be obtained by these capabilities in the health care industries. Using this model, we investigate 109 case descriptions, covering 63 healthcare organizations to explore the causal relationships between the big data analytics capabilities and business value and the path-to-value chains for big data analytics success. Our findings provide new insights to healthcare practitioners on how to constitute big data analytics capabilities for business transformation and offer an empirical basis that can stimulate a more detailed investigation of big data analytics implementation.}
}
@article{BROEDERS2017309,
title = {Big Data and security policies: Towards a framework for regulating the phases of analytics and use of Big Data},
journal = {Computer Law & Security Review},
volume = {33},
number = {3},
pages = {309-323},
year = {2017},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2017.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0267364917300675},
author = {Dennis Broeders and Erik Schrijvers and Bart {van der Sloot} and Rosamunde {van Brakel} and Josta {de Hoog} and Ernst {Hirsch Ballin}},
keywords = {Big Data, Security, Data protection, Privacy, Regulation, Fraud, Policing, Surveillance, Algorithmic accountability, the Netherlands},
abstract = {Big Data analytics in national security, law enforcement and the fight against fraud have the potential to reap great benefits for states, citizens and society but require extra safeguards to protect citizens' fundamental rights. This involves a crucial shift in emphasis from regulating Big Data collection to regulating the phases of analysis and use. In order to benefit from the use of Big Data analytics in the field of security, a framework has to be developed that adds new layers of protection for fundamental rights and safeguards against erroneous and malicious use. Additional regulation is needed at the levels of analysis and use, and the oversight regime is in need of strengthening. At the level of analysis – the algorithmic heart of Big Data processes – a duty of care should be introduced that is part of an internal audit and external review procedure. Big Data projects should also be subject to a sunset clause. At the level of use, profiles and (semi-) automated decision-making should be regulated more tightly. Moreover, the responsibility of the data processing party for accuracy of analysis – and decisions taken on its basis – should be anchored in legislation. The general and security-specific oversight functions should be strengthened in terms of technological expertise, access and resources. The possibilities for judicial review should be expanded to stimulate the development of case law.}
}
@article{SMIRNOVA201825,
title = {A practical guide to big data},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {25-29},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300592},
author = {Ekaterina Smirnova and Andrada Ivanescu and Jiawei Bai and Ciprian M. Crainiceanu},
keywords = {Big data, Wearable and implantable computing, Accelerometer},
abstract = {Big Data is increasingly prevalent in science and data analysis. We provide a short tutorial for adapting to these changes and making the necessary adjustments to the academic culture to keep Biostatistics truly impactful in scientific research.}
}
@article{WESTRA2016286,
title = {Big Data and Perioperative Nursing},
journal = {AORN Journal},
volume = {104},
number = {4},
pages = {286-292},
year = {2016},
note = {Special Focus Issue: Technology},
issn = {0001-2092},
doi = {https://doi.org/10.1016/j.aorn.2016.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0001209216304410},
author = {Bonnie L. Westra and Jessica J. Peterson},
keywords = {big data, perioperative nursing, quality care, nursing knowledge, nursing informatics},
abstract = {Big data are large volumes of digital data that can be collected from disparate sources and are challenging to analyze. These data are often described with the five “Vs”: volume, velocity, variety, veracity, and value. Perioperative nurses contribute to big data through documentation in the electronic health record during routine surgical care, and these data have implications for clinical decision making, administrative decisions, quality improvement, and big data science. This article explores methods to improve the quality of perioperative nursing data and provides examples of how these data can be combined with broader nursing data for quality improvement. We also discuss a national action plan for nursing knowledge and big data science and how perioperative nurses can engage in collaborative actions to transform health care. Standardized perioperative nursing data has the potential to affect care far beyond the original patient.}
}
@article{ZHOU2016215,
title = {Big data driven smart energy management: From big data to big insights},
journal = {Renewable and Sustainable Energy Reviews},
volume = {56},
pages = {215-225},
year = {2016},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2015.11.050},
url = {https://www.sciencedirect.com/science/article/pii/S1364032115013179},
author = {Kaile Zhou and Chao Fu and Shanlin Yang},
keywords = {Energy big data, Smart energy management, Big data analytics, Smart grid, Demand side management (DSM)},
abstract = {Large amounts of data are increasingly accumulated in the energy sector with the continuous application of sensors, wireless transmission, network communication, and cloud computing technologies. To fulfill the potential of energy big data and obtain insights to achieve smart energy management, we present a comprehensive study of big data driven smart energy management. We first discuss the sources and characteristics of energy big data. Also, a process model of big data driven smart energy management is proposed. Then taking smart grid as the research background, we provide a systematic review of big data analytics for smart energy management. It is discussed from four major aspects, namely power generation side management, microgrid and renewable energy management, asset management and collaborative operation, as well as demand side management (DSM). Afterwards, the industrial development of big data-driven smart energy management is analyzed and discussed. Finally, we point out the challenges of big data-driven smart energy management in IT infrastructure, data collection and governance, data integration and sharing, processing and analysis, security and privacy, and professionals.}
}
@article{LAREYRE2020e575,
title = {Artificial Intelligence in Vascular Surgery: Moving from Big Data to Smart Data},
journal = {Annals of Vascular Surgery},
volume = {67},
pages = {e575-e576},
year = {2020},
issn = {0890-5096},
doi = {https://doi.org/10.1016/j.avsg.2020.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0890509620303411},
author = {Fabien Lareyre and Cédric Adam and Marion Carrier and Juliette Raffort}
}
@article{MOGHADAM2018401,
title = {Information security governance in big data environments: A systematic mapping},
journal = {Procedia Computer Science},
volume = {138},
pages = {401-408},
year = {2018},
note = {CENTERIS 2018 - International Conference on ENTERprise Information Systems / ProjMAN 2018 - International Conference on Project MANagement / HCist 2018 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.057},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316909},
author = {Reza Saneei Moghadam and Ricardo Colomo-Palacios},
keywords = {information security governance, big data, framework, systematic mapping},
abstract = {Information security governance is an important aspects for all organizations. Given the crucial importance of IT systems and the increasing range of threats these systems are facing, there is an increasing interest on the topic. On the other hand, Big Data environments are also beginning to be more pervasive as IT is increasing its importance for organizations worldwide. In order to better know which aspects are the most important for the intersection of Big Data and information security governance, authors present in this paper a systematic mapping on this topic. Authors illustrate challenges and gaps concerning the topic and clarify these challenges by means of a classification of the environments they take place, the security risk spectrums they concern, and the security governance measures they take to mitigate them; by providing solutions as in a framework, model, software or tool, wherever possible. Results are expected to be useful for IT security professionals and information systems practitioners as a whole.}
}
@article{NUNEZREIZ201952,
title = {Big data and machine learning in critical care: Opportunities for collaborative research},
journal = {Medicina Intensiva (English Edition)},
volume = {43},
number = {1},
pages = {52-57},
year = {2019},
issn = {2173-5727},
doi = {https://doi.org/10.1016/j.medine.2018.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S217357271930013X},
author = {A. {Núñez Reiz}},
keywords = {Big data, Machine learning, Artificial intelligence, Clinical databases, MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases de datos clínicos, MIMIC III, Datathon, Trabajo colaborativo},
abstract = {The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.
Resumen
La aparición de los sistemas de información clínica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos clínicos en formato electrónico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas clínicas, para su uso en la gestión de recursos o para sugerir líneas de investigación que luego pueden ser explotadas mediante ensayos clínicos aleatorizados. Sin embargo, los médicos clínicos carecen de la formación necesaria para la explotación de grandes bases de datos, lo que supone un obstáculo para aprovechar esta oportunidad. Además, existen cuestiones de índole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estadísticos, epidemiólogos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigación clínica o análisis de resultados (benchmarking). Se describe la reunión multidisciplinar (Critical Care Datathon) realizada en Madrid los días 1, 2 y 3 de diciembre de 2017. Esta reunión, celebrada bajo los auspicios de la Sociedad Española de Medicina Intensiva, Crítica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovación y el Servicio de Medicina Intensiva del Hospital Clínico San Carlos, así como el grupo de investigación «Life Supporting Technologies» de la Universidad Politécnica de Madrid. Tras unas ponencias de formación sobre big data, seguridad y calidad de los datos, y su aplicación al entorno de la medicina intensiva, un grupo de clínicos, analistas de datos, estadísticos, expertos en seguridad informática de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas clínicas establecidas previamente a la reunión. El trabajo colaborativo permitió establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias líneas de investigación clínica a desarrollar en el futuro. Además, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en España, y se estableció un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparación de resultados entre unidades o la elaboración de nuevas líneas de investigación clínica.}
}
@article{ELOUAZZANI201852,
title = {A new technique ensuring privacy in big data: K-anonymity without prior value of the threshold k},
journal = {Procedia Computer Science},
volume = {127},
pages = {52-59},
year = {2018},
note = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.097},
url = {https://www.sciencedirect.com/science/article/pii/S187705091830108X},
author = {Zakariae {El Ouazzani} and Hanan {El Bakkali}},
keywords = {k-anonymity, quasi identifier attributes, big data, anonymization, privacy},
abstract = {Big data has become omnipresent and crucial for many application domains. Big data makes reference to the explosive quantity of data generated in today’s society that might contain personally identifiable information (PII). That’s why the challenge from the point of view of data privacy is one of the major hurdles for the application of big data. In that situation, several techniques were exposed in order to ensure privacy in big data including generalization, randomization and cryptographic techniques as well. It is well known that there exist two main types of attributes in the literature, quasi identifier and sensitive attributes. In this paper, we are going to focus on quasi identifier attributes. Over the years, k-anonymity has been treated with great interest as an anonymization technique ensuring privacy in big data when we are dealing with quasi identifier attributes. Despite the fact that many algorithms of k-anonymity have been proposed, most of them admit that the threshold k of k-anonymity has to be known before anonymizing the data set. Here, a novel way in applying k-anonymity for quasi identifier attributes is presented. It’s a new algorithm called “k-anonymity without prior value of the threshold k”. Our proposed algorithm was experimentally evaluated using a test table of quasi identifier attributes. Furthermore, we highlight all the steps of our proposed algorithm with detailed comments.}
}
@article{BEN2019403,
title = {A spatio-temporally weighted hybrid model to improve estimates of personal PM2.5 exposure: Incorporating big data from multiple data sources},
journal = {Environmental Pollution},
volume = {253},
pages = {403-411},
year = {2019},
issn = {0269-7491},
doi = {https://doi.org/10.1016/j.envpol.2019.07.034},
url = {https://www.sciencedirect.com/science/article/pii/S026974911930795X},
author = {YuJie Ben and FuJun Ma and Hao Wang and Muhammad Azher Hassan and Romanenko Yevheniia and WenHong Fan and Yubiao Li and ZhaoMin Dong},
keywords = {Exposure assessment, Indoor PM, Ambient PM, In-home monitors, Shanghai},
abstract = {An accurate estimation of population exposure to particulate matter with an aerodynamic diameter <2.5 μm (PM2.5) is crucial to hazard assessment and epidemiology. This study integrated annual data from 1146 in-home air monitors, air quality monitoring network, public applications, and traffic smart cards to determine the pattern of PM2.5 concentrations and activities in different microenvironments (including outdoors, indoors, subways, buses, and cars). By combining massive amounts of signaling data from cell phones, this study applied a spatio-temporally weighted model to improve the estimation of PM2.5 exposure. Using Shanghai as a case study, the annual average indoor PM2.5 concentration was estimated to be 29.3 ± 27.1 μg/m3 (n = 365), with an average infiltration factor of 0.63. The spatio-temporally weighted PM2.5 exposure was estimated to be 32.1 ± 13.9 μg/m3 (n = 365), with indoor PM2.5 contributing the most (85.1%), followed by outdoor (7.6%), bus (3.7%), subway (3.1%), and car (0.5%). However, considering that outdoor PM2.5 makes a significant contribution to indoor PM2.5, outdoor PM2.5 was responsible for most of the exposure in Shanghai. A heatmap of PM2.5 exposure indicated that the inner-city exposure index was significantly higher than that of the outskirts city, which demonstrated that the importance of spatial differences in population exposure estimation.}
}
@article{YASSINE2019563,
title = {IoT big data analytics for smart homes with fog and cloud computing},
journal = {Future Generation Computer Systems},
volume = {91},
pages = {563-573},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.08.040},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18311099},
author = {Abdulsalam Yassine and Shailendra Singh and M. Shamim Hossain and Ghulam Muhammad},
keywords = {Internet of Things (IoT), Cloud computing, Fog computing, Big data analytics, Energy management, Smart homes},
abstract = {Internet of Things (IoT) analytics is an essential mean to derive knowledge and support applications for smart homes. Connected appliances and devices inside the smart home produce a significant amount of data about consumers and how they go about their daily activities. IoT analytics can aid in personalizing applications that benefit both homeowners and the ever growing industries that need to tap into consumers profiles. This article presents a new platform that enables innovative analytics on IoT captured data from smart homes. We propose the use of fog nodes and cloud system to allow data-driven services and address the challenges of complexities and resource demands for online and offline data processing, storage, and classification analysis. We discuss in this paper the requirements and the design components of the system. To validate the platform and present meaningful results, we present a case study using a dataset acquired from real smart home in Vancouver, Canada. The results of the experiments show clearly the benefit and practicality of the proposed platform.}
}
@article{VALENCIAPARRA2021113450,
title = {DMN4DQ: When data quality meets DMN},
journal = {Decision Support Systems},
volume = {141},
pages = {113450},
year = {2021},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113450},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620302050},
author = {Álvaro Valencia-Parra and Luisa Parody and Ángel Jesús Varela-Vaca and Ismael Caballero and María Teresa Gómez-López},
keywords = {Data usability, Data quality, Decision model and notation, Data quality rule, Data quality assessment, Data quality measurement},
abstract = {To succeed in their business processes, organizations need data that not only attains suitable levels of quality for the task at hand, but that can also be considered as usable for the business. However, many researchers ground the potential usability of the data on its quality. Organizations would benefit from receiving recommendations on the usability of the data before its use. We propose that the recommendation on the usability of the data be supported by a decision process, which includes a context-dependent data-quality assessment based on business rules. Ideally, this recommendation would be generated automatically. Decision Model and Notation (DMN) enables the assessment of data quality based on the evaluation of business rules, and also, provides stakeholders (e.g., data stewards) with sound support for the automation of the whole process of generation of a recommendation regarding usability based on data quality. The main contribution of the proposal involves designing and enabling both DMN-driven mechanisms and a guiding methodology (DMN4DQ) to support the automatic generation of a decision-based recommendation on the potential usability of a data record in terms of its level of data quality. Furthermore, the validation of the proposal is performed through the application of a real dataset.}
}
@article{LEE2017293,
title = {Big data: Dimensions, evolution, impacts, and challenges},
journal = {Business Horizons},
volume = {60},
number = {3},
pages = {293-303},
year = {2017},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2017.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0007681317300046},
author = {In Lee},
keywords = {Big data, Internet of things, Data analytics, Sentiment analysis, Social network analysis, Web analytics},
abstract = {Big data represents a new technology paradigm for data that are generated at high velocity and high volume, and with high variety. Big data is envisioned as a game changer capable of revolutionizing the way businesses operate in many industries. This article introduces an integrated view of big data, traces the evolution of big data over the past 20 years, and discusses data analytics essential for processing various structured and unstructured data. This article illustrates the application of data analytics using merchant review data. The impacts of big data on key business performances are then evaluated. Finally, six technical and managerial challenges are discussed.}
}
@article{EREVELLES2016897,
title = {Big Data consumer analytics and the transformation of marketing},
journal = {Journal of Business Research},
volume = {69},
number = {2},
pages = {897-904},
year = {2016},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2015.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0148296315002842},
author = {Sunil Erevelles and Nobuyuki Fukawa and Linda Swayne},
keywords = {Big Data, Consumer analytics, Consumer insights, Resource-based theory, Induction, Ignorance},
abstract = {Consumer analytics is at the epicenter of a Big Data revolution. Technology helps capture rich and plentiful data on consumer phenomena in real time. Thus, unprecedented volume, velocity, and variety of primary data, Big Data, are available from individual consumers. To better understand the impact of Big Data on various marketing activities, enabling firms to better exploit its benefits, a conceptual framework that builds on resource-based theory is proposed. Three resources—physical, human, and organizational capital—moderate the following: (1) the process of collecting and storing evidence of consumer activity as Big Data, (2) the process of extracting consumer insight from Big Data, and (3) the process of utilizing consumer insight to enhance dynamic/adaptive capabilities. Furthermore, unique resource requirements for firms to benefit from Big Data are discussed.}
}
@article{WANG201864,
title = {An integrated big data analytics-enabled transformation model: Application to health care},
journal = {Information & Management},
volume = {55},
number = {1},
pages = {64-79},
year = {2018},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617303129},
author = {Yichuan Wang and LeeAnn Kung and William Yu Chung Wang and Casey G. Cegielski},
keywords = {Big data analytics, IT-enabled transformation, Practice-based view, Business value of IT, Healthcare, Content analysis},
abstract = {A big data analytics-enabled transformation model based on practice-based view is developed, which reveals the causal relationships among big data analytics capabilities, IT-enabled transformation practices, benefit dimensions, and business values. This model was then tested in healthcare setting. By analyzing big data implementation cases, we sought to understand how big data analytics capabilities transform organizational practices, thereby generating potential benefits. In addition to conceptually defining four big data analytics capabilities, the model offers a strategic view of big data analytics. Three significant path-to-value chains were identified for healthcare organizations by applying the model, which provides practical insights for managers.}
}
@article{NADAL201775,
title = {A software reference architecture for semantic-aware Big Data systems},
journal = {Information and Software Technology},
volume = {90},
pages = {75-92},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304287},
author = {Sergi Nadal and Victor Herrero and Oscar Romero and Alberto Abelló and Xavier Franch and Stijn Vansummeren and Danilo Valerio},
keywords = {Big Data, Software reference architecture, Semantic-aware, Data management, Data analysis},
abstract = {Context: Big Data systems are a class of software systems that ingest, store, process and serve massive amounts of heterogeneous data, from multiple sources. Despite their undisputed impact in current society, their engineering is still in its infancy and companies find it difficult to adopt them due to their inherent complexity. Existing attempts to provide architectural guidelines for their engineering fail to take into account important Big Data characteristics, such as the management, evolution and quality of the data. Objective: In this paper, we follow software engineering principles to refine the λ-architecture, a reference model for Big Data systems, and use it as seed to create Bolster, a software reference architecture (SRA) for semantic-aware Big Data systems. Method: By including a new layer into the λ-architecture, the Semantic Layer, Bolster is capable of handling the most representative Big Data characteristics (i.e., Volume, Velocity, Variety, Variability and Veracity). Results: We present the successful implementation of Bolster in three industrial projects, involving five organizations. The validation results show high level of agreement among practitioners from all organizations with respect to standard quality factors. Conclusion: As an SRA, Bolster allows organizations to design concrete architectures tailored to their specific needs. A distinguishing feature is that it provides semantic-awareness in Big Data Systems. These are Big Data system implementations that have components to simplify data definition and exploitation. In particular, they leverage metadata (i.e., data describing data) to enable (partial) automation of data exploitation and to aid the user in their decision making processes. This simplification supports the differentiation of responsibilities into cohesive roles enhancing data governance.}
}
@article{AHMED2017459,
title = {The role of big data analytics in Internet of Things},
journal = {Computer Networks},
volume = {129},
pages = {459-471},
year = {2017},
note = {Special Issue on 5G Wireless Networks for IoT and Body Sensors},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2017.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S1389128617302591},
author = {Ejaz Ahmed and Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Imran Khan and Abdelmuttlib Ibrahim Abdalla Ahmed and Muhammad Imran and Athanasios V. Vasilakos},
keywords = {Internet of Things, Big data, Analytics, Distributed computing, Smart city},
abstract = {The explosive growth in the number of devices connected to the Internet of Things (IoT) and the exponential increase in data consumption only reflect how the growth of big data perfectly overlaps with that of IoT. The management of big data in a continuously expanding network gives rise to non-trivial concerns regarding data collection efficiency, data processing, analytics, and security. To address these concerns, researchers have examined the challenges associated with the successful deployment of IoT. Despite the large number of studies on big data, analytics, and IoT, the convergence of these areas creates several opportunities for flourishing big data and analytics for IoT systems. In this paper, we explore the recent advances in big data analytics for IoT systems as well as the key requirements for managing big data and for enabling analytics in an IoT environment. We taxonomized the literature based on important parameters. We identify the opportunities resulting from the convergence of big data, analytics, and IoT as well as discuss the role of big data analytics in IoT applications. Finally, several open challenges are presented as future research directions.}
}
@article{KHAN2017923,
title = {A survey on scholarly data: From big data perspective},
journal = {Information Processing & Management},
volume = {53},
number = {4},
pages = {923-944},
year = {2017},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306457316305994},
author = {Samiya Khan and Xiufeng Liu and Kashish A. Shakil and Mansaf Alam},
keywords = {Scholarly data, Big data, Cloud-based big data analytics, Big scholarly data, Big data platform, Scholarly applications},
abstract = {Recently, there has been a shifting focus of organizations and governments towards digitization of academic and technical documents, adding a new facet to the concept of digital libraries. The volume, variety and velocity of this generated data, satisfies the big data definition, as a result of which, this scholarly reserve is popularly referred to as big scholarly data. In order to facilitate data analytics for big scholarly data, architectures and services for the same need to be developed. The evolving nature of research problems has made them essentially interdisciplinary. As a result, there is a growing demand for scholarly applications like collaborator discovery, expert finding and research recommendation systems, in addition to several others. This research paper investigates the current trends and identifies the existing challenges in development of a big scholarly data platform, with specific focus on directions for future research and maps them to the different phases of the big data lifecycle.}
}
@article{SHOUMY2020102447,
title = {Multimodal big data affective analytics: A comprehensive survey using text, audio, visual and physiological signals},
journal = {Journal of Network and Computer Applications},
volume = {149},
pages = {102447},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.102447},
url = {https://www.sciencedirect.com/science/article/pii/S1084804519303078},
author = {Nusrat J. Shoumy and Li-Minn Ang and Kah Phooi Seng and D.M.Motiur Rahaman and Tanveer Zia},
keywords = {Affective computing, Multimodal fusion, Sentiment databases, Sentiment analysis, Affective applications},
abstract = {Affective computing is an emerging multidisciplinary research field that is increasingly drawing the attention of researchers and practitioners in various fields, including artificial intelligence, natural language processing, cognitive and social sciences. Research in affective computing includes areas such as sentiment, emotion, and opinion modelling. The internet is an excellent source of data required for sentiment analysis, such as customer reviews of products, social media, forums, blogs, etc. Most of these data, called big data, are unstructured and unorganized. Hence there is a strong demand for developing suitable data processing techniques to process these rich and valuable data to produce useful information. Early surveys on sentiment and emotion recognition in the literature have been limited to discussions using text, audio, and visual modalities. So far, to the author's knowledge, a comprehensive survey combining physiological modalities with these other modalities for affective computing has yet to be reported. The objective of this paper is to fill the gap in this surveyed area. The usage of physiological modalities for affective computing brings several benefits in that the signals can be used in different environmental conditions, more robust systems can be constructed in combination with other modalities, and it has increased anti-spoofing characteristics. The paper includes extensive reviews on different frameworks and categories for state-of-the-art techniques, critical analysis of their performances, and discussions of their applications, trends and future directions to serve as guidelines for readers towards this emerging research area.}
}
@article{SYED2019136,
title = {Smart healthcare framework for ambient assisted living using IoMT and big data analytics techniques},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {136-151},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18321071},
author = {Liyakathunisa Syed and Saima Jabeen and Manimala S. and Abdullah Alsaeedi},
keywords = {Ambient Assisted Living (AAL), Big data analytics, Internet of Medical Things (IoMT), Machine learning techniques, Physical activities, Wearable sensors},
abstract = {In the era of pervasive computing, human living has become smarter by the latest advancements in IoMT (Internet of Medical Things), wearable sensors and telecommunication technologies in order to deliver smart healthcare services. IoMT has the potential to revolutionize the healthcare industry. IoMT interconnects wearable sensors, patients, healthcare providers and caregivers via software and ICT (Information and Communication Technology). AAL (Ambient Assisted Living) enables integration of new technologies to be part of our daily life activities. In this paper, we have provided a novel smart healthcare framework for AAL to monitor the physical activities of elderly people using IoMT and intelligent machine learning algorithms for faster analysis, decision making and better treatment recommendations. Data is collected from multiple wearable sensors placed on subject’s left ankle, right arm, and chest, is transmitted through IoMT devices to the integrated cloud and data analytics layer. To process huge amounts of data in parallel, Hadoop MapReduce techniques are used. Multinomial Naïve Bayes classifier, which fits into the MapReduce paradigm, is utilized to recognize the motion experienced by different body parts and provides higher scalability and better performance with parallel processing when compared to serial processor. Our proposed framework predicts 12 physical activities with an overall accuracy of 97.1%. This can be considered as an optimal solution for recognizing physical activities to remotely monitor health conditions of elderly people.}
}
@article{LI2016119,
title = {Geospatial big data handling theory and methods: A review and research challenges},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {115},
pages = {119-133},
year = {2016},
note = {Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science'},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2015.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0924271615002439},
author = {Songnian Li and Suzana Dragicevic and Francesc Antón Castro and Monika Sester and Stephan Winter and Arzu Coltekin and Christopher Pettit and Bin Jiang and James Haworth and Alfred Stein and Tao Cheng},
keywords = {Big data, Geospatial, Data handling, Analytics, Spatial modeling, Review},
abstract = {Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future.}
}
@incollection{KABALCI2019309,
title = {Chapter 8 - Big data, privacy and security in smart grids},
editor = {Ersan Kabalci and Yasin Kabalci},
booktitle = {From Smart Grid to Internet of Energy},
publisher = {Academic Press},
pages = {309-333},
year = {2019},
isbn = {978-0-12-819710-3},
doi = {https://doi.org/10.1016/B978-0-12-819710-3.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128197103000089},
author = {Ersan Kabalci and Yasin Kabalci},
keywords = {Internet of things (IoT), Machine-to-machine communication (M2M), Human to machine (H2M), Machine learning, Smart grid security, Hadoop, Data mining, Security, Privacy},
abstract = {The smart grid applications are related with monitoring and control operations of conventional power grid. The integration of information and communication technologies (ICT) to existing power network has leveraged interaction of different generators, controllers, monitoring and measurement devices, and intelligent loads. The two-way communication infrastructure is comprised by numerous sensor networks that increased deployment of massive data from measurement nodes to monitoring centers. Big data is a widespread concept which become a trend for massive data streams that are transferred and processed in an ecosystem. The enormous amount of data are generated, transferred and stored to improve operating and management quality of smart grid. The big data analytics are performed to improve quality of service in terms of grid operators and consumers. The big data acquisition, processing, storing, and clustering stages are widely researched by a wide variety of specialist. In this chapter, the all these stages, big data acquisition technologies, machine learning methods used in big data analytics, privacy and security of big data infrastructure are introduced in detail. The privacy preserving methods, big data processing technologies and firmware infrastructures are presented in the context of this chapter.}
}
@article{KOSELEVA2017544,
title = {Big Data in Building Energy Efficiency: Understanding of Big Data and Main Challenges},
journal = {Procedia Engineering},
volume = {172},
pages = {544-549},
year = {2017},
note = {Modern Building Materials, Structures and Techniques},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.02.064},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817305702},
author = {Natalija Koseleva and Guoda Ropaite},
keywords = {Energy Efficiency, Big Data, Characteristics of Big Data, Big Data analysis, Construction, Web of Science},
abstract = {Data generation has increased drastically over the past few years. Data management has also grown in importance because extracting the significant value out of a huge pile of raw data is of prime important thing to make different decisions. One of the important sectors nowadays is construction sector, especially building energy efficiency field. Collecting big amount of data, using different kinds of big data analysis can help to improve construction process from the energy efficiency perspective. This article reviews the understanding of Big Data, methods used for Big Data analysis and the main problems with Big Data in the field of energy.}
}
@article{GUALO2021110938,
title = {Data quality certification using ISO/IEC 25012: Industrial experiences},
journal = {Journal of Systems and Software},
volume = {176},
pages = {110938},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.110938},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221000352},
author = {Fernando Gualo and Moisés Rodriguez and Javier Verdugo and Ismael Caballero and Mario Piattini},
keywords = {Data quality evaluation process, Data quality certification, Data quality management, ISO/IEC 25012, ISO/IEC 25024, ISO/IEC 25040},
abstract = {The most successful organizations in the world are data-driven businesses. Data is at the core of the business of many organizations as one of the most important assets, since the decisions they make cannot be better than the data on which they are based. Due to this reason, organizations need to be able to trust their data. One important activity that helps to achieve data reliability is the evaluation and certification of the quality level of organizational data repositories. This paper describes the results of the application of a data quality evaluation and certification process to the repositories of three European organizations belonging to different sectors. We present findings from the point of view of both the data quality evaluation team and the organizations that underwent the evaluation process. In this respect, several benefits have been explicitly recognized by the involved organizations after achieving the data quality certification for their repositories (e.g., long-term organizational sustainability better internal knowledge of data, and a more efficient management of data quality). As a result of this experience, we have also identified a set of best practices aimed to enhance the data quality evaluation process.}
}
@article{KAMBATLA20142561,
title = {Trends in big data analytics},
journal = {Journal of Parallel and Distributed Computing},
volume = {74},
number = {7},
pages = {2561-2573},
year = {2014},
note = {Special Issue on Perspectives on Parallel and Distributed Processing},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2014.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731514000057},
author = {Karthik Kambatla and Giorgos Kollias and Vipin Kumar and Ananth Grama},
keywords = {Big-data, Analytics, Data centers, Distributed systems},
abstract = {One of the major applications of future generation parallel and distributed systems is in big-data analytics. Data repositories for such applications currently exceed exabytes and are rapidly increasing in size. Beyond their sheer magnitude, these datasets and associated applications’ considerations pose significant challenges for method and software development. Datasets are often distributed and their size and privacy considerations warrant distributed techniques. Data often resides on platforms with widely varying computational and network capabilities. Considerations of fault-tolerance, security, and access control are critical in many applications (Dean and Ghemawat, 2004; Apache hadoop). Analysis tasks often have hard deadlines, and data quality is a major concern in yet other applications. For most emerging applications, data-driven models and methods, capable of operating at scale, are as-yet unknown. Even when known methods can be scaled, validation of results is a major issue. Characteristics of hardware platforms and the software stack fundamentally impact data analytics. In this article, we provide an overview of the state-of-the-art and focus on emerging trends to highlight the hardware, software, and application landscape of big-data analytics.}
}
@article{ANDREOLINI201567,
title = {Adaptive, scalable and reliable monitoring of big data on clouds},
journal = {Journal of Parallel and Distributed Computing},
volume = {79-80},
pages = {67-79},
year = {2015},
note = {Special Issue on Scalable Systems for Big Data Management and Analytics},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2014.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S074373151400149X},
author = {Mauro Andreolini and Michele Colajanni and Marcello Pietri and Stefania Tosi},
keywords = {Adaptivity, Monitoring, Cloud computing, Big data, Scalability},
abstract = {Real-time monitoring of cloud resources is crucial for a variety of tasks such as performance analysis, workload management, capacity planning and fault detection. Applications producing big data make the monitoring task very difficult at high sampling frequencies because of high computational and communication overheads in collecting, storing, and managing information. We present an adaptive algorithm for monitoring big data applications that adapts the intervals of sampling and frequency of updates to data characteristics and administrator needs. Adaptivity allows us to limit computational and communication costs and to guarantee high reliability in capturing relevant load changes. Experimental evaluations performed on a large testbed show the ability of the proposed adaptive algorithm to reduce resource utilization and communication overhead of big data monitoring without penalizing the quality of data, and demonstrate our improvements to the state of the art.}
}
@article{NIMMAGADDA20191155,
title = {On Modelling Big Data Guided Supply Chains in Knowledge-Base Geographic Information Systems},
journal = {Procedia Computer Science},
volume = {159},
pages = {1155-1164},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.284},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314814},
author = {Shastri L Nimmagadda and Torsten Reiners and Lincoln C Wood},
keywords = {Supply Chain Management, Project Management, Laws of Geography, Domain Ontologies, Data Mining},
abstract = {We examine the existing goals of business- and geographic - information systems and their influence on logistics and supply chain management systems. Modelling supply chain management systems is held back because of lack of consistent and poorly aligned data with supply chain elements and processes. The issues constraining the decision-making process limit the connectivity between supply chains and geographically controlled database systems. The heterogeneous and unstructured data are added challenges to connectivity and integration processes. The research focus is on analysing the data heterogeneity and multidimensionality relevant to supply chain systems and geographically controlled databases. In pursuance of the challenges, a unified methodological framework is designed with data structuring, data warehousing and mining, visualization and interpretation artefacts to support connectivity and integration process. Multidimensional ontologies, ecosystem conceptualization and Big Data novelty are added motivations, facilitating the relationships between events of supply chain operations. The models construed for optimizing the resources are analysed in terms of effectiveness of the integrated framework articulations in global supply chains that obey laws of geography. The integrated articulations analysed with laws of geography can affect the operational costs, sure for better with reduced lead times and enhanced stock management.}
}
@article{SHARPLES2018105,
title = {The role of statistics in the era of big data: Electronic health records for healthcare research},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {105-110},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300890},
author = {Linda D. Sharples},
keywords = {Electronic healthcare records},
abstract = {The transferring of medical records into huge electronic databases has opened up opportunities for research but requires attention to data quality, study design and issues of bias and confounding.}
}
@incollection{SEBASTIANCOLEMAN2022187,
title = {Chapter 9 - Core Data Quality Management Capabilities},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {187-228},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000092},
author = {Laura Sebastian-Coleman},
keywords = {Data quality standards, assessing data quality, data quality monitoring, data quality reporting, data issue management, quality improvement methodology},
abstract = {This chapter describes the functions required to build organizational capacity to manage data for quality over time. They include: data quality standards, data quality assessment, data quality monitoring, data quality reporting, data quality issue management, and data quality improvement. These activities are likely to be executed more consistently and with greater impact if there is a data quality team specifically responsible for defining them and facilitating their adoption within the organization.}
}
@article{VERMA2018791,
title = {An extension of the technology acceptance model in the big data analytics system implementation environment},
journal = {Information Processing & Management},
volume = {54},
number = {5},
pages = {791-806},
year = {2018},
note = {In (Big) Data we trust: Value creation in knowledge organizations},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2018.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0306457317300043},
author = {Surabhi Verma and Som Sekhar Bhattacharyya and Saurav Kumar},
keywords = {Technology acceptance model, Big data analytics system, System quality, Information quality},
abstract = {Research on the adoption of systems for big data analytics has drawn enormous attention in Information Systems research. This study extends big data analytics adoption research by examining the effects of system characteristics on the attitude of managers towards the usage of big data analytics systems. A research model has been proposed in this study based on an extensive review of literature pertaining to the Technology Acceptance Model, with further validation by a survey of 150 big data analytics users. Results of this survey confirm that characteristics of the big data analytics system have significant direct and indirect effects on belief in the benefits of big data analytics systems and perceived usefulness, attitude and adoption. Moreover, there are mediation effects that exist among the system characteristics, benefits of big data analytics systems, perceived usefulness and the attitude towards using big data analytics system. This study expands the existing body of knowledge on the adoption of big data analytics systems, and benefits big data analytics providers and vendors while helping in the formulation of their business models.}
}
@article{KAUR20181049,
title = {Big Data and Machine Learning Based Secure Healthcare Framework},
journal = {Procedia Computer Science},
volume = {132},
pages = {1049-1059},
year = {2018},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S187705091830752X},
author = {Prableen Kaur and Manik Sharma and Mamta Mittal},
keywords = {Big Data, Healthcare, Big data analytics, disease diagnosis, predictive analysis, security, privacy},
abstract = {The paper presents a brief introduction to big data and its role in healthcare applications. It is observed that the use of big data architecture and techniques are continuously assisting in managing the expeditious data growth in healthcare industry. Here, initially an empirical study is performed to analyze the role of big data in healthcare industry. It has been observed that significant work has been done using big data in healthcare sector. Nowadays, it is intricate to envision the way the machine learning and big data can influence the healthcare industries. It has been observed that most of the authors who implemented the use of machine learning and big data analytics in disease diagnosis have not given significant weightage to the privacy and security of the data. Here, a novel design of smart and secure healthcare information system using machine learning and advanced security mechanism has been proposed to handle big data of medical industry. The innovation lies in the incorporation of optimal storage and data security layer used to maintain security and privacy. Different techniques like masking encryption, activity monitoring, granular access control, dynamic data encryption and end point validation have been incorporated. The proposed hybrid four layer healthcare model seems to be more effective disease diagnostic big data system.}
}
@article{OUYANG201860,
title = {Methodologies, principles and prospects of applying big data in safety science research},
journal = {Safety Science},
volume = {101},
pages = {60-71},
year = {2018},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2017.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925753517304320},
author = {Qiumei Ouyang and Chao Wu and Lang Huang},
keywords = {Safety big data (SBD), Safety small data (SSD), Safety science, Big data application, Method, Principle, Prospect and challenge},
abstract = {It is clear that big data has numerous potential impacts in many fields. However, few papers discussed its applications in the field of safety science research. Additionally, there exist many problems that cannot be ignored when big data is applied to safety science, most outstanding of which is lack of universal supporting theory that guides how to apply big data to safety science research like methods, principles and approaches, etc. In other terms, it is not enough for big data to be viewed asa strong enabler for safety science applications mainly due to lack of universal and basic theory from the perspective of safety science. Considering the above analyzes, the two key objectives of this paper are: (1) to propose the connotation of safety big data (SBD) and its applying rules, methods and principles, and (2) to put forward some application prospects and challenges of big data to safety science research seen from theoretical research. First, by comparing SBD and traditional safety small data (SSD) from four aspects including theoretical research, typical research method, specific analysis method and processing mode, this paper puts forward the definition and connotation of SBD. Subsequently this paper further summarizes and extracts the application rules and methods of SBD. And then nine principles of SBD are explored and their relationship and application are addressed from the view of theory architecture and working framework in data processing flow. At last, this paper also discusses the potential applications and some hot issues of SBD. Overall, this paper will play an essential role in supporting the application of SBD. In addition, it will fill in the theory gaps in the field of SBD beyond traditional safety statistics, and further enriches the contents of safety science.}
}
@article{CHATFIELD2018336,
title = {Customer agility and responsiveness through big data analytics for public value creation: A case study of Houston 311 on-demand services},
journal = {Government Information Quarterly},
volume = {35},
number = {2},
pages = {336-347},
year = {2018},
note = {Agile Government and Adaptive Governance in the Public Sector},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17300394},
author = {Akemi Takeoka Chatfield and Christopher G. Reddick},
keywords = {On-demand services, Customer agility, Systemic use, Big data, Big data analytics, IT assimilation, Process-level strategic alignment, Digital infrastructures, 311 services, Government},
abstract = {A theoretical framework for big data analytics-enabled customer agility and responsiveness was developed from extant IS research. In on-demand service environments, customer agility involves dynamic capabilities in sensing and responding to citizens. Using this framework, a case study examined a large city government's 311 on-demand services which had leveraged big data analytics. While we found the localized big data analytics use by some of the 22 departments for enhanced customer agility and on-demand 311 services, city-wide systemic change in on-demand service delivery through big data analytics use was not evident. From the case study we identified key institutional mechanisms for linking customer agility to public value creation through 311 services. We posit how systemic use of big data analytics embedded into critical processes enables the government to co-create public values with citizens through 311 on-demand services, indicating the importance of creating a culture of analytics driven by strong political leadership.}
}
@article{OUSSOUS2018431,
title = {Big Data technologies: A survey},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {30},
number = {4},
pages = {431-448},
year = {2018},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2017.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157817300034},
author = {Ahmed Oussous and Fatima-Zahra Benjelloun and Ayoub {Ait Lahcen} and Samir Belfkih},
keywords = {Big Data, Hadoop, Big Data distributions, Big Data analytics, NoSQL, Machine learning},
abstract = {Developing Big Data applications has become increasingly important in the last few years. In fact, several organizations from different sectors depend increasingly on knowledge extracted from huge volumes of data. However, in Big Data context, traditional data techniques and platforms are less efficient. They show a slow responsiveness and lack of scalability, performance and accuracy. To face the complex Big Data challenges, much work has been carried out. As a result, various types of distributions and technologies have been developed. This paper is a review that survey recent technologies developed for Big Data. It aims to help to select and adopt the right combination of different Big Data technologies according to their technological needs and specific applications’ requirements. It provides not only a global view of main Big Data technologies but also comparisons according to different system layers such as Data Storage Layer, Data Processing Layer, Data Querying Layer, Data Access Layer and Management Layer. It categorizes and discusses main technologies features, advantages, limits and usages.}
}
@article{HUANG201846,
title = {Big-data-driven safety decision-making: A conceptual framework and its influencing factors},
journal = {Safety Science},
volume = {109},
pages = {46-56},
year = {2018},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2018.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518300973},
author = {Lang Huang and Chao Wu and Bing Wang and Qiumei Ouyang},
keywords = {Big data (BD), Safety big data (SBD), Safety decision-making (SDM), Safety insight (SI), Data-driven},
abstract = {Safety data and information are the most valuable assets for organizations’ safety decision-making (SDM), especially in the era of big data (BD). In this study, a conceptual framework for SDM based on BD, known as BD-driven SDM, was developed and its detailed structure and elements as well as strategies were presented. Other theoretical and practical contributions include: (a) the description of the meta-process and interdisciplinary research area of BD-driven SDM, (b) the design of six types of general analytics and five types of special analytics for SBD mining according to different requirements of safety management applications, (c) the analysis of influencing factors of BD-driven SDM, and (d) the discussion of advantages and limitations in this research as well as suggestions for future research. The results obtained from this study are of important implications for research and practice on BD-driven SDM.}
}
@article{WESTRA2017549,
title = {Big data science: A literature review of nursing research exemplars},
journal = {Nursing Outlook},
volume = {65},
number = {5},
pages = {549-561},
year = {2017},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2016.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0029655416303967},
author = {Bonnie L. Westra and Martha Sylvia and Elizabeth F. Weinfurter and Lisiane Pruinelli and Jung In Park and Dianna Dodd and Gail M. Keenan and Patricia Senk and Rachel L. Richesson and Vicki Baukner and Christopher Cruz and Grace Gao and Luann Whittenburg and Connie W. Delaney},
keywords = {Big data, Data science, Nursing informatics, Nursing research, Nurse scientist},
abstract = {Background
Big data and cutting-edge analytic methods in nursing research challenge nurse scientists to extend the data sources and analytic methods used for discovering and translating knowledge.
Purpose
The purpose of this study was to identify, analyze, and synthesize exemplars of big data nursing research applied to practice and disseminated in key nursing informatics, general biomedical informatics, and nursing research journals.
Methods
A literature review of studies published between 2009 and 2015. There were 650 journal articles identified in 17 key nursing informatics, general biomedical informatics, and nursing research journals in the Web of Science database. After screening for inclusion and exclusion criteria, 17 studies published in 18 articles were identified as big data nursing research applied to practice.
Discussion
Nurses clearly are beginning to conduct big data research applied to practice. These studies represent multiple data sources and settings. Although numerous analytic methods were used, the fundamental issue remains to define the types of analyses consistent with big data analytic methods.
Conclusion
There are needs to increase the visibility of big data and data science research conducted by nurse scientists, further examine the use of state of the science in data analytics, and continue to expand the availability and use of a variety of scientific, governmental, and industry data resources. A major implication of this literature review is whether nursing faculty and preparation of future scientists (PhD programs) are prepared for big data and data science.}
}
@article{LI2019168,
title = {Lithium-ion battery modeling based on Big Data},
journal = {Energy Procedia},
volume = {159},
pages = {168-173},
year = {2019},
note = {Renewable Energy Integration with Mini/Microgrid},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2018.12.046},
url = {https://www.sciencedirect.com/science/article/pii/S1876610218313419},
author = {Shuangqi Li and Jianwei Li and Hongwen He and Hanxiao Wang},
keywords = {electric vehicle, lithium-ion power battery, modeling, battery management, bigdata, deeplearning},
abstract = {Battery is the bottleneck technology of electric vehicles. The complex chemical reactions inside the battery are difficult to monitor directly. The establishment of a precise mathematical model for the battery is of great significance in ensuring the secure and stable operation of the battery management system. First of all, a data cleaning method based on machine learning is put forward, which is applicable to the characteristics of big data from batteries in electric vehicles. Secondly, this paper establishes a lithium-ion battery model based on deep learning algorithm and the error of model based on different algorithms is compared. The data of electric buses are used for validating the effectiveness of the model. The result shows that the data cleaning method achieves good results, in the case of the terminal voltage missing, the mean absolute percentage error of filling is within 4%, and the battery modeling method in this paper is able to simulate the battery characteristics accurately, and the mean absolute percentage error of the terminal voltage estimation is within 2.5%.}
}
@article{SHENG201797,
title = {A multidisciplinary perspective of big data in management research},
journal = {International Journal of Production Economics},
volume = {191},
pages = {97-112},
year = {2017},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S092552731730169X},
author = {Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang},
keywords = {Big data, Management research, Literature review},
abstract = {In recent years, big data has emerged as one of the prominent buzzwords in business and management. In spite of the mounting body of research on big data across the social science disciplines, scholars have offered little synthesis on the current state of knowledge. To take stock of academic research that contributes to the big data revolution, this paper tracks scholarly work's perspectives on big data in the management domain over the past decade. We identify key themes emerging in management studies and develop an integrated framework to link the multiple streams of research in fields of organisation, operations, marketing, information management and other relevant areas. Our analysis uncovers a growing awareness of big data's business values and managerial changes led by data-driven approach. Stemming from the review is the suggestion for research that both structured and unstructured big data should be harnessed to advance understanding of big data value in informing organisational decisions and enhancing firm competitiveness. To discover the full value, firms need to formulate and implement a data-driven strategy. In light of these, the study identifies and outlines the implications and directions for future research.}
}
@article{AKOKA2017105,
title = {Research on Big Data – A systematic mapping study},
journal = {Computer Standards & Interfaces},
volume = {54},
pages = {105-115},
year = {2017},
note = {SI: New modeling in Big Data},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917300211},
author = {Jacky Akoka and Isabelle Comyn-Wattiau and Nabil Laoufi},
keywords = {Big Data, Systematic mapping study, Framework, Artefact, Usage, Analytics},
abstract = {Big Data has emerged as a significant area of study for both practitioners and researchers. Big Data is a term for massive data sets with large structure. In 2012, Big Data passed the top of the Gartner Hype Cycle, attesting the maturity level of this technology and its applications. The aim of this paper is to examine how do researchers grasp the big data concept? We will answer the following questions: How many research papers are produced? What is the annual trend of publications? What are the hot topics in big data research? What are the most investigated big data topics? Why the research is performed? What are the most frequently obtained research artefacts? What does big data research produces? Who are the active authors? Which journals include papers on Big Data? What are the active disciplines? For this purpose, we provide a framework identifying existing and emerging research areas of Big Data. This framework is based on eight dimensions, including the SMACIT (Social Mobile Analytics Cloud Internet of Things) perspective. Current and past research in Big Data are analyzed using a systematic mapping study of publications based on more than a decade of related academic publications. The results have shown that significant contributions have been made by the research community, attested by a continuous increase in the number of scientific publications that address Big Data. We found that researchers are increasingly involved in research combining Big Data and Analytics, Cloud, Internet of things, mobility or social media. As for quality objectives, besides an interest in performance, other topics as scalability is emerging. Moreover, security and quality aspects become important. Researchers on Big Data provide more algorithms, frameworks, and architectures than other artifacts. Finally, application domains such as earth, energy, medicine, ecology, marketing, and health attract more attention from researchers on big data. A complementary content analysis on a subset of papers sheds some light on the evolving field of big data research.}
}
@article{RUAN2021100110,
title = {Health-adjusted life expectancy (HALE) in Chongqing, China, 2017: An artificial intelligence and big data method estimating the burden of disease at city level},
journal = {The Lancet Regional Health - Western Pacific},
volume = {9},
pages = {100110},
year = {2021},
issn = {2666-6065},
doi = {https://doi.org/10.1016/j.lanwpc.2021.100110},
url = {https://www.sciencedirect.com/science/article/pii/S2666606521000195},
author = {Xiaowen Ruan and Yue Li and Xiaohui Jin and Pan Deng and Jiaying Xu and Na Li and Xian Li and Yuqi Liu and Yiyi Hu and Jingwen Xie and Yingnan Wu and Dongyan Long and Wen He and Dongsheng Yuan and Yifei Guo and Heng Li and He Huang and Shan Yang and Mei Han and Bojin Zhuang and Jiang Qian and Zhenjie Cao and Xuying Zhang and Jing Xiao and Liang Xu},
abstract = {Background
A universally applicable approach that provides standard HALE measurements for different regions has yet to be developed because of the difficulties of health information collection. In this study, we developed a natural language processing (NLP) based HALE estimation approach by using individual-level electronic medical records (EMRs), which made it possible to calculate HALE timely in different temporal or spatial granularities.
Methods
We performed diagnostic concept extraction and normalisation on 13•99 million EMRs with NLP to estimate the prevalence of 254 diseases in WHO Global Burden of Disease Study (GBD). Then, we calculated HALE in Chongqing, 2017, by using the life table technique and Sullivan's method, and analysed the contribution of diseases to the expected years “lost” due to disability (DLE).
Findings
Our method identified a life expectancy at birth (LE0) of 77•9 years and health-adjusted life expectancy at birth (HALE0) of 71•7 years for the general Chongqing population of 2017. In particular, the male LE0 and HALE0 were 76•3 years and 68•9 years, respectively, while the female LE0 and HALE0 were 80•0 years and 74•4 years, respectively. Cerebrovascular diseases, cancers, and injuries were the top three deterioration factors, which reduced HALE by 2•67, 2•15, and 1•19 years, respectively.
Interpretation
The results demonstrated the feasibility and effectiveness of EMRs-based HALE estimation. Moreover, the method allowed for a potentially transferable framework that facilitated a more convenient comparison of cross-sectional and longitudinal studies on HALE between regions. In summary, this study provided insightful solutions to the global ageing and health problems that the world is facing.
Funding
National Key R and D Program of China (2018YFC2000400).}
}
@incollection{WHELAN2020365,
title = {Chapter 27 - Genetics, imaging, and cognition: big data approaches to addiction research},
editor = {Antonio Verdejo-Garcia},
booktitle = {Cognition and Addiction},
publisher = {Academic Press},
pages = {365-377},
year = {2020},
isbn = {978-0-12-815298-0},
doi = {https://doi.org/10.1016/B978-0-12-815298-0.00027-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128152980000277},
author = {Robert Whelan and Zhipeng Cao and Laura O'Halloran and Brian Pennie},
keywords = {Addiction, Big Data, Cognition, Genetics, Machine learning, Neuroimaging, Online methods},
abstract = {The etiology and trajectory of addictions is complex, caused and moderated by individual differences in cognition that are themselves a function of genetics and of environment. In this chapter, we discuss how “Big Data” can shed light on the cognitive correlates of addiction. Big Data is primarily data-driven, using algorithms that search for patterns in data, with accurate prediction on previously unseen data as the metric of success. In this chapter, we introduce and provide practical advice on Big Data approaches for addiction. In the first part of this chapter, we describe how online methods of data collection facilitate the collection of large datasets. In the second section, we outline some recent advances in neuroimaging, with a focus on prediction of substance use using machine learning methods. In the final section, we present advances in genetics—meta- and megaanalyses—which may provide breakthroughs in our understanding of the genetics of addiction.}
}
@article{TCHAGNAKOUANOU201868,
title = {An optimal big data workflow for biomedical image analysis},
journal = {Informatics in Medicine Unlocked},
volume = {11},
pages = {68-74},
year = {2018},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2018.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352914818300844},
author = {Aurelle {Tchagna Kouanou} and Daniel Tchiotsop and Romanic Kengne and Djoufack Tansaa Zephirin and Ngo Mouelas {Adele Armele} and René Tchinda},
keywords = {Biomedical images, Big data, Artificial intelligence, Machine learning, Hadoop/spark},
abstract = {Background and objective
In the medical field, data volume is increasingly growing, and traditional methods cannot manage it efficiently. In biomedical computation, the continuous challenges are: management, analysis, and storage of the biomedical data. Nowadays, big data technology plays a significant role in the management, organization, and analysis of data, using machine learning and artificial intelligence techniques. It also allows a quick access to data using the NoSQL database. Thus, big data technologies include new frameworks to process medical data in a manner similar to biomedical images. It becomes very important to develop methods and/or architectures based on big data technologies, for a complete processing of biomedical image data.
Method
This paper describes big data analytics for biomedical images, shows examples reported in the literature, briefly discusses new methods used in processing, and offers conclusions. We argue for adapting and extending related work methods in the field of big data software, using Hadoop and Spark frameworks. These provide an optimal and efficient architecture for biomedical image analysis. This paper thus gives a broad overview of big data analytics to automate biomedical image diagnosis. A workflow with optimal methods and algorithm for each step is proposed.
Results
Two architectures for image classification are suggested. We use the Hadoop framework to design the first, and the Spark framework for the second. The proposed Spark architecture allows us to develop appropriate and efficient methods to leverage a large number of images for classification, which can be customized with respect to each other.
Conclusions
The proposed architectures are more complete, easier, and are adaptable in all of the steps from conception. The obtained Spark architecture is the most complete, because it facilitates the implementation of algorithms with its embedded libraries.}
}
@article{SANTOSO201793,
title = {Data Warehouse with Big Data Technology for Higher Education},
journal = {Procedia Computer Science},
volume = {124},
pages = {93-99},
year = {2017},
note = {4th Information Systems International Conference 2017, ISICO 2017, 6-8 November 2017, Bali, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.12.134},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917329022},
author = {Leo Willyanto Santoso and  Yulia},
keywords = {Data Warehouse, Big Data, Academic, Hadoop, Higher Education, Analysis},
abstract = {Nowadays, data warehouse tools and technologies cannot handle the load and analytic process of data into meaningful information for top management. Big data technology should be implemented to extend the existing data warehouse solutions. Universities already collect vast amounts of data so the academic data of university has been growing significantly and become a big academic data. These datasets are rich and growing. University’s top-level management needs tools to produce information from the records. The generated information is expected to support the decision-making process of top-level management. This paper explores how big data technology could be implemented with data warehouse to support decision making process. In this framework, we propose Hadoop as big data analytic tools to be implemented for data ingestion/staging. The paper concludes by outlining future directions relating to the development and implementation of an institutional project on Big Data.}
}
@article{VERBRUGGHE2019298,
title = {The electronic medical record: Big data, little information?},
journal = {Journal of Critical Care},
volume = {54},
pages = {298-299},
year = {2019},
issn = {0883-9441},
doi = {https://doi.org/10.1016/j.jcrc.2019.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0883944119313723},
author = {Walter Verbrugghe and Kirsten Colpaert}
}
@article{YAN2018457,
title = {Energy-efficient shipping: An application of big data analysis for optimizing engine speed of inland ships considering multiple environmental factors},
journal = {Ocean Engineering},
volume = {169},
pages = {457-468},
year = {2018},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2018.08.050},
url = {https://www.sciencedirect.com/science/article/pii/S0029801818316421},
author = {Xinping Yan and Kai Wang and Yupeng Yuan and Xiaoli Jiang and Rudy R. Negenborn},
keywords = {Ship energy efficiency, Speed optimization, Big data analysis, Parallel k-means algorithm, Hadoop},
abstract = {Energy efficiency of inland ships is significantly influenced by navigational environment, including wind speed and direction as well as water depth and speed. The complexity of the inland navigational environment makes it rather difficult to determine the optimal speeds under different environmental conditions to achieve the best energy efficiency. Route division according to the characteristics of these environmental factors could provide a good solution for the optimization of ship engine speed under different navigational environments. In this paper, the distributed parallel k-means clustering algorithm is adopted to achieve an elaborate route division by analyzing the corresponding environmental factors based on a self-developed big data analytics platform. Subsequently, a ship energy efficiency optimization model considering multiple environmental factors is established through analyzing the energy transfer among hull, propeller and main engine. Then, decisions are made concerning the optimal engine speeds in different segments along the path. Finally, a case study on the Yangtze River is performed to validate the present optimization method. The results show that the proposed method can effectively reduce energy consumption and CO2 emissions of ships.}
}
@article{SCOTT201820,
title = {The role of Statistics in the era of big data: Crucial, critical and under-valued},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {20-24},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.050},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300956},
author = {E. Marian Scott},
keywords = {Data, Sampling, Variability, Inference, Uncertainty},
abstract = {What is the role of Statistics in the era of big data, or is Statistics still relevant? I will start this rather personal view with my answer. Statistics remains highly relevant irrespective of ‘bigness’ of data, its role remains what is has always been, but is even more important now. As a community, we need to improve our explanations and presentations to make more visible our relevance.}
}
@article{LOFGREN2021R1312,
title = {Fungal biodiversity and conservation mycology in light of new technology, big data, and changing attitudes},
journal = {Current Biology},
volume = {31},
number = {19},
pages = {R1312-R1325},
year = {2021},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2021.06.083},
url = {https://www.sciencedirect.com/science/article/pii/S096098222100912X},
author = {Lotus A. Lofgren and Jason E. Stajich},
abstract = {Summary
Fungi have successfully established themselves across seemingly every possible niche, substrate, and biome. They are fundamental to biogeochemical cycling, interspecies interactions, food production, and drug bioprocessing, as well as playing less heroic roles as difficult to treat human infections and devastating plant pathogens. Despite community efforts to estimate and catalog fungal diversity, we have only named and described a minute fraction of the fungal world. The identification, characterization, and conservation of fungal diversity is paramount to preserving fungal bioresources, and to understanding and predicting ecosystem cycling and the evolution and epidemiology of fungal disease. Although species and ecosystem conservation are necessarily the foundation of preserving this diversity, there is value in expanding our definition of conservation to include the protection of biological collections, ecological metadata, genetic and genomic data, and the methods and code used for our analyses. These definitions of conservation are interdependent. For example, we need metadata on host specificity and biogeography to understand rarity and set priorities for conservation. To aid in these efforts, we need to draw expertise from diverse fields to tie traditional taxonomic knowledge to data obtained from modern -omics-based approaches, and support the advancement of diverse research perspectives. We also need new tools, including an updated framework for describing and tracking species known only from DNA, and the continued integration of functional predictions to link genetic diversity to functional and ecological diversity. Here, we review the state of fungal diversity research as shaped by recent technological advancements, and how changing viewpoints in taxonomy, -omics, and systematics can be integrated to advance mycological research and preserve fungal biodiversity.}
}
@article{JIN201559,
title = {Significance and Challenges of Big Data Research},
journal = {Big Data Research},
volume = {2},
number = {2},
pages = {59-64},
year = {2015},
note = {Visions on Big Data},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2015.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615000076},
author = {Xiaolong Jin and Benjamin W. Wah and Xueqi Cheng and Yuanzhuo Wang},
keywords = {Big data, Data complexity, Computational complexity, System complexity},
abstract = {In recent years, the rapid development of Internet, Internet of Things, and Cloud Computing have led to the explosive growth of data in almost every industry and business area. Big data has rapidly developed into a hot topic that attracts extensive attention from academia, industry, and governments around the world. In this position paper, we first briefly introduce the concept of big data, including its definition, features, and value. We then identify from different perspectives the significance and opportunities that big data brings to us. Next, we present representative big data initiatives all over the world. We describe the grand challenges (namely, data complexity, computational complexity, and system complexity), as well as possible solutions to address these challenges. Finally, we conclude the paper by presenting several suggestions on carrying out big data projects.}
}
@article{WIBISONO201929,
title = {Average Restrain Divider of Evaluation Value (ARDEV) in data stream algorithm for big data prediction},
journal = {Knowledge-Based Systems},
volume = {176},
pages = {29-39},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119301443},
author = {Ari Wibisono and Devvi Sarwinda},
keywords = {ARDEV, Big data prediction, FIMT-DD, Tree regression},
abstract = {Today, big data processing has become a challenging task due to the amount of data collected using various sensors increasingly significantly. To build knowledge and predict the data, traditional data mining methods calculate all numerical attributes into the memory simultaneously. The data stream method is a solution for processing and calculating data. The method streams incrementally in batch form; therefore, infrastructure memory is sufficient to develop knowledge. The existing method for data stream prediction is FIMT-DD (Fast Incremental Model Tree-Drift Detection). Using this method, knowledge is developed in tree form for every instance. In this paper, enhanced FIMT-DD is proposed using ARDEV (Average Restrain Divider of Evaluation Value). ARDEV utilizes the Chernoff bound approach with error evaluation, improvement in learning rate, modification of perceptron rule calculation, and utilization of activation function. Standard FIMT-DD separates the tree formation process and perceptron prediction. The proposed method evaluates and connects the development of the tree for knowledge formation and the perceptron rule for prediction. The prediction accuracy of the proposed method is measured using MAE, RMSE and MAPE. From the experiment performed, the utilization of ARDEV enhancement shows significant improvement in terms of accuracy prediction. Statistically, the overall accuracy prediction improvement is approximately 6.99 % compared to standard FIMT-DD with a traffic dataset.}
}
@article{AKHAVANHEJAZI201891,
title = {Power systems big data analytics: An assessment of paradigm shift barriers and prospects},
journal = {Energy Reports},
volume = {4},
pages = {91-100},
year = {2018},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352484717300616},
author = {Hossein Akhavan-Hejazi and Hamed Mohsenian-Rad},
keywords = {Energy, Big data analytics, Internet of energy, Smart grid},
abstract = {Electric power systems are taking drastic advances in deployment of information and communication technologies; numerous new measurement devices are installed in forms of advanced metering infrastructure, distributed energy resources (DER) monitoring systems, high frequency synchronized wide-area awareness systems that with great speed are generating immense volume of energy data. However, it is still questioned that whether the today’s power system data, the structures and the tools being developed are indeed aligned with the pillars of the big data science. Further, several requirements and especial features of power systems and energy big data call for customized methods and platforms. This paper provides an assessment of the distinguished aspects in big data analytics developments in the domain of power systems. We perform several taxonomy of the existing and the missing elements in the structures and methods associated with big data analytics in power systems. We also provide a holistic outline, classifications, and concise discussions on the technical approaches, research opportunities, and application areas for energy big data analytics.}
}
@article{GHASEMAGHAEI201969,
title = {Does big data enhance firm innovation competency? The mediating role of data-driven insights},
journal = {Journal of Business Research},
volume = {104},
pages = {69-84},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0148296319304138},
author = {Maryam Ghasemaghaei and Goran Calic},
keywords = {Big data characteristics, Descriptive insight, Predictive insight, Prescriptive insight, Innovation competency},
abstract = {Grounded in gestalt insight learning theory and organizational learning theory, we collected data from 280 middle and top-level managers to investigate the impact of each big data characteristic (i.e., data volume, data velocity, data variety, and data veracity) on firm innovation competency (i.e., exploitation competency and exploration competency), mediated through data-driven insight generation (i.e., descriptive insight, predictive insight, and prescriptive insight). Findings show that while data velocity, variety, and veracity enhance data-driven insight generation, data volume does not impact it. Additionally, results of the post hoc analysis indicate that while descriptive and predictive insights improve innovation competency, prescriptive insight does not affect it. These results provide interesting and unique theoretical and practical insights.}
}
@article{GUPTA201878,
title = {Big data with cognitive computing: A review for the future},
journal = {International Journal of Information Management},
volume = {42},
pages = {78-89},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218304110},
author = {Shivam Gupta and Arpan Kumar Kar and Abdullah Baabdullah and Wassan A.A. Al-Khowaiter},
keywords = {Big data, Cognitive computing, Literature review, Resource based View, Institutional theory},
abstract = {Analysis of data by humans can be a time-consuming activity and thus use of sophisticated cognitive systems can be utilized to crunch this enormous amount of data. Cognitive computing can be utilized to reduce the shortcomings of the concerns faced during big data analytics. The aim of the study is to provide readers a complete understanding of past, present and future directions in the domain big data and cognitive computing. A systematic literature review has been adopted for this study by using the Scopus, DBLP and Web of Science databases. The work done in the field of big data and cognitive computing is currently at the nascent stage and this is evident from the publication record. The characteristics of cognitive computing, namely observation, interpretation, evaluation and decision were mapped to the five V’s of big data namely volume, variety, veracity, velocity and value. Perspectives which touch all these parameters are yet to be widely explored in existing literature.}
}
@article{HASHEM201598,
title = {The rise of “big data” on cloud computing: Review and open research issues},
journal = {Information Systems},
volume = {47},
pages = {98-115},
year = {2015},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2014.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306437914001288},
author = {Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Nor Badrul Anuar and Salimah Mokhtar and Abdullah Gani and Samee {Ullah Khan}},
keywords = {Big data, Cloud computing, Hadoop},
abstract = {Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized.}
}
@article{COX2018111,
title = {Big data: Some statistical issues},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {111-115},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300609},
author = {D.R. Cox and Christiana Kartsonaki and Ruth H. Keogh},
keywords = {Big data, Electronic health records, Epidemiology, Metrology, Precision},
abstract = {A broad review is given of the impact of big data on various aspects of investigation. There is some but not total emphasis on issues in epidemiological research.}
}
@article{SUN201827,
title = {Lossless Pruned Naive Bayes for Big Data Classifications},
journal = {Big Data Research},
volume = {14},
pages = {27-36},
year = {2018},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S2214579616301320},
author = {Nanfei Sun and Bingjun Sun and Jian (Denny) Lin and Michael Yu-Chi Wu},
keywords = {Big data, Classification, Naive Bayes, Large-scale taxonomy, Lossless, Pruned},
abstract = {In a fast growing big data era, volume and varieties of data processed in Internet applications drastically increase. Real-world search engines commonly use text classifiers with thousands of classes to improve relevance or data quality. These large scale classification problems lead to severe runtime performance challenges, so practitioners often resort to fast approximation techniques. However, the increase in classification speed comes at a cost, as approximations are lossy, mis-assigning classes relative to the original reference classification algorithm. To address this problem, we introduce a Lossless Pruned Naive Bayes (LPNB) classification algorithm tailored to real-world, big data applications with thousands of classes. LPNB achieves significant speed-ups by drawing on Information Retrieval (IR) techniques for efficient posting list traversal and pruning. We show empirically that LPNB can classify text up to eleven times faster than standard Naive Bayes on a real-world data set with 7205 classes, with larger gains extrapolated for larger taxonomies. In practice, the achieved acceleration is significant as it can greatly cut required computation time. In addition, it is lossless: the output is identical to standard Naive Bayes, in contrast to extant techniques such as hierarchical classification. The acceleration does not rely on the taxonomy structure, and it can be used for both hierarchical and flat taxonomies.}
}
@article{TAI2019101704,
title = {Machine learning and big data: Implications for disease modeling and therapeutic discovery in psychiatry},
journal = {Artificial Intelligence in Medicine},
volume = {99},
pages = {101704},
year = {2019},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2019.101704},
url = {https://www.sciencedirect.com/science/article/pii/S0933365717301781},
author = {Andy M.Y. Tai and Alcides Albuquerque and Nicole E. Carmona and Mehala Subramanieapillai and Danielle S. Cha and Margarita Sheko and Yena Lee and Rodrigo Mansur and Roger S. McIntyre},
keywords = {Big data, Machine learning, Precision medicine, AI, Mental health, Mental disease, Psychiatry, Data mining, RDoC, Research domain criteria, DSM-5. Schizophrenia, ADHD, Alzheimer, Depression, fMRI, MRI, Algorithms, IBM Watson, Neuro networking, Random forests, Decision trees, Support vector machines},
abstract = {Introduction
Machine learning capability holds promise to inform disease models, the discovery and development of novel disease modifying therapeutics and prevention strategies in psychiatry. Herein, we provide an introduction on how machine learning/Artificial Intelligence (AI) may instantiate such capabilities, as well as provide rationale for its application to psychiatry in both research and clinical ecosystems.
Methods
Databases PubMed and PsycINFO were searched from 1966 to June 2016 for keywords:Big Data, Machine Learning, Precision Medicine, Artificial Intelligence, Mental Health, Mental Disease, Psychiatry, Data Mining, RDoC, and Research Domain Criteria. Articles selected for review were those that were determined to be aligned with the objective of this particular paper.
Results
Results indicate that AI is a viable option to build useful predictors of outcome while offering objective and comparable accuracy metrics, a unique opportunity, particularly in mental health research. The approach has also consistently brought notable insight into disease models through processing the vast amount of already available multi-domain, semi-structured medical data. The opportunity for AI in psychiatry, in addition to disease-model refinement, is in characterizing those at risk, and it is likely also relevant to personalizing and discovering therapeutics.
Conclusions
Machine learning currently provides an opportunity to parse disease models in complex, multi-factorial disease states (e.g. mental disorders) and could possibly inform treatment selection with existing therapies and provide bases for domain-based therapeutic discovery.}
}
@article{SAGGI2018758,
title = {A survey towards an integration of big data analytics to big insights for value-creation},
journal = {Information Processing & Management},
volume = {54},
number = {5},
pages = {758-790},
year = {2018},
note = {In (Big) Data we trust: Value creation in knowledge organizations},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2018.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0306457316307178},
author = {Mandeep Kaur Saggi and Sushma Jain},
keywords = {Big data, Data analytics, Machine learning, Big data visualization, Decision-making, Smart agriculture, Smart city application, Value-creation, Value-discover, Value-realization},
abstract = {Big Data Analytics (BDA) is increasingly becoming a trending practice that generates an enormous amount of data and provides a new opportunity that is helpful in relevant decision-making. The developments in Big Data Analytics provide a new paradigm and solutions for big data sources, storage, and advanced analytics. The BDA provide a nuanced view of big data development, and insights on how it can truly create value for firm and customer. This article presents a comprehensive, well-informed examination, and realistic analysis of deploying big data analytics successfully in companies. It provides an overview of the architecture of BDA including six components, namely: (i) data generation, (ii) data acquisition, (iii) data storage, (iv) advanced data analytics, (v) data visualization, and (vi) decision-making for value-creation. In this paper, seven V's characteristics of BDA namely Volume, Velocity, Variety, Valence, Veracity, Variability, and Value are explored. The various big data analytics tools, techniques and technologies have been described. Furthermore, it presents a methodical analysis for the usage of Big Data Analytics in various applications such as agriculture, healthcare, cyber security, and smart city. This paper also highlights the previous research, challenges, current status, and future directions of big data analytics for various application platforms. This overview highlights three issues, namely (i) concepts, characteristics and processing paradigms of Big Data Analytics; (ii) the state-of-the-art framework for decision-making in BDA for companies to insight value-creation; and (iii) the current challenges of Big Data Analytics as well as possible future directions.}
}
@article{DEFREITASVISCONDI201954,
title = {A Systematic Literature Review on big data for solar photovoltaic electricity generation forecasting},
journal = {Sustainable Energy Technologies and Assessments},
volume = {31},
pages = {54-63},
year = {2019},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2018.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S2213138818301036},
author = {Gabriel {de Freitas Viscondi} and Solange N. Alves-Souza},
keywords = {Systematic Literature Review, Solar energy forecasting, Machine learning, Data mining},
abstract = {Solar power is expected to play a substantial role globally, due to it being one of the leading renewable electricity sources for future use. Even though the use of solar irradiation to generate electricity is currently at a fast deployment pace and technological evolution, its natural variability still presents an important barrier to overcome. Machine learning and data mining techniques arise as alternatives to aid solar electricity generation forecast reducing the impacts of its natural inconstant power supply. This paper presents a literature review on big data models for solar photovoltaic electricity generation forecasts, aiming to evaluate the most applicable and accurate state-of-art techniques to the problem, including the motivation behind each project proposal, the characteristics and quality of data used to address the problem, among other issues. A Systematic Literature Review (SLR) method was used, in which research questions were defined and translated into search strings. The search returned 38 papers for final evaluation, affirming that the use of these models to predict solar electricity generation is currently an ongoing academic research question. Machine learning is widely used, and neural networks is considered the most accurate algorithm. Extreme learning machine learning has reduced time and raised precision.}
}
@article{COBB2018640,
title = {Big data: More than big data sets},
journal = {Surgery},
volume = {164},
number = {4},
pages = {640-642},
year = {2018},
issn = {0039-6060},
doi = {https://doi.org/10.1016/j.surg.2018.06.022},
url = {https://www.sciencedirect.com/science/article/pii/S0039606018303660},
author = {Adrienne N. Cobb and Andrew J. Benjamin and Erich S. Huang and Paul C. Kuo},
abstract = {The term big data has been popularized over the past decade and is often used to refer to data sets that are too large or complex to be analyzed by traditional means. Although the term has been utilized for some time in business and engineering, the concept of big data is relatively new to medicine. The reception from the medical community has been mixed; however, the widespread utilization of electronic health records in the United States, the creation of large clinical data sets and national registries that capture information on numerous vectors affecting healthcare delivery and patient outcomes, and the sequencing of the human genome are all opportunities to leverage big data. This review was inspired by a lively panel discussion on big data that took place at the 75th Central Surgical Association Annual Meeting. The authors’ aim was to describe big data, the methodologies used to analyze big data, and their practical clinical application.}
}
@article{BAYNE2018481,
title = {Big Data in Neonatal Health Care: Big Reach, Big Reward?},
journal = {Critical Care Nursing Clinics of North America},
volume = {30},
number = {4},
pages = {481-497},
year = {2018},
note = {Neonatal Nursing},
issn = {0899-5885},
doi = {https://doi.org/10.1016/j.cnc.2018.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0899588518309754},
author = {Lynn E. Bayne},
keywords = {Big data, Electronic health record (EHR), Neonatology, Cost-savings, Clinical decision making, Healthcare analytics}
}
@article{SAFHI201930,
title = {Assessing reliability of Big Data Knowledge Discovery process},
journal = {Procedia Computer Science},
volume = {148},
pages = {30-36},
year = {2019},
note = {THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919300055},
author = {Hicham Moad Safhi and Bouchra Frikh and Brahim Ouhbi},
keywords = {Knowledge discovery, Reliability, Trustworthiness, Quality, Big Data mining},
abstract = {Extracting knowledge from Big Data is the process of transforming this data into actionable information. The exponential growth of data has initiated a myriad of new opportunities, and made data become the most valuable raw material of production for many organizations. Mining Big Data is coupled with some challenges, known as the 3V’s of Big Data: Volume, Variety and Velocity. However, a major challenge that needs to be addressed, and often is ignored in the literature, concerns reliability. Actually, data is agglomerated from multiple disparate sources, and each of Knowledge Discovery (KDD) process steps may be carried out by different organizations. These considerations lead us to ask a critical question that is weather the information we have at each step is reliable enough to proceed to the next one? This paper therefore aims to provide a framework that automatically assesses reliability of the knowledge discovery process. We focus on Linked Open Data (LOD) as a source of data, as it constitutes a relevant data provider in many Big Data applications. However, our framework can also be adapted for unstructured data. This framework will assist scientists to automatically and efficiently measure the reliability of each KDD process stage as well as detect unreliable steps that should be revised. Following this methodology, KDD process will be optimized and therefore produce knowledge with higher quality.}
}
@article{MOKTADIR20191063,
title = {Barriers to big data analytics in manufacturing supply chains: A case study from Bangladesh},
journal = {Computers & Industrial Engineering},
volume = {128},
pages = {1063-1075},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2018.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0360835218301505},
author = {Md. Abdul Moktadir and Syed Mithun Ali and Sanjoy Kumar Paul and Nagesh Shukla},
keywords = {AHP, Big data analytics, Barriers to BDA, Delphi, Information and communication technology (ICT), Manufacturing supply chains},
abstract = {Recently, big data (BD) has attracted researchers and practitioners due to its potential usefulness in decision-making processes. Big data analytics (BDA) is becoming increasingly popular among manufacturing companies as it helps gain insights and make decisions based on BD. However, there many barriers to the adoption of BDA in manufacturing supply chains. It is therefore necessary for manufacturing companies to identify and examine the nature of each barrier. Previous studies have mostly built conceptual frameworks for BDA in a given situation and have ignored examining the nature of the barriers to BDA. Due to the significance of both BD and BDA, this research aims to identify and examine the critical barriers to the adoption of BDA in manufacturing supply chains in the context of Bangladesh. This research explores the existing body of knowledge by examining these barriers using a Delphi-based analytic hierarchy process (AHP). Data were obtained from five Bangladeshi manufacturing companies. The findings of this research are as follows: (i) data-related barriers are most important, (ii) technology-related barriers are second, and (iii) the five most important components of these barriers are (a) lack of infrastructure, (b) complexity of data integration, (c) data privacy, (d) lack of availability of BDA tools and (e) high cost of investment. The findings can assist industrial managers to understand the actual nature of the barriers and potential benefits of using BDA and to make policy regarding BDA adoption in manufacturing supply chains. A sensitivity analysis was carried out to justify the robustness of the barrier rankings.}
}
@article{XIA2018191,
title = {Using spatiotemporal patterns to optimize Earth Observation Big Data access: Novel approaches of indexing, service modeling and cloud computing},
journal = {Computers, Environment and Urban Systems},
volume = {72},
pages = {191-203},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0198971518300401},
author = {Jizhe Xia and Chaowei Yang and Qingquan Li},
keywords = {Spatiotemporal optimization, Big Data, Cloud computing, Global operation, GEOSS},
abstract = {Based on our GEOSS Clearinghouse operating experience, we summarized the three Earth Observation (EO) Big Data access challenges, namely, fast access, accurate service estimation and global access, and two essential research questions: are there any spatiotemporal patterns when end users access EO data, and how can these spatiotemporal patterns be utilized to better facilitate EO Big Data access? To tackle these two research questions, we conducted a two-year pattern analysis with 2+ million user access records. The spatial pattern, temporal pattern and spatiotemporal pattern of user-data interactions were explored. For the second research question, we developed three spatiotemporal optimization strategies to respond to the three access challenges: a) spatiotemporal indexing to accelerate data access, b) spatiotemporal service modeling to improve data access accuracy and c) spatiotemporal cloud computing to enhance global access. This research is a pioneering framework for spatiotemporal optimization of EO Big Data access and valuable for other multidisciplinary geographic data and information research.}
}
@article{HONG2018175,
title = {Big Data in Health Care: Applications and Challenges},
journal = {Data and Information Management},
volume = {2},
number = {3},
pages = {175-197},
year = {2018},
issn = {2543-9251},
doi = {https://doi.org/10.2478/dim-2018-0014},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122000791},
author = {Liang Hong and Mengqi Luo and Ruixue Wang and Peixin Lu and Wei Lu and Long Lu},
keywords = {Big Data, public health, cloud computing, medical applications},
abstract = {The concept of Big Data is popular in a variety of domains. The purpose of this review was to summarize the features, applications, analysis approaches, and challenges of Big Data in health care. Big Data in health care has its own features, such as heterogeneity, incompleteness, timeliness and longevity, privacy, and ownership. These features bring a series of challenges for data storage, mining, and sharing to promote health-related research. To deal with these challenges, analysis approaches focusing on Big Data in health care need to be developed and laws and regulations for making use of Big Data in health care need to be enacted. From a patient perspective, application of Big Data analysis could bring about improved treatment and lower costs. In addition to patients, government, hospitals, and research institutions could also benefit from the Big Data in health care.}
}
@article{GUPTA2019466,
title = {Circular economy and big data analytics: A stakeholder perspective},
journal = {Technological Forecasting and Social Change},
volume = {144},
pages = {466-474},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.06.030},
url = {https://www.sciencedirect.com/science/article/pii/S0040162517314488},
author = {Shivam Gupta and Haozhe Chen and Benjamin T. Hazen and Sarabjot Kaur and Ernesto D.R. {Santibañez Gonzalez}},
keywords = {Circular economy, Big data, Stakeholder theory, Relational view, Supply chain management, Sustainability},
abstract = {The business concept of the circular economy (CE) has gained significant momentum among practitioners and researchers alike. However, successful adoption and implementation of this paradigm of managing business remains a challenge. In this article, we build a case for utilizing big data analytics (BDA) as a fundamental basis for informed and data driven decision making in supply chain networks supporting CE. We view this from a stakeholder perspective and argue that a collaborative association among all supply chain members can positively affect CE implementation. We propose a model highlighting the facilitating role of big data analytics for achieving shared sustainability goals. The model is based on integrating thematic categories coming out of 10 semi-structured interviews with key position holders in industry. We argue that mutual support and coordination driven by a stakeholder perspective coupled with holistic information processing and sharing along the entire supply chain network can effectively create a basis for achieving the triple bottom line of economic, ecological and social benefits. The proposed model is useful for managers in that it provides a reference point for aligning activities with the circular economy paradigm. The conceptual model provides a theoretical basis for future empirical research in this domain.}
}
@article{JANSSEN2017338,
title = {Factors influencing big data decision-making quality},
journal = {Journal of Business Research},
volume = {70},
pages = {338-345},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316304945},
author = {Marijn Janssen and Haiko {van der Voort} and Agung Wahyudi},
keywords = {Big data, Big data analytics, Big data chain, E-government, Governance, Decision-making, Decision-making quality},
abstract = {Organizations are looking for ways to harness the power of big data (BD) to improve their decision making. Despite its significance the effects of BD on decision-making quality has been given scant attention in the literature. In this paper factors influencing decision-making based on BD are identified using a case study. BD is collected from different sources that have various data qualities and are processed by various organizational entities resulting in the creation of a big data chain. The veracity (manipulation, noise), variety (heterogeneity of data) and velocity (constantly changing data sources) amplified by the size of big data calls for relational and contractual governance mechanisms to ensure BD quality and being able to contextualize data. The case study reveals that taking advantage of big data is an evolutionary process in which the gradually understanding of the potential of big data and the routinization of processes plays a crucial role.}
}
@article{LI20191259,
title = {Big data driven lithium-ion battery modeling method based on SDAE-ELM algorithm and data pre-processing technology},
journal = {Applied Energy},
volume = {242},
pages = {1259-1273},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.03.154},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919305495},
author = {Shuangqi Li and Hongwen He and Jianwei Li},
keywords = {Electric vehicles, Battery energy storage, Temperature-dependent model, Battery management system, Big data, Deep learning},
abstract = {As one of the bottleneck technologies of electric vehicles (EVs), the battery hosts complex and hardly observable internal chemical reactions. Therefore, a precise mathematical model is crucial for the battery management system (BMS) to ensure the secure and stable operation of the battery in a multi-variable environment. First, a Cloud-based BMS (C-BMS) is established based on a database containing complete battery status information. Next, a data cleaning method based on machine learning is applied to the big data of batteries. Meanwhile, to improve the model stability under dynamic conditions, an F-divergence-based data distribution quality assessment method and a sampling-based data preprocess method is designed. Then, a lithium-ion battery temperature-dependent model is built based on Stacked Denoising Autoencoders- Extreme Learning Machine (SDAE-ELM) algorithm, and a new training method combined with data preprocessing is also proposed to improve the model accuracy. Finally, to improve reliability, a conjunction working mode between the C-BMS and the BMS in vehicles (V-BMS) is also proposed, providing as an applied case of the model. Using the battery data extracted from electric buses, the effectiveness and accuracy of the model are validated. The error of the estimated battery terminal voltage is within 2%, and the error of the estimated State of Charge (SoC) is within 3%.}
}
@article{CORTEREAL2019160,
title = {Unlocking the drivers of big data analytics value in firms},
journal = {Journal of Business Research},
volume = {97},
pages = {160-173},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2018.12.072},
url = {https://www.sciencedirect.com/science/article/pii/S0148296318306908},
author = {Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira and Aleš Popovič},
keywords = {IT business value, Big data analytics (BDA), Delphi method, Mixed methodology, Competitive advantage},
abstract = {Although big data analytics (BDA) is considered the next “frontier” in data science by creating potential business opportunities, the way to extract those opportunities is unclear. This paper aims to understand the antecedents of BDA value at a firm level. The authors performed a study using a mixed methodology approach. First, by carrying out a Delphi study to explore and rank the antecedents affecting the creation of BDA value. Based on the Delphi results, we propose an empirically validated model supported by a survey conducted on 175 European firms to explain the antecedents of BDA sustained value. The results show that the proposed model explains 62% of BDA sustained value at the firm level, where the most critical contributor is BDA use. We provide directions for managers to support their decisions on BDA strategy definition and refinement. For academics, we extend BDA value literature and outline some potential research opportunities.}
}
@incollection{SHARMA202137,
title = {Chapter 2 - Deep learning in big data and data mining},
editor = {Vincenzo Piuri and Sandeep Raj and Angelo Genovese and Rajshree Srivastava},
booktitle = {Trends in Deep Learning Methodologies},
publisher = {Academic Press},
pages = {37-61},
year = {2021},
series = {Hybrid Computational Intelligence for Pattern Analysis},
isbn = {978-0-12-822226-3},
doi = {https://doi.org/10.1016/B978-0-12-822226-3.00002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222263000027},
author = {Deepak Kumar Sharma and Bhanu Tokas and Leo Adlakha},
keywords = {Aspect extraction, Big data, CRISP-DM, Customer relations, Data mining, Data visualization, Deep learning, Distributed computing, LSTM, Machine learning},
abstract = {The growth of the digital age has led to a colossal leap in data generated by the average user. This growing data has several applications: businesses can use it to give a more personalized touch to their services, governments can use it to better allocate their funds, and companies can utilize it to select the best candidates for a job. While these applications may seem extremely enticing, there are a couple of problems that must be solved first, namely, data collection and extraction of useful patterns from the data. The disciplines of data mining and big data deal with these problems, respectively. But, as we have already discussed, the amount of data is so vast that any manual approach is extremely time intensive and costly. Thus this limits the potential outcomes from this data. This problem has been solved by the application of deep learning. Deep learning has allowed us to automate processes that were not only time intensive but also mentally arduous. It has achieved better than human accuracy in several types of discriminative and recognition tasks making it a viable alternative to inefficient human labor. Deep learning plays a vital role in this analysis and has enabled several businesses to comprehend customer needs and accordingly improve their own services, thus giving them the opportunity to outdo their competitors. Similarly, deep learning has also been instrumental in analyzing the trends and associations of securities in the financial market. It has even helped to create fraud detection and loan underwriting applications, which have contributed to making financial institutions more transparent and efficient. Apart from directly improving the efficiency in these fields, deep learning has also been instrumental in improving the fields of data mining and big data. Machine learning algorithms can actually utilize the existing data to predict the unknowns, including future trends in data. Due to its potential applications the field of machine learning is deeply interconnected with data mining. Nevertheless, machine learning algorithms are often heavily dependent on the availability of huge datasets to ensure useful accuracy. Deep learning algorithms have allowed the different components of data (i.e., multimedia data) in the data mining process itself to be identified. Similarly, semantic indexing and tagging algorithms have allowed the processes of big data to speed up. In this chapter, we will discuss the applications of deep learning in these fields and give a brief overview of the concepts involved.}
}
@article{PEZOULAS2019270,
title = {Medical data quality assessment: On the development of an automated framework for medical data curation},
journal = {Computers in Biology and Medicine},
volume = {107},
pages = {270-283},
year = {2019},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010482519300733},
author = {Vasileios C. Pezoulas and Konstantina D. Kourou and Fanis Kalatzis and Themis P. Exarchos and Aliki Venetsanopoulou and Evi Zampeli and Saviana Gandolfo and Fotini Skopouli and Salvatore {De Vita} and Athanasios G. Tzioufas and Dimitrios I. Fotiadis},
keywords = {Big data, Data quality, Data quality assessment, Data curation, Data standardization},
abstract = {Data quality assessment has gained attention in the recent years since more and more companies and medical centers are highlighting the importance of an automated framework to effectively manage the quality of their big data. Data cleaning, also known as data curation, lies in the heart of the data quality assessment and is a key aspect prior to the development of any data analytics services. In this work, we present the objectives, functionalities and methodological advances of an automated framework for data curation from a medical perspective. The steps towards the development of a system for data quality assessment are first described along with multidisciplinary data quality measures. A three-layer architecture which realizes these steps is then presented. Emphasis is given on the detection and tracking of inconsistencies, missing values, outliers, and similarities, as well as, on data standardization to finally enable data harmonization. A case study is conducted in order to demonstrate the applicability and reliability of the proposed framework on two well-established cohorts with clinical data related to the primary Sjögren's Syndrome (pSS). Our results confirm the validity of the proposed framework towards the automated and fast identification of outliers, inconsistencies, and highly-correlated and duplicated terms, as well as, the successful matching of more than 85% of the pSS-related medical terms in both cohorts, yielding more accurate, relevant, and consistent clinical data.}
}
@article{REN20191343,
title = {A comprehensive review of big data analytics throughout product lifecycle to support sustainable smart manufacturing: A framework, challenges and future research directions},
journal = {Journal of Cleaner Production},
volume = {210},
pages = {1343-1365},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618334255},
author = {Shan Ren and Yingfeng Zhang and Yang Liu and Tomohiko Sakao and Donald Huisingh and Cecilia M.V.B. Almeida},
keywords = {Big data analytics, Smart manufacturing, Servitization, Sustainable production, Conceptual framework, Product lifecycle},
abstract = {Smart manufacturing has received increased attention from academia and industry in recent years, as it provides competitive advantage for manufacturing companies making industry more efficient and sustainable. As one of the most important technologies for smart manufacturing, big data analytics can uncover hidden knowledge and other useful information like relations between lifecycle decisions and process parameters helping industrial leaders to make more-informed business decisions in complex management environments. However, according to the literature, big data analytics and smart manufacturing were individually researched in academia and industry. To provide theoretical foundations for the research community to further develop scientific insights in applying big data analytics to smart manufacturing, it is necessary to summarize the existing research progress and weakness. In this paper, through combining the key technologies of smart manufacturing and the idea of ubiquitous servitization in the whole lifecycle, the term of sustainable smart manufacturing was coined. A comprehensive overview of big data in smart manufacturing was conducted, and a conceptual framework was proposed from the perspective of product lifecycle. The proposed framework allows analyzing potential applications and key advantages, and the discussion of current challenges and future research directions provides valuable insights for academia and industry.}
}
@article{SIEMSANDERSON2019100071,
title = {An adaptive big data weather system for surface transportation},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {3},
pages = {100071},
year = {2019},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2019.100071},
url = {https://www.sciencedirect.com/science/article/pii/S2590198219300703},
author = {Amanda R. Siems-Anderson and Curtis L. Walker and Gerry Wiener and William P. Mahoney and Sue Ellen Haupt},
keywords = {Big data, Pikalert, Road weather, Surface transportation, Pavement condition, Weather forecasts},
abstract = {Operating modern multi-modal surface transportation systems are becoming increasingly automated and driven by decision support systems. One aspect necessary for successful, safe, reliable, and efficient operation of any transportation network is real-time and forecasted weather and pavement condition information. Providing such information requires an adaptive system capable of blending large amounts of observational and model data that arrives quickly, in disparate formats and times, and blends and optimizes their use via expert systems and machine-learning algorithms. Quality control of the data is also essential, and historical data is required to both develop expert-based empirical algorithms and train machine learning models. This paper reports on the open-source Pikalert® system that brings together weather information and real-time data from connected vehicles to provide crucial information to enhance the safety and efficiency of surface transportation systems. This robust framework can be applied to a diverse array of user community specifications and is designed to rapidly ingest more, unique data sets as they become available. Ultimately, the developmental framework of this system will provide critical environmental information necessary to promote the development, growth, refinement, and expanded adoption of automated and connected multi-modal vehicular systems globally.}
}
@article{ALBADI2018271,
title = {Exploring Big Data Governance Frameworks},
journal = {Procedia Computer Science},
volume = {141},
pages = {271-277},
year = {2018},
note = {The 9th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2018) / The 8th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918318313},
author = {Ali Al-Badi and Ali Tarhini and Asharul Islam Khan},
keywords = {Big Data, Big Data model, Big Data governance, Data management, Big Data governance framework, Big Data analytic},
abstract = {The recent explosion in ICT and digital data has led organizations, both private and public, to efficient decision-making. Nowadays organizations can store huge amounts of data, which can be accessible at any time. Big Data governance refers to the management of huge volumes of an organization’s data, exploiting it in the organization’s decision-making using different analytical tools. Big Data emergence provides great convenience, but it also brings challenges. Nevertheless, for Big Data governance, data has to be prepared in a timely manner, keeping in view the consistency and reliability of the data, and being able to trust its source and the meaningfulness of the result. Hence, a framework for Big Data governance would have many advantages. There are Big Data governance frameworks, which guide the management of Big Data. However, there are also limitations associated with these frameworks. Therefore, this study aims to explore the existing Big Data governance frameworks and their shortcomings, and propose a new framework. The proposed framework consists of eight components. As a framework validation, the proposed framework has been compared with the ISO 8000 data governance framework.}
}
@article{SILVA2020893,
title = {Identification of Patterns of Fatal Injuries in Humans through Big Data},
journal = {Procedia Computer Science},
volume = {170},
pages = {893-898},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.114},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920305524},
author = {Jesus Silva and Jack Zilberman and Ligia Romero and Omar Bonerge Pineda and Yaneth Herazo-Beltran},
keywords = {Recognition of automated standards, mining, decision trees},
abstract = {External cause injuries are defined as intentionally or unintentionally harm or injury to a person, which may be caused by trauma, poisoning, assault, accidents, etc., being fatal (fatal injury) or not leading to death (non-fatal injury). External injuries have been considered a global health problem for two decades. This work aims to determine criminal patterns using data mining techniques to a sample of patients from Mumbai city in India.}
}
@incollection{HAYMOND202137,
title = {Chapter 3 - Machine learning and big data in pediatric laboratory medicine},
editor = {Dennis Dietzen and Michael Bennett and Edward Wong and Shannon Haymond},
booktitle = {Biochemical and Molecular Basis of Pediatric Disease (Fifth Edition)},
publisher = {Academic Press},
edition = {Fifth Edition},
pages = {37-70},
year = {2021},
isbn = {978-0-12-817962-8},
doi = {https://doi.org/10.1016/B978-0-12-817962-8.00018-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128179628000184},
author = {Shannon Haymond and Randall K. Julian and Emily L. Gill and Stephen R. Master},
keywords = {Artificial intelligence, Big data, Laboratory medicine, Machine learning, Pediatrics, Regulation},
abstract = {Clinical laboratories generate a large number of test results, creating opportunities for improved data management and the use of analytics. Aggregate analyses of these data have potential diagnostic value but require labs to utilize computational tools for the analysis of high-dimensional data. Machine learning can be used to aid decision-making, whether for clinical or operational purposes, using a variety of algorithms to analyze complex data sets and make reliable predictions. This chapter discusses key concepts related to big data and its application to pediatric laboratory medicine. Machine learning workflows, concepts, common algorithms, and related infrastructure requirements are also covered.}
}
@article{LIN2019197,
title = {Data source selection for information integration in big data era},
journal = {Information Sciences},
volume = {479},
pages = {197-213},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518309162},
author = {Yiming Lin and Hongzhi Wang and Jianzhong Li and Hong Gao},
keywords = {Source selection, Data integration, Data cleaning},
abstract = {In big data era, information integration often requires abundant data extracted from massive data sources. Due to a large number of data sources, data source selection plays a crucial role in information integration, since it is costly and even impossible to access all data sources. Data Source selection should consider both efficiency and effectiveness issues. For efficiency, the approach should scale to large data source amount. From effectiveness aspect, data quality and overlapping of sources are to be considered. In this paper, we study source selection problem in Big Data and propose methods which can scale to datasets with up to millions of data sources and guarantee the quality of results. Motivated by this, we propose a new metric taking the expected number of true values a source can provide as a criteria to evaluate the contribution of a data source. Based on our proposed index, we present a scalable algorithm and two pruning strategies to improve the efficiency without sacrificing precision. Experimental results on both real world and synthetic data sets show that our methods can select sources providing a large proportion of true values efficiently and can scale to massive data sources.}
}
@article{ZARRINPAR2020599,
title = {What Can We Learn About Drug Safety and Other Effects in the Era of Electronic Health Records and Big Data That We Would Not Be Able to Learn From Classic Epidemiology?},
journal = {Journal of Surgical Research},
volume = {246},
pages = {599-604},
year = {2020},
issn = {0022-4804},
doi = {https://doi.org/10.1016/j.jss.2019.09.053},
url = {https://www.sciencedirect.com/science/article/pii/S0022480419306985},
author = {Ali Zarrinpar and Ting-Yuan {David Cheng} and Zhiguang Huo},
keywords = {Electronic health record, Big data, Drug safety, Health care database, Cancer risk},
abstract = {As more and more health systems have converted to the use of electronic health records, the amount of searchable and analyzable data is exploding. This includes not just provider or laboratory created data but also data collected by instruments, personal devices, and patients themselves, among others. This has led to more attention being paid to the analysis of these data to answer previously unaddressed questions. This is especially important given the number of therapies previously found to be beneficial in clinical trials that are currently being re-scrutinized. Because there are orders of magnitude more information contained in these data sets, a fundamentally different approach needs to be taken to their processing and analysis and the generation of knowledge. Health care and medicine are drivers of this phenomenon and will ultimately be the main beneficiaries. Concurrently, many different types of questions can now be asked using these data sets. Research groups have become increasingly active in mining large data sets, including nationwide health care databases, to learn about associations of medication use and various unrelated diseases such as cancer. Given the recent increase in research activity in this area, its promise to radically change clinical research, and the relative lack of widespread knowledge about its potential and advances, we surveyed the available literature to understand the strengths and limitations of these new tools. We also outline new databases and techniques that are available to researchers worldwide, with special focus on work pertaining to the broad and rapid monitoring of drug safety and secondary effects.}
}
@article{NASHIPUDIMATH2020100033,
title = {An efficient integration and indexing method based on feature patterns and semantic analysis for big data},
journal = {Array},
volume = {7},
pages = {100033},
year = {2020},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2020.100033},
url = {https://www.sciencedirect.com/science/article/pii/S2590005620300187},
author = {Madhu Mahesh Nashipudimath and Subhash K. Shinde and Jayshree Jain},
keywords = {Big data, Integration, Feature patterns, Indexing, Semantic analysis},
abstract = {Big Data has received much attention in the multi-domain industry. In the digital and computing world, information is generated and collected at a rate that quickly exceeds the boundaries. The traditional data integration system interconnects the limited number of resources and is built with relatively stable and generally complex and time-consuming design activities. However, the rapid growth of these large data sets creates difficulties in learning heterogeneous data structures for integration and indexing. It also creates difficulty in information retrieval for the various data analysis requirements. In this paper, a probabilistic feature Patterns (PFP) approach using feature transformation and selection method is proposed for efficient data integration and utilizing the features latent semantic analysis (F-LSA) method for indexing the unsupervised multiple heterogeneous integrated cluster data sources. The PFP approach takes the advantage of the features transformation and selection mechanism to map and cluster the data for the integration, and an analysis of the data features context relation using LSA to provide the appropriate index for fast and accurate data extraction. A huge volume of BibText dataset from different publication sources are processed to evaluated to understand the effectiveness of the proposal. The analytical study and the outcome results show the improvisation in integration and indexing of the work.}
}
@article{LI2018301,
title = {Big data in tourism research: A literature review},
journal = {Tourism Management},
volume = {68},
pages = {301-323},
year = {2018},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2018.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0261517718300591},
author = {Jingjing Li and Lizhi Xu and Ling Tang and Shouyang Wang and Ling Li},
keywords = {Tourism research, Big data, Literature review, Tourism management, Tourist behavior},
abstract = {Even at an early stage, diverse big data have been applied to tourism research and made an amazing improvement. This paper might be the first attempt to present a comprehensive literature review on different types of big data in tourism research. By data sources, the tourism-related big data fall into three primary categories: UGC data (generated by users), including online textual data and online photo data; device data (by devices), including GPS data, mobile roaming data, Bluetooth data, etc.; transaction data (by operations), including web search data, webpage visiting data, online booking data, etc. Carrying different information, different data types address different tourism issues. For each type, a systematical analysis is conducted from the perspectives of research focuses, data characteristics, analytic techniques, major challenges and further directions. This survey facilitates a thorough understanding of this sunrise research and offers valuable insights into its future prospects.}
}
@article{VANDERVOORT201927,
title = {Rationality and politics of algorithms. Will the promise of big data survive the dynamics of public decision making?},
journal = {Government Information Quarterly},
volume = {36},
number = {1},
pages = {27-38},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2018.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X17304951},
author = {H.G. {van der Voort} and A.J. Klievink and M. Arnaboldi and A.J. Meijer},
abstract = {Big data promises to transform public decision-making for the better by making it more responsive to actual needs and policy effects. However, much recent work on big data in public decision-making assumes a rational view of decision-making, which has been much criticized in the public administration debate. In this paper, we apply this view, and a more political one, to the context of big data and offer a qualitative study. We question the impact of big data on decision-making, realizing that big data – including its new methods and functions – must inevitably encounter existing political and managerial institutions. By studying two illustrative cases of big data use processes, we explore how these two worlds meet. Specifically, we look at the interaction between data analysts and decision makers. In this we distinguish between a rational view and a political view, and between an information logic and a decision logic. We find that big data provides ample opportunities for both analysts and decision makers to do a better job, but this doesn't necessarily imply better decision-making, because big data also provides opportunities for actors to pursue their own interests. Big data enables both data analysts and decision makers to act as autonomous agents rather than as links in a functional chain. Therefore, big data's impact cannot be interpreted only in terms of its functional promise; it must also be acknowledged as a phenomenon set to impact our policymaking institutions, including their legitimacy.}
}
@article{MIKALEF2020103169,
title = {Exploring the relationship between big data analytics capability and competitive performance: The mediating roles of dynamic and operational capabilities},
journal = {Information & Management},
volume = {57},
number = {2},
pages = {103169},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618301022},
author = {Patrick Mikalef and John Krogstie and Ilias O. Pappas and Paul Pavlou},
keywords = {Big data analytics, Dynamic capabilities, Operational capabilities, Business value, Resource-based view},
abstract = {A central question for information systems (IS) researchers and practitioners is if, and how, big data can help attain a competitive advantage. To address this question, this study draws on the resource-based view, dynamic capabilities view, and on recent literature on big data analytics, and examines the indirect relationship between a firm’s big data analytics capability (BDAC) and competitive performance. The study extends existing research by proposing that BDACs enable firms to generate insight that can help strengthen their dynamic capabilities, which, in turn, positively impact marketing and technological capabilities. To test our proposed research model, we used survey data from 202 chief information officers and IT managers working in Norwegian firms. By means of partial least squares structural equation modeling, results show that a strong BDAC can help firms build a competitive advantage. This effect is not direct but fully mediated by dynamic capabilities, which exerts a positive and significant effect on two types of operational capabilities: marketing and technological capabilities. The findings suggest that IS researchers should look beyond direct effects of big data investments and shift their attention on how a BDAC can be leveraged to enable and support organizational capabilities.}
}
@article{ANEJIONU2019456,
title = {Spatial urban data system: A cloud-enabled big data infrastructure for social and economic urban analytics},
journal = {Future Generation Computer Systems},
volume = {98},
pages = {456-473},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.03.052},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18319046},
author = {Obinna C.D. Anejionu and Piyushimita (Vonu) Thakuriah and Andrew McHugh and Yeran Sun and David McArthur and Phil Mason and Rod Walpole},
keywords = {Urban big data infrastructure, Urban analytics, Spatial urban indicators, Small area assessment, Spatial big data},
abstract = {The Spatial Urban Data System (SUDS) is a spatial big data infrastructure to support UK-wide analytics of the social and economic aspects of cities and city-regions. It utilises data generated from traditional as well as new and emerging sources of urban data. The SUDS deploys geospatial technology, synthetic small area urban metrics, and cloud computing to enable urban analytics, and geovisualization with the goal of deriving actionable knowledge for better urban management and data-driven urban decision making. At the core of the system is a programme of urban indicators generated by using novel forms of data and urban modelling and simulation programme. SUDS differs from other similar systems by its emphasis on the generation and use of regularly updated spatially-activated urban area metrics from real or near-real time data sources, to enhance understanding of intra-city interactions and dynamics. By deploying public transport, labour market accessibility and housing advertisement data in the system, we were able to identify spatial variations of key urban services at intra-city levels as well as social and economically-marginalised output areas in major cities across the UK. This paper discusses the design and implementation of SUDS, the challenges and limitations encountered, and considerations made during its development. The innovative approach adopted in the design of SUDS will enable it to support research and analysis of urban areas, policy and city administration, business decision-making, private sector innovation, and public engagement. Having been tested with housing, transport and employment metrics, efforts are ongoing to integrate information from other sources such as IoT, and User Generated Content into the system to enable urban predictive analytics.}
}
@article{RAJABION2019271,
title = {Healthcare big data processing mechanisms: The role of cloud computing},
journal = {International Journal of Information Management},
volume = {49},
pages = {271-289},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0268401217304917},
author = {Lila Rajabion and Abdusalam Abdulla Shaltooki and Masoud Taghikhah and Amirhossein Ghasemi and Arshad Badfar},
keywords = {Cloud computing, Processing, Healthcare, Big data, Review},
abstract = {Recently, patient safety and healthcare have gained high attention in professional and health policy-makers. This rapid growth causes generating a high amount of data, which is known as big data. Therefore, handling and processing of this data are attracted great attention. Cloud computing is one of the main choices for handling and processing of this type of data. But, as far as we know, the detailed review and deep discussion in this filed are very rare. Therefore, this paper reviews and discusses the recently introduced mechanisms in this field as well as providing a deep analysis of their applied mechanisms. Moreover, the drawbacks and benefits of the reviewed mechanisms have been discussed and the main challenges of these mechanisms are highlighted for developing more efficient healthcare big data processing techniques over cloud computing in the future.}
}
@article{UEDA2019150,
title = {Delineation of Nitrogen Signaling Networks: Computational Approaches in the Big Data Era},
journal = {Molecular Plant},
volume = {12},
number = {2},
pages = {150-152},
year = {2019},
issn = {1674-2052},
doi = {https://doi.org/10.1016/j.molp.2019.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1674205219300139},
author = {Yoshiaki Ueda and Shuichi Yanagisawa}
}
@article{LIANG2019290,
title = {A survey on big data-driven digital phenotyping of mental health},
journal = {Information Fusion},
volume = {52},
pages = {290-307},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1566253518305244},
author = {Yunji Liang and Xiaolong Zheng and Daniel D. Zeng},
keywords = {Digital phenotyping, Big data, Mental health, Data mining, Information fusion},
abstract = {The landscape of mental health has undergone tremendous changes within the last two decades, but the research on mental health is still at the initial stage with substantial knowledge gaps and the lack of precise diagnosis. Nowadays, big data and artificial intelligence offer new opportunities for the screening and prediction of mental problems. In this review paper, we outline the vision of digital phenotyping of mental health (DPMH) by fusing the enriched data from ubiquitous sensors, social media and healthcare systems, and present a broad overview of DPMH from sensing and computing perspectives. We first conduct a systematical literature review and propose the research framework, which highlights the key aspects related with mental health, and discuss the challenges elicited by the enriched data for digital phenotyping. Next, five key research strands including affect recognition, cognitive analytics, behavioral anomaly detection, social analytics, and biomarker analytics are unfolded in the psychiatric context. Finally, we discuss various open issues and the corresponding solutions to underpin the digital phenotyping of mental health.}
}
@article{BRADLOW201779,
title = {The Role of Big Data and Predictive Analytics in Retailing},
journal = {Journal of Retailing},
volume = {93},
number = {1},
pages = {79-95},
year = {2017},
note = {The Future of Retailing},
issn = {0022-4359},
doi = {https://doi.org/10.1016/j.jretai.2016.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0022435916300835},
author = {Eric T. Bradlow and Manish Gangwar and Praveen Kopalle and Sudhir Voleti},
keywords = {Big data, Predictive analytics, Retailing, Pricing},
abstract = {The paper examines the opportunities in and possibilities arising from big data in retailing, particularly along five major data dimensions—data pertaining to customers, products, time, (geo-spatial) location and channel. Much of the increase in data quality and application possibilities comes from a mix of new data sources, a smart application of statistical tools and domain knowledge combined with theoretical insights. The importance of theory in guiding any systematic search for answers to retailing questions, as well as for streamlining analysis remains undiminished, even as the role of big data and predictive analytics in retailing is set to rise in importance, aided by newer sources of data and large-scale correlational techniques. The Statistical issues discussed include a particular focus on the relevance and uses of Bayesian analysis techniques (data borrowing, updating, augmentation and hierarchical modeling), predictive analytics using big data and a field experiment, all in a retailing context. Finally, the ethical and privacy issues that may arise from the use of big data in retailing are also highlighted.}
}
@article{CUQUET201874,
title = {The societal impact of big data: A research roadmap for Europe},
journal = {Technology in Society},
volume = {54},
pages = {74-86},
year = {2018},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2018.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X17300131},
author = {Martí Cuquet and Anna Fensel},
keywords = {Big data, Research roadmap, Societal externalities, Skills development, Standardisation},
abstract = {With its rapid growth and increasing adoption, big data is producing a substantial impact in society. Its usage is opening both opportunities such as new business models and economic gains and risks such as privacy violations and discrimination. Europe is in need of a comprehensive strategy to optimise the use of data for a societal benefit and increase the innovation and competitiveness of its productive activities. In this paper, we contribute to the definition of this strategy with a research roadmap to capture the economic, social and ethical, legal and political benefits associated with the use of big data in Europe. The present roadmap considers the positive and negative externalities associated with big data, maps research and innovation topics in the areas of data management, processing, analytics, protection, visualisation, as well as non-technical topics, to the externalities they can tackle, and provides a time frame to address these topics in order to deliver social impact, skills development and standardisation. Finally, it also identifies what sectors will be most benefited by each of the research efforts. The goal of the roadmap is to guide European research efforts to develop a socially responsible big data economy, and to allow stakeholders to identify and meet big data challenges and proceed with a shared understanding of the societal impact, positive and negative externalities and concrete problems worth investigating in future programmes.}
}
@article{HUANCHUN2020102024,
title = {Analyzing the Influencing Factors of Urban Thermal Field Intensity Using Big-Data-Based GIS},
journal = {Sustainable Cities and Society},
volume = {55},
pages = {102024},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102024},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720300111},
author = {Huang Huanchun and Yang Hailin and Deng Xin and Hao Cui and Liu Zhifeng and Liu Wei and Zeng Peng},
keywords = {land-surface temperature, thermal field pattern, POI data, GIS, air temperature},
abstract = {The effects of human activities and land cover changes on urban thermal field patterns are closely related to the land surface temperature (LST) and air temperature. At present, the number of studies on the quantitative relationship between these two indexes and the effect of the observational scale on their influence is insufficient. In this study, spatial analysis methods such as geographic modeling were combined with remote sensing images, meteorological data, and points of insert and used to investigate the composition and scale of the factors influencing the temperature field in Beijing. The results showed that there are differences in the positive and negative correlations between LST and air temperature and various influencing factors. At a spatial resolution of 90 m, LST had a strong linear relationship with the average air temperature. Indicators reflecting elements of human activity, such as buildings, roads, and entertainment, were easily measured by meteorological stations at a small scale, and the natural green space ratio could also be easily captured by satellite thermal sensors at small scales. These results have substantial implications for environmental impact assessments in areas experiencing an increasing urban heat island effect due to rapid urbanization.}
}
@article{GALETSI2019112533,
title = {Values, challenges and future directions of big data analytics in healthcare: A systematic review},
journal = {Social Science & Medicine},
volume = {241},
pages = {112533},
year = {2019},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2019.112533},
url = {https://www.sciencedirect.com/science/article/pii/S0277953619305271},
author = {P. Galetsi and K. Katsaliaki and S. Kumar},
keywords = {Systematic review, Big data analytics, Health-medicine, Decision-making, Organizational and societal values, Preferred reporting items for systematic reviews and meta-analyses},
abstract = {The emergence of powerful software has created conditions and approaches for large datasets to be collected and analyzed which has led to informed decision-making towards tackling health issues. The objective of this study is to systematically review 804 scholarly publications related to big data analytics in health in order to identify the organizational and social values along with associated challenges. Key principles of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology were followed for conducting systematic reviews. Following a research path, we present the values, challenges and future directions of the scientific area using indicative examples from relevant published articles. The study reveals that one of the main values created is the development of analytical techniques which provides personalized health services to users and supports human decision-making using automated algorithms, challenging the power issues in the doctor-patient relationship and creating new working conditions. A main challenge to data analytics is data management and security when processing large volumes of sensitive, personal health data. Future research is directed towards the development of systems that will standardize and secure the process of extracting private healthcare datasets from relevant organizations. Our systematic literature review aims to provide to governments and health policy-makers a better understanding of how the development of a data driven strategy can improve public health and the functioning of healthcare organizations but also how can create challenges that need to be addressed in the near future to avoid societal malfunctions.}
}
@incollection{LYTRAS2021xvii,
title = {Preface: artificial intelligence and big data analytics for smart healthcare: a digital transformation of healthcare primer},
editor = {Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui},
booktitle = {Artificial Intelligence and Big Data Analytics for Smart Healthcare},
publisher = {Academic Press},
pages = {xvii-xxvii},
year = {2021},
series = {Next Gen Tech Driven Personalized Med&Smart Healthcare},
isbn = {978-0-12-822060-3},
doi = {https://doi.org/10.1016/B978-0-12-822060-3.00018-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220603000188},
author = {Miltiadis D. Lytras and Anna Visvizi and Akila Sarirete and Kwok Tai Chui}
}
@article{URREHMAN2019247,
title = {The role of big data analytics in industrial Internet of Things},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {247-259},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18313645},
author = {Muhammad Habib {ur Rehman} and Ibrar Yaqoob and Khaled Salah and Muhammad Imran and Prem Prakash Jayaraman and Charith Perera},
keywords = {Internet of Things, Cyber-physical systems, Cloud computing, Analytics, Big data},
abstract = {Big data production in industrial Internet of Things (IIoT) is evident due to the massive deployment of sensors and Internet of Things (IoT) devices. However, big data processing is challenging due to limited computational, networking and storage resources at IoT device-end. Big data analytics (BDA) is expected to provide operational- and customer-level intelligence in IIoT systems. Although numerous studies on IIoT and BDA exist, only a few studies have explored the convergence of the two paradigms. In this study, we investigate the recent BDA technologies, algorithms and techniques that can lead to the development of intelligent IIoT systems. We devise a taxonomy by classifying and categorising the literature on the basis of important parameters (e.g. data sources, analytics tools, analytics techniques, requirements, industrial analytics applications and analytics types). We present the frameworks and case studies of the various enterprises that have benefited from BDA. We also enumerate the considerable opportunities introduced by BDA in IIoT. We identify and discuss the indispensable challenges that remain to be addressed, serving as future research directions.}
}
@article{NIMMAGADDA2018143,
title = {On big data-guided upstream business research and its knowledge management},
journal = {Journal of Business Research},
volume = {89},
pages = {143-158},
year = {2018},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2018.04.029},
url = {https://www.sciencedirect.com/science/article/pii/S0148296318302054},
author = {Shastri L. Nimmagadda and Torsten Reiners and Lincoln C. Wood},
keywords = {Upstream business, Heterogeneous and multidimensional data, Data warehousing and mining, Big Data paradigm, Spatial-temporal dimensions},
abstract = {The emerging Big Data integration imposes diverse challenges, compromising the sustainable business research practice. Heterogeneity, multi-dimensionality, velocity, and massive volumes that challenge Big Data paradigm may preclude the effective data and system integration processes. Business alignments get affected within and across joint ventures as enterprises attempt to adapt to changes in industrial environments rapidly. In the context of the Oil and Gas industry, we design integrated artefacts for a resilient multidimensional warehouse repository. With access to several decades of resource data in upstream companies, we incorporate knowledge-based data models with spatial-temporal dimensions in data schemas to minimize ambiguity in warehouse repository implementation. The design considerations ensure uniqueness and monotonic properties of dimensions, maintaining the connectivity between artefacts and achieving the business alignments. The multidimensional attributes envisage Big Data analysts a scope of business research with valuable new knowledge for decision support systems and adding further business values in geographic scales.}
}
@incollection{DAS2020127,
title = {Chapter 7 - A Framework Development on Big Data Analytics for Terahertz Healthcare},
editor = {Amit Banerjee and Basabi Chakraborty and Hiroshi Inokawa and Jitendra {Nath Roy}},
booktitle = {Terahertz Biomedical and Healthcare Technologies},
publisher = {Elsevier},
pages = {127-143},
year = {2020},
isbn = {978-0-12-818556-8},
doi = {https://doi.org/10.1016/B978-0-12-818556-8.00007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128185568000070},
author = {Debashis Das and Chinmay Chakraborty and Sourav Banerjee},
keywords = {3D imaging, Big data, Genomic expression, Healthcare technology, Medical sensors, Personal medical information, Radiology images},
abstract = {This chapter is mainly focused on the development of big data analytics in terahertz healthcare technology. In today's world, “big data” is a very familiar term, but the way it is interpreted is modified day by day. Healthcare is one aspect in which big data can be utilized to improve the overall system of healthcare, as, in the context of healthcare, the three primary “V's” of big data definition, volume, variety, and velocity, are very well suited. According to big data analysts, error-free analysis and outcome are ensured from big data, but this is really difficult for medical data due to the issues regarding the data quality. The final V related to big data is value, i.e., how much leverage the data can provide. Thus we can conclude that healthcare is a much preferred area for expert big data analysis. But there are several challenges to be faced in every aspect, starting from data collection to storage, analysis, prediction, etc. The main challenge is the unstructured nature of the data and the organizations from where the data are collected, following no standardized rules, which forms a big gap in the processing of information. Also, a huge investment is required for resources such as high-level expertise, knowledge, technologies used for data analytics, common data warehouses (for obtaining homogeneous data), etc. Despite having so many obstructions, big data analytics has already started growing rapidly in the healthcare sector.}
}
@article{HOLMLUND2020356,
title = {Customer experience management in the age of big data analytics: A strategic framework},
journal = {Journal of Business Research},
volume = {116},
pages = {356-365},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320300345},
author = {Maria Holmlund and Yves {Van Vaerenbergh} and Robert Ciuchita and Annika Ravald and Panagiotis Sarantopoulos and Francisco Villarroel Ordenes and Mohamed Zaki},
keywords = {Customer experience, Customer experience management, Customer experience insight, Big data analytics},
abstract = {Customer experience (CX) has emerged as a sustainable source of competitive differentiation. Recent developments in big data analytics (BDA) have exposed possibilities to unlock customer insights for customer experience management (CXM). Research at the intersection of these two fields is scarce and there is a need for conceptual work that (1) provides an overview of opportunities to use BDA for CXM and (2) guides management practice and future research. The purpose of this paper is therefore to develop a strategic framework for CXM based on CX insights resulting from BDA. Our conceptualisation is comprehensive and is particularly relevant for researchers and practitioners who are less familiar with the potential of BDA for CXM. For managers, we provide a step-by-step guide on how to kick-start or implement our strategic framework. For researchers, we propose some opportunities for future studies in this promising research area.}
}
@article{BIESIALSKA2021106448,
title = {Big Data analytics in Agile software development: A systematic mapping study},
journal = {Information and Software Technology},
volume = {132},
pages = {106448},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106448},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301981},
author = {Katarzyna Biesialska and Xavier Franch and Victor Muntés-Mulero},
keywords = {Agile software development, Software analytics, Data analytics, Machine learning, Artificial intelligence, Literature review},
abstract = {Context:
Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity.
Objective:
Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA).
Method:
As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019.
Results:
In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics.
Conclusions:
As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.}
}
@article{BENASSULI20197,
title = {Bringing big data analytics closer to practice: A methodological explanation and demonstration of classification algorithms},
journal = {Health Policy and Technology},
volume = {8},
number = {1},
pages = {7-13},
year = {2019},
issn = {2211-8837},
doi = {https://doi.org/10.1016/j.hlpt.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2211883718301631},
author = {Ofir Ben-Assuli and Tsipi Heart and Nir Shlomo and Robert Klempfner},
keywords = {Congestive heart failure, Machine learning, Logistic regression, Boosted decision tree, Support vector machine, Neural network},
abstract = {Background
Big data analytics are becoming more prevalent due to the recent availability of health data. Yet in spite of evidence supporting the potential contribution of big data analytics to health policy makers and care providers, these tools are still too complex to be routinely used. Further, access to comprehensive datasets required for more accurate results is complex and costly. Consequently, big data analytics are mostly used by researchers and experts who are far removed from actual clinical practice. Hence, policy makers should allocate resources to encourage studies that clarify and simplify big data analytics so it can be used by non-experts (e.g., clinicians, practitioners and decision-makers who may not have advanced computer skills). It is also important to fund data collection and integration from various health IT, a pre-condition for any big data analytics project.
Objectives
To methodologically clarify the rationale and logic behind several analytics algorithms to help non-expert users employ big data analytics by understanding how to implement relatively easy to use platforms as Azure ML.
Methods
We demonstrate the predictive power of four known algorithms and compare their accuracy in predicting early mortality of Congestive Heart Failure (CHF) patients.
Results
The results of our models outperform those reported in the literature, attesting to the strength of some of the models, and the utility of comprehensive data.
Conclusions
The results support our call to policy makers to allocate resources to establishing comprehensive, integrated health IT systems, and to projects aimed at simplifying ML analytics.}
}
@article{RAUT201910,
title = {Linking big data analytics and operational sustainability practices for sustainable business management},
journal = {Journal of Cleaner Production},
volume = {224},
pages = {10-24},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.03.181},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619308753},
author = {Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Bhaskar B. Gardas and Pragati Priyadarshinee and Balkrishna E. Narkhede},
keywords = {Big-data analytics, Ecological-economic-social sustainability, Green practices, SustainableOperations management, Structural equation modelling-artificial neural network, Emerging economies},
abstract = {Big data analytics is becoming very popular concept in academia as well as in industry. It has come up with new decision tools to design data-driven supply chains. The manufacturing industry is under huge pressure to integrate sustainable practices into their overall business for sustainbale operations management. The purpose of this study is to analyse the predictors of sustainable business performance through big data analytics in the context of developing countries. Data was collected from manufacturing firms those have adopted sustainable practices. A hybrid Structural Equation Modelling - Artificial Neural Network model is used to analyse 316 responses of Indian professional experts. Factor analysis results shows that management and leadership style, state and central-government policy, supplier integration, internal business process, and customer integration have a significant influence on big data analytics and sustainability practices. Furthermore, the results obtained from structural equation modelling were feed as input to the artificial neural network model. The study findings shows that management and leadership style, state and central-government policy as the two most important predictors of big data analytics and sustainability practices. The results provide unique insights into manufacturing firms to improve their sustainable business performance from an operations management viewpoint. The study provides theoretical and practical insights into big data implementation issues in accomplishing sustainability practices in business organisations of emerging economies.}
}
@article{GHANI2019417,
title = {Social media big data analytics: A survey},
journal = {Computers in Human Behavior},
volume = {101},
pages = {417-428},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.08.039},
url = {https://www.sciencedirect.com/science/article/pii/S074756321830414X},
author = {Norjihan Abdul Ghani and Suraya Hamid and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed},
keywords = {Big data, Social media, Machine learning, Analytics},
abstract = {Big data analytics has recently emerged as an important research area due to the popularity of the Internet and the advent of the Web 2.0 technologies. Moreover, the proliferation and adoption of social media applications have provided extensive opportunities and challenges for researchers and practitioners. The massive amount of data generated by users using social media platforms is the result of the integration of their background details and daily activities. This enormous volume of generated data known as “big data” has been intensively researched recently. A review of the recent works is presented to obtain a broad perspective of the social media big data analytics research topic. We classify the literature based on important aspects. This study also compares possible big data analytics techniques and their quality attributes. Moreover, we provide a discussion on the applications of social media big data analytics by highlighting the state-of-the-art techniques, methods, and the quality attributes of various studies. Open research challenges in big data analytics are described as well.}
}
@incollection{LOSHIN201339,
title = {Chapter 5 - Data Governance for Big Data Analytics: Considerations for Data Policies and Processes},
editor = {David Loshin},
booktitle = {Big Data Analytics},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {39-48},
year = {2013},
isbn = {978-0-12-417319-4},
doi = {https://doi.org/10.1016/B978-0-12-417319-4.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124173194000053},
author = {David Loshin},
keywords = {Data governance, accuracy, completeness, consistency, currency, data requirements, consumer expectations, data quality dimensions, metadata, reference data, repurposing, enrichment, enhancement},
abstract = {In this chapter we look at the need for oversight and governance for the data, especially when those developing big data applications often bypass traditional IT and data management channels. Some of the key issues involve the fact that for big data applications that consume massive amounts of data streamed from external sources, there is little or no control that can be exerted to ensure data quality and usability. We consider five key concepts, namely managing data consumer expectations, defining critical data quality dimensions, monitoring consistency of metadata, data repurposing and reinterpretation, and data enrichment when possible.}
}
@article{BOLDOSOVA2020122,
title = {Telling stories that sell: The role of storytelling and big data analytics in smart service sales},
journal = {Industrial Marketing Management},
volume = {86},
pages = {122-134},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118303353},
author = {Valeriia Boldosova},
keywords = {Storytelling, Big data analytics, Smart service, Customer reference, Customer-supplier relationships},
abstract = {The emergence of digitally connected products and big data analytics (BDA) in industrial marketing has attracted academic and managerial interest in smart services. However, suppliers' provision of smart services and customers' adoption of these services have received scarce attention in the literature, demonstrating the need to address the changing nature of customer-supplier interactions in the digital era. Responding to prior research calls, this study utilizes ethnographic research and a storytelling lens to advance our knowledge of how stories and BDA can enhance customers' attitudes toward suppliers' smart services, their behavioral intentions and their actual adoption of smart services. The study's findings demonstrate that storytelling is a collective sensemaking and sensegiving process that occurs in interactions between customers and suppliers in which both parties contribute to the story development. The use of BDA in storytelling enhances customer sensemaking of smart services by highlighting the business value extracted from the digitized data of a reference customer. By synthesizing insights from servitization, storytelling, BDA and the customer reference literature, this study offers managers practical guidance regarding how to increase smart service sales. An example of a story used to facilitate customer adoption of a supplier's smart services in the manufacturing sector is provided.}
}
@article{FENG2021103636,
title = {Tunnel boring machines (TBM) performance prediction: A case study using big data and deep learning},
journal = {Tunnelling and Underground Space Technology},
volume = {110},
pages = {103636},
year = {2021},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2020.103636},
url = {https://www.sciencedirect.com/science/article/pii/S0886779820305903},
author = {Shangxin Feng and Zuyu Chen and Hua Luo and Shanyong Wang and Yufei Zhao and Lipeng Liu and Daosheng Ling and Liujie Jing},
keywords = {TBM performance prediction, Deep belief network (DBN), Yingsong Water Diversion Project, Field penetration index prediction},
abstract = {This work explores the potential for predicting TBM performance using deep learning. It focuses on a 17.5-km-long tunnel excavated for the Yingsong Water Diversion Project in Northeastern China with its 728 days’ continuous monitoring of mechanical data. The prediction uses the deep belief network (DBN) proposed by Hinton et al. (2006),on the penetration rate, cutter rotation speed, torque, and thrust force. Field Penetration Index (FPI) is introduced to quantify TBM performance in the field. The DBN algorithm trains on nth number of preceding elements and predicts the performance of the n + 1th element. Prior to the implementation of the DBN, a pilot test was performed to find the optimal values for the network structural parameters (number of input nodes, number of hidden layers, number of nodes in the hidden layers, and learning rate). Predictions on FPIs in all the three rock types were then proceeded with good agreement with the field measured data. The mean relative errors for the predicted measured FPIs are generally less than 0.15 and the correlation coefficients (R) can be higher than 0.78. The predicted and measured FPI values along the length of the tunnel graphically follow the same trends. These results confirm the usefulness of big data and the deep learning in predicting TBM performance.}
}
@article{ELAGGOUNE2020465,
title = {A fuzzy agent approach for smart data extraction in big data environments},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {32},
number = {4},
pages = {465-478},
year = {2020},
note = {Emerging Software Systems},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819302010},
author = {Zakarya Elaggoune and Ramdane Maamri and Imane Boussebough},
keywords = {Big data, Multi-agent systems, Wireless sensor network, Fuzzy logic, Smart data},
abstract = {The era of big data has brought new challenges in data processing ad management. Existing analytical tools are now close to facing ongoing challenges thus providing satisfactory results at a reasonable cost. However, the velocity at which new data are flooded and the noise generated from such a large volume leads to various new challenges. The present research combines two artificial intelligence fields the represented by multi-agent technologies and fuzzy logic inference systems in order to extract the needed smart data from big noisy ones. A multi-fuzzy agent-based large-scale wireless sensor network has been used to demonstrate the effectiveness of the proposed approach. It handles sensors as autonomous fuzzy agents to measure the relevance of the collected data and eliminate the irrelevant ones. The results of the simulation exhibit a high quality of the data with a decrease in the sensors energy consumption, leading to a longer lifetime of the network.}
}
@incollection{HULSEN202169,
title = {Chapter 4 - Challenges and solutions for big data in personalized healthcare},
editor = {Ahmed A. Moustafa},
booktitle = {Big Data in Psychiatry #x0026; Neurology},
publisher = {Academic Press},
pages = {69-94},
year = {2021},
isbn = {978-0-12-822884-5},
doi = {https://doi.org/10.1016/B978-0-12-822884-5.00016-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228845000167},
author = {Tim Hulsen},
keywords = {Big data, Precision medicine, Personalized healthcare, Data science, Big data analytics},
abstract = {“Big data” is a term that has been used often in the past decade to describe datasets that are extremely large and complex so that traditional software is unable to store and analyze them in an accurate way. It can refer to “long data,” “wide data,” and both. Big data is of increasing importance in healthcare as well: new methods dedicated to improving data collection, storage, cleaning, processing, and interpretation for medical research continue to be developed. Exploiting new tools and methods to extract meaning from large volume information has the potential to drive real change in clinical practice, and combining this novel data-driven research with the classical hypothesis-driven research will have a large impact on personalized healthcare. However, significant challenges remain. Here we discuss the challenges (and possible solutions) posed to biomedical research by our increasing ability to collect, store, and analyze large datasets. Important challenges include: (1) the need for standardization of data content, format, and clinical definitions, adhering to the FAIR guiding principles; (2) the need for collaborative networks with sharing of both data and expertise, for example through a federated approach; (3) stricter privacy and ethics regulations, in particular the GDPR in the European Union; and (4) a need to reconsider how and when analytic methodology (data science) is taught to medical researchers. Overcoming these challenges will help to make a success of the use of big data in medical and translational research.}
}
@article{MILNE2019235,
title = {Big data and understanding change in the context of planning transport systems},
journal = {Journal of Transport Geography},
volume = {76},
pages = {235-244},
year = {2019},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2017.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0966692317300984},
author = {Dave Milne and David Watling},
abstract = {This paper considers the implications of so-called ‘big data’ for the analysis, modelling and planning of transport systems. The primary conceptual focus is on the needs of the practical context of medium-term planning and decision-making, from which perspective the paper seeks to achieve three goals: (i) to try to identify what is truly ‘special’ about big data; (ii) to provoke debate on the future relationship between transport planning and big data; and (iii) to try to identify promising themes for research and application. Differences in the information that can be derived from the data compared to more traditional surveys are discussed, and the respects in which they may impact on the role of models in supporting transport planning and decision-making are identified. It is argued that, over time, changes to the nature of data may lead to significant differences in both modelling approaches and in the expectations placed upon them. Furthermore, it is suggested that the potential widespread availability of data to commercial actors and travellers will affect the performance of the transport systems themselves, which might be expected to have knock-on effects for planning functions. We conclude by proposing a series of research challenges that we believe need to be addressed and warn against adaptations based on minimising change from the status quo.}
}
@article{VIEIRA2020125,
title = {Are Simulation Tools Ready For Big Data? Computational Experiments with Supply Chain Models Developed in Simio},
journal = {Procedia Manufacturing},
volume = {42},
pages = {125-131},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.093},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920306582},
author = {António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira},
keywords = {Simulation, Supply Chain, Big Data, Industry 4.0},
abstract = {The need and potential benefits for the combined use of Simulation and Big Data in Supply Chains (SCs) has been widely recognized. Having worked on such project, some simulation experiments of the modelled SC system were conducted in SIMIO. Different circumstances were tested, including running the model based on the stored data, on statistical distributions and considering risk situations. Thus, this paper aimed to evaluate such experiments, to evaluate the performance of simulations in these contexts. After analyzing the obtained results, it was found that whilst running the model based on the real data required considerable amounts of computer memory, running the model based on statistical distributions reduced such values, albeit required considerable higher time to run a single replication. In all the tested experiments, the simulation took considerable time to run and was not smooth, which can reduce the stakeholders’ interest in the developed tool, despite its benefits for the decision-making process. For future researches, it would be beneficial to test other simulation tools and other strategies and compare those results to the ones provided in this paper.}
}
@article{DAISSAOUI2020161,
title = {IoT and Big Data Analytics for Smart Buildings: A Survey},
journal = {Procedia Computer Science},
volume = {170},
pages = {161-168},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920304506},
author = {Abdellah Daissaoui and Azedine Boulmakoul and Lamia Karim and Ahmed Lbath},
keywords = {Smart buildings, IoT, Big data analytics, Reactif systems, Complex event processing},
abstract = {The processes of digital transformation have involved a variety of socio-technical activities, with the objective of increasing productivity, safety and quality of execution, sustainable development, collaborative working and solutions for the sustainable smart city. The major digital trends, changing the building sector and revealing new trends of understanding information technologies to integrate in this sector. Current smart building management systems incorporate a variety of sensors, actuators and dedicated networks. Their objectives are to observe the condition of specific areas and apply appropriate rules to preserve or improve comfort while saving energy. In this paper, we propose a review of related works to IoT, Big Data Analytics in smart buildings.}
}
@article{BELHADI2019106099,
title = {Understanding Big Data Analytics for Manufacturing Processes: Insights from Literature Review and Multiple Case Studies},
journal = {Computers & Industrial Engineering},
volume = {137},
pages = {106099},
year = {2019},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2019.106099},
url = {https://www.sciencedirect.com/science/article/pii/S0360835219305686},
author = {Amine Belhadi and Karim Zkik and Anass Cherrafi and Sha'ri M. Yusof and Said {El fezazi}},
keywords = {Big Data Analytics, Manufacturing process, Big Data Analytics capabilities, Business intelligence, Literature review, Multiple case study},
abstract = {Today, we are undoubtedly in the era of data. Big Data Analytics (BDA) is no longer a perspective for all level of the organization. This is of special interest in the manufacturing process with their high capital intensity, time constraints and given the huge amount of data already captured. However, there is a paucity in past literature on BDA to develop better understanding of the capabilities and strategic implications to extract value from BDA. In that vein, the central aim of this paper is to develop a novel model that summarizes the main capabilities of BDA in the context of manufacturing process. This is carried out by relying on the findings of a review of the ongoing research along with a multiple case studies within a leading phosphate derivatives manufacturer to point out the capabilities of BDA in manufacturing processes and outline recommendations to advance research in the field. The findings will help companies to understand the big data analytics capabilities and its potential implications for their manufacturing processes and support them seeking to design more effective BDA-enabler infrastructure.}
}
@article{HAMALAINEN2019100105,
title = {Industrial applications of big data in disruptive innovations supporting environmental reporting},
journal = {Journal of Industrial Information Integration},
volume = {16},
pages = {100105},
year = {2019},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2019.100105},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X19300044},
author = {Esa Hämäläinen and Tommi Inkinen},
keywords = {Big data, Disruption, Responsible, Process industry, Economic efficiency, Economic geography},
abstract = {Disruptive innovations are usually identified as ideas that are created ‘outside the box’. They are expected to fundamentally change existing business models and processes founded on technological applications. Disruptive innovations can be challenging to define. Information technology (IT) solutions focus on collecting, processing, and reporting different types of data. Commonly, is the solutions are expected (in cybernetics or self-regulating processes) to provide feedback to original processes and to steer them based on the data. To achieve continuous improvement with regard to environmental responsibility and profitability, new thinking and, in particular, accurate and reliable data are needed for decision-making. Very large data storages, known as big data, contain an increasing mass of different types of homogenous and non-homogenous information, as well as extensive time-series. New, innovative algorithms are required to reveal relevant information and opportunities hidden in these data storages. Global environmental challenges and zero-emission responsible production issues can only be solved using relevant and reliable continuous data as the basis. The final goal should be the creation of scalable environmental solutions based on disruptive innovations and accurate data. The aim of this paper is to determine the explicit steps for replacing silo-based reporting with company-wide, refined information, which enables decision-makers in all industries the chance to make responsible choices.}
}
@article{ZHANG2019814,
title = {Big data driven decision-making for batch-based production systems},
journal = {Procedia CIRP},
volume = {83},
pages = {814-818},
year = {2019},
note = {11th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.05.023},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119310169},
author = {Yongheng Zhang and Rui Zhang and Yizhong Wang and Hongfei Guo and Ray Y Zhong and Ting Qu and Zhiwu Li},
keywords = {Big data, Smart Product-Service System, Sales predict, Economic batch quantity, production plan},
abstract = {The era of big data has brought new challenges to chemical enterprises. In order to maximize the benefits, enterprises are considering to implement intelligent service technology into traditional production systems to improve the level of intelligence in business. This paper proposes a service framework based on big data driven prediction, which includes information perception layer, information application layer and big data service layer. In this paper, the composition of big data service layer is described in detail, and a sales predicting method based on neural network is introduced. The salability of products is divided, and the qualitative economic production volume mechanism is finally given. Based on the framework, an intelligent service system for enterprises with the characteristics of mass production is implemented. Experimental results show that the big data service framework can support chemical enterprises to make decisions to reduce costs, and provides an effective method for Smart Product Service System (PSS).}
}
@article{TU2020101428,
title = {Portraying the spatial dynamics of urban vibrancy using multisource urban big data},
journal = {Computers, Environment and Urban Systems},
volume = {80},
pages = {101428},
year = {2020},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2019.101428},
url = {https://www.sciencedirect.com/science/article/pii/S0198971519302674},
author = {Wei Tu and Tingting Zhu and Jizhe Xia and Yulun Zhou and Yani Lai and Jincheng Jiang and Qingquan Li},
keywords = {Urban vibrancy, Geographically weighted regression, mobile phone data, Social media, Points-of-interest, Big data},
abstract = {Understanding urban vibrancy aids policy-making to foster urban space and therefore has long been a goal of urban studies. Recently, the emerging urban big data and urban analytic methods have enabled us to portray citywide vibrancy. From the social sensing perspective, this study presents a comprehensive and comparative framework to cross-validate urban vibrancy and uncover associated spatial effects. Spatial patterns of urban vibrancy indicated by multisource urban sensing data (points-of-interest, social media check-ins, and mobile phone records) were investigated. A comprehensive urban vibrancy metric was formed by adaptively weighting these metrics. The association between urban vibrancy and demographic, economic, and built environmental factors was revealed with global regression models and local regression models. An empirical experiment was conducted in Shenzhen. The results demonstrate that four urban vibrancy metrics are all higher in the special economic zone (SEZ) and lower in non-SEZs but with different degrees of spatial aggregation. The influences of employment and road density on all vibrancy metrics are significant and positive. However, the effects of metro stations, land use mix, building footprints, and distance to district center depend on the vibrancy indicator and location. These findings unravel the commonalities and differences in urban vibrancy metrics derived from multisource urban big data and the hidden spatial dynamics of the influences of associated factors. They further suggest that urban policies should be proposed to foster vibrancy in Shenzhen therefore benefit social wellbeing and urban development in the long term. They also provide valuable insights into the reliability of urban big data-driven urban studies.}
}
@incollection{MCGILVRAY20217,
title = {Chapter 1 - Data Quality and the Data-Dependent World},
editor = {Danette McGilvray},
booktitle = {Executing Data Quality Projects (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {7-14},
year = {2021},
isbn = {978-0-12-818015-0},
doi = {https://doi.org/10.1016/B978-0-12-818015-0.00021-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128180150000219},
author = {Danette McGilvray},
keywords = {Data-dependent world, data-driven, assets, technology, legal and regulatory tsunami, Internet of Things (IoT), big data, artificial intelligence (AI), machine learning (ML), The Leader’s Data Manifesto, data literacy, change, COVID-19},
abstract = {Chapter 1 addresses topics in our world today, shows how they are dependent on data and information, why data quality is more relevant and critical now than ever before, and how the Ten Steps methodology will help. Topics include COVID-19, the legal and regulatory tsunami, big data, Internet of Things (IoT), 5G, artificial intelligence (AI) and machine learning (ML). Our data-dependent world is broader than, yet encompasses, being data-driven. Data and information are assets to be managed and are compared to how human and financial resources are managed. The Leader’s Data Manifesto is introduced as a starting point for conversations about the importance of managing data and information assets. While the Ten Steps methodology is meant for specific audiences, three recommendations were provided that anyone, in any organization, can do to help raise awareness of the importance of data quality: add data and information to the conversation, increase data literacy in the workplace, and include data (quality) management in learning institutions at all levels.}
}
@article{URBAN2020117792,
title = {Application of big data analysis technique on high-velocity airblast atomization: Searching for optimum probability density function},
journal = {Fuel},
volume = {273},
pages = {117792},
year = {2020},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2020.117792},
url = {https://www.sciencedirect.com/science/article/pii/S0016236120307870},
author = {András Urbán and Axel Groniewsky and Milan Malý and Viktor Józsa and Jan Jedelský},
keywords = {Big data, Airblast, Rapeseed oil, PDA, Probability density function, Likelihood},
abstract = {In this paper, the droplet size distributions of high-velocity airblast atomization were analyzed. The spray measurement was performed by a Phase-Doppler anemometer at several points and different diameters across the spray for diesel oil, light heating oil, crude rapeseed oil, and water. The atomizing gauge pressure and the liquid preheating temperature varied from 0.3 to 2.4 bar and 25 to 100 °C, respectively. Approximately 400 million individual droplets were recorded; therefore, a big data evaluation technique was applied. 18 of the most commonly used probability density functions (PDF) were fitted to the histogram of each measuring point and evaluated by their relative log-likelihood. Among the three-parameter PDFs, Generalized Extreme Value and Burr PDFs provided the most desirable result to describe a complete drop size distribution. With restriction to two-parameter PDFs, the Nakagami PDF unexpectedly outperformed all the others, including Weibull (Rosin-Rammler) PDF, which is commonly used in atomization. However, if the spray is characterized by a single value, such as the Sauter Mean Diameter, i.e. an expected value-like parameter is of primary importance over the distribution, Gamma PDF is the best option, used in several papers of the atomization literature.}
}
@article{PERAKIS2020107035,
title = {CYBELE – Fostering precision agriculture & livestock farming through secure access to large-scale HPC enabled virtual industrial experimentation environments fostering scalable big data analytics},
journal = {Computer Networks},
volume = {168},
pages = {107035},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.107035},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619305353},
author = {Konstantinos Perakis and Fenareti Lampathaki and Konstantinos Nikas and Yiannis Georgiou and Oskar Marko and Jarissa Maselyne},
keywords = {Precision agriculture, Precision livestock farming, High performance computing, Big data analytics},
abstract = {According to McKinsey & Company, about a third of food produced is lost or wasted every year, amounting to a $940 billion economic hit. Inefficiencies in planting, harvesting, water use, reduced animal contributions, as well as uncertainty about weather, pests, consumer demand and other intangibles contribute to the loss. Precision Agriculture (PA) and Precision Livestock Farming (PLF) come to assist in optimizing agricultural and livestock production and minimizing the wastes and costs aforementioned. PA is a technology-enabled, data-driven approach to farming management that observes, measures, and analyzes the needs of individual fields and crops. PLF is also a technology-enabled, data-driven approach to livestock production management, which exploits technology to quantitatively measure the behavior, health and performance of animals. Big data delivered by a plethora of data sources related to these domains, has a multitude of payoffs including precision monitoring of fertilizer and fungicide levels to optimize crop yields, risk mitigation that results from monitoring when temperature and humidity levels reach dangerous levels for crops, increasing livestock production while minimizing the environmental footprint of livestock farming, ensuring high levels of welfare and health for animals, and more. By adding analytics to these sensor and image data, opportunities also exist to further optimize PA and PLF by having continuous data on how a field or the livestock is responding to a protocol. For these domains, two main challenges exist: 1) to exploit this multitude of data facilitating dedicated improvements in performance, and 2) to make available advanced infrastructure so as to harness the power of this information in order to benefit from the new insights, practices and products, efficiently time-wise, lowering responsiveness down to seconds so as to cater for time-critical decisions. The current paper aims to introduce CYBELE, a platform aspiring to safeguard that the stakeholders involved in the agri-food value chain (research community, SMEs, entrepreneurs, etc.) have integrated, unmediated access to a vast amount of very large scale datasets of diverse types and coming from a variety of sources, and that they are capable of actually generating value and extracting insights out of these data, by providing secure and unmediated access to large-scale High Performance Computing (HPC) infrastructures supporting advanced data discovery, processing, combination and visualization services, solving computationally-intensive challenges modelled as mathematical algorithms requiring very high computing power and capability.}
}
@article{ALWAN2022101951,
title = {Data quality challenges in large-scale cyber-physical systems: A systematic review},
journal = {Information Systems},
volume = {105},
pages = {101951},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101951},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921001484},
author = {Ahmed Abdulhasan Alwan and Mihaela Anca Ciupala and Allan J. Brimicombe and Seyed Ali Ghorashi and Andres Baravalle and Paolo Falcarin},
keywords = {Cyber-physical systems (CPS), Wireless Sensor Networks(WSN), Data quality management, Data quality dimensions, Smart cities, Quality of observations},
abstract = {Cyber-physical systems (CPSs) are integrated systems engineered to combine computational control algorithms and physical components such as sensors and actuators, effectively using an embedded communication core. Smart cities can be viewed as large-scale, heterogeneous CPSs that utilise technologies like the Internet of Things (IoT), surveillance, social media, and others to make informed decisions and drive the innovations of automation in urban areas. Such systems incorporate multiple layers and complex structure of hardware, software, analytical algorithms, business knowledge and communication networks, and operate under noisy and dynamic conditions. Thus, large-scale CPSs are vulnerable to enormous technical and operational challenges that may compromise the quality of data of their applications and accordingly reduce the quality of their services. This paper presents a systematic literature review to investigate data quality challenges in smart-cities large-scale CPSs and to identify the most common techniques used to address these challenges. This systematic literature review showed that significant work had been conducted to address data quality management challenges in smart cities, large-scale CPS applications. However, still, more is required to provide a practical, comprehensive data quality management solution to detect errors in sensor nodes’ measurements associated with the main data quality dimensions of accuracy, timeliness, completeness, and consistency. No systematic or generic approach was demonstrated for detecting sensor nodes and sensor node networks failures in large-scale CPS applications. Moreover, further research is required to address the challenges of ensuring the quality of the spatial and temporal contextual attributes of sensor nodes’ observations.}
}
@article{RANA2019807,
title = {How Big Data Science Can Improve Linkage and Retention in Care},
journal = {Infectious Disease Clinics of North America},
volume = {33},
number = {3},
pages = {807-815},
year = {2019},
note = {HIV},
issn = {0891-5520},
doi = {https://doi.org/10.1016/j.idc.2019.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0891552019300455},
author = {Aadia I. Rana and Michael J. Mugavero},
keywords = {HIV, AIDS, Prevention, Treatment, Continuum, Data, Surveillance}
}
@article{WORDSWORTH20181048,
title = {Using “Big Data” in the Cost-Effectiveness Analysis of Next-Generation Sequencing Technologies: Challenges and Potential Solutions},
journal = {Value in Health},
volume = {21},
number = {9},
pages = {1048-1053},
year = {2018},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2018.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S1098301518322654},
author = {Sarah Wordsworth and Brett Doble and Katherine Payne and James Buchanan and Deborah A. Marshall and Christopher McCabe and Dean A. Regier},
keywords = {Big data, cost-effectiveness, next generation sequencing},
abstract = {Next-generation sequencing (NGS) is considered to be a prominent example of “big data” because of the quantity and complexity of data it produces and because it presents an opportunity to use powerful information sources that could reduce clinical and health economic uncertainty at a patient level. One obstacle to translating NGS into routine health care has been a lack of clinical trials evaluating NGS technologies, which could be used to populate cost-effectiveness analyses (CEAs). A key question is whether big data can be used to partially support CEAs of NGS. This question has been brought into sharp focus with the creation of large national sequencing initiatives. In this article we summarize the main methodological and practical challenges of using big data as an input into CEAs of NGS. Our focus is on the challenges of using large observational datasets and cohort studies and linking these data to the genomic information obtained from NGS, as is being pursued in the conduct of large genomic sequencing initiatives. We propose potential solutions to these key challenges. We conclude that the use of genomic big data to support and inform CEAs of NGS technologies holds great promise. Nevertheless, health economists face substantial challenges when using these data and must be cognizant of them before big data can be confidently used to produce evidence on the cost-effectiveness of NGS.}
}
@article{PEDRO20193,
title = {Capabilities and Readiness for Big Data Analytics},
journal = {Procedia Computer Science},
volume = {164},
pages = {3-10},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.147},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321866},
author = {Jenifer Pedro and Irwin Brown and Mike Hart},
keywords = {Big data analytics, organisational readiness, organisational capabilities, frameworks, business analytics, thematic analysis},
abstract = {Despite some of the initial hype from marketers and consultants, the use of big data is now firmly established in many organisations worldwide. Big data analytics (BDA) is making use of huge volumes of data from a wide range of structured and unstructured sources. Surveys have however reported a number of barriers to organisational effectiveness with BDA. This research aims to determine what capabilities large organisations require to be ready for a successful BDA initiative. Drawing mainly on relevant results of two published research articles, key informed stakeholders from a large South African telecommunications company were interviewed on this topic. Thematic analysis identified the key themes and sub-themes relating to capabilities needed for the organization to be ready for effective BDA. These proved to be very similar to those given in the earlier research, although a new capability of legal compliance for data protection was now added.}
}
@article{AITHAMMOU2020102122,
title = {Towards a real-time processing framework based on improved distributed recurrent neural network variants with fastText for social big data analytics},
journal = {Information Processing & Management},
volume = {57},
number = {1},
pages = {102122},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2019.102122},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319305163},
author = {Badr {Ait Hammou} and Ayoub {Ait Lahcen} and Salma Mouline},
keywords = {Big data, FastText, Recurrent neural networks, LSTM, BiLSTM, GRU, Natural language processing, Sentiment analysis, Social big data analytics},
abstract = {Big data generated by social media stands for a valuable source of information, which offers an excellent opportunity to mine valuable insights. Particularly, User-generated contents such as reviews, recommendations, and users’ behavior data are useful for supporting several marketing activities of many companies. Knowing what users are saying about the products they bought or the services they used through reviews in social media represents a key factor for making decisions. Sentiment analysis is one of the fundamental tasks in Natural Language Processing. Although deep learning for sentiment analysis has achieved great success and allowed several firms to analyze and extract relevant information from their textual data, but as the volume of data grows, a model that runs in a traditional environment cannot be effective, which implies the importance of efficient distributed deep learning models for social Big Data analytics. Besides, it is known that social media analysis is a complex process, which involves a set of complex tasks. Therefore, it is important to address the challenges and issues of social big data analytics and enhance the performance of deep learning techniques in terms of classification accuracy to obtain better decisions. In this paper, we propose an approach for sentiment analysis, which is devoted to adopting fastText with Recurrent neural network variants to represent textual data efficiently. Then, it employs the new representations to perform the classification task. Its main objective is to enhance the performance of well-known Recurrent Neural Network (RNN) variants in terms of classification accuracy and handle large scale data. In addition, we propose a distributed intelligent system for real-time social big data analytics. It is designed to ingest, store, process, index, and visualize the huge amount of information in real-time. The proposed system adopts distributed machine learning with our proposed method for enhancing decision-making processes. Extensive experiments conducted on two benchmark data sets demonstrate that our proposal for sentiment analysis outperforms well-known distributed recurrent neural network variants (i.e., Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), and Gated Recurrent Unit (GRU)). Specifically, we tested the efficiency of our approach using the three different deep learning models. The results show that our proposed approach is able to enhance the performance of the three models. The current work can provide several benefits for researchers and practitioners who want to collect, handle, analyze and visualize several sources of information in real-time. Also, it can contribute to a better understanding of public opinion and user behaviors using our proposed system with the improved variants of the most powerful distributed deep learning and machine learning algorithms. Furthermore, it is able to increase the classification accuracy of several existing works based on RNN models for sentiment analysis.}
}
@article{WANG2020119299,
title = {Big data driven Hierarchical Digital Twin Predictive Remanufacturing paradigm: Architecture, control mechanism, application scenario and benefits},
journal = {Journal of Cleaner Production},
volume = {248},
pages = {119299},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.119299},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619341691},
author = {Yankai Wang and Shilong Wang and Bo Yang and Lingzi Zhu and Feng Liu},
keywords = {multi-life-cycle remanufacturing, Sustainable products, Big data, CPS-Digital-twin(CPSDT), IoT-cloud, Reconfiguration},
abstract = {Remanufacturing is deemed to be an effective method for recycling resources, achieving sustainable production. However, little importance of remanufacturing has been attached in PLM. Surely, there are many problems in implementation of the remanufacturing strategy, such as inability to effectively reduce uncertainty, lack of product multi-life-cycle remanufacturing process tracking management, lack of smart enabling technology application in the full lifecycle that focusing on multi-life-cycle remanufacturing. After analyzing the reasons, through integrating smart enabling technologies, a new PLM paradigm focusing on the multi-life-cycle remanufacturing process: Big Data driven Hierarchical Digital Twin Predictive Remanufacturing (BDHDTPREMfg) is proposed. And the definition of BDHDTPREMfg is proposed. A big data driven layered architecture and the hierarchical CPS-Digital-Twin(CPSDT) reconfiguration control mechanism of BDHDTPREMfg are respectively developed. Then, this paper presents an application scenario of BDHDTPREMfg to validate the feasibility and effectiveness. Based on the above application analysis, the benefits of penetrating BDHDTPREMfg into the entire lifecycle are demonstrated. The summary of this paper and future research work is discussed in the end.}
}
@article{DREMEL2020103121,
title = {Actualizing big data analytics affordances: A revelatory case study},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103121},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308522},
author = {Christian Dremel and Matthias M. Herterich and Jochen Wulf and Jan {vom Brocke}},
keywords = {Big data analytics, Affordance theory, Socio-technical approach, Organizational transformation, Organizational benefits, Affordance actualization},
abstract = {Drawing on a revelatory case study, we identify four big data analytics (BDA) actualization mechanisms: (1) enhancing, (2) constructing, (3) coordinating, and (4) integrating, which manifest in actions on three socio-technical system levels, i.e., the structure, actor, and technology levels. We investigate the actualization of four BDA affordances at an automotive manufacturing company, i.e., establishing customer-centric marketing, provisioning vehicle-data-driven services, data-driven vehicle developing, and optimizing production processes. This study introduces a theoretical perspective to BDA research that explains how organizational actions contribute to actualizing BDA affordances. We further provide practical implications that can help guide practitioners in BDA adoption.}
}
@article{BACHECHI2022100292,
title = {Big Data Analytics and Visualization in Traffic Monitoring},
journal = {Big Data Research},
volume = {27},
pages = {100292},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100292},
url = {https://www.sciencedirect.com/science/article/pii/S221457962100109X},
author = {Chiara Bachechi and Laura Po and Federica Rollo},
keywords = {Traffic, Time series, Air quality maps, Real-time data, Spatio-temporal data, Smart cities},
abstract = {This paper presents a system that employs information visualization techniques to analyze urban traffic data and the impact of traffic emissions on urban air quality. Effective visualizations allow citizens and public authorities to identify trends, detect congested road sections at specific times, and perform monitoring and maintenance of traffic sensors. Since road transport is a major source of air pollution, also the impact of traffic on air quality has emerged as a new issue that traffic visualizations should address. Trafair Traffic Dashboard exploits traffic sensor data and traffic flow simulations to create an interactive layout focused on investigating the evolution of traffic in the urban area over time and space. The dashboard is the last step of a complex data framework that starts from the ingestion of traffic sensor observations, anomaly detection, traffic modeling, and also air quality impact analysis. We present the results of applying our proposed framework on two cities (Modena, in Italy, and Santiago de Compostela, in Spain) demonstrating the potential of the dashboard in identifying trends, seasonal events, abnormal behaviors, and understanding how urban vehicle fleet affects air quality. We believe that the framework provides a powerful environment that may guide the public decision-makers through effective analysis of traffic trends devoted to reducing traffic issues and mitigating the polluting effect of transportation.}
}
@article{LIM201886,
title = {Smart cities with big data: Reference models, challenges, and considerations},
journal = {Cities},
volume = {82},
pages = {86-99},
year = {2018},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2018.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0264275117308545},
author = {Chiehyeon Lim and Kwang-Jae Kim and Paul P. Maglio},
keywords = {Smart city, Big data, Reference model, Challenge, Consideration},
abstract = {Cities worldwide are attempting to transform themselves into smart cities. Recent cases and studies show that a key factor in this transformation is the use of urban big data from stakeholders and physical objects in cities. However, the knowledge and framework for data use for smart cities remain relatively unknown. This paper reports findings from an analysis of various use cases of big data in cities worldwide and the authors' four projects with government organizations toward developing smart cities. Specifically, this paper classifies the urban data use cases into four reference models and identifies six challenges in transforming data into information for smart cities. Furthermore, building upon the relevant literature, this paper proposes five considerations for addressing the challenges in implementing the reference models in real-world applications. The reference models, challenges, and considerations collectively form a framework for data use for smart cities. This paper will contribute to urban planning and policy development in the modern data-rich economy.}
}
@article{MIKALEF2019261,
title = {Big data analytics and firm performance: Findings from a mixed-method approach},
journal = {Journal of Business Research},
volume = {98},
pages = {261-276},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.01.044},
url = {https://www.sciencedirect.com/science/article/pii/S014829631930061X},
author = {Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie},
keywords = {Big data analytics, Complexity theory, fsQCA, Business value, Mixed-method, Environmental uncertainty},
abstract = {Big data analytics has been widely regarded as a breakthrough technological development in academic and business communities. Despite the growing number of firms that are launching big data initiatives, there is still limited understanding on how firms translate the potential of such technologies into business value. The literature argues that to leverage big data analytics and realize performance gains, firms must develop strong big data analytics capabilities. Nevertheless, most studies operate under the assumption that there is limited heterogeneity in the way firms build their big data analytics capabilities and that related resources are of similar importance regardless of context. This paper draws on complexity theory and investigates the configurations of resources and contextual factors that lead to performance gains from big data analytics investments. Our empirical investigation followed a mixed methods approach using survey data from 175 chief information officers and IT managers working in Greek firms, and three case studies to show that depending on the context, big data analytics resources differ in significance when considering performance gains. Applying a fuzzy-set qualitative comparative analysis (fsQCA) method on the quantitative data, we show that there are four different patterns of elements surrounding big data analytics that lead to high performance. Outcomes of the three case studies highlight the inter-relationships between these elements and outline challenges that organizations face when orchestrating big data analytics resources.}
}
@article{GUO20181,
title = {Research on case retrieval of Bayesian network under big data},
journal = {Data & Knowledge Engineering},
volume = {118},
pages = {1-13},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18300624},
author = {Yuan Guo and Yuan Guo and K. Wu},
keywords = {Case retrieval, Big data, BN model, Hadoop platform},
abstract = {Although case retrieval of Bayesian network has greatly promoted the application of CBR technique in engineering fields, it is facing huge challenges with the arrival of the era of big data. First, huge computation task of BN learning caused by big data seriously hampers the efficiency of case retrieval; Second, with the increasing data size, the accuracy of case retrieval becomes poorer and poorer because existing methods of improving probability learning become unfit for new situation. Aiming at the first problem, this paper proposes Within-Cross algorithm to assign computation task to improve the result of parallel data processing and gain better efficiency of case retrieval. For the second problem, this paper proposes a new method called Weighted Index Coefficient of Dirichlet Distribution (WICDD) algorithm, which first measures the influence of different factors on probability learning and then gives a weight to each super parameter of Dirichlet Distribution to adjust the result of probability learning. Thus with WICDD algorithm, the effect of probability learning is greatly improved, which then further enhances the accuracy of case retrieval. Finally, lots of experiments are executed to validate the effectiveness of the proposed method.}
}
@article{LI2021103928,
title = {Search query of English translation text based on embedded system and big data},
journal = {Microprocessors and Microsystems},
volume = {82},
pages = {103928},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.103928},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121001071},
author = {Zhihong Li},
keywords = {Cross-language information retrieval, Optical character recognition, Embedded applications},
abstract = {Cross-Language Information Retrieval (CLIR) the purpose of another language (target language), a collection of documents written question from one language (source language).CLIR employees publish documents based on user queries, dictionary translation, machine translation methods, and promotion problems. Through various detection methods and translation, this word applies to single words, translation, transliteration of names, including transcription and translation and disambiguation. CLIR Recovery in Question Semantics Technology is the most appropriate) Translation to retrieve documents (dictionary related method) based on English Question Concentrations and Question translation. To the proposed model of translating Arabic to English, and provides the high accuracy Optical Character Recognition (OCR) errors in handling orthography, expanding outside and transliteration dubbed gives higher accuracy in resolving ambiguities. Thus, the single question expands with additional meanings and related words that improve significantly with semantic input. However, the documents related to the questions recover those cross language boundaries. The development of large data systems is completely different from the actual goal of small (traditional, structured) data system development. When the in-depth learning technology is developed, face some space and environmental barriers in different laboratory environments. To describe the requirements when running embedded applications on computers for deep learning.}
}
@article{ARIYALURANHABEEB2019289,
title = {Real-time big data processing for anomaly detection: A Survey},
journal = {International Journal of Information Management},
volume = {45},
pages = {289-307},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218301658},
author = {Riyaz Ahamed {Ariyaluran Habeeb} and Fariza Nasaruddin and Abdullah Gani and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed and Muhammad Imran},
keywords = {Real-time, Big data processing, Anomaly detection and machine learning algorithms},
abstract = {The advent of connected devices and omnipresence of Internet have paved way for intruders to attack networks, which leads to cyber-attack, financial loss, information theft in healthcare, and cyber war. Hence, network security analytics has become an important area of concern and has gained intensive attention among researchers, off late, specifically in the domain of anomaly detection in network, which is considered crucial for network security. However, preliminary investigations have revealed that the existing approaches to detect anomalies in network are not effective enough, particularly to detect them in real time. The reason for the inefficacy of current approaches is mainly due the amassment of massive volumes of data though the connected devices. Therefore, it is crucial to propose a framework that effectively handles real time big data processing and detect anomalies in networks. In this regard, this paper attempts to address the issue of detecting anomalies in real time. Respectively, this paper has surveyed the state-of-the-art real-time big data processing technologies related to anomaly detection and the vital characteristics of associated machine learning algorithms. This paper begins with the explanation of essential contexts and taxonomy of real-time big data processing, anomalous detection, and machine learning algorithms, followed by the review of big data processing technologies. Finally, the identified research challenges of real-time big data processing in anomaly detection are discussed.}
}
@article{SHADROO201819,
title = {Systematic survey of big data and data mining in internet of things},
journal = {Computer Networks},
volume = {139},
pages = {19-47},
year = {2018},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618301579},
author = {Shabnam Shadroo and Amir Masoud Rahmani},
keywords = {Internet of things, Systematic survey, Big data, Data mining},
abstract = {In recent years, the Internet of Things (IoT) has emerged as a new opportunity. Thus, all devices such as smartphones, transportation facilities, public services, and home appliances are used as data creator devices. All the electronic devices around us help our daily life. Devices such as wrist watches, emergency alarms, and garage doors and home appliances such as refrigerators, microwaves, air conditioning, and water heaters are connected to an IoT network and controlled remotely. Methods such as big data and data mining can be used to improve the efficiency of IoT and storage challenges of a large data volume and the transmission, analysis, and processing of the data volume on the IoT. The aim of this study is to investigate the research done on IoT using big data as well as data mining methods to identify subjects that must be emphasized more in current and future research paths. This article tries to achieve the goal by following the conference and journal articles published on IoT-big data and also IoT-data mining areas between 2010 and August 2017. In order to examine these articles, the combination of Systematic Mapping and literature review was used to create an intended review article. In this research, 44 articles were studied. These articles are divided into three categories: Architecture & Platform, framework, and application. In this research, a summary of the methods used in the area of IoT-big data and IoT-data mining is presented in three categories to provide a starting point for researchers in the future.}
}
@article{MEHTA201857,
title = {Concurrence of big data analytics and healthcare: A systematic review},
journal = {International Journal of Medical Informatics},
volume = {114},
pages = {57-65},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618302466},
author = {Nishita Mehta and Anil Pandit},
keywords = {Big data, Analytics, Healthcare, Predictive analytics, Evidence-based medicine},
abstract = {Background
The application of Big Data analytics in healthcare has immense potential for improving the quality of care, reducing waste and error, and reducing the cost of care.
Purpose
This systematic review of literature aims to determine the scope of Big Data analytics in healthcare including its applications and challenges in its adoption in healthcare. It also intends to identify the strategies to overcome the challenges.
Data sources
A systematic search of the articles was carried out on five major scientific databases: ScienceDirect, PubMed, Emerald, IEEE Xplore and Taylor & Francis. The articles on Big Data analytics in healthcare published in English language literature from January 2013 to January 2018 were considered.
Study selection
Descriptive articles and usability studies of Big Data analytics in healthcare and medicine were selected.
Data extraction
Two reviewers independently extracted information on definitions of Big Data analytics; sources and applications of Big Data analytics in healthcare; challenges and strategies to overcome the challenges in healthcare.
Results
A total of 58 articles were selected as per the inclusion criteria and analyzed. The analyses of these articles found that: (1) researchers lack consensus about the operational definition of Big Data in healthcare; (2) Big Data in healthcare comes from the internal sources within the hospitals or clinics as well external sources including government, laboratories, pharma companies, data aggregators, medical journals etc.; (3) natural language processing (NLP) is most widely used Big Data analytical technique for healthcare and most of the processing tools used for analytics are based on Hadoop; (4) Big Data analytics finds its application for clinical decision support; optimization of clinical operations and reduction of cost of care (5) major challenge in adoption of Big Data analytics is non-availability of evidence of its practical benefits in healthcare.
Conclusion
This review study unveils that there is a paucity of information on evidence of real-world use of Big Data analytics in healthcare. This is because, the usability studies have considered only qualitative approach which describes potential benefits but does not take into account the quantitative study. Also, majority of the studies were from developed countries which brings out the need for promotion of research on Healthcare Big Data analytics in developing countries.}
}
@article{SHAW2021108953,
title = {Marine big data analysis of ships for the energy efficiency changes of the hull and maintenance evaluation based on the ISO 19030 standard},
journal = {Ocean Engineering},
volume = {232},
pages = {108953},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.108953},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821003887},
author = {Heiu-Jou Shaw and Cheng-Kuan Lin},
keywords = {Energy efficiency management, ISO 19030, Hull and propeller maintenance},
abstract = {This study analyzes the energy efficiency of ships based on ISO 19030, which is a standard for the measurement of changes in hull and propeller performance. The goal is to provide energy efficiency management with digital indicators that have not been easily provided. The ship navigation information platform (SNIP) is developed to determine the dynamic information of each ship, including the fuel consumption, ship speed, horsepower of the engine, rotation speed of the engine, wind direction, and wind speed. In addition, model test data and computational fluid dynamics (CFD) calculation data are applied to calculate the energy efficiency performance indicators. Finally, the relationship of the speed through water and the speed over ground enables us to modify the effects of the ocean currents. The results verify that these indicators can be used as a reference for performance monitoring and maintenance prediction of international maritime affairs.}
}
@article{MANTELERO2018754,
title = {AI and Big Data: A blueprint for a human rights, social and ethical impact assessment},
journal = {Computer Law & Security Review},
volume = {34},
number = {4},
pages = {754-772},
year = {2018},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2018.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0267364918302012},
author = {Alessandro Mantelero},
keywords = {Data protection, Impact assessment, Data protection impact assessment, Human rights, Human rights impact assessment, Ethical impact assessment, Social impact assessment, General Data Protection Regulation},
abstract = {The use of algorithms in modern data processing techniques, as well as data-intensive technological trends, suggests the adoption of a broader view of the data protection impact assessment. This will force data controllers to go beyond the traditional focus on data quality and security, and consider the impact of data processing on fundamental rights and collective social and ethical values. Building on studies of the collective dimension of data protection, this article sets out to embed this new perspective in an assessment model centred on human rights (Human Rights, Ethical and Social Impact Assessment-HRESIA). This self-assessment model intends to overcome the limitations of the existing assessment models, which are either too closely focused on data processing or have an extent and granularity that make them too complicated to evaluate the consequences of a given use of data. In terms of architecture, the HRESIA has two main elements: a self-assessment questionnaire and an ad hoc expert committee. As a blueprint, this contribution focuses mainly on the nature of the proposed model, its architecture and its challenges; a more detailed description of the model and the content of the questionnaire will be discussed in a future publication drawing on the ongoing research.}
}
@article{JONES20193,
title = {What we talk about when we talk about (big) data},
journal = {The Journal of Strategic Information Systems},
volume = {28},
number = {1},
pages = {3-16},
year = {2019},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2018.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0963868718302622},
author = {Matthew Jones},
abstract = {In common with much contemporary discourse around big data, recent discussion of datafication in the Journal of Strategic Information Systems has focused on its effects on individuals, organisations and society. Generally missing from such analysis, however, is any consideration of data themselves. What is it that is having these effects? In this Viewpoint article I therefore present a critical analysis of a number of widely-held assumptions about data in general and big data in particular. Rather than being a referential, natural, foundational, objective and equal representation of the world, it will be argued, data are partial and contingent and are brought into being through situated practices of conceptualization, recording and use. Big data are also not as revolutionary voluminous, universal or exhaustive as they are often presented. Some initial implications of this reconceptualization of data are explored. A distinction is made between “data in principle” as they are recorded, and the “data in practice” as they are used. It is only the latter, typically a small and not necessarily representative subset of the former, that will contribute directly to the effects of datafication.}
}
@article{LI2019103149,
title = {Experience and reflection from China’s Xiangya medical big data project},
journal = {Journal of Biomedical Informatics},
volume = {93},
pages = {103149},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103149},
url = {https://www.sciencedirect.com/science/article/pii/S153204641930067X},
author = {Bei Li and Jianbin Li and Yuqiao Jiang and Xiaoyun Lan},
keywords = {Medical big data, Data sharing, Information security, Cooperation mechanism, Medical data acquisition, Medical data centre},
abstract = {The construction of medical big data includes several problems that need to be solved, such as integration and data sharing of many heterogeneous information systems, efficient processing and analysis of large-scale medical data with complex structure or low degree of structure, and narrow application range of medical data. Therefore, medical big data construction is not only a simple collection and application of medical data but also a complex systematic project. This paper introduces China's experience in the construction of a regional medical big data ecosystem, including the overall goal of the project; establishment of policies to encourage data sharing; handling the relationship between personal privacy, information security, and information availability; establishing a cooperation mechanism between agencies; designing a polycentric medical data acquisition system; and establishing a large data centre. From the experience gained from one of China's earliest established medical big data projects, we outline the challenges encountered during its development and recommend approaches to overcome these challenges to design medical big data projects in China more rationally. Clear and complete top-level design of a project requires to be planned in advance and considered carefully. It is essential to provide a culture of information sharing and to facilitate the opening of data, and changes in ideas and policies need the guidance of the government. The contradiction between data sharing and data security must be handled carefully, that is not to say data openness could be abandoned. The construction of medical big data involves many institutions, and high-level management and cooperation can significantly improve efficiency and promote innovation. Compared with infrastructure construction, it is more challenging and time-consuming to develop appropriate data standards, data integration tools and data mining tools.}
}
@incollection{ILMUDEEN202133,
title = {Chapter 3 - Big data-based frameworks for healthcare systems},
editor = {Pradeep N and Sandeep Kautish and Sheng-Lung Peng},
booktitle = {Demystifying Big Data, Machine Learning, and Deep Learning for Healthcare Analytics},
publisher = {Academic Press},
pages = {33-56},
year = {2021},
isbn = {978-0-12-821633-0},
doi = {https://doi.org/10.1016/B978-0-12-821633-0.00003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128216330000039},
author = {Aboobucker Ilmudeen},
keywords = {Big data, Frameworks, Healthcare, Healthcare systems},
abstract = {Today, the term big data involves various applications, technologies, architectures, services, and standards in the healthcare domain. The advanced technology and healthcare systems have been extremely knotted together in recent times. As the adoption of wearable biosensors and their applications have begun across the world, eHealth and mHealth have emerged. Hence, wearable sensor devices produce organized and unorganized big data that cannot be easily processed and analyzed due to its complexity; that hinders effective medical decision making. Modern advances in healthcare systems have increased the size of health records such as through electronic health records, patient care, clinical reports, regulations, and compliance requirements. However, the present data-processing technologies are not capable of handling the growing amount of large datasets. This chapter discusses theoretical illustrations that pinpoint various aspects of big data-related frameworks and proposes a large data-based conceptual framework in healthcare systems.}
}
@article{CANITO20181,
title = {Unfolding the relations between companies and technologies under the Big Data umbrella},
journal = {Computers in Industry},
volume = {99},
pages = {1-8},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S016636151830040X},
author = {João Canito and Pedro Ramos and Sérgio Moro and Paulo Rita},
keywords = {Big data companies, Big data technologies, Online news, Gartner magic quadrant},
abstract = {Big Data is dominating the landscape as data originated in many sources keeps piling up. Information Technology (IT) business companies are making tremendous efforts to keep the pace with this wave of innovative technologies. This study aims to identify how the different IT companies are aligned with emerging Big Data technologies. The approach consisted in analyzing 11,505 news published between 2013 and 2016 and aggregated through Google News. The companies were categorized according to their position in the 2017 Gartner Magic Quadrant for advanced analytics. A text mining and topic modeling procedure assisted in summarizing the main findings. Leaders dominated a large fraction of the published news. Challengers are making a significant effort in investing in predictive analytics, overlooking other technologies such as those related to data preparation and integration. The results helped to shed light on the emerging field of Big Data from a corporate perspective.}
}
@article{HUANG2019592,
title = {Challenges, opportunities and paradigm of applying big data to production safety management: From a theoretical perspective},
journal = {Journal of Cleaner Production},
volume = {231},
pages = {592-599},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.05.245},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619317810},
author = {Lang Huang and Chao Wu and Bing Wang},
keywords = {Big data, Production safety management, Big-data-driven, Challenges, Opportunities},
abstract = {Big data has caused the scientific community to re-examine the scientific research methodologies and has triggered a revolution in scientific thinking. As a branch of scientific research, production safety management is also exploring methods to take advantage of big data. This research aims to provide a theoretical basis for promoting the application of big data in production safety management. First, four different types of production safety management paradigms were identified, namely small-data-based, static-oriented, interpretation-based and causal-oriented paradigm, and the challenges to these paradigms in the presence of big data were introduced. Second, the opportunities of employing big data in production safety management were identified from four aspects, including better predict the future production safety phenomena, promote production safety management highlight relevance, achieve the balance between deductive and inductive approaches and promote the interdisciplinary development of production safety management. Third, the paradigm shifting trend of production safety management was concluded, and the discipline foundation of the new paradigm was considered as the integration of data science, production management and safety science. Fourth, a new big-data-driven production safety management paradigm was developed, which consists of the logical line of production safety management, the macro-meso-micro data spectrum, the key big data analytics, and the four-dimensional morphology. At last, the strengths (e.g., supporting better-informed safety description, safety inquisition, safety prediction) and future research direction (e.g., theory research focuses on safety-related data mining/capturing/cleansing) of the new paradigm were discussed. The research results not only can provide theoretical and practical basis for big-data-driven production safety management, but also can offer advice to managerial consideration and scholarly investigation.}
}
@article{SILVA2020107828,
title = {Can big data explain yield variability and water productivity in intensive cropping systems?},
journal = {Field Crops Research},
volume = {255},
pages = {107828},
year = {2020},
issn = {0378-4290},
doi = {https://doi.org/10.1016/j.fcr.2020.107828},
url = {https://www.sciencedirect.com/science/article/pii/S0378429019318039},
author = {João Vasco Silva and Tomás R. Tenreiro and Léon Spätjens and Niels P.R. Anten and Martin K. {van Ittersum} and Pytrik Reidsma},
keywords = {Arable crops, Yield gaps, Crop ecology, Crop coefficients (kc), The Netherlands},
abstract = {Yield gaps and water productivity are key indicators to monitor the progress towards more sustainable and productive cropping systems. Individual farmers are collecting increasing amounts of data (‘big data’), which can help monitor the process of sustainable intensification at local level. In this study, we build upon such data to quantify the magnitude and identify the biophysical and management determinants of on-farm yield gaps and water productivity for the main arable crops cultivated in the Netherlands. The analysis focused on ware, seed and starch potatoes, sugar beet, spring onion, winter wheat and spring barley and covered the period 2015–2017. A crop modelling approach based on crop coefficients (kc) and daily weather data was used to estimate the potential yield (Yp), radiation intercepted and potential evapotranspiration (ETP) for each crop. Yield gaps were estimated to be ca. 10% of Yp for sugar beet, 25–30% of Yp for ware, seed and starch potato and spring barley, and 35–40% of Yp for spring onion and winter wheat. Variation in actual yields was associated with water availability in key periods of the growing season as well as with sowing and harvest dates. However, the R2 of the fitted regressions was rather low (20–49%). Current levels of crop water productivity ranged between 13 kg DM ha−1 mm−1 for spring barley, ca. 15 kg DM ha−1 mm−1 for seed potato, spring onion and winter wheat, 23 kg DM ha−1 mm−1 for ware potato and ca. 25 kg DM ha−1 mm−1 for starch potato and sugar beet. These values are about half of their potential, but increasing actual water productivity further is restricted by rainfall amount and distribution. However, doing so should not be prioritized over reducing environmental impacts of these intensive cropping systems in the short-term and may require large investments from farm to regional levels in the long-term. Although these findings are most relevant to similar cropping systems in NW Europe, the underlying methods are generic and can be used to benchmark crop performance in other cropping systems. Based on this work, we argue that ‘big data’ are currently most useful to describe cropping systems at regional scale and derive benchmarks of farm performance but not as much to predict and explain crop yield variability in time and space.}
}
@article{LI2019991,
title = {Prospects for energy economy modelling with big data: Hype, eliminating blind spots, or revolutionising the state of the art?},
journal = {Applied Energy},
volume = {239},
pages = {991-1002},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919302922},
author = {Francis G.N. Li and Chris Bataille and Steve Pye and Aidan O'Sullivan},
keywords = {Energy modelling, Climate policy, Energy policy, Decarbonisation, Energy data, Big data},
abstract = {Energy economy models are central to decision making on energy and climate issues in the 21st century, such as informing the design of deep decarbonisation strategies under the Paris Agreement. Designing policies that are aimed at achieving such radical transitions in the energy system will require ever more in-depth modelling of end-use demand, efficiency and fuel switching, as well as an increasing need for regional, sectoral, and agent disaggregation to capture technological, jurisdictional and policy detail. Building and using these models entails complex trade-offs between the level of detail, the size of the system boundary, and the available computing resources. The availability of data to characterise key energy system sectors and interactions is also a key driver of model structure and parameterisation, and there are many blind spots and design compromises that are caused by data scarcity. We may soon, however, live in a world of data abundance, potentially enabling previously impossible levels of resolution and coverage in energy economy models. But while big data concepts and platforms have already begun to be used in a number of selected energy research applications, their potential to improve or even completely revolutionise energy economy modelling has been almost completely overlooked in the existing literature. In this paper, we explore the challenges and possibilities of this emerging frontier. We identify critical gaps and opportunities for the field, as well as developing foundational concepts for guiding the future application of big data to energy economy modelling, with reference to the existing literature on decision making under uncertainty, scenario analysis and the philosophy of science.}
}
@article{MCISAAC2020510,
title = {Real-world evaluation of enhanced recovery after surgery: big data under the microscope},
journal = {British Journal of Anaesthesia},
volume = {124},
number = {5},
pages = {510-512},
year = {2020},
issn = {0007-0912},
doi = {https://doi.org/10.1016/j.bja.2020.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0007091220300593},
author = {Daniel I. McIsaac},
keywords = {big data, enhanced recovery, epidemiology, orthopedic surgery, postoperative outcome, study design}
}
@article{LI202018,
title = {Teaching Natural Language Processing through Big Data Text Summarization with Problem-Based Learning},
journal = {Data and Information Management},
volume = {4},
number = {1},
pages = {18-43},
year = {2020},
issn = {2543-9251},
doi = {https://doi.org/10.2478/dim-2020-0003},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122000572},
author = {Liuqing Li and Jack Geissinger and William A. Ingram and Edward A. Fox},
keywords = {information system education, computer science education, problem-based learning, natural language processing, NLP, big data text analytics, machine learning, deep learning},
abstract = {Natural language processing (NLP) covers a large number of topics and tasks related to data and information management, leading to a complex and challenging teaching process. Meanwhile, problem-based learning is a teaching technique specifically designed to motivate students to learn efficiently, work collaboratively, and communicate effectively. With this aim, we developed a problem-based learning course for both undergraduate and graduate students to teach NLP. We provided student teams with big data sets, basic guidelines, cloud computing resources, and other aids to help different teams in summarizing two types of big collections: Web pages related to events, and electronic theses and dissertations (ETDs). Student teams then deployed different libraries, tools, methods, and algorithms to solve the task of big data text summarization. Summarization is an ideal problem to address learning NLP since it involves all levels of linguistics, as well as many of the tools and techniques used by NLP practitioners. The evaluation results showed that all teams generated coherent and readable summaries. Many summaries were of high quality and accurately described their corresponding events or ETD chapters, and the teams produced them along with NLP pipelines in a single semester. Further, both undergraduate and graduate students gave statistically significant positive feedback, relative to other courses in the Department of Computer Science. Accordingly, we encourage educators in the data and information management field to use our approach or similar methods in their teaching and hope that other researchers will also use our data sets and synergistic solutions to approach the new and challenging tasks we addressed.}
}
@article{CONNELLY20161,
title = {The role of administrative data in the big data revolution in social science research},
journal = {Social Science Research},
volume = {59},
pages = {1-12},
year = {2016},
note = {Special issue on Big Data in the Social Sciences},
issn = {0049-089X},
doi = {https://doi.org/10.1016/j.ssresearch.2016.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0049089X1630206X},
author = {Roxanne Connelly and Christopher J. Playford and Vernon Gayle and Chris Dibben},
keywords = {Big data, Administrative data, Data management, Data quality, Data access},
abstract = {The term big data is currently a buzzword in social science, however its precise meaning is ambiguous. In this paper we focus on administrative data which is a distinctive form of big data. Exciting new opportunities for social science research will be afforded by new administrative data resources, but these are currently under appreciated by the research community. The central aim of this paper is to discuss the challenges associated with administrative data. We emphasise that it is critical for researchers to carefully consider how administrative data has been produced. We conclude that administrative datasets have the potential to contribute to the development of high-quality and impactful social science research, and should not be overlooked in the emerging field of big data.}
}
@article{BARJAMARTINEZ2021111459,
title = {Artificial intelligence techniques for enabling Big Data services in distribution networks: A review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {150},
pages = {111459},
year = {2021},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2021.111459},
url = {https://www.sciencedirect.com/science/article/pii/S1364032121007413},
author = {Sara Barja-Martinez and Mònica Aragüés-Peñalba and Íngrid Munné-Collado and Pau Lloret-Gallego and Eduard Bullich-Massagué and Roberto Villafafila-Robles},
keywords = {Machine learning, Deep learning, Smart grid, Distribution grid, Smart energy service},
abstract = {Artificial intelligence techniques lead to data-driven energy services in distribution power systems by extracting value from the data generated by the deployed metering and sensing devices. This paper performs a holistic analysis of artificial intelligence applications to distribution networks, ranging from operation, monitoring and maintenance to planning. The potential artificial intelligence techniques for power system applications and needed data sources are identified and classified. The following data-driven services for distribution networks are analyzed: topology estimation, observability, fraud detection, predictive maintenance, non-technical losses detection, forecasting, energy management systems, aggregated flexibility services and trading. A review of the artificial intelligence methods implemented in each of these services is conducted. Their interdependencies are mapped, proving that multiple services can be offered as a single clustered service to different stakeholders. Furthermore, the dependencies between the AI techniques with each energy service are identified. In recent years there has been a significant rise of deep learning applications for time series prediction tasks. Another finding is that unsupervised learning methods are mainly being applied to customer segmentation, buildings efficiency clustering and consumption profile grouping for non-technical losses detection. Reinforcement learning is being widely applied to energy management systems design, although more testing in real environments is needed. Distribution network sensorization should be enhanced and increased in order to obtain larger amounts of valuable data, enabling better service outcomes. Finally, the future opportunities and challenges for applying artificial intelligence in distribution grids are discussed.}
}
@incollection{RISTEVSKI202185,
title = {4 - Healthcare and medical Big Data analytics},
editor = {Ashish Khanna and Deepak Gupta and Nilanjan Dey},
booktitle = {Applications of Big Data in Healthcare},
publisher = {Academic Press},
pages = {85-112},
year = {2021},
isbn = {978-0-12-820203-6},
doi = {https://doi.org/10.1016/B978-0-12-820203-6.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202036000059},
author = {Blagoj Ristevski and Snezana Savoska},
keywords = {Big Data, medical and healthcare Big Data, Big Data Analytics, databases, healthcare information systems},
abstract = {In the era of big data, a huge volume of heterogeneous healthcare and medical data are generated daily. These heterogeneous data, that are stored in diverse data formats, have to be integrated and stored in a standard way and format to perform suitable efficient and effective data analysis and visualization. These data, which are generated from different sources such as mobile devices, sensors, lab tests, clinical notes, social media, demographics data, diverse omics data, etc., can be structured, semistructured, or unstructured. These varieties of data structures require these big data to be stored not only in the standard relational databases but also in NoSQL databases. To provide effective data analysis, suitable classification and standardization of big data in medicine and healthcare are necessary, as well as excellent design and implementation of healthcare information systems. Regarding the security and privacy of the patient’s data, we suggest employing suitable data governance policies. Additionally, we suggest choosing of proper software development frameworks, tools, databases, in-database analytics, stream computing and data mining algorithms (supervised, unsupervised and semisupervised) to reveal valuable knowledge and insights from these healthcare and medical big data. Ultimately we propose the development of not only patient-oriented but also decision- and population-centric healthcare information systems.}
}
@article{MCCOY201774,
title = {Geospatial Big Data and archaeology: Prospects and problems too great to ignore},
journal = {Journal of Archaeological Science},
volume = {84},
pages = {74-94},
year = {2017},
note = {Archaeological GIS Today: Persistent Challenges, Pushing Old Boundaries, and Exploring New Horizons},
issn = {0305-4403},
doi = {https://doi.org/10.1016/j.jas.2017.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0305440317300821},
author = {Mark D. McCoy},
keywords = {Geospatial, Big Data, Spatial technology, Cyberinfrastructure, Data science},
abstract = {As spatial technology has evolved and become integrated in to archaeology, we face a new set of challenges posed by the sheer size and complexity of data we use and produce. In this paper I discuss the prospects and problems of Geospatial Big Data (GBD) – broadly defined as data sets with locational information that exceed the capacity of widely available hardware, software, and/or human resources. While the datasets we create today remain within available resources, we nonetheless face the same challenges as many other fields that use and create GBD, especially in apprehensions over data quality and privacy. After reviewing the kinds of archaeological geospatial data currently available I discuss the near future of GBD in writing culture histories, making decisions, and visualizing the past. I use a case study from New Zealand to argue for the value of taking a data quantity-in-use approach to GBD and requiring applications of GBD in archaeology be regularly accompanied by a Standalone Quality Report.}
}
@article{QIAN2021645,
title = {Visual recognition processing of power monitoring data based on big data computing},
journal = {Energy Reports},
volume = {7},
pages = {645-657},
year = {2021},
note = {2021 International Conference on Energy Engineering and Power Systems},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.09.205},
url = {https://www.sciencedirect.com/science/article/pii/S235248472101009X},
author = {Jianguo Qian and Bingquan Zhu and Ying Li and Zhengchai Shi},
keywords = {Power control data, Monitoring, Visual identification, Iterative screening, CARIMA},
abstract = {The operation control of power units is usually carried out by the control personnel with the help of distributed control system. Although it can ensure the safety of unit operation and meet the requirements of power generation loads, the economy of unit operation and the accuracy of control process still need to be further improved. Therefore, by designing multiple view mapping and association, it provides interactive visualization support for relevant experts in the key links of model establishment and evaluation. In the exploration stage of estimating model parameter, the user can get the delay range by line chart and focus + context technology, while in the model screening stage, the user can provide the combination of screening views, selecting the model by its accuracy on different data sets, and finding the model anomalies by the model structure view. Besides, in the model evaluation stage, the user can get the delay range by predicting line chart and model accuracy radar chart. In addition, the method in this paper keeps between 4.2–7.2 in most distributions, and the maximum value is 18. The time series trend of the data segment is consistent, and the absolute value of the weight coefficient is basically 0 after being superimposed, which has great advantages compared with other methods, proving the effective results of the research content in this paper.}
}
@article{CAISSIE2020e773,
title = {Radiotherapy (RT) Patterns Of Practice Variability Identified As A Challenge To Real-World Big Data: Recommendations From The Learning From Analysis Of Multicenter Big Data Aggregation (LAMBDA) Consortium},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {108},
number = {3, Supplement },
pages = {e773-e774},
year = {2020},
note = {Proceedings of the American Society for Radiation Oncology},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2020.07.224},
url = {https://www.sciencedirect.com/science/article/pii/S0360301620316436},
author = {A.L. Caissie and M.L. Mierzwa and C.D. Fuller and M. Rajaraman and A. Lin and A.M. McDonald and R.A. Popple and Y. Xiao and L. {van Dijk} and P. Balter and H. Fong and H. Ping and M. Kovoor and J. Lee and A. Rao and M.K. Martel and R.F. Thompson and B. Merz and J. Yao and C. Mayo}
}
@article{KUMARI2018169,
title = {Multimedia big data computing and Internet of Things applications: A taxonomy and process model},
journal = {Journal of Network and Computer Applications},
volume = {124},
pages = {169-195},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518303011},
author = {Aparna Kumari and Sudeep Tanwar and Sudhanshu Tyagi and Neeraj Kumar and Michele Maasberg and Kim-Kwang Raymond Choo},
keywords = {Multimedia big data, Data acquisition, Data representation, Data reduction, Data analysis, Data security and privacy, System intelligence, Distributed system, Social media},
abstract = {With an exponential increase in the provisioning of multimedia devices over the Internet of Things (IoT), a significant amount of multimedia data (also referred to as multimedia big data – MMBD) is being generated. Current research and development activities focus on scalar sensor data based IoT or general MMBD and overlook the complexity of facilitating MMBD over IoT. This paper examines the unique nature and complexity of MMBD computing for IoT applications and develops a comprehensive taxonomy for MMBD abstracted into a novel process model reflecting MMBD over IoT. This process model addresses a number of research challenges associated with MMBD, such as scalability, accessibility, reliability, heterogeneity, and Quality of Service (QoS) requirements. A case study is presented to demonstrate the process model.}
}
@article{ZHAO2020132,
title = {Privacy-preserving clustering for big data in cyber-physical-social systems: Survey and perspectives},
journal = {Information Sciences},
volume = {515},
pages = {132-155},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519309764},
author = {Yaliang Zhao and Samwel K. Tarus and Laurence T. Yang and Jiayu Sun and Yunfei Ge and Jinke Wang},
keywords = {CPSS, Big data, Cloud computing, Privacy preserving, Clustering},
abstract = {Clustering technique plays a critical role in data mining, and has received great success to solve application problems like community analysis, image retrieval, personalized recommendation, activity prediction, etc. This paper first reviews the traditional clustering and the emerging multiple clustering methods, respectively. Although the existing methods have superior performance on some small or certain datasets, they fall short when clustering is performed on CPSS big data because of the high cost of computation and storage. With the powerful cloud computing, this challenge can be effectively addressed, but it brings enormous threat to individual or company’s privacy. Currently, privacy preserving data mining has attracted widespread attention in academia. Compared to other reviews, this paper focuses on privacy preserving clustering technique, guiding a detailed overview and discussion. Specifically, we introduce a novel privacy-preserving tensor-based multiple clustering, propose a privacy-preserving tensor-based multiple clustering analytic and service framework, and give an illustrated case study on the public transportation dataset. Furthermore, we indicate the remaining challenges of privacy preserving clustering and discuss the future significant research in this area.}
}
@article{ULLAH201981,
title = {Architectural Tactics for Big Data Cybersecurity Analytics Systems: A Review},
journal = {Journal of Systems and Software},
volume = {151},
pages = {81-118},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.01.051},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300172},
author = {Faheem Ullah and Muhammad {Ali Babar}},
keywords = {Big data, Cybersecurity, Quality attribute, Architectural tactic},
abstract = {Context
Big Data Cybersecurity Analytics (BDCA) systems leverage big data technologies for analyzing security events data to protect organizational networks, computers, and data from cyber attacks.
Objective
We aimed at identifying the most frequently reported quality attributes and architectural tactics for BDCA systems.
Method
We used Systematic Literature Review (SLR) method for reviewing 74 papers.
Result
Our findings are twofold: (i) identification of 12 most frequently reported quality attributes for BDCA systems; and (ii) identification and codification of 17 architectural tactics for addressing the identified quality attributes. The identified tactics include six performance tactics, four accuracy tactics, two scalability tactics, three reliability tactics, and one security and usability tactic each.
Conclusion
Our study reveals that in the context of BDCA (a) performance, accuracy and scalability are the most important quality concerns (b) data analytics is the most critical architectural component (c) despite the significance of interoperability, modifiability, adaptability, generality, stealthiness, and privacy assurance, these quality attributes lack explicit architectural support (d) empirical investigation is required to evaluate the impact of the codified tactics and explore the quality trade-offs and dependencies among the tactics and (e) the reported tactics need to be modelled using a standardized modelling language such as UML.}
}
@article{LI2020106143,
title = {Ensemble-based deep learning for estimating PM2.5 over California with multisource big data including wildfire smoke},
journal = {Environment International},
volume = {145},
pages = {106143},
year = {2020},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2020.106143},
url = {https://www.sciencedirect.com/science/article/pii/S0160412020320985},
author = {Lianfa Li and Mariam Girguis and Frederick Lurmann and Nathan Pavlovic and Crystal McClure and Meredith Franklin and Jun Wu and Luke D. Oman and Carrie Breton and Frank Gilliland and Rima Habre},
keywords = {PM, Machine learning, Air pollution exposure, Wildfires, Remote sensing, California, High spatiotemporal resolution},
abstract = {Introduction
Estimating PM2.5 concentrations and their prediction uncertainties at a high spatiotemporal resolution is important for air pollution health effect studies. This is particularly challenging for California, which has high variability in natural (e.g, wildfires, dust) and anthropogenic emissions, meteorology, topography (e.g. desert surfaces, mountains, snow cover) and land use.
Methods
Using ensemble-based deep learning with big data fused from multiple sources we developed a PM2.5 prediction model with uncertainty estimates at a high spatial (1 km × 1 km) and temporal (weekly) resolution for a 10-year time span (2008–2017). We leveraged autoencoder-based full residual deep networks to model complex nonlinear interrelationships among PM2.5 emission, transport and dispersion factors and other influential features. These included remote sensing data (MAIAC aerosol optical depth (AOD), normalized difference vegetation index, impervious surface), MERRA-2 GMI Replay Simulation (M2GMI) output, wildfire smoke plume dispersion, meteorology, land cover, traffic, elevation, and spatiotemporal trends (geo-coordinates, temporal basis functions, time index). As one of the primary predictors of interest with substantial missing data in California related to bright surfaces, cloud cover and other known interferences, missing MAIAC AOD observations were imputed and adjusted for relative humidity and vertical distribution. Wildfire smoke contribution to PM2.5 was also calculated through HYSPLIT dispersion modeling of smoke emissions derived from MODIS fire radiative power using the Fire Energetics and Emissions Research version 1.0 model.
Results
Ensemble deep learning to predict PM2.5 achieved an overall mean training RMSE of 1.54 μg/m3 (R2: 0.94) and test RMSE of 2.29 μg/m3 (R2: 0.87). The top predictors included M2GMI carbon monoxide mixing ratio in the bottom layer, temporal basis functions, spatial location, air temperature, MAIAC AOD, and PM2.5 sea salt mass concentration. In an independent test using three long-term AQS sites and one short-term non-AQS site, our model achieved a high correlation (>0.8) and a low RMSE (<3 μg/m3). Statewide predictions indicated that our model can capture the spatial distribution and temporal peaks in wildfire-related PM2.5. The coefficient of variation indicated highest uncertainty over deciduous and mixed forests and open water land covers.
Conclusion
Our method can be generalized to other regions, including those having a mix of major urban areas, deserts, intensive smoke events, snow cover and complex terrains, where PM2.5 has previously been challenging to predict. Prediction uncertainty estimates can also inform further model development and measurement error evaluations in exposure and health studies.}
}
@article{LOPEZROBLES2019729,
title = {The last five years of Big Data Research in Economics, Econometrics and Finance: Identification and conceptual analysis},
journal = {Procedia Computer Science},
volume = {162},
pages = {729-736},
year = {2019},
note = {7th International Conference on Information Technology and Quantitative Management (ITQM 2019): Information technology and quantitative management based on Artificial Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919320551},
author = {José Ricardo López-Robles and Marisela Rodríguez-Salvador and Nadia Karina Gamboa-Rosales and Selene Ramirez-Rosales and Manuel Jesús Cobo},
keywords = {Type your keywords here, separated by semicolons},
abstract = {Today, the Big Data term has a multidimensional approach where five main characteristics stand out: volume, velocity, veracity, value and variety. It has changed from being an emerging theme to a growing research area. In this respect, this study analyses the literature on Big Data in the Economics, Econometrics and Finance field. To do that, 1.034 publications from 2015 to 2019 were evaluated using SciMAT as a bibliometric and network analysis software. SciMAT offers a complete approach of the field and evaluates the most cited and productive authors, countries and subject areas related to Big Data. Lastly, a science map is performed to understand the intellectual structure and the main research lines (themes).}
}
@article{VYDRA2019101383,
title = {Techno-optimism and policy-pessimism in the public sector big data debate},
journal = {Government Information Quarterly},
volume = {36},
number = {4},
pages = {101383},
year = {2019},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2019.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X18302326},
author = {Simon Vydra and Bram Klievink},
keywords = {Big data, Analytics, Government, Public administration, Policy-making, Decision-making, Science-policy interface, Network governance},
abstract = {Despite great potential, high hopes and big promises, the actual impact of big data on the public sector is not always as transformative as the literature would suggest. In this paper, we ascribe this predicament to an overly strong emphasis the current literature places on technical-rational factors at the expense of political decision-making factors. We express these two different emphases as two archetypical narratives and use those to illustrate that some political decision-making factors should be taken seriously by critiquing some of the core ‘techno-optimist’ tenets from a more ‘policy-pessimist’ angle. In the conclusion we have these two narratives meet ‘eye-to-eye’, facilitating a more systematized interrogation of big data promises and shortcomings in further research, paying appropriate attention to both technical-rational and political decision-making factors. We finish by offering a realist rejoinder of these two narratives, allowing for more context-specific scrutiny and balancing both technical-rational and political decision-making concerns, resulting in more realistic expectations about using big data for policymaking in practice.}
}
@article{BELL2021453,
title = {Exploring future challenges for big data in the humanitarian domain},
journal = {Journal of Business Research},
volume = {131},
pages = {453-468},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320306172},
author = {David Bell and Mark Lycett and Alaa Marshan and Asmat Monaghan},
keywords = {Big data, Veracity, Granularity, Heterogeneous datasets, Humanitarian, Value},
abstract = {This paper examines the challenges of leveraging big data in the humanitarian sector in support of UN Sustainable Development Goal 17 “Partnerships for the Goals”. The full promise of Big Data is underpinned by a tacit assumption that the heterogeneous ‘exhaust trail’ of data is contextually relevant and sufficiently granular to be mined for value. This promise, however, relies on relationality – that patterns can be derived from combining different pieces of data that are of corresponding detail or that there are effective mechanisms to resolve differences in detail. Here, we present empirical work integrating eight heterogeneous datasets from the humanitarian domain to provide evidence of the inherent challenge of complexity resulting from differing levels of data granularity. In clarifying this challenge, we explore the reasons why it is manifest, discuss strategies for addressing it and, as our principal contribution, identify five propositions to guide future research.}
}
@article{WANG202116,
title = {A digital twin-based big data virtual and real fusion learning reference framework supported by industrial internet towards smart manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {16-32},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301990},
author = {Pei Wang and Ming Luo},
keywords = {Virtual and real fusion learning, Big data learning and analysis models, Digital twin, Industrial internet, Smart manufacturing},
abstract = {Digital twin takes Industrial Internet as a carrier deeply coordinating and integrating virtual spaces with physical spaces, which effectively promotes smart factory development. Digital twin-based big data learning and analysis (BDLA) deepens virtual and real fusion, interaction and closed-loop iterative optimization in smart factories. This paper proposes a digital twin-based big data virtual and real fusion (DT-BDVRL) reference framework supported by Industrial Internet towards smart manufacturing. The reference framework is synthetically designed from three perspectives. The first one is an overall framework of DT-BDVRL supported by Industrial Internet. The second one is the establishment method and flow of BDLA models based on digital twin. The final one is digital thread of DT-BDVRL in virtual and real fusion analysis, iteration and closed-loop feedback in product full life cycle processes. For different virtual scenes, iterative optimization and verification methods and processes of BDLA models in virtual spaces are established. Moreover, the BDLA results can drive digital twin running in virtual spaces. By this, the BDLA results can be validated iteratively multiple times in virtual spaces. At same time, the BDLA results that run in virtual spaces are synchronized and executed in physical spaces through Industrial Internet platforms, effectively improving the physical execution effect of BDLA models. Finally, the above contents were applied and verified in the actual production case study of power switchgear equipment.}
}
@article{LIU2020123,
title = {Urban big data fusion based on deep learning: An overview},
journal = {Information Fusion},
volume = {53},
pages = {123-133},
year = {2020},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2019.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S1566253519301393},
author = {Jia Liu and Tianrui Li and Peng Xie and Shengdong Du and Fei Teng and Xin Yang},
keywords = {Urban computing, Big data, Data fusion, Deep learning},
abstract = {Urban big data fusion creates huge values for urban computing in solving urban problems. In recent years, various models and algorithms based on deep learning have been proposed to unlock the power of knowledge from urban big data. To clarify the methodologies of urban big data fusion based on deep learning (DL), this paper classifies them into three categories: DL-output-based fusion, DL-input-based fusion and DL-double-stage-based fusion. These methods use deep learning to learn feature representation from multi-source big data. Then each category of fusion methods is introduced and some examples are shown. The difficulties and ideas of dealing with urban big data will also be discussed.}
}
@incollection{PLOTKIN2021245,
title = {Chapter 10 - Big Data Stewardship and Data Lakes},
editor = {David Plotkin},
booktitle = {Data Stewardship (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {245-255},
year = {2021},
isbn = {978-0-12-822132-7},
doi = {https://doi.org/10.1016/B978-0-12-822132-7.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128221327000103},
author = {David Plotkin},
keywords = {Big data, data lake, unstructured data, zone},
abstract = {Big Data Governance and big data stewardship are not so different from what we’ve been doing prior to the advent of big data and data lakes. Most of the same roles still need to be filled, and accountability for making data decisions is even more important because of the vast quantity of data, the many ways in which it can be changed, and increased consequences of “getting it wrong” due to not only the large quantities of data and metadata, but also the speed at which the data can change.}
}
@article{ELIA2020617,
title = {A multi-dimension framework for value creation through Big Data},
journal = {Industrial Marketing Management},
volume = {90},
pages = {617-632},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2020.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0019850120302212},
author = {Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante},
keywords = {Big Data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation},
abstract = {Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.}
}
@article{MAJOR202056,
title = {Using big data in pediatric oncology: Current applications and future directions},
journal = {Seminars in Oncology},
volume = {47},
number = {1},
pages = {56-64},
year = {2020},
note = {Pediatric Oncology},
issn = {0093-7754},
doi = {https://doi.org/10.1053/j.seminoncol.2020.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0093775420300063},
author = {Ajay Major and Suzanne M. Cox and Samuel L. Volchenboum},
keywords = {Pediatric oncology, Pediatric cancer, Big data, Data sharing, Data science, Informatics},
abstract = {Pediatric cancer is a rare disease with a low annual incidence, which presents a significant challenge in being able to collect enough data to fuel clinical discoveries. Big data registry trials hold promise to advance the study of pediatric cancers by allowing for the combination of traditional randomized controlled trials with the power of larger cohort sizes. The emergence of big data resources and data-sharing initiatives are becoming transformative for pediatric cancer diagnosis and treatment. This review discusses the uses of big data in pediatric cancer, existing pediatric cancer registry initiatives and research, the challenges in harmonizing these data to improve accessibility for study, and building pediatric data commons and other important future endeavors.}
}
@article{TAMYM2021102,
title = {A big data based architecture for collaborative networks: Supply chains mixed-network},
journal = {Computer Communications},
volume = {175},
pages = {102-111},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421001924},
author = {Lahcen Tamym and Lyes Benyoucef and Ahmed {Nait Sidi Moh} and Moulay Driss {El Ouadghiri}},
keywords = {Big data architecture, Collaborative networks, Enterprises network, Supply chain network, Flexibility, Robustness},
abstract = {Nowadays, the world knows a high-speed development and evolution of technologies, vulnerable economic environments, market changes, and personalised consumer trends. The issue and challenge related to enterprises networks design are more and more critical. These networks are often designed for short terms since their strategies must be competitive and better adapted to the environment, social and economical changes. As a solution, to design a flexible and robust network, it is necessary to deal with the trade-off between conflicting qualitative and quantitative criteria such as cost, quality, delivery time, and competition, etc. To this end, using Big Data (BD) as emerging technology will enhance the real performances of these kinds of networks. Moreover, even if the literature is rich with BD models and frameworks developed for a single supply chain network (SCN), there is a real need to scale and extend these BD models to networked supply chains (NSCs). To do so, this paper proposes a BD architecture to drive a mixed-network of SCs that collaborate in serial and parallel fashions. The collaboration is set up by sharing their resources, capabilities, competencies, and information to imitate a unique organisation. The objective is to increase internal value to their shareholders (where value is seen as wealth) and deliver better external value to the end-customer (where value represents customer satisfaction). Within a mixed-network of SCs, both values are formally calculated considering both serial and parallel networks configurations. Besides, some performance factors of the proposed BD architecture such as security, flexibility, robustness and resilience are discussed.}
}
@article{ZHAO2021101196,
title = {Prediction model of ecological environmental water demand based on big data analysis},
journal = {Environmental Technology & Innovation},
volume = {21},
pages = {101196},
year = {2021},
issn = {2352-1864},
doi = {https://doi.org/10.1016/j.eti.2020.101196},
url = {https://www.sciencedirect.com/science/article/pii/S2352186420314966},
author = {Lihong Zhao},
keywords = {Big data analysis, Ecological environment, Water demand, Prediction},
abstract = {The existing prediction model of eco-environmental water demand has the problem of large prediction error. In order to solve the above problems, the prediction model of eco-environmental water demand is constructed based on big data analysis. In order to reduce the prediction error of the ecological environment water demand prediction model, the framework of the ecological environment water demand prediction model is built. On this basis, the principal component analysis method is used to select the auxiliary variables of the model. Based on the selected auxiliary variables, the minimum monthly average flow method is used to analyze the basic water demand of the ecological environment, the leakage water demand and the water surface evaporation ecological environment water demand, so as to analyze based on the results, the water demand of ecological environment is predicted by big data analysis technology, and the prediction of water demand of ecological environment is realized. The experimental results show that compared with the existing ecological environment water demand prediction model, the prediction error of the model is within 19.3, which fully shows that the constructed ecological environment water demand prediction model has better prediction effect and can provide a certain reference value for the actual use of water resources.}
}
@article{VALENCIAPARRA2020101180,
title = {Unleashing Constraint Optimisation Problem solving in Big Data environments},
journal = {Journal of Computational Science},
volume = {45},
pages = {101180},
year = {2020},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101180},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320304816},
author = {Álvaro Valencia-Parra and Ángel Jesús Varela-Vaca and Luisa Parody and María Teresa Gómez-López},
keywords = {Big Data, Optimisation problem, Constraint programming, Distributed data, Heterogeneous data format},
abstract = {The application of the optimisation problems in the daily decisions of companies is able to be used for finding the best management according to the necessities of the organisations. However, optimisation problems imply a high computational complexity, increased by the current necessity to include a massive quantity of data (Big Data), for the creation of optimisation problems to customise products and services for their clients. The irruption of Big Data technologies can be a challenge but also an important mechanism to tackle the computational difficulties of optimisation problems, and the possibility to distribute the problem performance. In this paper, we propose a solution that lets the query of a data set supported by Big Data technologies that imply the resolution of Constraint Optimisation Problem (COP). This proposal enables to: (1) model COPs whose input data are obtained from distributed and heterogeneous data; (2) facilitate the integration of different data sources to create the COPs; and, (3) solve the optimisation problems in a distributed way, to improve the performance. It is done by means of a framework and supported by a tool capable of modelling, solving and querying the results of optimisation problems. The tool integrates the Big Data technologies and commercial solvers of constraint programming. The suitability of the proposal and the development have been evaluated with real data sets whose computational study and results are included and discussed.}
}
@article{KUN2019556,
title = {Application of Big Data Technology in Scientific Research Data Management of Military Enterprises},
journal = {Procedia Computer Science},
volume = {147},
pages = {556-561},
year = {2019},
note = {2018 International Conference on Identification, Information and Knowledge in the Internet of Things},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.221},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919302406},
author = {Wang Kun and Liu Tong and Xie Xiaodan},
keywords = {big data technology, scientific research data, data analysis, decision},
abstract = {Scientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology started late in the military enterprises, while military enterprises research data often has the characteristics of decentralization, low relevance, and diverse data types. It cannot fully utilize the advantages of data resources to enhance the core competitiveness of enterprises. To this end, this paper deeply explores the application methods of big data technology in military scientific research data management, and lays a foundation for the construction of scientific research big data platform.}
}
@article{CARPIOPINEDO2020102859,
title = {Consumption and symbolic capital in the metropolitan space: Integrating ‘old’ retail data sources with social big data},
journal = {Cities},
volume = {106},
pages = {102859},
year = {2020},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2020.102859},
url = {https://www.sciencedirect.com/science/article/pii/S0264275120312075},
author = {José Carpio-Pinedo and Javier Gutiérrez},
keywords = {Commercial space, Retail, Retail geography, Symbolic capital, Big data, Foursquare, Madrid},
abstract = {While commerce is one of the key activities in cities, its spatial description still requires further attention, especially by considering the different dimensions of commercial space: physical, economic and socio-symbolic. The latter is becoming more and more important in an era where consumption is at the centre of social relations. Further, although data availability has been an enduring obstacle in commercial research, we are witnessing the advent of new data sources, and social-network big data is an opportunity to unveil the places to which consumers attribute prestige or symbolic capital, at the extent of entire metropolitan areas. This paper compares the physical, economic and socio-symbolic dimensions of commercial spaces through the analysis of three different commercial data sources: cadastral micro-data, business register and social-network big data. For the case of Madrid Metropolitan Area, the three databases are compared with correlation analysis and density maps, coming out as partly redundant and partly complementary. Getis-Ord's hotspot statistics integrated into a cluster analysis enable a comprehensive understanding of commercial environments, enriching previous spatial hierarchies. The spatial distribution of symbolic capital unveils a relation with socio-spatial segregation and paves the way to new reflections on the spatiality of consumption as a social practice.}
}
@article{TRUJILLO2021101911,
title = {Conceptual modeling in the era of Big Data and Artificial Intelligence: Research topics and introduction to the special issue},
journal = {Data & Knowledge Engineering},
volume = {135},
pages = {101911},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101911},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000380},
author = {Juan Trujillo and Karen C. Davis and Xiaoyong Du and Ernesto Damiani and Veda C. Storey},
keywords = {Conceptual modeling, Big Data, Machine learning, Artificial Intelligence},
abstract = {Since the first version of the Entity–Relationship (ER) model proposed by Peter Chen over forty years ago, both the ER model and conceptual modeling activities have been key success factors for modeling computer-based systems. During the last decade, conceptual modeling has been recognized as an important research topic in academia, as well as a necessity for practitioners. However, there are many research challenges for conceptual modeling in contemporary applications such as Big Data, data-intensive applications, decision support systems, e-health applications, and ontologies. In addition, there remain challenges related to the traditional efforts associated with methodologies, tools, and theory development. Recently, novel research is uniting contributions from both the conceptual modeling area and the Artificial Intelligence discipline in two directions. The first one is efforts related to how conceptual modeling can aid in the design of Artificial Intelligence (AI) and Machine Learning (ML) algorithms. The second one is how Artificial Intelligence and Machine Learning can be applied in model-based solutions, such as model-based engineering, to infer and improve the generated models. For the first time in the history of Conceptual Modeling (ER) conferences, we encouraged the submission of papers based on AI and ML solutions in an attempt to highlight research from both communities. In this paper, we present some of important topics in current research in conceptual modeling. We introduce the selected best papers from the 37th International Conference on Conceptual Modeling (ER’18) held in Xi’an, China and summarize some of the valuable contributions made based on the discussions of these papers. We conclude with suggestions for continued research.}
}
@article{MIRACOLO2022S206,
title = {POSB319 Predictive Analytic Techniques and Big Data for Improved Health Outcomes in the Context of Value Based Health Care and Coverage Decisions: A Scoping Review},
journal = {Value in Health},
volume = {25},
number = {1, Supplement },
pages = {S206},
year = {2022},
note = {Emerging Frontiers and Opportunities},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2021.11.1002},
url = {https://www.sciencedirect.com/science/article/pii/S1098301521027972},
author = {A Miracolo and M Mills and P Kanavos}
}
@article{LV2021103298,
title = {Detecting the true urban polycentric pattern of Chinese cities in morphological dimensions: A multiscale analysis based on geospatial big data},
journal = {Cities},
volume = {116},
pages = {103298},
year = {2021},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2021.103298},
url = {https://www.sciencedirect.com/science/article/pii/S0264275121001980},
author = {Yongqiang Lv and Lin Zhou and Guobiao Yao and Xinqi Zheng},
keywords = {Polycentricity, Urban centers, Multi-scale, Street blocks, Geospatial big data, Chinese cities},
abstract = {With current decentralization trends and polycentric planning efforts, the urban spatial structures of Chinese cities have been changing tremendously. To detect the true urban polycentric pattern of Chinese cities, this article analyzed the urban polycentricity characteristics of 294 cities. The natural cities were delineated by points of interest (POIs), and road networks constituted street blocks. Based on check-in data and new spatial units, centers within both metropolitan areas and central cities were identified and examined. We discovered that all Chinese cities have at least one natural city in their metropolitan areas because of rapid urban sprawl. Although a monocentric structure is still the most common urban spatial structure, 110 Chinese cities displayed different degrees of polycentricity at the metropolitan level. Many natural cities beyond central cities contribute to polycentric development at the metropolitan level. Central cities have maintained their original vitality and importance, most Chinese cities have dispersed urban structures in central cities, and 45 central cities are polycentric. The spatial structures in metropolitan areas are more polycentric than those in central cities. The only 36 cities with polycentric urban structures at both the metropolitan and central city levels are all national or regional central cities in eastern China.}
}
@article{SBAI2020938,
title = {A real-time Decision Support System for Big Data Analytic: A case of Dynamic Vehicle Routing Problems},
journal = {Procedia Computer Science},
volume = {176},
pages = {938-947},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319876},
author = {Ines Sbai and Saoussen Krichen},
keywords = {Big data analytic, Decision Support System, DVRP, S-GA, Spark},
abstract = {Recently, the explosion of large amounts of traffic data has guided data scientists to create models with big data for a better decision-making. Big Data applications process and analyze this huge amounts of data (collected from a variety of heterogeneous data sources) that cannot be processed with traditional technologies. In this paper, Big Data frameworks are used for solving an optimization problem known as Dynamic Vehicle Routing Problem (DVRP). Hence, due to the NP-Hardness of the problem and to deal with a large size of data, we develop a parallel Spark Genetic Algorithm named (S-GA). This parallelism aims to take the advantage of Spark’s in-memory computing ability (as a master-slave distribution computing) and GA’s iterations operations. Parallel operations were used for fitness evaluation and genetic operations. Based on the parallel S-GA a decision support system is developed for the DVRP in order to generate the best routes. The experiments show that our proposed architecture is improved due to its capacity when coping with Big Data optimization problems by interconnecting components and deploying on different nodes of a cluster.}
}
@article{SMALEC20215156,
title = {Big Data as a tool helpful in communication management},
journal = {Procedia Computer Science},
volume = {192},
pages = {5156-5165},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.293},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020329},
author = {Agnieszka Smalec},
keywords = {Big Data, marketing communication, management, data processing, collection, communication management},
abstract = {The boundaries between the online and offline worlds have become irretrievably blurred, especially as mobile devices have proliferated. As a result, more and more activities are transferred to the Internet. Every activity in the network leaves a trace, which is why the volume of available data is growing rapidly. The amount of increasing information affects all market participants, and the necessity to constantly collect and process large amounts of data becomes an everyday reality. The aim of the article is to present the concept of big data and to indicate examples of the use of big data to manage marketing communication with the environment. It should be emphasized that not only data transfer devices, but also human interaction contribute to the creation of very large data sets. Acquiring and correctly interpreting them plays an important role in market entities in terms of management, including communication management. Contemporary multi-directional communication, including communication in a hypermedia environment, creates new challenges and threats. The article was prepared based on a literature review, research reports and an analysis of secondary sources. It also outlines the practical implications. The considerations provided are the basis for further activities and empirical research.}
}
@article{HADJSASSI2019534,
title = {A New Architecture for Cognitive Internet of Things and Big Data},
journal = {Procedia Computer Science},
volume = {159},
pages = {534-543},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.208},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919313924},
author = {Mohamed Saifeddine {Hadj Sassi} and Faiza Ghozzi Jedidi and Lamia Chaari Fourati},
keywords = {Internet of Things, Big-Data, Architecture, Cognitive, Data-flow},
abstract = {Big data and the Internet of Things (IoT) are considered as the main paradigms when defining new information architecture projects. Accordingly, technologies that make up these solutions could have an important role to play in business information architecture. Solutions that have approached big data and the IoT as unique technology initiatives, struggle in finding value in such efforts and in the technology itself. A connection to the requirements (volume, velocity, and variety) is mandatory to reach the potential business goals. In this context, we propose a new architecture for Cognitive Internet of Things (CIoT) and big data. The proposed architecture benefits computing mechanisms by combining the data WareHouse (DWH) and Data Lake (DL), and defining a tool for heterogeneous data collection.}
}
@article{KHALAJZADEH2020100964,
title = {An end-to-end model-based approach to support big data analytics development},
journal = {Journal of Computer Languages},
volume = {58},
pages = {100964},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2020.100964},
url = {https://www.sciencedirect.com/science/article/pii/S2590118420300241},
author = {Hourieh Khalajzadeh and Andrew J. Simmons and Mohamed Abdelrazek and John Grundy and John Hosking and Qiang He},
keywords = {Big data analytics, Big data modeling, Big data toolkits, Domain-specific visual languages, Multidisciplinary teams, End-user tools},
abstract = {We present BiDaML 2.0, an integrated suite of visual languages and supporting tool to help multidisciplinary teams with the design of big data analytics solutions. BiDaML tool support provides a platform for efficiently producing BiDaML diagrams and facilitating their design, creation, report and code generation. We evaluated BiDaML using two types of evaluations, a theoretical analysis using the “physics of notations”, and an empirical study with 1) a group of 12 target end-users and 2) five individual end-users. Participants mostly agreed that BiDaML was straightforward to understand/learn, and prefer BiDaML for supporting complex data analytics solution modeling than other modeling languages.}
}
@article{YIN2022104285,
title = {Perception model of surrounding rock geological conditions based on TBM operational big data and combined unsupervised-supervised learning},
journal = {Tunnelling and Underground Space Technology},
volume = {120},
pages = {104285},
year = {2022},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2021.104285},
url = {https://www.sciencedirect.com/science/article/pii/S0886779821004764},
author = {Xin Yin and Quansheng Liu and Xing Huang and Yucong Pan},
keywords = {TBM, Surrounding rock class, Perception model, Unsupervised learning, Supervised learning},
abstract = {The perception of surrounding rock geological conditions ahead the tunnel face is essential for TBM safe and efficient tunnelling. This paper developed a perception approach of surrounding rock class based on TBM operational big data and combined unsupervised-supervised learning. In data preprocessing, four data mining techniques (i.e., Z-score, K-NN, Kalman filtering, and wavelet packet decomposition) were used to detect outliers, substitute outliers, suppress noise, and extract features, respectively. Then, GMM was used to revise the original surrounding rock class through clustering TBM load parameters and performance parameters in view of the shortcomings of the HC method in the TBM-excavated tunnel. After that, five various ensemble learning classification models were constructed to identify the surrounding rock class, in which model hyper-parameters were automatically tuned by Bayes optimization. In order to evaluate model performance, balanced accuracy, Kappa, F1-score, and training time were taken into account, and a novel multi-metric comprehensive ranking system was designed. Engineering application results indicated that LightGBM achieved the most superior performance with the highest comprehensive score of 6.9066, followed by GBDT (5.9228), XGBoost (5.4964), RF (3.7581), and AdaBoost (0.9946). Through the weighted purity reduction algorithm, the contributions of input features on the five models were quantitatively analyzed. Finally, the impact of class imbalance on model performance was discussed using the ADASYN algorithm, showing that eliminating class imbalance can further improve the model's perception ability.}
}
@article{ZHAO20201624,
title = {Advancing computer-aided drug discovery (CADD) by big data and data-driven machine learning modeling},
journal = {Drug Discovery Today},
volume = {25},
number = {9},
pages = {1624-1638},
year = {2020},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2020.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1359644620302646},
author = {Linlin Zhao and Heather L. Ciallella and Lauren M. Aleksunes and Hao Zhu},
abstract = {Advancing a new drug to market requires substantial investments in time as well as financial resources. Crucial bioactivities for drug candidates, including their efficacy, pharmacokinetics (PK), and adverse effects, need to be investigated during drug development. With advancements in chemical synthesis and biological screening technologies over the past decade, a large amount of biological data points for millions of small molecules have been generated and are stored in various databases. These accumulated data, combined with new machine learning (ML) approaches, such as deep learning, have shown great potential to provide insights into relevant chemical structures to predict in vitro, in vivo, and clinical outcomes, thereby advancing drug discovery and development in the big data era.}
}
@article{DECAMARGOFIORINI2018112,
title = {Management theory and big data literature: From a review to a research agenda},
journal = {International Journal of Information Management},
volume = {43},
pages = {112-129},
year = {2018},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S026840121830553X},
author = {Paula {de Camargo Fiorini} and Bruno Michel {Roman Pais Seles} and Charbel Jose {Chiappetta Jabbour} and Enzo {Barberio Mariano} and Ana Beatriz Lopes {de Sousa Jabbour}},
keywords = {Big data, Big data analytics, Organizational theory, Firms’ performance, Research agenda},
abstract = {The purpose of this study is to enrich the existing state-of-the-art literature on the impact of big data on business growth by examining how dozens of organizational theories can be applied to enhance the understanding of the effects of big data on organizational performance. While the majority of management disciplines have had research dedicated to the conceptual discussion of how to link a variety of organizational theories to empirically quantified research topics, the body of research into big data so far lacks an academic work capable of systematising the organizational theories supporting big data domain. The three main contributions of this work are: (a) it addresses the application of dozens of organizational theories to big data research; (b) it offers a research agenda on how to link organizational theories to empirical research in big data; and (c) it foresees promising linkages between organizational theories and the effects of big data on organizational performance, with the aim of contributing to further research in this field. This work concludes by presenting implications for researchers and managers, and by highlighting intrinsic limitations of the research.}
}
@article{BALTI2020101136,
title = {A review of drought monitoring with big data: Issues, methods, challenges and research directions},
journal = {Ecological Informatics},
volume = {60},
pages = {101136},
year = {2020},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2020.101136},
url = {https://www.sciencedirect.com/science/article/pii/S1574954120300868},
author = {Hanen Balti and Ali {Ben Abbes} and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle},
keywords = {Drought monitoring, Artificial intelligence, Big data, Machine learning, Statistical approach, Remote sensing},
abstract = {Over recent years, the frequency and intensity of droughts have increased and there has been a large drying trend over many parts of the world. Consequently, drought monitoring using big data analytic has gained an explosive interest. Droughts stand among the most damaging natural disasters. It threatens agricultural production, ecological environment, and socio-economic development. For this reason, early warning, accurate evaluation, and efficient prediction are an emergency especially for the nations that are the most menaced by this danger. There are numerous emerging studies addressing big data and its applications in drought monitoring. In fact, big data handle data heterogeneity which is an additive value for the prediction of drought, it offers a view of the different dimensions such as the spatial distribution, the temporal distribution and the severity detection of this phenomenon. Big data analytic and drought are introduced and reviewed in this paper. Besides, this review includes different studies, researches and applications of big data to drought monitoring. Challenges related to data life cycle such as data challenges, data processing challenges and data infrastructure management challenges are also discussed. Finally, we conclude that big data analytic can be beneficial in drought monitoring but there is a need for statistical and artificial intelligence-based approaches.}
}
@article{LV2020101937,
title = {Achieving secure big data collection based on trust evaluation and true data discovery},
journal = {Computers & Security},
volume = {96},
pages = {101937},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101937},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820302133},
author = {Denglong Lv and Shibing Zhu},
keywords = {Big data collection, Trust evaluation, Trust model, True data discovery, Wireless sensor network},
abstract = {Data collection is an important process in the life cycle of big data processing. It is the key part that must be completed first in all kinds of data applications, which determines the results of data analysis and application service quality. However, untrusted data sources and transmission links expose the data collection process to attacks and malicious threats such as counterfeiting, replay, and denial of service, and ultimately lead to untrustworthy data. In order to cope with the threat of data collection process and ensure data quality, this paper proposes trust evaluation scheme for data security collection based on wireless sensor network, one of the data collection applications, including direct trust, recommendation trust, link trust, and backhaul trust. Meanwhile, in order to realize the dynamic update of the trust of the data sources, a true data discovery and trust dynamic update mechanism based on ω-FCM (Weight Fuzzy C-Mean) algorithm is proposed. The results of a large number of simulation experiments show that the proposed scheme, model and algorithm can effectively evaluate the trust of data sources and ensure the authenticity of the collected data.}
}
@article{MOHARM2019100945,
title = {State of the art in big data applications in microgrid: A review},
journal = {Advanced Engineering Informatics},
volume = {42},
pages = {100945},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100945},
url = {https://www.sciencedirect.com/science/article/pii/S147403461830702X},
author = {Karim Moharm},
keywords = {Big data, Microgrid},
abstract = {The prospering Big data era is emerging in the power grid. Multiple world-wide studies are emphasizing the big data applications in the microgrid due to the huge amount of produced data. Big data analytics can impact the design and applications towards safer, better, more profitable, and effective power grid. This paper presents the recognition and challenges of the big data and the microgrid. The construction of big data analytics is introduced. The data sources, big data opportunities, and enhancement areas in the microgrid like stability improvement, asset management, renewable energy prediction, and decision-making support are summarized. Diverse case studies are presented including different planning, operation control, decision making, load forecasting, data attacks detection, and maintenance aspects of the microgrid. Finally, the open challenges of big data in the microgrid are discussed.}
}
@article{PAIGE20211467,
title = {A Versatile Big Data Health System for Australia: Driving Improvements in Cardiovascular Health},
journal = {Heart, Lung and Circulation},
volume = {30},
number = {10},
pages = {1467-1476},
year = {2021},
issn = {1443-9506},
doi = {https://doi.org/10.1016/j.hlc.2021.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S1443950621005175},
author = {Ellie Paige and Kerry Doyle and Louisa Jorm and Emily Banks and Meng-Ping Hsu and Lee Nedkoff and Tom Briffa and Dominique A. Cadilhac and Ray Mahoney and Johan W. Verjans and Girish Dwivedi and Michael Inouye and Gemma A. Figtree},
keywords = {Big data, Datasets, Cardiovascular disease, National platform},
abstract = {Cardiovascular diseases (CVD) are leading causes of death and morbidity in Australia and worldwide. Despite improvements in treatment, there remain large gaps in our understanding to prevent, treat and manage CVD events and associated morbidities. This article lays out a vision for enhancing CVD research in Australia through the development of a Big Data system, bringing together the multitude of rich administrative and health datasets available. The article describes the different types of Big Data available for CVD research in Australia and presents an overview of the potential benefits of a Big Data system for CVD research and some of the major challenges in establishing the system for Australia. The steps for progressing this vision are outlined.}
}
@article{HUGHES2020120300,
title = {Sowing the seeds of value? Persuasive practices and the embedding of big data analytics},
journal = {Technological Forecasting and Social Change},
volume = {161},
pages = {120300},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120300},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520311264},
author = {Jeffrey Hughes and Kirstie Ball},
keywords = {Big data analytics, Persuasion, Practice, Capabilities, Value},
abstract = {This paper draws on data from three organisational case studies and expert interviews to propose that persuasive practices are the precursors and enablers of analytical capability development. A bundle of seven practices was identified and observed to bridge multiple gaps between technical and non-technical colleagues on big data analytics (BDA) projects. The deployment of these practices varied according to the level of BDA maturity and featured a host of socio-material elements. This paper complements existing technical case studies with a fine-grained qualitative account of the managerial and human elements of BDA implementation. Effective deployment of persuasive practices potentially both embeds the benefits and mitigates the risks of BDA, sowing the seeds of many different forms of value.}
}
@article{VANAM2021,
title = {Analysis of twitter data through big data based sentiment analysis approaches},
journal = {Materials Today: Proceedings},
year = {2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.11.486},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320391501},
author = {Harika Vanam and Jeberson {Retna Raj R}},
keywords = {Sentiment analysis, Twitter, Unstructured data analysis, Big data analytics, Machine learning algorithm},
abstract = {The common and various forms of Twitter information render this one of the best controlling and recording virtual environments of information. The growth in social media nowadays gives internet users immense interest. In several pups like prediction, advertisement, sentiment analysis …, the data on such a social network platform is used. People exchange good or bad views on problems, items and administrations through the web and informal communities. The capacity to assess such a data productively is presently observed as a noteworthy upper hand in settling on choices all the more proficiently. In this sense, associations use methods, for example, Sentiment Analysis (SA). The utilization of web based life around the globe is growing, however, greatly speeding up mass data generation and stopping us from providing useful insights in conventional SA systems. These data volumes can be processed effectively, using SA and Big Data technology. Big data is not a luxury, in fact, but an important prediction.}
}
@article{SU2020138984,
title = {Carbon emissions and environmental management based on Big Data and Streaming Data: A bibliometric analysis},
journal = {Science of The Total Environment},
volume = {733},
pages = {138984},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.138984},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720325018},
author = {Yuan Su and Yanni Yu and Ning Zhang},
keywords = {Big data, Streaming data, Carbon emission, Environmental management, Bibliometric analysis, Net-work analysis},
abstract = {Climate change and environmental management are issues of global concern. The advent of the era of Big Data has created a new research platform for the assessment of environmental governance and policies. However, little is known about Big Data application to climate change and environmental management research. This paper adopts bibliometric analysis in conjunction with network analysis to systematically evaluate the publications on carbon emissions and environmental management based on Big Data and Streaming Data using R package and VOSviewer software. The analysis involves 274 articles after rigorous screening and includes citation analysis, co-citation analysis, and co-word analysis. Main findings include (1) Carbon emissions and environmental management based on big data and streaming data is an emerging multidisciplinary research topic, which has been applied in the fields of computer science, supply chain design, transportation, carbon price assessment, environmental policy evaluation, and CO2 emissions reduction. (2) This field has attracted the attention of nations which are major contributors to the world economy. In particular, European and American scholars have made the main contributions to this topic, and Chinese researchers have also had great impact. (3) The research content of this topic is primarily divided into four categories, including empirical studies of specific industries, air pollution governance, technological innovation, and low-carbon transportation. Our findings suggest that future research should bring greater depth of practical and modeling analysis to environmental policy assessment based on Big Data.}
}
@article{HAJLI2020135,
title = {Understanding market agility for new product success with big data analytics},
journal = {Industrial Marketing Management},
volume = {86},
pages = {135-143},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118304735},
author = {Nick Hajli and Mina Tajvidi and Ayantunji Gbadamosi and Waqar Nadeem},
keywords = {Big data analytics, Customer agility, Effective use of data, New product success},
abstract = {The complexity that characterises the dynamic nature of the various environmental factors makes it very compelling for firms to be capable of addressing the changing customers' needs. The current study examines the role of big data in new product success. We develop a qualitative research with case study approach to look at this. Specifically, we look at multiple cases to get in-depth understanding of customer agility for new product success with big data analytics. The findings of the study provide insight into the role of customer agility in new product success. This study unpacks the interconnectedness of the effective use of data aggregation tools, the effectiveness of data analysis tools and customer agility. It also explores the link between all of these factors and new product success. The study is reasonably telling in that it shows that the effective use of data aggregation and data analysis tools results in customer agility which in itself explains how an organisation senses and responds speedily to opportunities for innovation in the competitive marketing environment. The current study provides significant theoretical contributions by providing evidence for the role of big data analytics, big data aggregation tools, customer agility, organisational slack and environmental turbulence in new product success.}
}
@article{NEILSON201935,
title = {Systematic Review of the Literature on Big Data in the Transportation Domain: Concepts and Applications},
journal = {Big Data Research},
volume = {17},
pages = {35-44},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214579617303866},
author = {Alex Neilson and  Indratmo and Ben Daniel and Stevanus Tjandra},
keywords = {Big Data, Smart city, Intelligent transportation system, Connected vehicle, Road traffic safety, Vision Zero},
abstract = {Research in Big Data and analytics offers tremendous opportunities to utilize evidence in making decisions in many application domains. To what extent can the paradigms of Big Data and analytics be used in the domain of transport? This article reports on an outcome of a systematic review of published articles in the last five years that discuss Big Data concepts and applications in the transportation domain. The goal is to explore and understand the current research, opportunities, and challenges relating to the utilization of Big Data and analytics in transportation. The review shows the potential of Big Data and analytics to garner insights and improve transportation systems through the analysis of various forms of data obtained from traffic monitoring systems, connected vehicles, crowdsourcing, and social media. We discuss some platforms and software architecture for the transport domain, along with a wide array of storage, processing, and analytical techniques, and describe challenges associated with the implementation of Big Data and analytics. This review contributes broadly to the various ways in which cities can utilize Big Data in transportation to guide the creation of sustainable and safer traffic systems. Since research in Big Data and transportation is, by and large, at infancy, this article does not prescribe recommendations to the various challenges identified, which also constitutes the limitation of the article.}
}
@article{YADEGARIDEHKORDI2018199,
title = {Influence of big data adoption on manufacturing companies' performance: An integrated DEMATEL-ANFIS approach},
journal = {Technological Forecasting and Social Change},
volume = {137},
pages = {199-210},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.07.043},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518304141},
author = {Elaheh Yadegaridehkordi and Mehdi Hourmand and Mehrbakhsh Nilashi and Liyana Shuib and Ali Ahani and Othman Ibrahim},
keywords = {Big data, Firm performance, Manufacturing companies, DEMATEL, ANFIS},
abstract = {Big Data is one of the recent technological advances with the strong applicability in almost every industry, including manufacturing. However, despite business opportunities offered by this technology, its adoption is still in early stage in many industries. Thus, this study aimed to identify and rank the significant factors influencing adoption of big data and in turn to predict the influence of big data adoption on manufacturing companies' performance using a hybrid approach of decision-making trial and evaluation laboratory (DEMATEL)- adaptive neuro-fuzzy inference systems (ANFIS). This study identified the critical adoption factors from literature review and categorized them into technological, organizational and environmental dimensions. Data was collected from 234 industrial managers who were involved in the decision-making process regarding IT procurement in Malaysian manufacturing companies. Research results showed that technological factors (perceived benefits, complexity, technology resources, big data quality and integration) have the highest influence on the big data adoption and firms' performance. This study is one of the pioneers in using DEMATEL-ANFIS approach in the big data adoption context. In addition to the academic contribution, findings of this study can hopefully assist manufacturing industries, big data service providers, and governments to precisely focus on vital factors found in this study in order to improve firm performance by adopting big data.}
}
@article{TSAI2019306,
title = {Big Data in Cancer Research: Real-World Resources for Precision Oncology to Improve Cancer Care Delivery},
journal = {Seminars in Radiation Oncology},
volume = {29},
number = {4},
pages = {306-310},
year = {2019},
note = {Big Data in Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1053429619300347},
author = {Chiaojung Jillian Tsai and Nadeem Riaz and Scarlett Lin Gomez},
abstract = {In oncology, the term “big data” broadly describes the rapid acquisition and generation of massive amounts of information, typically from population cancer registries, electronic health records, or large-scale genetic sequencing studies. The challenge of using big data in cancer research lies in interdisciplinary collaboration and information processing to unify diverse data sources and provide valid analytics to harness meaningful information. This article provides an overview of how big data approaches can be applied in cancer research, and how they can be used to translate information into new ways to ultimately make informed decisions that improve cancer care and delivery.}
}
@incollection{FENG2021145,
title = {Chapter 10 - Spatiotemporal Big Data-Driven Vessel Traffic Risk Estimation for Promoting Maritime Healthcare: Lessons Learnt from Another Domain than Healthcare},
editor = {Miltiadis D. Lytras and Akila Sarirete and Anna Visvizi and Kwok Tai Chui},
booktitle = {Artificial Intelligence and Big Data Analytics for Smart Healthcare},
publisher = {Academic Press},
pages = {145-160},
year = {2021},
series = {Next Gen Tech Driven Personalized Med&Smart Healthcare},
isbn = {978-0-12-822060-3},
doi = {https://doi.org/10.1016/B978-0-12-822060-3.00006-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220603000061},
author = {Zikun Feng and Yan Li and Zhao Liu and Ryan Wen Liu},
keywords = {Data mining, maritime healthcare, maritime risk estimation, automatic identification system (AIS), maritime safety, artificial intelligence},
abstract = {With the rapid development of maritime industries, the vessel traffic density has been gradually increased leading to increasing the potential risk of ship collision accidents in crowded inland waterways. It will bring negative effects on human life safety and global maritime economy. Therefore it is of vital significance to study the risk of ship collision using big data mining techniques. The big data–driven computational results are beneficial for guaranteeing smart maritime healthcare in the fields of ocean engineering and maritime management. This chapter proposes to quantitatively estimate the ship collision risk based on ship domain modeling and real-time vessel trajectory data. In particular, the trajectory data quality is first improved using the cubic spline interpolation method. We assume that the ship collision risk is highly related to the cross areas of ship domains between different ships, which are then computed using the Monte Carlo simulation strategy. For the sake of better understanding, the kernel density estimation method is finally adopted to visually generate the ship collision risk in maps. Experimental results on realistic spatiotemporal big data have illustrated the effectiveness of the proposed method in crowded inland waterways.}
}
@article{ATITALLAH2020100303,
title = {Leveraging Deep Learning and IoT big data analytics to support the smart cities development: Review and future directions},
journal = {Computer Science Review},
volume = {38},
pages = {100303},
year = {2020},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2020.100303},
url = {https://www.sciencedirect.com/science/article/pii/S1574013720304032},
author = {Safa Ben Atitallah and Maha Driss and Wadii Boulila and Henda Ben Ghézala},
keywords = {Internet of Things, Deep Learning, Smart city, Big data analytics, Review},
abstract = {The rapid growth of urban populations worldwide imposes new challenges on citizens’ daily lives, including environmental pollution, public security, road congestion, etc. New technologies have been developed to manage this rapid growth by developing smarter cities. Integrating the Internet of Things (IoT) in citizens’ lives enables the innovation of new intelligent services and applications that serve sectors around the city, including healthcare, surveillance, agriculture, etc. IoT devices and sensors generate large amounts of data that can be analyzed to gain valuable information and insights that help to enhance citizens’ quality of life. Deep Learning (DL), a new area of Artificial Intelligence (AI), has recently demonstrated the potential for increasing the efficiency and performance of IoT big data analytics. In this survey, we provide a review of the literature regarding the use of IoT and DL to develop smart cities. We begin by defining the IoT and listing the characteristics of IoT-generated big data. Then, we present the different computing infrastructures used for IoT big data analytics, which include cloud, fog, and edge computing. After that, we survey popular DL models and review the recent research that employs both IoT and DL to develop smart applications and services for smart cities. Finally, we outline the current challenges and issues faced during the development of smart city services.}
}
@incollection{WANG202135,
title = {Chapter 2 - Big data in personalized healthcare},
editor = {Ahmed A. Moustafa},
booktitle = {Big Data in Psychiatry #x0026; Neurology},
publisher = {Academic Press},
pages = {35-49},
year = {2021},
isbn = {978-0-12-822884-5},
doi = {https://doi.org/10.1016/B978-0-12-822884-5.00017-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228845000179},
author = {Lidong Wang and Cheryl Alexander},
keywords = {Big data, Precision healthcare, Personalized healthcare, Big Data analytics, Telepsychiatry, Deep learning},
abstract = {Big data technologies enable correlation of multiple data sources into a coherent view. Big data and Big Data analytics have been used in public health, electronic consultation (e-consultation), real-time telediagnosis, precision healthcare, and personalized healthcare. e-Consultation is one aspect of telemedicine related to remote communication between medical specialists and clinicians, or clinicians and patients. It is generally implemented via the Internet or mobile communication devices (e.g., smartphone) and often generates big data. The concepts, characteristics, methods, emerging technologies, and software platforms or tools of big data and Big Data analytics are introduced in this chapter. Big data and applications in general healthcare are presented. Specifically, big data in precision healthcare and personalized healthcare are introduced. Challenges of big data and Big Data analytics in personalized healthcare are also outlined.}
}
@article{BAG2021120420,
title = {Role of institutional pressures and resources in the adoption of big data analytics powered artificial intelligence, sustainable manufacturing practices and circular economy capabilities},
journal = {Technological Forecasting and Social Change},
volume = {163},
pages = {120420},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120420},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520312464},
author = {Surajit Bag and Jan Ham Christiaan Pretorius and Shivam Gupta and Yogesh K. Dwivedi},
keywords = {Big data, Artificial intelligence, Industry 4.0, Circular economy, Sustainable manufacturing},
abstract = {ABSTRACT
The significance of big data analytics-powered artificial intelligence has grown in recent years. The literature indicates that big data analytics-powered artificial intelligence has the ability to enhance supply chain performance, but there is limited research concerning the reasons for which firms engaging in manufacturing activities adopt big data analytics-powered artificial intelligence. To address this gap, our study employs institutional theory and resource-based view theory to elucidate the way in which automotive firms configure tangible resources and workforce skills to drive technological enablement and improve sustainable manufacturing practices and furthermore develop circular economy capabilities. We tested the research hypothesis using primary data collected from 219 automotive and allied manufacturing companies operating in South Africa. The contribution of this work lies in the statistical validation of the theoretical framework, which provides insight regarding the role of institutional pressures on resources and their effects on the adoption of big data analytics-powered artificial intelligence, and how this affects sustainable manufacturing and circular economy capabilities under the moderating effects of organizational flexibility and industry dynamism.}
}
@article{NARAYANAN2020,
title = {A novel system architecture for secure authentication and data sharing in cloud enabled Big Data Environment},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820303700},
author = {Uma Narayanan and Varghese Paul and Shelbi Joseph},
keywords = {Big data outsourcing, Big data sharing, Big data management, SALSA encryption with MapReduce, Fractal index tree, SHA-3},
abstract = {With the rapid growth of data sources, Big data security in Cloud is a big challenge. Different issues have ascended in the area of Big data security such as infrastructure security, data privacy, data management and data integrity. Currently, Big data processing, analytics and storage is secured using cryptography algorithms, which are not appropriate for Big data protection over Cloud. In this paper, we present a solution for addressing the main issues in Big data security over Cloud. We propose a novel system architecture called the Secure Authentication and Data Sharing in Cloud (SADS-Cloud). There are three processes involved in this paper including (i). Big Data Outsourcing, (ii). Big Data Sharing and (iii). Big Data Management. In Big data outsourcing, the data owners are registered to a Trust Center using SHA-3 hashing algorithm. The MapReduce model is used to split the input file into fixed-size of blocks of data and SALSA20 encryption algorithm is applied over each block. In Big data sharing, data users participate in a secure file retrieval. For this purpose, user's credentials (ID, password, secure ID, and current timestamp, email id) are hashed and compared with that stored in a database. In Big data management, there are three important processes implemented to organize data. They are as follows: Compression using Lemperl Ziv Markow Algorithm (LZMA), Clustering using Density-based Clustering of Applications with Noise (DBSCAN), and Indexing using Fractal Index Tree. The proposed scheme for these processes are implemented using Java Programming and performance tested for the following metrics: Information Loss, Compression Ratio, Throughput, Encryption Time and Decryption Time.}
}
@article{LI202149,
title = {The critical need to establish standards for data quality in intelligent medicine},
journal = {Intelligent Medicine},
volume = {1},
number = {2},
pages = {49-50},
year = {2021},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2021.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2667102621000073},
author = {Ruiyang Li and Yahan Yang and Haotian Lin},
keywords = {Artificial intelligence, Intelligent medicine, Big data, Data collection, Data storage, Data management},
abstract = {Medical artificial intelligence (AI) is an important technical asset to support medical supply-side reforms and national development in the big data era. Clinical data from multiple disciplines represent building blocks for the development and application of AI-aided diagnostic and treatment systems based on medical big data. However, the inconsistent quality of these data resources in AI research leads to waste and inefficiencies. Therefore, it is crucial that the field formulates the requirements and content related to data processing as part of the development of intelligent medicine. To promote medical AI research worldwide, the “Belt and Road” International Ophthalmic Artificial Intelligence Research and Development Alliance will establish a series of expert recommendations for data quality in intelligent medicine.}
}
@article{SHAMIM2019103135,
title = {Role of big data management in enhancing big data decision-making capability and quality among Chinese firms: A dynamic capabilities view},
journal = {Information & Management},
volume = {56},
number = {6},
pages = {103135},
year = {2019},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0378720618302854},
author = {Saqib Shamim and Jing Zeng and Syed Muhammad Shariq and Zaheer Khan},
keywords = {Big data management, Dynamic capabilities, Big data decision-making capability, Decision-making quality, China},
abstract = {This study examines the antecedents and influence of big data decision-making capabilities on decision-making quality among Chinese firms. We propose that such capabilities are influenced by big data management challenges such as leadership, talent management, technology, and organisational culture. By using primary data from 108 Chinese firms and utilising partial least squares, we tested the antecedents of big data decision-making capability and its impact on decision-making quality. Findings suggest that big data management challenges are the key antecedents of big data decision-making capability. Furthermore, the latter is vital for big data decision-making quality.}
}
@incollection{CYCHOSZ20221,
title = {Chapter One - Using big data from long-form recordings to study development and optimize societal impact},
editor = {Rick O. Gilmore and Jeffrey J. Lockman},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {62},
pages = {1-36},
year = {2022},
booktitle = {New Methods and Approaches for Studying Child Development},
issn = {0065-2407},
doi = {https://doi.org/10.1016/bs.acdb.2021.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065240721000434},
author = {Margaret Cychosz and Alejandrina Cristia},
keywords = {Big data, Wearable technology, Algorithm bias, Automatic measurement, Language, Audio recording, Children},
abstract = {Big data are everywhere. In this chapter, we focus on one source: long-form, child-centered recordings collected using wearable technologies. Because these recordings are simultaneously unobtrusive and encompassing, they may be a breakthrough technology for clinicians and researchers from several diverse fields. We demonstrate this possibility by outlining three applications for the recordings—clinical treatment, large-scale interventions, and language documentation—where we see the greatest potential. We argue that incorporating these recordings into basic and applied research will result in more equitable treatment of patients, more reliable measurements of the effects of interventions on real-world behavior, and deeper scientific insights with less observational bias. We conclude by outlining a proposal for a semistructured online platform where vast numbers of long-form recordings could be hosted and more representative, less biased algorithms could be trained.}
}
@article{SILVA2020111,
title = {Ion beam analysis and big data: How data science can support next-generation instrumentation},
journal = {Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms},
volume = {478},
pages = {111-115},
year = {2020},
issn = {0168-583X},
doi = {https://doi.org/10.1016/j.nimb.2020.05.027},
url = {https://www.sciencedirect.com/science/article/pii/S0168583X2030272X},
author = {Tiago F. Silva and Cleber L. Rodrigues and Manfredo H. Tabacniks and Hugo D.C. Pereira and Thiago B. Saramela and Renato O. Guimarães},
keywords = {Ion beam analysis, Big data, Data quality assurance, Artificial intelligence},
abstract = {With a growing demand for accurate ion beam analysis on a large number of samples, it becomes an issue of how to ensure the quality standards and consistency over hundreds or thousands of samples. In this sense, a virtual assistant that checks the data quality, emitting certificates of quality, is highly desired. Even the processing of a massive number of spectra is a problem regarding the consistency of the analysis. In this work, we report the design and first results of a virtual layer under implementation in our laboratory. It consists of a series of systems running in the cloud that perform the mentioned tasks and serves as a virtual assistant for member staff and users. We aim to bring the concept of the Internet of Things and artificial intelligence closer to the laboratory to support a new generation of instrumentation.}
}
@article{LIN201949,
title = {Strategic orientations, developmental culture, and big data capability},
journal = {Journal of Business Research},
volume = {105},
pages = {49-60},
year = {2019},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2019.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0148296319304333},
author = {Canchu Lin and Anand Kunnathur},
keywords = {Big data capability, Customer orientation, Entrepreneurial orientation, Technology orientation, And developmental culture},
abstract = {Prior research articulated the importance of developing a big data analytics capability but did not show how to cultivate this development. Drawing on the literature on this topic, this study develops the concept of Big Data capability, which enhances our understanding of Big Data practice beyond that captured in previous literature on the concept of big data analytics capability. This study further highlights the strategic implications of the concept by testing its relationship to three strategic orientations and one aspect of organizational culture. Findings show that customer, entrepreneurial, and technology orientations, and developmental culture are important contributors to the development of Big Data capability.}
}
@article{MA2021107580,
title = {A big data-driven root cause analysis system: Application of Machine Learning in quality problem solving},
journal = {Computers & Industrial Engineering},
volume = {160},
pages = {107580},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107580},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221004848},
author = {Qiuping Ma and Hongyan Li and Anders Thorstenson},
keywords = {Quality management, Data mining, Machine Learning, Multi-class classification, Neural Network},
abstract = {Root cause analysis for quality problem solving is critical to improve product quality performance and reduce the quality risk for manufacturers. Subjective conventional methods have been applied frequently in past decades. However, due to increasingly complex product and supply chain structures, diverse working conditions, and massive amounts of components, accuracy and efficiency of root cause analysis are progressively challenged in practice. Therefore, data-driven root cause analysis methods have attracted attention lately. In this paper, taking advantage of the availability of big operations data and the rapid development of data science, we design a big data-driven root cause analysis system utilizing Machine Learning techniques to improve the performance of root cause analysis. More specifically, we first propose a conceptual framework of the big data-driven root cause analysis system including three modules of Problem Identification, Root Cause Identification, and Permanent Corrective Action. Furthermore, in the Problem Identification Module, we construct a unified feature-based approach to describe multiple and different types of quality problems by applying a data mining method. In the Root Cause Identification Module, we use supervised Machine Learning (classification) methods to automatically predict the root causes of multiple quality problems. Finally, we illustrate the accuracy and efficiency of the proposed system and algorithms based on actual quality data from a case company. This study contributes to the literature from the following aspects: (i) the integrated system and algorithms can be used directly to develop a computer application to manage and solve quality problems with high concurrences and complexities in any manufacturing process; (ii) a general procedure and method are provided to formulate and describe a large quantity and different types of quality problems; (iii) compared with traditional methods, it is demonstrated using real case data that manufacturing companies can save significant time and cost with our proposed data-driven root cause analysis system; (iv) this study not only aims at improving the quality problem solving practices for a complex manufacturing process but also bridges a gap between the theoretical development of Machining Learning methods and their application in the operations management domain.}
}
@article{GARCIAGIL2019135,
title = {Enabling Smart Data: Noise filtering in Big Data classification},
journal = {Information Sciences},
volume = {479},
pages = {135-152},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518309460},
author = {Diego García-Gil and Julián Luengo and Salvador García and Francisco Herrera},
keywords = {Big Data, Smart Data, Classification, Class noise, Label noise.},
abstract = {In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.}
}
@article{LV2020103,
title = {Analysis of healthcare big data},
journal = {Future Generation Computer Systems},
volume = {109},
pages = {103-110},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20304829},
author = {Zhihan Lv and Liang Qiao},
keywords = {Big data, Health care, Privacy security risk, Privacy measures},
abstract = {In order to explore the development of healthcare in China and the privacy and security risk factors in medical data under the background of big data, the development status of China’s healthcare sector is analyzed. The questionnaire is used to analyze the privacy and security risk factors of healthcare big data and protection measures are put forward based on the data privacy and security risk factors in the context of cloud services in the literature. The results show that in recent years, the number of health institutions, the number of medical personnel, the assets of medical institutions, the per capita hospitalization cost, and the insured population all show a trend of increasing year by year; while in 2017, the crude mortality rate of malignant tumor patients was the highest in China, and the mortality rate of rural patients was higher than that of urban patients. The results of the questionnaire show that the probability of data analysis, medical treatment process, disease diagnosis process, lack of protective measures, and imperfect access system is all greater than 0.8 when medical care big data is oriented to cloud services. Based on this, two levels of privacy protection measures are proposed: technology and management. It indicates that medical institutions need to pay attention to data privacy protection and grasp the use of digital medical data to provide decision support for subsequent medical data analysis.}
}
@article{COZZINI2022133422,
title = {Computational methods on food contact chemicals: Big data and in silico screening on nuclear receptors family},
journal = {Chemosphere},
volume = {292},
pages = {133422},
year = {2022},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2021.133422},
url = {https://www.sciencedirect.com/science/article/pii/S0045653521038960},
author = {Pietro Cozzini and Francesca Cavaliere and Giulia Spaggiari and Gianluca Morelli and Marco Riani},
keywords = {Computational chemistry, Consensus prediction, Database, Nuclear receptors, Toxicology},
abstract = {According to Eurostat, the EU production of chemicals hazardous to health reached 211 million tonnes in 2019. Thus, the possibility that some of these chemical compounds interact negatively with the human endocrine system has received, especially in the last decade, considerable attention from the scientific community. It is obvious that given the large number of chemical compounds it is impossible to use in vitro/in vivo tests for identifying all the possible toxic interactions of these chemicals and their metabolites. In addition, the poor availability of highly curated databases from which to retrieve and download the chemical, structure, and regulative information about all food contact chemicals has delayed the application of in silico methods. To overcome these problems, in this study we use robust computational approaches, based on a combination of highly curated databases and molecular docking, in order to screen all food contact chemicals against the nuclear receptor family in a cost and time-effective manner.}
}
@article{GILL202051,
title = {Big Data Everywhere: The Impact of Data Disjunction in the Direct-to-Consumer Testing Model},
journal = {Clinics in Laboratory Medicine},
volume = {40},
number = {1},
pages = {51-59},
year = {2020},
note = {Direct-to-Consumer Testing: The Role of Laboratory Medicine},
issn = {0272-2712},
doi = {https://doi.org/10.1016/j.cll.2019.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0272271219300927},
author = {Emily L. Gill and Stephen R. Master},
keywords = {Big Data, Laboratory medicine, Machine learning, Direct-to-consumer testing, DTC, Harmonization}
}
@article{ARBEX2020102671,
title = {Estimating the influence of crowding and travel time variability on accessibility to jobs in a large public transport network using smart card big data},
journal = {Journal of Transport Geography},
volume = {85},
pages = {102671},
year = {2020},
issn = {0966-6923},
doi = {https://doi.org/10.1016/j.jtrangeo.2020.102671},
url = {https://www.sciencedirect.com/science/article/pii/S0966692319300092},
author = {Renato Arbex and Claudio B. Cunha},
keywords = {Public transport, Accessibility, Smart card data, In-vehicle crowding, Travel time reliability},
abstract = {Accessibility metrics are gaining momentum in public transportation planning and policy-making. However, critical user experience issues such as crowding discomfort and travel time unreliability are still not considered in those accessibility indicators. This paper aims to apply a methodology to build spatiotemporal crowding data and estimate travel time variability in a congested public transport network to improve accessibility calculations. It relies on using multiple big data sources available in most transit systems such as smart card and automatic vehicle location (AVL) data. São Paulo, Brazil, is used as a case study to show the impact of crowding and travel time variability on accessibility to jobs. Our results evidence a population-weighted average reduction of 56.8% in accessibility to jobs in a regular workday morning peak due to crowding discomfort, as well as reductions of 6.2% due to travel time unreliability and 59.2% when both are combined. The findings of this study can be of invaluable help to public transport planners and policymakers, as they show the importance of including both aspects in accessibility indicators for better decision making. Despite some limitations due to data quality and consistency throughout the study period, the proposed approach offers a new way to leverage big data in public transport to enhance policy decisions.}
}
@article{BAIG2019102095,
title = {Big data adoption: State of the art and research challenges},
journal = {Information Processing & Management},
volume = {56},
number = {6},
pages = {102095},
year = {2019},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2019.102095},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319301773},
author = {Maria Ijaz Baig and Liyana Shuib and Elaheh Yadegaridehkordi},
keywords = {Big data adoption, Technology–Organization–Environment, Diffusion of Innovations},
abstract = {Big data adoption is a process through which businesses find innovative ways to enhance productivity and predict risk to satisfy customers need more efficiently. Despite the increase in demand and importance of big data adoption, there is still a lack of comprehensive review and classification of the existing studies in this area. This research aims to gain a comprehensive understanding of the current state-of-the-art by highlighting theoretical models, the influence factors, and the research challenges of big data adoption. By adopting a systematic selection process, twenty studies were identified in the domain of big data adoption and were reviewed in order to extract relevant information that answers a set of research questions. According to the findings, Technology–Organization–Environment and Diffusion of Innovations are the most popular theoretical models used for big data adoption in various domains. This research also revealed forty-two factors in technology, organization, environment, and innovation that have a significant influence on big data adoption. Finally, challenges found in the current research about big data adoption are represented, and future research directions are recommended. This study is helpful for researchers and stakeholders to take initiatives that will alleviate the challenges and facilitate big data adoption in various fields.}
}
@article{ALIC2019243,
title = {BIGSEA: A Big Data analytics platform for public transportation information},
journal = {Future Generation Computer Systems},
volume = {96},
pages = {243-269},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18304448},
author = {Andy S. Alic and Jussara Almeida and Giovanni Aloisio and Nazareno Andrade and Nuno Antunes and Danilo Ardagna and Rosa M. Badia and Tania Basso and Ignacio Blanquer and Tarciso Braz and Andrey Brito and Donatello Elia and Sandro Fiore and Dorgival Guedes and Marco Lattuada and Daniele Lezzi and Matheus Maciel and Wagner Meira and Demetrio Mestre and Regina Moraes and Fabio Morais and Carlos Eduardo Pires and Nádia P. Kozievitch and Walter dos Santos and Paulo Silva and Marco Vieira},
abstract = {Analysis of public transportation data in large cities is a challenging problem. Managing data ingestion, data storage, data quality enhancement, modelling and analysis requires intensive computing and a non-trivial amount of resources. In EUBra-BIGSEA (Europe–Brazil Collaboration of Big Data Scientific Research Through Cloud-Centric Applications) we address such problems in a comprehensive and integrated way. EUBra-BIGSEA provides a platform for building up data analytic workflows on top of elastic cloud services without requiring skills related to either programming or cloud services. The approach combines cloud orchestration, Quality of Service and automatic parallelisation on a platform that includes a toolbox for implementing privacy guarantees and data quality enhancement as well as advanced services for sentiment analysis, traffic jam estimation and trip recommendation based on estimated crowdedness. All developments are available under Open Source licenses (http://github.org/eubr-bigsea, https://hub.docker.com/u/eubrabigsea/).}
}
@article{SUNDARAKANI2021102452,
title = {Big data driven supply chain design and applications for blockchain: An action research using case study approach},
journal = {Omega},
volume = {102},
pages = {102452},
year = {2021},
issn = {0305-0483},
doi = {https://doi.org/10.1016/j.omega.2021.102452},
url = {https://www.sciencedirect.com/science/article/pii/S030504832100061X},
author = {Balan Sundarakani and Aneesh Ajaykumar and Angappa Gunasekaran},
keywords = {Big data architecture, Action research, Case study research, Blockchain adoption, Supply chain management},
abstract = {Blockchain appears to still be nascent in its growth and a relatively untapped asset. This research investigates the need of blockchain in Industry 4.0 environment from Big Data perspective in supply chain management. The research method used in this study involves a combination of an Action Research method and Case Study research. More specifically, the action research method was applied in two industry case studies that implemented and tested the designed architecture in a global logistics environment. Case Study A examined the blockchain application in cross-border cargo movements whereas Case Study B investigated the application in a liquid chemical logistics company serving to petroleum industries. Our research analysis has identified that the Case A subject had disconnected systems and services for blockchain wherein the big data interactions had failed (failure case). Whereas in Case B, the company has achieved nearly 25% increase in revenue through its customer service after the blockchain implementation and thereby reduction in paperwork and carbon emissions (success case). This research contributes to the advancement of the body of knowledge to big data and blockchain by identifying key implementation guideline and issues for blockchain in supply chain management. Further, action-based research coupled with a case study approach has been used to evaluate the application aspects of the architecture's scalability and functionality of bigdata and blockchain in supply chain management.}
}
@article{ELIA2020508,
title = {A multi-dimension framework for value creation through big data},
journal = {Industrial Marketing Management},
volume = {90},
pages = {508-522},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118307600},
author = {Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante},
keywords = {Big data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation},
abstract = {Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.}
}
@article{WANG202110,
title = {An interview with Shouyang Wang: research frontier of big data-driven economic and financial forecasting},
journal = {Data Science and Management},
volume = {1},
number = {1},
pages = {10-12},
year = {2021},
issn = {2666-7649},
doi = {https://doi.org/10.1016/j.dsm.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666764921000011},
author = {Shouyang Wang},
keywords = {Big data, Economic forecasting, Data mining, Spatio-temporal},
abstract = {The development of big data generation, acquisition, storage, processing, and other technologies has greatly enriched our sensory world and fundamentally changed the basis of traditional economic and financial forecasting. Unexpected events in the economic and financial fields challenge our confidence in the performance of forecasting models. Obviously, the big data-driven decision theories and analysis methods are different from the traditional methods. In view of the important role of big data-driven economic and financial forecasting in social stability, innovative development, and sustainability, the research frontiers of big data-driven economic and financial forecasting in the future include: feature mining of complex economic systems with big data representation; accurate real-time correction of theories and methods of dynamic forecasting and early warning; general paradigm of big data forecasting research; formation and process of big data-driven economic and financial system management mechanism, etc. Systematic research on such issues will contribute to the formation of decision-making theories and research systems in the context of big data, thus improving the adaptability and scientificity of management decisions.}
}
@article{VIEIRA2020101985,
title = {On the use of simulation as a Big Data semantic validator for supply chain management},
journal = {Simulation Modelling Practice and Theory},
volume = {98},
pages = {101985},
year = {2020},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2019.101985},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X19301182},
author = {António AC Vieira and Luís MS Dias and Maribel Y Santos and Guilherme AB Pereira and José A Oliveira},
keywords = {Simulation, Big Data, Data issues, Semantic validation, Supply chain management, Industry 4.0},
abstract = {Simulation stands out as an appropriate method for the Supply Chain Management (SCM) field. Nevertheless, to produce accurate simulations of Supply Chains (SCs), several business processes must be considered. Thus, when using real data in these simulation models, Big Data concepts and technologies become necessary, as the involved data sources generate data at increasing volume, velocity and variety, in what is known as a Big Data context. While developing such solution, several data issues were found, with simulation proving to be more efficient than traditional data profiling techniques in identifying them. Thus, this paper proposes the use of simulation as a semantic validator of the data, proposed a classification for such issues and quantified their impact in the volume of data used in the final achieved solution. This paper concluded that, while SC simulations using Big Data concepts and technologies are within the grasp of organizations, their data models still require considerable improvements, in order to produce perfect mimics of their SCs. In fact, it was also found that simulation can help in identifying and bypassing some of these issues.}
}
@article{LOZADA2019e02541,
title = {Big data analytics capability and co-innovation: An empirical study},
journal = {Heliyon},
volume = {5},
number = {10},
pages = {e02541},
year = {2019},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e02541},
url = {https://www.sciencedirect.com/science/article/pii/S2405844019362012},
author = {Nelson Lozada and Jose Arias-Pérez and Geovanny Perdomo-Charry},
keywords = {Business, Economics, Information science, Big data analytics capabilities, Co-innovation, Big data, Co-creation},
abstract = {There are numerous emerging studies addressing big data and its application in different organizational aspects, especially regarding its impact on the business innovation process. This study in particular aims at analyzing the existing relationship between Big Data Analytics Capabilities and Co-innovation. To test the hypothesis model, structural equations by the partial least squares method were used in a sample of 112 Colombian firms. The main findings allow to positively relate Big Data Analytics Capabilities with better and more agile processes of product and service co-creation and with more robust collaboration networks with stakeholders internal and external to the firm.}
}
@article{GHOLIZADEH2020120640,
title = {A robust fuzzy stochastic programming for sustainable procurement and logistics under hybrid uncertainty using big data},
journal = {Journal of Cleaner Production},
volume = {258},
pages = {120640},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.120640},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620306879},
author = {Hadi Gholizadeh and Hamed Fazlollahtabar and Mohammad Khalilzadeh},
keywords = {logistics, Robust fuzzy stochastic programming, Sustainable procurement, Big data, Hybrid uncertainty, ε-Constraint},
abstract = {Today, in many organizations, the debate about the difference in core capabilities has become an important factor for market competition. Companies, based on the field of activity, decide to strengthen some of their capabilities, capacities, and expertise. Therefore, the focus of an organization on the strengths and efforts to develop its sustainability will lead to a competitive advantage in the marketplace. Due to changes in environmental factors, organizations have focused on carbon emissions in procurement and transportation that have the highest carbon footprint. This paper proposes a multi-objective, eco-sustainability model for a supply chain. The objectives are to minimize overall costs, maximize the efficiency of transportation vehicles and minimize information fraud in the process of information sharing within supply chain elements. Big data is considered in the amount of information exchanged between customers and other elements of the proposed supply chain; since there are frauds in information sharing then using big data 5Vs the model is adapted to control the cost of information loss leading to customer dissatisfaction. Since uncertainty is inevitable in the real environments, in this research hybrid uncertainty is considered. Because two sources of uncertainty are considered in most of the parameters, thus it is necessary to robustify the decision-making process. The model is a mixed integer nonlinear program including big data for an optimal sustainable procurement and transportation decision. A heuristic method is used to solve the big data problem that makes use of a robust fuzzy stochastic programming approach. The proposed model can prevent disturbances by using a scenario-based stochastic programming approach. An effective hybrid robust fuzzy stochastic method is also employed for controlling uncertainty in parameters and risk taking out of outbound decisions. To solve the multi-objective model, augmented ε-constraint method is utilized. The model performance is investigated in a comprehensive computational study.}
}
@article{JHA2020113382,
title = {A note on big data analytics capability development in supply chain},
journal = {Decision Support Systems},
volume = {138},
pages = {113382},
year = {2020},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2020.113382},
url = {https://www.sciencedirect.com/science/article/pii/S0167923620301378},
author = {Ashish Kumar Jha and Maher A.N. Agi and Eric W.T. Ngai},
keywords = {Big data, Analytics, Capability development, Qualitative study, Supply chain},
abstract = {Big data analytics (BDA) are gaining importance in all aspects of business management. This is driven by both the presence of large-scale data and management's desire to root decisions in data. Extant research demonstrates that supply chain and operations management functions are among the biggest sources and users of data in the company. Therefore, their decision-making processes would benefit from increased use of BDA technologies. However, there is still a lack of understanding of what determines a company's ability to build BDA capability to gain a competitive advantage. In this study, we attempt to answer this fundamental question by identifying the factors that assist a company in or inhibit it from building its BDA capability and maximizing its gains through BDA technologies. We base our findings on a qualitative analysis of data collected from field visits, interviews with senior management, and secondary resources. We find that, in addition to technical capacity, competitive landscape and intra-firm power dynamics play an important role in building BDA capability and using BDA technologies.}
}
@article{KEZUNOVIC2020106788,
title = {Big data analytics for future electricity grids},
journal = {Electric Power Systems Research},
volume = {189},
pages = {106788},
year = {2020},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2020.106788},
url = {https://www.sciencedirect.com/science/article/pii/S0378779620305915},
author = {Mladen Kezunovic and Pierre Pinson and Zoran Obradovic and Santiago Grijalva and Tao Hong and Ricardo Bessa},
keywords = {Electricity grids, Analytics, Big data, Decision-making},
abstract = {This paper provides a survey of big data analytics applications and associated implementation issues. The emphasis is placed on applications that are novel and have demonstrated value to the industry, as illustrated using field data and practical applications. The paper reflects on the lessons learned from initial implementations, as well as ideas that are yet to be explored. The various data science trends treated in the literature are outlined, while experiences from applying them in the electricity grid setting are emphasized to pave the way for future applications. The paper ends with opportunities and challenges, as well as implementation goals and strategies for achieving impactful outcomes.}
}
@incollection{KOLTAY202249,
title = {Chapter 3 - Data quality, the essential “ingredient”},
editor = {Tibor Koltay},
booktitle = {Research Data Management and Data Literacies},
publisher = {Chandos Publishing},
pages = {49-75},
year = {2022},
series = {Chandos Information Professional Series},
isbn = {978-0-12-824475-3},
doi = {https://doi.org/10.1016/B978-0-12-824475-3.00004-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128244753000047},
author = {Tibor Koltay},
keywords = {Research data quality, Stakeholders, Trust, Intrinsic and extrinsic data quality, Semiotic representation, Time-related dimensions, Data retrievability, Data reuse, Data governance},
abstract = {This chapter acquaints the reader with the general and often changing nature of research on data quality. It is emphasized that research data quality is closely related to business data; however, the goals of scholarly research have become different, especially as the environments shaping the two are different. From among data quality’s attributes, trust receives particular attention. Technical and scientific quality, the relationship of data quality to data reuse, and other quality factors are also examined, including big data quality, intrinsic and extrinsic data quality, and the semiotic representation of quality attributes, as well as their time-related dimensions and retrievability. Although data reuse was addressed in an earlier chapter, its relationship to data quality is touched on in this chapter as well. Sharing the previously mentioned origin with data quality and being closely associated with it, data governance is also portrayed.}
}
@article{CUI2020101861,
title = {Manufacturing big data ecosystem: A systematic literature review},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {62},
pages = {101861},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.101861},
url = {https://www.sciencedirect.com/science/article/pii/S0736584519300559},
author = {Yesheng Cui and Sami Kara and Ka C. Chan},
keywords = {Smart manufacturing, Big data, Cloud computing, Cloud manufacturing, Internet of things, NoSQL},
abstract = {Advanced manufacturing is one of the core national strategies in the US (AMP), Germany (Industry 4.0) and China (Made-in China 2025). The emergence of the concept of Cyber Physical System (CPS) and big data imperatively enable manufacturing to become smarter and more competitive among nations. Many researchers have proposed new solutions with big data enabling tools for manufacturing applications in three directions: product, production and business. Big data has been a fast-changing research area with many new opportunities for applications in manufacturing. This paper presents a systematic literature review of the state-of-the-art of big data in manufacturing. Six key drivers of big data applications in manufacturing have been identified. The key drivers are system integration, data, prediction, sustainability, resource sharing and hardware. Based on the requirements of manufacturing, nine essential components of big data ecosystem are captured. They are data ingestion, storage, computing, analytics, visualization, management, workflow, infrastructure and security. Several research domains are identified that are driven by available capabilities of big data ecosystem. Five future directions of big data applications in manufacturing are presented from modelling and simulation to real-time big data analytics and cybersecurity.}
}
@article{LOPEZMARTINEZ2021263,
title = {A big data-centric architecture metamodel for Industry 4.0},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {263-284},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002156},
author = {Patricia {López Martínez} and Ricardo Dintén and José María Drake and Marta Zorrilla},
keywords = {Data-centric architecture, Data-intensive applications, Model-based development, Industry 4.0, Big data, Metamodel},
abstract = {The effective implementation of Industry 4.0 requires the reformulation of industrial processes in order to achieve the vertical and horizontal digitalization of the value chain. For this purpose, it is necessary to provide tools that enable their successful implementation. This paper therefore proposes a data-centric, distributed, dynamically scalable reference architecture that integrates cutting-edge technologies being aware of the existence of legacy technology typically present in these environments. In order to make its implementation easier, we have designed a metamodel that collects the description of all the elements involved in a digital platform (data, resources, applications and monitoring metrics) as well as the necessary information to configure, deploy and execute applications on it. Likewise, we provide a tool compliant to the metamodel that automates the generation of configuration, deployment and launch files and their corresponding transference and execution in the nodes of the platform. We show the flexibility, extensibility and validity of our software artefacts through their application in two case studies, one addressed to preprocess and store pollution data and the other one, more complex, which simulates the management of an electric power distribution of a smart city.}
}
@article{NORTHCOTT202096,
title = {Big data and prediction: Four case studies},
journal = {Studies in History and Philosophy of Science Part A},
volume = {81},
pages = {96-104},
year = {2020},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2019.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0039368119300652},
author = {Robert Northcott},
keywords = {Big data, Prediction, Case studies, Explanation, Elections, Weather},
abstract = {Has the rise of data-intensive science, or ‘big data’, revolutionized our ability to predict? Does it imply a new priority for prediction over causal understanding, and a diminished role for theory and human experts? I examine four important cases where prediction is desirable: political elections, the weather, GDP, and the results of interventions suggested by economic experiments. These cases suggest caution. Although big data methods are indeed very useful sometimes, in this paper's cases they improve predictions either limitedly or not at all, and their prospects of doing so in the future are limited too.}
}
@article{TRIPATHI20201245,
title = {Big-data driven approaches in materials science: A survey},
journal = {Materials Today: Proceedings},
volume = {26},
pages = {1245-1249},
year = {2020},
note = {10th International Conference of Materials Processing and Characterization},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.02.249},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320310026},
author = {Manwendra K. Tripathi and Randhir Kumar and Rakesh Tripathi},
keywords = {Material science, Big data, Machine learning, Data analytics, Predictive Algorithms},
abstract = {The data volume is growing rapidly in material science. Every year data volume is getting double in many context of material science. The growing rate of data in material science is demanding for new computational infrastructures that can speed-up material discovery and deployment. In this survey, we are focusing on the challenges in material science due to growing data rate, and how Big Data technology can play a major role in research of material science. This survey includes various disciplines that can be used with Big Data to provide better analysis in the material science research.}
}
@article{FAHEEM2021100236,
title = {CBI4.0: A cross-layer approach for big data gathering for active monitoring and maintenance in the manufacturing industry 4.0},
journal = {Journal of Industrial Information Integration},
volume = {24},
pages = {100236},
year = {2021},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100236},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000364},
author = {Muhammad Faheem and Rizwan Aslam Butt and Rashid Ali and Basit Raza and Md. Asri Ngadi and Vehbi Cagri Gungor},
keywords = {Internet of things, Industry 4.0, Big data, Multi-channel communication, Wireless sensor network},
abstract = {Industry 4.0 (I4.0) defines a new paradigm to produce high-quality products at the low cost by reacting quickly and effectively to changing demands in the highly volatile global markets. In Industry 4.0, the adoption of Internet of Things (IoT)-enabled Wireless Sensors (WSs) in the manufacturing processes, such as equipment, machining, assembly, material handling, inspection, etc., generates a huge volume of data known as Industrial Big Data (IBD). However, the reliable and efficient gathering and transmission of this big data from the source sensors to the floor inspection system for the real-time monitoring of unexpected changes in the production and quality control processes is the biggest challenge for Industrial Wireless Sensor Networks (IWSNs). This is because of the harsh nature of the indoor industrial environment that causes high noise, signal fading, multipath effects, heat and electromagnetic interference, which reduces the transmission quality and trigger errors in the IWSNs. Therefore, this paper proposes a novel cross-layer data gathering approach called CBI4.0 for active monitoring and control of manufacturing processes in the Industry 4.0. The key aim of the proposed CBI4.0 scheme is to exploit the multi-channel and multi-radio architecture of the sensor network to guarantee quality of service (QoS) requirements, such as higher data rates, throughput, and low packet loss, corrupted packets, and latency by dynamically switching between different frequency bands in the Multichannel Wireless Sensor Networks (MWSNs). By performing several simulation experiments through EstiNet 9.0 simulator, the performance of the proposed CBI4.0 scheme is compared against existing studies in the automobile Industry 4.0. The experimental outcomes show that the proposed scheme outperforms existing schemes and is suitable for effective control and monitoring of various events in the automobile Industry 4.0.}
}
@incollection{BACHHETY202145,
title = {2 - Big Data Analytics for healthcare: theory and applications},
editor = {Ashish Khanna and Deepak Gupta and Nilanjan Dey},
booktitle = {Applications of Big Data in Healthcare},
publisher = {Academic Press},
pages = {45-67},
year = {2021},
isbn = {978-0-12-820203-6},
doi = {https://doi.org/10.1016/B978-0-12-820203-6.00008-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202036000084},
author = {Shivam Bachhety and Shivani Kapania and Rachna Jain},
keywords = {Big Data, healthcare, Big Data Analytics, Hadoop},
abstract = {In the past 10 years, the healthcare industry is growing at a remarkable rate. The healthcare industry is generating enormous amounts of data in terms of volume, velocity, and variety. Big Data methodologies in healthcare can not only increase the business value but will also add to the improvement of healthcare services. Several techniques can be implemented to develop early disease diagnose systems and improve treatment procedures using detailed analysis over time. In such a situation, Big data Analytics proposes to connect intricate databases to achieve more useful results. In this chapter, we will discuss the procedure of big data analytics in the healthcare sector with some practical applications along with its challenges. We will also have a look at the various big data techniques and their tools for implementation. We conclude this chapter with a discussion on potential opportunities for analytics in the healthcare sector.}
}
@article{MIKALEF2020103361,
title = {The role of information governance in big data analytics driven innovation},
journal = {Information & Management},
volume = {57},
number = {7},
pages = {103361},
year = {2020},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2020.103361},
url = {https://www.sciencedirect.com/science/article/pii/S0378720620302998},
author = {Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie},
keywords = {Big data analytics capabilities, Information governance, Incremental innovation, Radical innovation, Environmental uncertainty, FIMIX-PLS},
abstract = {The age of big data analytics is now here, with companies increasingly investing in big data initiatives to foster innovation and outperform competition. Nevertheless, while researchers and practitioners started to examine the shifts that these technologies entail and their overall business value, it is still unclear whether and under what conditions they drive innovation. To address this gap, this study draws on the resource-based view (RBV) of the firm and information governance theory to explore the interplay between a firm’s big data analytics capabilities (BDACs) and their information governance practices in shaping innovation capabilities. We argue that a firm’s BDAC helps enhance two distinct types of innovative capabilities, incremental and radical capabilities, and that information governance positively moderates this relationship. To examine our research model, we analyzed survey data collected from 175 IT and business managers. Results from partial least squares structural equation modelling analysis reveal that BDACs have a positive and significant effect on both incremental and radical innovative capabilities. Our analysis also highlights the important role of information governance, as it positively moderates the relationship between BDAC’s and a firm’s radical innovative capability, while there is a nonsignificant moderating effect for incremental innovation capabilities. Finally, we examine the effect of environmental uncertainty conditions in our model and find that information governance and BDACs have amplified effects under conditions of high environmental dynamism.}
}
@article{ABOELMAGED2020102234,
title = {Influencing models and determinants in big data analytics research: A bibliometric analysis},
journal = {Information Processing & Management},
volume = {57},
number = {4},
pages = {102234},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102234},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319313366},
author = {Mohamed Aboelmaged and Samar Mouakket},
keywords = {Big data analytics, Technology adoption, Literature review, Bibliometric analysis, Theoretical models, Adoption frameworks},
abstract = {Incorporating big data analytics into a particular context brings various challenges that rest on the model or framework through which individuals or organisations adopt big data to achieve their objectives. Although these models have recently triggered scholars’ attention in various domains, in-depth knowledge of using each of these models in big data research is still blurred. This study enriches our knowledge on emerging models and theories that shape big data analytics adoption (BDAD) research through a bibliometric analysis of 229 studies (143 journal articles and 86 conference papers) published in indexed sources between 2013 and 2019. As a result, twenty models on BDAD have emerged (e.g., “Dynamic Capabilities”, “Resource-Based View”, “Technology Acceptance Model”, “Diffusion of Innovation”, etc.). The analysis reveals that BDAD research to demonstrate attributes suggestive of a topic at an initial stage of development as it is broadly dispersed across different domains employs a wide range of models, some of which overlap. Most of the applied models are generic in nature focusing on variance-based relationships and snapshot prediction with little consensus. There is a conspicuous dearth of process models, firm-level analysis and cultural orientation in contemporary BDAD research. Insights of this bibliometric study could guide rigorous big data research and practice in various contexts. The study concludes with research implications and limitations that offer promising prospects for forthcoming research.}
}
@incollection{MORRA2021841,
title = {Fresh Outlook on Numerical Methods for Geodynamics. Part 2: Big Data, HPC, Education},
editor = {David Alderton and Scott A. Elias},
booktitle = {Encyclopedia of Geology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {841-855},
year = {2021},
isbn = {978-0-08-102909-1},
doi = {https://doi.org/10.1016/B978-0-08-102908-4.00111-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081029084001119},
author = {Gabriele Morra and David A. Yuen and Henry M. Tufo and Matthew G. Knepley},
keywords = {Big Data, High performance computing, Education, Modeling, Geodynamics},
abstract = {Since 2015 much has developed in geodynamical modeling because of the arrival of Big Data. We present in two parts an overview of numerical techniques but also a scan of the new opportunities in this age of Big Data and prepare the community for the coming decade, the roaring twenties, when Data Analytics will reign. We begin with a review of traditional numerical methods (Part I), followed by a survey of the current techniques used for data analytics and high-performance computing (HPC) (Part II). Our aim is to cover topics of machine learning, neural networks and deep learning, unsupervised learning as well as the role that HPC will play in the Big Data era, especially in hardware of various calibers. Finally, we will address the need for education of students and professionals, in particular, on the use of the emerging programming languages and the importance of scientific software communities.}
}
@article{OPREA2021106902,
title = {Insights into demand-side management with big data analytics in electricity consumers’ behaviour},
journal = {Computers & Electrical Engineering},
volume = {89},
pages = {106902},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106902},
url = {https://www.sciencedirect.com/science/article/pii/S0045790620307540},
author = {Simona-Vasilica Oprea and Adela Bâra and Bogdan George Tudorică and Maria Irène Călinoiu and Mihai Alexandru Botezatu},
keywords = {Big data, Machine learning, Smart meters, Electricity consumption, Clustering, Questionnaire analytics},
abstract = {The consumption data from smart meters and complex questionnaires reveals the electricity consumers’ willingness to adapt their lifestyle to reduce or change their behaviour in electricity usage to flatten the peak in electricity consumption and release the stress in the power grid. Thus, the electricity consumption can support the enforcement of tariff and demand response strategies. Although the plethora of complex, unstructured and heterogeneous data is collected from various devices connected to the Internet, smart meters, plugs, sensors and complex questionnaires, there is an undoubted challenge to handle the data flow that does not provide much information as it remains unprocessed. Therefore, in this paper, we propose an innovative methodology that organizes and extracts valuable information from the increasing volume of data, such as data about the electricity consumption measured and recorded at 30 min intervals, as well as data collected from complex questionnaires.}
}
@article{HUANG2021144535,
title = {An updated model-ready emission inventory for Guangdong Province by incorporating big data and mapping onto multiple chemical mechanisms},
journal = {Science of The Total Environment},
volume = {769},
pages = {144535},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.144535},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720380669},
author = {Zhijiong Huang and Zhuangmin Zhong and Qinge Sha and Yuanqian Xu and Zhiwei Zhang and Lili Wu and Yuzheng Wang and Lihang Zhang and Xiaozhen Cui and MingShuang Tang and Bowen Shi and Chuanzeng Zheng and Zhen Li and Mingming Hu and Linlin Bi and Junyu Zheng and Min Yan},
keywords = {Emission inventory, Guangdong Province, Ship emissions, Big data, VOCs speciation},
abstract = {An accurate characterization of spatial-temporal emission patterns and speciation of volatile organic compounds (VOCs) for multiple chemical mechanisms is important to improving the air quality ensemble modeling. In this study, we developed a 2017-based high-resolution (3 km × 3 km) model-ready emission inventory for Guangdong Province (GD) by updating estimation methods, emission factors, activity data, and allocation profiles. In particular, a full-localized speciation profile dataset mapped to five chemical mechanisms was developed to promote the determination of VOC speciation, and two dynamic approaches based on big data were used to improve the estimation of ship emissions and open fire biomass burning (OFBB). Compared with previous emissions, more VOC emissions were classified as oxygenated volatile organic compound (OVOC) species, and their contributions to the total ozone formation potential (OFP) in the Pearl River Delta (PRD) region increased by 17%. Formaldehyde became the largest OFP species in GD, accounting for 11.6% of the total OFP, indicating that the model-ready emission inventory developed in this study is more reactive. The high spatial-temporal variability of ship sources and OFBB, which were previously underestimated, was also captured by using big data. Ship emissions during typhoon days and holidays decreased by 23–55%. 95% of OFBB emissions were concentrated in 9% of the GD area and 31% of the days in 2017, demonstrating their strong spatial-temporal variability. In addition, this study revealed that GD emissions have changed rapidly in recent years due to the leap-forward control measures implemented, and thus, they needed to be updated regularly. All of these updates led to a 5–17% decrease in the emission uncertainty for most pollutants. The results of this study provide a reference for how to reduce uncertainties in developing model-ready emission inventories.}
}
@article{GE2018601,
title = {Big Data for Internet of Things: A Survey},
journal = {Future Generation Computer Systems},
volume = {87},
pages = {601-614},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.04.053},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17316953},
author = {Mouzhi Ge and Hind Bangui and Barbora Buhnova},
keywords = {Big Data, Data analytics, Internet of Things, Healthcare, Energy, Transportation, Building automation, Smart Cities},
abstract = {With the rapid development of the Internet of Things (IoT), Big Data technologies have emerged as a critical data analytics tool to bring the knowledge within IoT infrastructures to better meet the purpose of the IoT systems and support critical decision making. Although the topic of Big Data analytics itself is extensively researched, the disparity between IoT domains (such as healthcare, energy, transportation and others) has isolated the evolution of Big Data approaches in each IoT domain. Thus, the mutual understanding across IoT domains can possibly advance the evolution of Big Data research in IoT. In this work, we therefore conduct a survey on Big Data technologies in different IoT domains to facilitate and stimulate knowledge sharing across the IoT domains. Based on our review, this paper discusses the similarities and differences among Big Data technologies used in different IoT domains, suggests how certain Big Data technology used in one IoT domain can be re-used in another IoT domain, and develops a conceptual framework to outline the critical Big Data technologies across all the reviewed IoT domains.}
}
@article{RAHUL2020364,
title = {Data Life Cycle Management in Big Data Analytics},
journal = {Procedia Computer Science},
volume = {173},
pages = {364-371},
year = {2020},
note = {International Conference on Smart Sustainable Intelligent Computing and Applications under ICITETM2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.06.042},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920315465},
author = {Kumar Rahul and Rohitash Kumar Banyal},
keywords = {Data life cycle, Data creation, Data usability, Healthcare, Big data},
abstract = {Data life cycle management is very much useful for any enterprise or application where data is being used and processed for producing results. Data’s appearance for a certain period time ensures accessibility and usability in the system. Data generated through different sources and it is available in various forms for accessibility. A big data-based application such as the healthcare sector generates lots of data through sensors and other electronic devices which can be further classified into a model for report generations and predictions for various purposes for the benefits of patients and hospitals as well. The data life cycle presents the entire data process in the system. The lifecycle of data starts from creation, store, usability, sharing, and archive and destroy in the system and applications. It defines the data flow in an organization. For the successful implementation of the model, there is a need to maintain the life cycle of data under a secured system of data management. This paper deals with the data life cycle with different steps and various works are done for data management in different sectors and benefits of the data life cycle for industrial and healthcare applications including challenges, conclusions, and future scope.}
}
@article{WANG2020120175,
title = {Tension in big data using machine learning: Analysis and applications},
journal = {Technological Forecasting and Social Change},
volume = {158},
pages = {120175},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120175},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520310015},
author = {Huamao Wang and Yumei Yao and Said Salhi},
keywords = {Big data, Machine learning, Data size, Prediction accuracy, Social media},
abstract = {The access of machine learning techniques in popular programming languages and the exponentially expanding big data from social media, news, surveys, and markets provide exciting challenges and invaluable opportunities for organizations and individuals to explore implicit information for decision making. Nevertheless, the users of machine learning usually find that these sophisticated techniques could incur a high level of tensions caused by the selection of the appropriate size of the training data set among other factors. In this paper, we provide a systematic way of resolving such tensions by examining practical examples of predicting popularity and sentiment of posts on Twitter and Facebook, blogs on Mashable, news on Google and Yahoo, the US house survey, and Bitcoin prices. Interesting results show that for the case of big data, using around 20% of the full sample often leads to a better prediction accuracy than opting for the full sample. Our conclusion is found to be consistent across a series of experiments. The managerial implication is that using more is not necessarily the best and users need to be cautious about such an important sensitivity as the simplistic approach may easily lead to inferior solutions with potentially detrimental consequences.}
}
@article{RAUT2021102170,
title = {Big Data Analytics as a mediator in Lean, Agile, Resilient, and Green (LARG) practices effects on sustainable supply chains},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {145},
pages = {102170},
year = {2021},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2020.102170},
url = {https://www.sciencedirect.com/science/article/pii/S1366554520308139},
author = {Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Manoj Dora and Mengqi Liu},
keywords = {Big data analytics, Manufacturing firms, Supply chain and logistics management, LARG, Business performance and innovation, Sustainability},
abstract = {The effect of big data on the lean, agile, resilient, and green (LARG) supply chain has not been explored much in the literature. This study investigates the role of ‘Big Data Analytics’ (BDA) as a mediator between ‘sustainable supply chain business performance’ and key factors, namely, lean practices, social practices, environmental practices, organisational practices, supply chain practices, financial practices, and total quality management. A sample of 297 responses from thirty-seven Indian manufacturing firms was collected. The paper is beneficial for managers and practitioners to understand supply chain analytics, and it addresses challenges in the management of LARG practices to contribute to a sustainable supply chain.}
}
@article{YILDIRIM2021114840,
title = {Big data analytics for default prediction using graph theory},
journal = {Expert Systems with Applications},
volume = {176},
pages = {114840},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114840},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002815},
author = {Mustafa Yıldırım and Feyza Yıldırım Okay and Suat Özdemir},
keywords = {Big data analytics, Graph theory, Machine learning, Default prediction, SHAP value},
abstract = {With the unprecedented increase in data all over the world, financial sector such as companies and industries try to remain competitive by transforming themselves into data-driven organizations. By analyzing a huge amount of financial data, companies are able to obtain valuable information to determine their strategic plans such as risk control, crisis management, or growth management. However, as the amount of data increase dramatically, traditional data analytic platforms confront with storing, managing, and analyzing difficulties. Emerging Big Data Analytics (BDA) overcome these problems by providing decentralized and distributed processing. In this study, we propose two new models for default prediction. In the first model, called DPModel-1, statistical (logistic regression), and machine learning methods (decision tree, random forest, gradient boosting) are employed to predict company default. Derived from the first model, we propose DPModel-2 based on graph theory. DPModel-2 also comprises new variables obtained from the trading interactions of companies. In both models, grid search optimization and SHapley Additive exPlanations (SHAP) value are utilized in order to determine the best hyperparameters and make the models interpretable, respectively. By leveraging balance sheet, credit, and invoice datasets, default prediction is realized for about one million companies in Turkey between the years 2010–2018. The default rates of companies range between 3%-6% by year. The experimental results are conducted on a BDA platform. According to the DPModel-1 results, the highest AUC score is ensured by random forest with 0.87. In addition, the results are improved for each technique separately by adjusting new variables with graph theory. According to DPModel-2 results, the best AUC score is achieved by random forest with 0.89.}
}
@article{MATHIS2020582,
title = {Making Sense of Big Data to Improve Perioperative Care: Learning Health Systems and the Multicenter Perioperative Outcomes Group},
journal = {Journal of Cardiothoracic and Vascular Anesthesia},
volume = {34},
number = {3},
pages = {582-585},
year = {2020},
issn = {1053-0770},
doi = {https://doi.org/10.1053/j.jvca.2019.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S1053077019311590},
author = {Michael R. Mathis and Timur Z. Dubovoy and Matthew D. Caldwell and Milo C. Engoren}
}
@article{RAKIPI2021100357,
title = {Correlates of the internal audit function’s use of data analytics in the big data era: Global evidence},
journal = {Journal of International Accounting, Auditing and Taxation},
volume = {42},
pages = {100357},
year = {2021},
issn = {1061-9518},
doi = {https://doi.org/10.1016/j.intaccaudtax.2020.100357},
url = {https://www.sciencedirect.com/science/article/pii/S1061951820300586},
author = {Romina Rakipi and Federica {De Santis} and Giuseppe D'Onza},
keywords = {Internal audit, Data analytics, Big data, Soft skills, Fraud detection, IT audit},
abstract = {In the big data era, internal audit functions (IAFs) should innovate their techniques so as to add value to their organizations. The use of data analytics (DA) increases IAFs’ ability to extract value from big data, helping IAFs to enhance their activities’ efficiency and effectiveness. We use responses from 1,681 Chief Audit Executives (CAEs) in 82 countries to investigate the correlates of IAFs’ DA usage. From the literature, we identify five main variables expected to be associated with IAFs’ DA use. We find a positive and significant association between DA use and (i) the IAF reporting to the audit committee (AC) and (ii) CAEs’ ability to build positive relationships with managers. These findings suggest that IAF independence and CAEs’ soft skills are important to innovate IAF techniques favoring DA use. We also find a positive association between DA use and IAFs’ involvement in the assurance of enterprise risk management, fraud detection, and IT risk audit activities. Our findings contribute to the internal auditing and DA literatures, and should be of interest to CAEs, ACs, corporate boards, and professional associations.}
}
@article{ZHANG2020483,
title = {Linking big data analytical intelligence to customer relationship management performance},
journal = {Industrial Marketing Management},
volume = {91},
pages = {483-494},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2020.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0019850120308762},
author = {Chubing Zhang and Xinchun Wang and Annie Peng Cui and Shenghao Han},
keywords = {Big data, Data-driven culture, Competitive pressures, Mass customization, Marketing capability, CRM performance},
abstract = {This study investigates the driving forces of a firm's assimilation of big data analytical intelligence (BDAI) and how the assimilation of BDAI improve customer relationship management (CRM) performance. Drawing on the resource-based view, this study argues that a firm's data-driven culture and the competitive pressure it faces in the industry motivate a firm's assimilation of BDAI. As a firm resource, BDAI enables an organization to develop superior mass-customization capability, which in turn positively influences its CRM performance. In addition, this study proposes that a firm's marketing capability can moderate the impact of BDAI assimilation on its mass-customization capability. Using survey data collected from 147 business-to-business companies, this study finds support for most of the hypotheses. The findings of this study uncover compelling insights about the dynamics involved in the process of using BDAI to improve CRM performance.}
}
@article{GAO2020668,
title = {Big data analytics for smart factories of the future},
journal = {CIRP Annals},
volume = {69},
number = {2},
pages = {668-692},
year = {2020},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0007850620301359},
author = {Robert X. Gao and Lihui Wang and Moneer Helu and Roberto Teti},
keywords = {Digital manufacturing system, Information, Learning},
abstract = {Continued advancement of sensors has led to an ever-increasing amount of data of various physical nature to be acquired from production lines. As rich information relevant to the machines and processes are embedded within these “big data”, how to effectively and efficiently discover patterns in the big data to enhance productivity and economy has become both a challenge and an opportunity. This paper discusses essential elements of and promising solutions enabled by data science that are critical to processing data of high volume, velocity, variety, and low veracity, towards the creation of added-value in smart factories of the future.}
}
@article{HAUSLADEN2020101721,
title = {Towards a maturity model for big data analytics in airline network planning},
journal = {Journal of Air Transport Management},
volume = {82},
pages = {101721},
year = {2020},
issn = {0969-6997},
doi = {https://doi.org/10.1016/j.jairtraman.2019.101721},
url = {https://www.sciencedirect.com/science/article/pii/S0969699718304988},
author = {Iris Hausladen and Maximilian Schosser},
keywords = {Maturity model, Network planning, Big data analytics, Airlines, Case study},
abstract = {The evaluation, acquisition and use of newly available big data sources has become a major strategic and organizational challenge for airline network planners. We address this challenge by developing a maturity model for big data readiness for airline network planning. The development of the maturity model is grounded in literature, expert interviews and case study research involving nine airlines. Four airline business models are represented, namely full-service carriers, low-cost airlines, scheduled charter airlines and cargo airlines. The maturity model has been well received with seven change requests in the model development phase. The revised version has been evaluated as exhaustive and useful by airline network planners. The self-assessment of airlines revealed low to medium maturity for most domains. Organizational factors show the lowest average maturity, IT architecture the highest. Full-service carriers seem to be more mature than airlines with different business models.}
}
@article{SHAH2020106970,
title = {Feature engineering in big data analytics for IoT-enabled smart manufacturing – Comparison between deep learning and statistical learning},
journal = {Computers & Chemical Engineering},
volume = {141},
pages = {106970},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106970},
url = {https://www.sciencedirect.com/science/article/pii/S0098135420300363},
author = {Devarshi Shah and Jin Wang and Q. Peter He},
keywords = {Internet-of-Things, Smart manufacturing, Big data, Data analytics, Feature engineering, Deep learning, Statistical learning},
abstract = {As IoT-enabled manufacturing is still in its infancy, there are several key research gaps that need to be addressed. These gaps include the understanding of the characteristics of the big data generated from industrial IoT sensors, the challenges they present to process data analytics, as well as the specific opportunities that the IoT big data could bring to advance manufacturing. In this paper, we use an inhouse-developed IoT-enabled manufacturing testbed to study the characteristics of the big data generated from the testbed. Since the quality of the data usually has the most impact on process modeling, data veracity is often the most challenging characteristic of big data. To address that, we explore the role of feature engineering in developing effective machine learning models for predicting key process variables. We compare complex deep learning approaches to a simple statistical learning approach, with different level or extent of feature engineering, to explore their pros and cons for potential industrial IoT-enabled manufacturing applications.}
}
@article{ZHANG2019103231,
title = {Orchestrating big data analytics capability for sustainability: A study of air pollution management in China},
journal = {Information & Management},
pages = {103231},
year = {2019},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.103231},
url = {https://www.sciencedirect.com/science/article/pii/S0378720619302010},
author = {Dan Zhang and Shan L. Pan and Jiaxin Yu and Wenyuan Liu},
keywords = {Big data, Big data analytics, Sustainability, Air pollution, Resource orchestration},
abstract = {Under rapid urbanization, cities are facing many societal challenges that impede sustainability. Big data analytics (BDA) gives cities unprecedented potential to address these issues. As BDA is still a new concept, there is limited knowledge on how to apply BDA in a sustainability context. Thus, this study investigates a case using BDA for sustainability, adopting the resource orchestration perspective. A process model is generated, which provides novel insights into three aspects: data resource orchestration, BDA capability development, and big data value creation. This study benefits both researchers and practitioners by contributing to theoretical developments as well as by providing practical insights.}
}
@article{LIU2020123646,
title = {Investment decision and coordination of green agri-food supply chain considering information service based on blockchain and big data},
journal = {Journal of Cleaner Production},
volume = {277},
pages = {123646},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123646},
url = {https://www.sciencedirect.com/science/article/pii/S095965262033691X},
author = {Pan Liu and Yue Long and Hai-Cao Song and Yan-Dong He},
keywords = {Big data, Blockchain, Agri-food supply chain, Investment decision, Coordination},
abstract = {Researches about the fusion application of Big Data and blockchain have appeared for a long time, many information service providers have launched information service business based on Big Data and blockchain (hereafter, ISBD). However, in the green agri-food area, the ISBD application does not popularized. A vital reason is that many decision makers do not know how to make an optimal investment decision and coordinate chain members after adopting ISBD. The core of this problem is to study the issue of investment decision and coordination in a green agri-food supply chain. To solve this problem, firstly, combining with the status of Chinese agricultural development, we proposed a more suitable supply chain structure in the fusion application environment of Big Data and blockchain. Then, we chose a green agri-food supply chain with one producer and one retailer as research object and revised the demand function. Afterwards, considering the changes of agri-food freshness and greenness, we built and analysed the benefit models of producer and retailer before and after using ISBD, and then a cost-sharing and revenue-sharing contract was put forward to coordinate the supply chain. Findings: 1) When the total investment cost payed by producer and retailer is in a certain range, using ISBD will help chain members gain more benefits. 2) If chain members want to gain more benefits after using ISBD, they should try their best to optimize costs by extracting valuable information. Results can offer a theoretical guidance for producer and retailer in investing in ISBD, pricing decision and supply chain coordination after applying ISBD.}
}
@article{REN2021128154,
title = {Research on big data analysis model of multi energy power generation considering pollutant emission—Empirical analysis from Shanxi Province},
journal = {Journal of Cleaner Production},
volume = {316},
pages = {128154},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128154},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621023726},
author = {Dongfang Ren and Xiaopeng Guo and Cunbin Li},
keywords = {Multi-energy power generation, Pollutant emission, Big data analysis, Renewable energy generation, Thermal power},
abstract = {With the development of the integrated energy Internet, energy structure optimization and emission reduction have led to higher requirements for developing various energy sources to enable coordinated and sustainable development. However, data-mining methods are rarely used to study the coordination of multi-energy generation in published research results. In this study, from the perspective of power industry emissions, coordinated generation of various energy sources, and balance of power generation and consumption, a data-mining algorithm was used to analyze the development of thermal power, hydropower, wind power, waste heat, gas, and other power sources. The chi-square automatic interaction detection tree (CHAID), logistic regression, and two-step clustering methods were applied. The results show that: a) CO2 and SO2 emissions were mainly affected by thermal power generation, whereas NOx emissions were jointly affected by thermal power, garbage power, and gas-fired power, and the emissions of various pollutants increased with an increase in power consumption. The optimal power-generation scheme under minimum emission can be obtained. b) There was a strong correlation between thermal power generation and residential electricity consumption, and renewable energy (wind energy, photovoltaic, hydropower) exhibited the highest correlation with the electricity consumption of the tertiary industry, which indicates that renewable energy generation can be promoted by managing electricity consumption in the tertiary industry. c) When the electricity demand of all users was small, the proportion of renewable energy power generation increased; in contrast, the thermal power generation was larger. This indicates the importance of improving the sustainable and stable power supply of renewable energy. This study provides a data analysis model for the coordinated development of multiple energies, which will contribute to the decision-making basis for controlling power emissions, improving the utilization rate of renewable energy, and optimizing the energy structure.}
}
@article{MAJEED2021102026,
title = {A big data-driven framework for sustainable and smart additive manufacturing},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {67},
pages = {102026},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2020.102026},
url = {https://www.sciencedirect.com/science/article/pii/S0736584520302374},
author = {Arfan Majeed and Yingfeng Zhang and Shan Ren and Jingxiang Lv and Tao Peng and Saad Waqar and Enhuai Yin},
keywords = {Big data, Additive manufacturing, Sustainable manufacturing, Smart manufacturing, Optimization},
abstract = {From the last decade, additive manufacturing (AM) has been evolving speedily and has revealed the great potential for energy-saving and cleaner environmental production due to a reduction in material and resource consumption and other tooling requirements. In this modern era, with the advancements in manufacturing technologies, academia and industry have been given more interest in smart manufacturing for taking benefits for making their production more sustainable and effective. In the present study, the significant techniques of smart manufacturing, sustainable manufacturing, and additive manufacturing are combined to make a unified term of sustainable and smart additive manufacturing (SSAM). The paper aims to develop framework by combining big data analytics, additive manufacturing, and sustainable smart manufacturing technologies which is beneficial to the additive manufacturing enterprises. So, a framework of big data-driven sustainable and smart additive manufacturing (BD-SSAM) is proposed which helped AM industry leaders to make better decisions for the beginning of life (BOL) stage of product life cycle. Finally, an application scenario of the additive manufacturing industry was presented to demonstrate the proposed framework. The proposed framework is implemented on the BOL stage of product lifecycle due to limitation of available resources and for fabrication of AlSi10Mg alloy components by using selective laser melting (SLM) technique of AM. The results indicate that energy consumption and quality of the product are adequately controlled which is helpful for smart sustainable manufacturing, emission reduction, and cleaner production.}
}
@article{XIANG2020106538,
title = {Dynamic game strategies of a two-stage remanufacturing closed-loop supply chain considering Big Data marketing, technological innovation and overconfidence},
journal = {Computers & Industrial Engineering},
volume = {145},
pages = {106538},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106538},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220302722},
author = {Zehua Xiang and Minli Xu},
keywords = {Supply chain management, Big Data marketing, Technological innovation, Closed-loop supply chain, Overconfidence},
abstract = {In the “Internet+” era, involving third-party Internet recycling platforms (IRPs) has revolutionized the operation models of closed-loop supply chains (CLSCs) in China. This study explores the impact of technological innovation, Big Data marketing and overconfidence on supply chain member decision-making. We propose a two-stage remanufacturing CLSC dynamic model consisting of a manufacturer, an IRP, and a supplier based on differential game theory. By comparing the optimal decisions of each member in three scenarios, we find that the IRP’s overconfident behavior is beneficial to both the manufacturer and the IRP but will damage the supplier's profit. Although a suitable cost-sharing ratio can enable the manufacturer and IRP to achieve a “win–win” situation, an excessive level of confidence will inhibit the incentives of the cost-sharing strategy, negatively affecting the manufacturer's interests. Interestingly, a cost-sharing contract will become inefficient under certain conditions, i.e., highly efficient level of technological innovation, highly efficient Big Data marketing, and a high level of overconfidence, negatively affecting the manufacturer’s interests. Additionally, technological innovation efficiency and marketing efficiency will have different effects on the IRP's recycling price. A cost-sharing contract and the IRP’s overconfidence will prompt the IRP to exert more efforts on technological innovation and Big Data marketing and to significantly reduce the manufacturing costs and recycling costs for all members. Notably, although the IRP’s overconfidence and cost-sharing strategies may damage the supplier’s profit, the total profit of the CLSC increases.}
}
@incollection{CHANG202221,
title = {Chapter 2 - Investigation of COVID-19 and scientific analysis big data analytics with the help of machine learning},
editor = {Victor Chang and Mohamed Abdel-Basset and Muthu Ramachandran and Nicolas F. Green and Gary Wills},
booktitle = {Novel AI and Data Science Advancements for Sustainability in the Era of COVID-19},
publisher = {Academic Press},
pages = {21-66},
year = {2022},
isbn = {978-0-323-90054-6},
doi = {https://doi.org/10.1016/B978-0-323-90054-6.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323900546000076},
author = {Victor Chang and Mohamed Aleem Ali and Alamgir Hossain},
keywords = {COVID-19 review, AU methods for COVID-19, Machine learning for COVID-19},
abstract = {This book chapter presents the review of COVID-19 and its status, as well as the scientific Analysis big data analytics with the help of machine learning. We provide in-depth literature review, and provide a summary of the current AI and machine learning methods, which have become increasingly important to provide accurate analyses. Various conceptual diagrams have been used to illustrate how different technologies can contribute to effective analyses for COVID-19. We demonstrate our work from both theoretical contributions and practical implementations.}
}
@article{RUSSELL2022108709,
title = {Physics-informed deep learning for signal compression and reconstruction of big data in industrial condition monitoring},
journal = {Mechanical Systems and Signal Processing},
volume = {168},
pages = {108709},
year = {2022},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2021.108709},
url = {https://www.sciencedirect.com/science/article/pii/S0888327021010293},
author = {Matthew Russell and Peng Wang},
keywords = {Physics-informed deep learning, Prognostics and health management, Data compression, Big data},
abstract = {The onset of the Internet of Things enables machines to be outfitted with always-on sensors that can provide health information to cloud-based monitoring systems for prognostics and health management (PHM), which greatly improves reliability and avoids downtime of machines and processes on the shop floor. On the other hand, real-time monitoring produces large amounts of data, leading to significant challenges for efficient and effective data transmission (from the shop floor to the cloud) and analysis (in the cloud). Restricted by industrial hardware capability, especially Internet bandwidth, most solutions approach data transmission from the perspective of data compression (before transmission, at local computing devices) coupled with data reconstruction (after transmission, in the cloud). However, existing data compression techniques may not adapt to domain-specific characteristics of data, and hence have limitations in addressing high compression ratios where full restoration of signal details is important for revealing machine conditions. This study integrates Deep Convolutional Autoencoders (DCAE) with local structure and physics-informed loss terms that incorporate PHM domain knowledge such as the importance of frequency content for machine fault diagnosis. Furthermore, Fault Division Autoencoder Multiplexing (FDAM) is proposed to mitigate the negative effects of multiple disjoint operating conditions on reconstruction fidelity. The proposed methods are evaluated on two case studies, and autocorrelation-based noise analysis provides insight into the relative performance across machine health and operating conditions. Results indicate that physically-informed DCAE compression outperforms prevalent data compression approaches, such as compressed sensing, Principal Component Analysis (PCA), Discrete Cosine Transform (DCT), and DCAE with a standard loss function. FDAM can further improve the data reconstruction quality for certain machine conditions.}
}
@article{AMANULLAH2020495,
title = {Deep learning and big data technologies for IoT security},
journal = {Computer Communications},
volume = {151},
pages = {495-517},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419315361},
author = {Mohamed Ahzam Amanullah and Riyaz Ahamed Ariyaluran Habeeb and Fariza Hanum Nasaruddin and Abdullah Gani and Ejaz Ahmed and Abdul Salam Mohamed Nainar and Nazihah Md Akim and Muhammad Imran},
keywords = {Deep learning, Big data, IoT security},
abstract = {Technology has become inevitable in human life, especially the growth of Internet of Things (IoT), which enables communication and interaction with various devices. However, IoT has been proven to be vulnerable to security breaches. Therefore, it is necessary to develop fool proof solutions by creating new technologies or combining existing technologies to address the security issues. Deep learning, a branch of machine learning has shown promising results in previous studies for detection of security breaches. Additionally, IoT devices generate large volumes, variety, and veracity of data. Thus, when big data technologies are incorporated, higher performance and better data handling can be achieved. Hence, we have conducted a comprehensive survey on state-of-the-art deep learning, IoT security, and big data technologies. Further, a comparative analysis and the relationship among deep learning, IoT security, and big data technologies have also been discussed. Further, we have derived a thematic taxonomy from the comparative analysis of technical studies of the three aforementioned domains. Finally, we have identified and discussed the challenges in incorporating deep learning for IoT security using big data technologies and have provided directions to future researchers on the IoT security aspects.}
}
@article{SEDLMAYR202081,
title = {Evaluation eines Zukunftsszenarios zur Nutzung von Big-Data-Anwendungen für die Verbesserung der Versorgung von Menschen mit seltenen Erkrankungen},
journal = {Zeitschrift für Evidenz, Fortbildung und Qualität im Gesundheitswesen},
volume = {158-159},
pages = {81-91},
year = {2020},
issn = {1865-9217},
doi = {https://doi.org/10.1016/j.zefq.2020.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1865921720301744},
author = {Brita Sedlmayr and Andreas Knapp and Michéle Kümmel and Franziska Bathelt and Martin Sedlmayr},
keywords = {Evaluation, Akzeptanz, Nutzen, Barrieren, Befragung, Szenario, Big Data, Seltene Erkrankungen, Evaluation, Acceptance, Benefits, Barriers, Survey, Scenario, Big data, Rare diseases},
abstract = {Zusammenfassung
Hintergrund
In Deutschland leben etwa 4 Millionen Menschen mit einer seltenen Erkrankung. Einzelne Studien belegen bereits, dass mit Hilfe von Big Data Verfahren die Diagnostik verbessert und seltene Erkrankungen effektiver erforscht werden können. Deutschlandweit existiert aber bisher kein konkretes, umfassendes Konzept für den Einsatz von Big Data zur Versorgung von Menschen mit seltenen Erkrankungen. Im Rahmen des BMG-geförderten Projekts „BIDA-SE“ wurde ein erstes Szenario entworfen, wie Big-Data-basierte Anwendungen sinnvoll in der Versorgungspraxis von Menschen mit seltenen Erkrankungen einfließen können.
Methode
Ziel der vorliegenden Studie war es, das entwickelte Szenario hinsichtlich der Akzeptanz, des (klinischen) Nutzens, ökonomischer Implikationen sowie Grenzen und Barrieren für dessen mittelfristige Umsetzung zu evaluieren. Für die Bewertung des Szenarios wurde im Zeitraum Oktober-November 2019 eine Online-Befragung innerhalb Deutschlands mit insgesamt N = 9 Ärzt*innen, N = 69 Patient*innen mit seltenen Erkrankungen/Patientenvertreter*innen, N = 14 IT-Expert*innen und N = 21 Versorgungsforscher*innen durchgeführt. Für die Entwicklung des Online-Fragebogens wurden standardisierte, validierte Fragen bereits erprobter Erhebungsinstrumente eingesetzt und eigene Fragen auf Basis einer vorausgegangenen Literaturanalyse konstruiert. Die Auswertung der Befragung erfolgte primär deskriptiv durch eine Analyse von Häufigkeiten, Mittelwerten und Standardabweichungen.
Ergebnisse
Die Ergebnisse zeigen, dass das entwickelte Szenario von allen befragten Gruppen (Ärzt*innen, Patient*innen/Patientenvertreter*innen, IT-Expert*innen und Versorgungsforscher*innen) mehrheitlich Akzeptanz erfährt. Aus Sicht der Ärzt*innen, Patient*innen/Patientenvertreter*innen und Versorgungsforscher*innen hätte das Szenario das Potential, die Diagnosestellung und Therapieeinleitung zu beschleunigen und die sektorenübergreifende Behandlung zu verbessern. Investitionen in die vorgestellte Anwendung würden sich aus Sicht der Ärzt*innen und Versorgungsforscher*innen rentieren. Zur Finanzierung des vorgestellten Szenarios müsste jedoch eine Anpassung der Vergütungssituation erfolgen. Die von allen Gruppen benannten Grenzen und Barrieren für eine mittelfristige Umsetzung des Szenarios lassen sich in sieben Themenfelder mit Handlungsbedarf gruppieren: (1) Finanzierung und Investition, (2) Datenschutz und Datensicherheit, (3) Standards/Datenquellen/Datenqualität, (4) Technologieakzeptanz, (5) Integration in den Arbeitsalltag, (6) Wissen um Verfügbarkeit sowie (7) Gewohnheiten und Präferenzen/Arztrolle.
Diskussion
Mit der vorliegenden Studie wurde ein erstes fachübergreifendes, praxisnahes Szenario unter Nutzung von Big-Data-basierten Anwendungen hinsichtlich Akzeptanz, Nutzen und Grenzen/Barrieren evaluiert und analysiert, inwiefern dieses Szenario zukünftig im Kontext seltener Erkrankungen implementiert werden kann. Das Szenario erfährt von allen befragten Zielgruppen mehrheitlich eine hohe Akzeptanz und wird mehrheitlich als (klinisch) nützlich bewertet, wenngleich noch rechtliche, organisatorische und technische Barrieren für dessen mittelfristige Umsetzung überwunden werden müssen. Die Evaluationsergebnisse tragen dazu bei, Handlungsempfehlungen abzuleiten, um eine mittelfristige Umsetzung des Szenarios zu gewährleiten und den Zugang zu den Zentren für Seltene Erkrankungen zukünftig zu kanalisieren.
Schlussfolgerung
Auf nationaler Ebene wurden zahlreiche Aktivitäten angestoßen, um die Versorgungssituation von Menschen mit seltenen Erkrankungen zu verbessern. Das im Rahmen des Projekts „BIDA-SE“ entwickelte Szenario ergänzt diese Forschungsaktivitäten und verdeutlicht, wie Big-Data-basierte Anwendungen sinnvoll in der Praxis genutzt werden können, um die Diagnosestellung und Therapie von Menschen mit seltenen Erkrankungen nachhaltig verbessern zu können.
Introduction
In Germany there are about 4 million people living with a rare disease. Studies have shown that big data applications can improve diagnosis of and research on rare diseases more effectively. However, no concrete comprehensive concept for the use of big data in the care of people with rare diseases has so far been established in Germany. As part of the project “BIDA-SE”, which is funded by the German Ministry of Health, a first scenario has been designed to show how big data applications can be usefully incorporated into the care of people with rare diseases.
Methods
The aim of the present study was to evaluate this scenario with regard to acceptance, (clinical) benefits, economic aspects, and limitations and barriers to its implementation. To evaluate the scenario, an online survey was conducted in Germany in October/November 2019 amongst a total of N = 9 physicians, N = 69 patients with rare diseases/patient representatives, N = 14 IT experts and N = 21 health care researchers. The online questionnaire consisted of both standardized, validated questions taken from already tested survey instruments and additional questions which were constructed on the basis of a preceding literature analysis. The evaluation of the survey was primarily descriptive, with a calculation of frequencies, mean values and standard deviations.
Results
The results of the evaluation show that the scenario has been accepted by a majority of all groups surveyed (physicians, patients/patient representatives, IT experts and health care researchers). From the point of view of physicians, patients/patient representatives and health care researchers, the scenario has the potential to accelerate the diagnosis and initiation of therapy and to improve cross-sectoral treatment. From the physician’s and health care researcher’s perspective, investments in the application presented in the scenario would be profitable. Financing the scenario would, however, require adjusting the reimbursement situation. The limitations and barriers identified by all groups for a medium-term implementation of the scenario can be grouped into seven thematic areas where action is needed: (1) financing and investment, (2) data protection and data security, (3) standards/data sources/data quality, (4) acceptance of technology, (5) integration into the daily work routine, (6) knowledge about availability as well as (7) habits and preferences/physician's role.
Discussion
With the present study, a first interdisciplinary, practical scenario using big data applications was evaluated with regard to acceptance, benefits and limitations/barriers. The scenario is widely accepted among the majority of all surveyed target groups and is considered (clinically) useful, although legal, organisational and technical barriers still need to be overcome for its medium-term implementation. The evaluation results contribute to the derivation of recommendations for action to ensure the medium-term implementation of the scenario and to channel access to the Centres for Rare Diseases in the future.
Conclusion
Many activities have been initiated at a national level to improve the health care situation of people with rare diseases. The scenario developed in the “BIDA-SE” project complements these research activities and illustrates how big data applications can be usefully implemented into practice to improve the diagnosis and therapy of people with rare diseases in a sustainable way.}
}
@article{BRESCIANI2021102347,
title = {Using big data for co-innovation processes: Mapping the field of data-driven innovation, proposing theoretical developments and providing a research agenda},
journal = {International Journal of Information Management},
volume = {60},
pages = {102347},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102347},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221000402},
author = {Stefano Bresciani and Francesco Ciampi and Francesco Meli and Alberto Ferraris},
keywords = {Big data, Co-innovation, Open innovation, Bibliometric analysis, Literature review},
abstract = {This is the first systematic literature review concerning the interconnections between big data (BD) and co-innovation. It uses BD as a common perspective of analysis as well as a concept aggregating different research streams (open innovation, co-creation and collaborative innovation). The review is based on the results of a bibliographic coupling analysis performed with 51 peer-reviewed papers published before the end of 2019. Three thematic clusters were discovered, which respectively focused on BD as a knowledge creation enabler within co-innovation contexts, BD as a driver of co-innovation processes based on customer engagement, and the impact of BD on co-innovation within service ecosystems. The paper theoretically argues that the use of BD, in addition to enhancing intentional and direct collaborative innovation processes, allows the development of passive and unintentional co-innovation that can be implemented through indirect relationships between the collaborative actors. This study also makes eleven unique research propositions concerning further theoretical developments and managerial implementations in the field of BD-driven co-innovation.}
}
@article{YADEGARIDEHKORDI2020100921,
title = {The impact of big data on firm performance in hotel industry},
journal = {Electronic Commerce Research and Applications},
volume = {40},
pages = {100921},
year = {2020},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2019.100921},
url = {https://www.sciencedirect.com/science/article/pii/S1567422319300985},
author = {Elaheh Yadegaridehkordi and Mehrbakhsh Nilashi and Liyana Shuib and Mohd {Hairul Nizam Bin Md Nasir} and Shahla Asadi and Sarminah Samad and Nor {Fatimah Awang}},
keywords = {Firm performance, Big data, Hotel industry, Fuzzy logic, Structural equation modelling},
abstract = {Big data has increasingly appeared as a frontier of opportunity in enhancing firm performance. However, it still is in early stages of introduction and many enterprises are still un-decisive in its adoption. The aim of this study is to propose a theoretical model based on integration of Human-Organization-Technology fit and Technology-Organization-Environment frameworks to identify the key factors affecting big data adoption and its consequent impact on the firm performance. The significant factors are gained from the literature and the research model is developed. Data was collected from top managers and/or owners of SMEs hotels in Malaysia using online survey questionnaire. Structural Equation Modelling (SEM) is used to assess the developed model and Adaptive Neuro-Fuzzy Inference Systems (ANFIS) technique is used to prioritize adoption factors based on their importance levels. The results showed that relative advantage, management support, IT expertise, and external pressure are the most important factors in the technological, organizational, human, and environmental dimensions. The results further revealed that technology is the most important influential dimension. The outcomes of this study can assist the policy makers, businesses and governments to make well-informed decisions in adopting big data.}
}
@article{MARIANI2020338,
title = {Exploring how consumer goods companies innovate in the digital age: The role of big data analytics companies},
journal = {Journal of Business Research},
volume = {121},
pages = {338-352},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320305956},
author = {Marcello M. Mariani and Samuel {Fosso Wamba}},
keywords = {Big data analytics, Forecasting, Innovation, Online review crowdsourcing, Consumer goods companies, Digital data},
abstract = {The advent and development of digital technologies have brought about a proliferation of online consumer reviews (OCRs), i.e., real-time customers’ evaluations of products, services, and brands. Increasingly, e-commerce platforms are using them to gain insights from customer feedback. Meanwhile, a new generation of big data analytics (BDA) companies are crowdsourcing large volumes of OCRs by means of controlled ad hoc online experiments and advanced machine learning (ML) techniques to forecast demand and determine the market potential for new products in several industries. We illustrate how this process is taking place for consumer goods companies by exploring the case of UK digital BDA company, SoundOut. Based on an in-depth qualitative analysis, we develop the consumer goods company innovation (CGCI) conceptual framework, which illustrates how digital BDA firms help consumer goods companies to test new products before they are launched on the market, and innovate. Theoretical and managerial implications are discussed.}
}
@article{JI2021108267,
title = {Building life-span prediction for life cycle assessment and life cycle cost using machine learning: A big data approach},
journal = {Building and Environment},
volume = {205},
pages = {108267},
year = {2021},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.108267},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321006673},
author = {Sukwon Ji and Bumho Lee and Mun Yong Yi},
keywords = {Building life span, Life cycle cost, Life cycle assessment, Big data, Machine learning, Deep neural network},
abstract = {Life cycle assessment (LCA) and life cycle cost (LCC) are two primary methods used to assess the environmental and economic feasibility of building construction. An estimation of the building's life span is essential to carrying out these methods. However, given the diverse factors that affect the building's life span, it was estimated typically based on its main structural type. However, different buildings have different life spans. Simply assuming that all buildings with the same structural type follow an identical life span can cause serious estimation errors. In this study, we collected 1,812,700 records describing buildings built and demolished in South Korea, analysed the actual life span of each building, and developed a building life-span prediction model using deep-learning and traditional machine learning. The prediction models examined in this study produced root mean square errors of 3.72–4.6 and the coefficients of determination of 0.932–0.955. Among those models, a deep-learning based prediction model was found the most powerful. As anticipated, the conventional method of determining a building's life expectancy using a discrete set of specific factors and associated assumptions of life span did not yield realistic results. This study demonstrates that an application of deep learning to the LCA and LCC of a building is a promising direction, effectively guiding business planning and critical decision making throughout the construction process.}
}
@article{LIOUTAS2019100297,
title = {Key questions on the use of big data in farming: An activity theory approach},
journal = {NJAS - Wageningen Journal of Life Sciences},
volume = {90-91},
pages = {100297},
year = {2019},
issn = {1573-5214},
doi = {https://doi.org/10.1016/j.njas.2019.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1573521418302197},
author = {Evagelos D. Lioutas and Chrysanthi Charatsari and Giuseppe {La Rocca} and Marcello {De Rosa}},
keywords = {Big data, Smart farming, Value, Farmers, Cyber-physical-social systems, Activity theory},
abstract = {Big data represent a pioneering development in the field of agriculture. By producing intuition, intelligence, and insights, these data have the potential to recast conventional process-driven agriculture, plotting the course for a smarter, data-driven farming. However, many open issues about the use of big data in agriculture remain unanswered. In this work, conceptualizing smart agricultural systems as cyber-physical-social systems, and building upon activity theory, we aim at highlighting some key questions that need to be addressed. To our view, big data constitute a tool reciprocally produced by all the actors involved in the agrifood supply chains. The constant flux of this tool and the intricate nature of the interactions among the actors who share it complicate the translation of big data into value. Moreover, farmers’ limited capacity to deal with data complexity, along with their dual role as producers and users of big data, impedes the institutionalization of this tool at the farm level. Although the approach used left us with more questions than answers, we suggest that unraveling the institutional arrangements that govern value co-creation, capturing the motivations of farmers and other actors, and detailing the direct and indirect effects that big data (and the technologies used to generate them) have in farms are important preconditions for setting forth rules that facilitate the extraction and equal exchange of value from big data.}
}
@article{LI2019393,
title = {Functional Neuroimaging in the New Era of Big Data},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {17},
number = {4},
pages = {393-401},
year = {2019},
note = {Big Data in Brain Science},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1672022919301603},
author = {Xiang Li and Ning Guo and Quanzheng Li},
keywords = {Big data, Neuroimaging, Machine learning, Health informatics, fMRI},
abstract = {The field of functional neuroimaging has substantially advanced as a big data science in the past decade, thanks to international collaborative projects and community efforts. Here we conducted a literature review on functional neuroimaging, with focus on three general challenges in big data tasks: data collection and sharing, data infrastructure construction, and data analysis methods. The review covers a wide range of literature types including perspectives, database descriptions, methodology developments, and technical details. We show how each of the challenges was proposed and addressed, and how these solutions formed the three core foundations for the functional neuroimaging as a big data science and helped to build the current data-rich and data-driven community. Furthermore, based on our review of recent literature on the upcoming challenges and opportunities toward future scientific discoveries, we envisioned that the functional neuroimaging community needs to advance from the current foundations to better data integration infrastructure, methodology development toward improved learning capability, and multi-discipline translational research framework for this new era of big data.}
}
@article{VIEIRA2020132,
title = {Bypassing Data Issues of a Supply Chain Simulation Model in a Big Data Context},
journal = {Procedia Manufacturing},
volume = {42},
pages = {132-139},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.033},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920305825},
author = {António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira},
keywords = {Simulation, Supply Chain, Big Data, Data issues, Industry 4.0},
abstract = {Supply Chains (SCs) are complex and dynamic networks, where certain events may cause severe problems. To avoid them, simulation can be used, allowing the uncertainty of these systems to be considered. Furthermore, the data that is generated at increasingly high volumes, velocities and varieties by relevant data sources allow, on one hand, the simulation model to capture all the relevant elements. While developing such solution, due to the inherent use of simulation, several data issues were identified and bypassed, so that the incorporated elements comprise a coherent SC simulation model. Thus, the purpose of this paper is to present the main issues that were faced, and discuss how these were bypassed, while working on a SC simulation model in a Big Data context and using real industrial data from an automotive electronics SC. This paper highlights the role of simulation in this task, since it worked as a semantic validator of the data. Moreover, this paper also presents the results that can be obtained from the developed model.}
}
@article{LUNDBERG2021100244,
title = {Editorial to the Special Issue on Big Data in Industrial and Commercial Applications},
journal = {Big Data Research},
volume = {26},
pages = {100244},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100244},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000617},
author = {Lars Lundberg and Håkan Grahn and Valeria Cardellini and Andreas Polze and Sogand Shirinbab}
}
@article{MCNUTT2019326,
title = {Use of Big Data for Quality Assurance in Radiation Therapy},
journal = {Seminars in Radiation Oncology},
volume = {29},
number = {4},
pages = {326-332},
year = {2019},
note = {Big Data in Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1053429619300384},
author = {Todd R. McNutt and Kevin L. Moore and Binbin Wu and Jean L. Wright},
abstract = {The application of big data to the quality assurance of radiation therapy is multifaceted. Big data can be used to detect anomalies and suboptimal quality metrics through both statistical means and more advanced machine learning and artificial intelligence. The application of these methods to clinical practice is discussed through examples of guideline adherence, contour integrity, treatment delivery mechanics, and treatment plan quality. The ultimate goal is to apply big data methods to direct measures of patient outcomes for care quality. The era of big data and machine learning is maturing and the implementation for quality assurance promises to improve the quality of care for patients.}
}
@article{MERINO2016123,
title = {A Data Quality in Use model for Big Data},
journal = {Future Generation Computer Systems},
volume = {63},
pages = {123-130},
year = {2016},
note = {Modeling and Management for Big Data Analytics and Visualization},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15003817},
author = {Jorge Merino and Ismael Caballero and Bibiano Rivas and Manuel Serrano and Mario Piattini},
keywords = {Data Quality, Big Data, Measurement, Quality-in-Use, Model},
abstract = {Beyond the hype of Big Data, something within business intelligence projects is indeed changing. This is mainly because Big Data is not only about data, but also about a complete conceptual and technological stack including raw and processed data, storage, ways of managing data, processing and analytics. A challenge that becomes even trickier is the management of the quality of the data in Big Data environments. More than ever before the need for assessing the Quality-in-Use gains importance since the real contribution–business value–of data can be only estimated in its context of use. Although there exists different Data Quality models for assessing the quality of regular data, none of them has been adapted to Big Data. To fill this gap, we propose the “3As Data Quality-in-Use model”, which is composed of three Data Quality characteristics for assessing the levels of Data Quality-in-Use in Big Data projects: Contextual Adequacy, Operational Adequacy and Temporal Adequacy. The model can be integrated into any sort of Big Data project, as it is independent of any pre-conditions or technologies. The paper shows the way to use the model with a working example. The model accomplishes every challenge related to Data Quality program aimed for Big Data. The main conclusion is that the model can be used as an appropriate way to obtain the Quality-in-Use levels of the input data of the Big Data analysis, and those levels can be understood as indicators of trustworthiness and soundness of the results of the Big Data analysis.}
}
@article{SHARDT2020104,
title = {Data Quality Assessment for System Identification in the Age of Big Data and Industry 4.0},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {104-113},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.103},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320303591},
author = {Yuri A.W. Shardt and Xu Yang and Kevin Brooks and Andrei Torgashov},
keywords = {data quality assessment, system identification, big data, Industry 4.0, soft sensors},
abstract = {As the amount of data stored from industrial processes increases with the demands of Industry 4.0, there is an increasing interest in finding uses for the stored data. However, before the data can be used its quality must be determined and appropriate regions extracted. Initially, such testing was done manually using graphs or basic rules, such as the value of a variable. With large data sets, such an approach will not work, since the amount of data to tested and the number of potential rules is too large. Therefore, there is a need for automated segmentation of the data set into different components. Such an approach has recently been proposed and tested using various types of industrial data. Although the industrial results are promising, there still remain many unanswered questions including how to handle a priori knowledge, over- or undersegmentation of the data set, and setting the appropriate thresholds for a given application. Solving these problems will provide a robust and reliable method for determining the data quality of a given data set.}
}
@article{LI2021441,
title = {A Big Data and Artificial Intelligence Framework for Smart and Personalized Air Pollution Monitoring and Health Management in Hong Kong},
journal = {Environmental Science & Policy},
volume = {124},
pages = {441-450},
year = {2021},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121001714},
author = {Victor O.K. Li and Jacqueline C.K. Lam and Yang Han and Kenyon Chow},
keywords = {Air Pollution Monitoring, Health Management, Artificial Intelligence, Big Data, PM, Personalization, Smart Behavioural Intervention, Health and Well-being Improvement},
abstract = {All people in the world are entitled to enjoy a clean environment and a good quality of life. With big data and artificial intelligence technologies, it is possible to estimate personalized air pollution exposure and synchronize it with activity, health, quality of life and behavioural data, and provide real-time, personalized and interactive alert and advice to improve the health and well-being of individual citizens. In this paper, we propose an overarching framework outlining five major challenges to personalized air pollution monitoring and health management, and respective methodologies in an integrated interdisciplinary manner. First, urban air quality data is sparse, rendering it difficult to provide timely personalized alert and advice. Second, collected data, especially those involving human inputs such as health perception, are often missing and erroneous. Third, the data collected are heterogeneous, and highly complex, not easily comprehensible to facilitate individual and collective decision-making. Fourth, the causal relationships between personal air pollutants exposure (specifically, PM2.5 and PM1.0 and NO2) and personal health conditions, and health-related quality of life perception, of young asthmatics and young healthy citizens in Hong Kong (HK), are yet to be established. Fifth, whether personalized and smart information and advice provided can induce behavioural change and improve health and quality of life are yet to be determined. To overcome these challenges, our first novelty is to develop an AI and big data framework to estimate and forecast air quality in high temporal-spatial resolution and real-time. Our second novelty includes the deployment of mobile pollution sensor platforms to substantially improve the accuracy of estimated and forecasted air quality data, and the collection of activity, health condition and perception data. Our third novelty is the development of visualization tools and comprehensible indexes, by correlating personal exposure with four types of personal data, to provide timely, personalized pollution, health and travel alerts and advice. Our fourth novelty is determining causal relationship, if any, between personal pollutants, PM1.0 and PM2.5, NO2 exposure and personal health condition, and personal health perception, based on a clinical experiment of 150 young asthmatics and 150 young healthy citizens in HK. Our fifth novelty is an intervention study to determine if smart information, presented via our proposed visualized platform, will induce personal behavioural change. Our novel big data AI-driven approach, when integrated with other analytical approaches, provides an integrated interdisciplinary framework for personalized air pollution monitoring and health management, easily transferrable to and applicable in other domains and countries.}
}
@article{LEE2022,
title = {Quality assurance of integrative big data for medical research within a multihospital system},
journal = {Journal of the Formosan Medical Association},
year = {2022},
issn = {0929-6646},
doi = {https://doi.org/10.1016/j.jfma.2021.12.024},
url = {https://www.sciencedirect.com/science/article/pii/S092966462100591X},
author = {Yi-Chia Lee and Ying-Ting Chao and Pei-Ju Lin and Yen-Yun Yang and Yu-Cih Yang and Cheng-Chieh Chu and Yu-Chun Wang and Chin-Hao Chang and Shu-Lin Chuang and Wei-Chun Chen and Hsing-Jen Sun and Hsin-Cheng Tsou and Cheng-Fu Chou and Wei-Shiung Yang},
keywords = {Big data, Electronic health record, Evidence based healthcare management, Validation study},
abstract = {Background
The need is growing to create medical big data based on the electronic health records collected from different hospitals. Errors for sure occur and how to correct them should be explored.
Methods
Electronic health records of 9,197,817 patients and 53,081,148 visits, totaling about 500 million records for 2006–2016, were transmitted from eight hospitals into an integrated database. We randomly selected 10% of patients, accumulated the primary keys for their tabulated data, and compared the key numbers in the transmitted data with those of the raw data. Errors were identified based on statistical testing and clinical reasoning.
Results
Data were recorded in 1573 tables. Among these, 58 (3.7%) had different key numbers, with the maximum of 16.34/1000. Statistical differences (P < 0.05) were found in 34 (58.6%), of which 15 were caused by changes in diagnostic codes, wrong accounts, or modified orders. For the rest, the differences were related to accumulation of hospital visits over time. In the remaining 24 tables (41.4%) without significant differences, three were revised because of incorrect computer programming or wrong accounts. For the rest, the programming was correct and absolute differences were negligible. The applicability was confirmed using the data of 2,730,883 patients and 15,647,468 patient-visits transmitted during 2017–2018, in which 10 (3.5%) tables were corrected.
Conclusion
Significant magnitude of inconsistent data does exist during the transmission of big data from diverse sources. Systematic validation is essential. Comparing the number of data tabulated using the primary keys allow us to rapidly identify and correct these scattered errors.}
}
@article{CREMIN2022138,
title = {Big data: Historic advances and emerging trends in biomedical research},
journal = {Current Research in Biotechnology},
volume = {4},
pages = {138-151},
year = {2022},
issn = {2590-2628},
doi = {https://doi.org/10.1016/j.crbiot.2022.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S2590262822000090},
author = {Conor John Cremin and Sabyasachi Dash and Xiaofeng Huang},
keywords = {Big data, Big data in biomedicine, Data analytics in biomedical research, Multiomics big data, Biomedical data management, Big data in personalized medicine},
abstract = {Big data is transforming biomedical research by integrating massive amounts of data from laboratory experiments, clinical investigations, healthcare records, and the internet of things. Specifically, the increasing rate at which information is obtained from omics technologies (genomics, epigenomics, transcriptomics, proteomics, metabolomics, and pharmacogenomics) is providing an opportunity for future advances in personalized medicine that are paving the way to improved patient care. The recent advances in omics technologies are profoundly contributing to big data in biomedicine and are anticipated to aid in disease diagnosis and patient care management. Herein, we critically review the major computational techniques, algorithms, and their outcomes that have contributed to recent advances in big data generated from biomedical research in various complex human diseases, such as cancer and infectious diseases. Finally, we discuss trends in the field and the future directions that must be considered to advance the influence of big data on biomedical research and its translation in the healthcare industry.}
}
@article{ZHANG202134,
title = {Big data and human resource management research: An integrative review and new directions for future research},
journal = {Journal of Business Research},
volume = {133},
pages = {34-50},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2021.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0148296321002563},
author = {Yucheng Zhang and Shan Xu and Long Zhang and Mengxi Yang},
keywords = {Human resource management research, Big data, Integrative review, Inductive and deductive paradigms},
abstract = {The lack of sufficient big data-based approaches impedes the development of human resource management (HRM) research and practices. Although scholars have realized the importance of applying a big data approach to HRM research, clear guidance is lacking regarding how to integrate the two. Using a clustering algorithm based on the big data research paradigm, we first conduct a bibliometric review to quantitatively assess and scientifically map the evolution of the current big data HRM literature. Based on this systematic review, we propose a general theoretical framework described as “Inductive (Prediction paradigm: Data mining/Theory building) vs. Deductive (Explanation paradigm: Theory testing)”. In this framework, we discuss potential research questions, their corresponding levels of analysis, relevant methods, data sources and software. We then summarize the general procedures for conducting big data research within HRM research. Finally, we propose a future agenda for applying big data approaches to HRM research and identify five promising HRM research topics at the micro, meso and macro levels along with three challenges and limitations that HRM scholars may face in the era of big data.}
}
@article{KEDDY2022e130,
title = {Using big data and mobile health to manage diarrhoeal disease in children in low-income and middle-income countries: societal barriers and ethical implications},
journal = {The Lancet Infectious Diseases},
volume = {22},
number = {5},
pages = {e130-e142},
year = {2022},
issn = {1473-3099},
doi = {https://doi.org/10.1016/S1473-3099(21)00585-5},
url = {https://www.sciencedirect.com/science/article/pii/S1473309921005855},
author = {Karen H Keddy and Senjuti Saha and Samuel Kariuki and John Bosco Kalule and Farah Naz Qamar and Zoya Haq and Iruka N Okeke},
abstract = {Summary
Diarrhoea is an important cause of morbidity and mortality in children from low-income and middle-income countries (LMICs), despite advances in the management of this condition. Understanding of the causes of diarrhoea in children in LMICs has advanced owing to large multinational studies and big data analytics computing the disease burden, identifying the important variables that have contributed to reducing this burden. The advent of the mobile phone has further enabled the management of childhood diarrhoea by providing both clinical support to health-care workers (such as diagnosis and management) and communicating preventive measures to carers (such as breastfeeding and vaccination reminders) in some settings. There are still challenges in addressing the burden of diarrhoeal diseases, such as incomplete patient information, underrepresented geographical areas, concerns about patient confidentiality, unequal partnerships between study investigators, and the reactive approach to outbreaks. A transparent approach to promote the inclusion of researchers in LMICs could address partnership imbalances. A big data umbrella encompassing cloud-based centralised databases to analyse interlinked human, animal, agricultural, social, and climate data would provide an informative solution to the development of appropriate management protocols in LMICs.}
}
@article{IBRAHIM2021121171,
title = {The convergence of big data and accounting: innovative research opportunities},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121171},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121171},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006041},
author = {Awad Elsayed Awad Ibrahim and Ahmed A. Elamer and Amr Nazieh Ezat},
keywords = {Big data, Analytics, Accounting, Data science, Business intelligence},
abstract = {This study aims to develop accounting standards, curriculums, and research to cope with the rapid development of big data. The study presents several potential convergence points between big data and different accounting techniques and theories. The study discusses how big data can overcome the data limitations of six accounting issues: financial reporting, performance measurement, audit evidence, risk management, corporate budgeting and activity-based techniques. It presents six exciting research questions for future research. Then, the study explains the potential convergence between big data and agency theory, stakeholders theory, and legitimacy theory. This theoretical study develops new convergence points between big data and accounting by reviewing the literature and proposing new ideas and research questions. The conclusion indicates a significant convergence between big data and accounting on the premise that data is the heart of accounting. Big data and advanced analytics have the potential to overcome the data limitations of accounting techniques that require estimations and predictions. A remarkable convergence is argued between big data and three accounting theories. Overall, the study presents helpful insights to members of the accounting and auditing community on the potential of big data.}
}
@article{BRAVE2021,
title = {The perils of working with big data, and a SMALL checklist you can use to recognize them},
journal = {Business Horizons},
year = {2021},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0007681321001178},
author = {Scott A. Brave and R. Andrew Butters and Michael Fogarty},
keywords = {Big data, Data analysis, Economic forecasting, Selection bias, Reporting lags, High-frequency data, Real-time forecasts, Leading indicator},
abstract = {The use of big data to help explain fluctuations in the broader economy and key business performance indicators is now so commonplace that in some instances it has even begun to rival more traditional measures. Big data sources can very often provide advantages when compared with these more traditional data sources, but with these advantages also come potential pitfalls. We lay out a checklist called SMALL that we have developed in order to help interested parties as they navigate the big data minefield. Based on a set of five questions, the SMALL checklist should help users of big data draw justifiable conclusions and avoid making mistakes in matters of interpretation. To demonstrate, we provide several case studies that demonstrate the subtle nuances of several of these new big data sets and show how the problems they face often closely relate to age-old concerns that more traditional data sources are also forced to tackle.}
}
@article{TANG2022,
title = {The National Inpatient Sample: A Primer for Neurosurgical Big Data Research and Systematic Review},
journal = {World Neurosurgery},
year = {2022},
issn = {1878-8750},
doi = {https://doi.org/10.1016/j.wneu.2022.02.113},
url = {https://www.sciencedirect.com/science/article/pii/S1878875022002601},
author = {Oliver Y. Tang and Alisa Pugacheva and Ankush I. Bajaj and Krissia M. {Rivera Perla} and Robert J. Weil and Steven A. Toms},
keywords = {Big data, Disparities, Health care costs, Health policy, Hospital volume, Machine learning, National Inpatient Sample, Nationwide Inpatient Sample, NIS},
abstract = {Objective
The National Inpatient Sample (NIS) (the largest all-payer inpatient database in the United States) is an important instrument for big data analysis of neurosurgical inquiries. However, earlier research has determined that many NIS studies are limited by common methodological pitfalls. In this study, we provide the first primer of NIS methodological procedures in the setting of neurosurgical research and review all reported neurosurgical studies using the NIS.
Methods
We designed a protocol for neurosurgical big data research using the NIS, based on our subject matter expertise, NIS documentation, and input and verification from the Healthcare Cost and Utilization Project. We subsequently used a comprehensive search strategy to identify all neurosurgical studies using the NIS in the PubMed and MEDLINE, Embase, and Web of Science databases from inception to August 2021. Studies underwent qualitative categorization (years of NIS studied, neurosurgical subspecialty, age group, and thematic focus of study objective) and analysis of longitudinal trends.
Results
We identified a canonical, 4-step protocol for NIS analysis: study population selection; defining additional clinical variables; identification and coding of outcomes; and statistical analysis. Methodological nuances discussed include identifying neurosurgery-specific admissions, addressing missing data, calculating additional severity and hospital-specific metrics, coding perioperative complications, and applying survey weights to make nationwide estimates. Inherent database limitations and common pitfalls of NIS studies discussed include lack of disease process–specific variables and data after the index admission, inability to calculate certain hospital-specific variables after 2011, performing state-level analyses, conflating hospitalization charges and costs, and not following proper statistical methodology for performing survey-weighted regression. In a systematic review, we identified 647 neurosurgical studies using the NIS. Although almost 60% of studies were reported after 2015, <10% of studies analyzed NIS data after 2015. The average sample size of studies was 507,352 patients (standard deviation = 2,739,900). Most studies analyzed cranial procedures (58.1%) and adults (68.1%). The most prevalent topic areas analyzed were surgical outcome trends (35.7%) and health policy and economics (17.8%), whereas patient disparities (9.4%) and surgeon or hospital volume (6.6%) were the least studied.
Conclusions
We present a standardized methodology to analyze the NIS, systematically review the state of the NIS neurosurgical literature, suggest potential future directions for neurosurgical big data inquiries, and outline recommendations to improve the design of future neurosurgical data instruments.}
}
@article{HU2021107994,
title = {A blockchain-based trading system for big data},
journal = {Computer Networks},
volume = {191},
pages = {107994},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.107994},
url = {https://www.sciencedirect.com/science/article/pii/S138912862100116X},
author = {Donghui Hu and Yifan Li and Lixuan Pan and Meng Li and Shuli Zheng},
keywords = {Big data trading, Blockchain, Smart contract, Proxy re-encryption, Price negotiation, Value reward},
abstract = {Data are an extremely important asset. Governments around the world encourage big data sharing and trading to promote the big data economy. However, existing data trading platforms are not fully trusted. Such platforms face the problems of a single point of failure (SPOF), opaque transactions, uncontrollability, untraceability, and issues of data privacy. Several blockchain-based big data trading methods have been proposed; however, they do not adequately address the security issues introduced by dishonesty in the data provider and data agent or the fairness of data revenue distribution and price bargaining. In this paper, we propose a blockchain-based decentralized data trading system in which data trading is completed by smart contract-based data matching, price negotiation, and reward assigning. Moreover, the proposed data trading system evaluates the data quality on the basis of three metrics, records the evaluation results in a side-chain, and distributes the data users’ application revenue to the data provider according to the evaluated data quality. We verify the security, usability, and efficiency of the proposed big data trading system.}
}
@article{ARFANUZZAMAN2021100127,
title = {Harnessing artificial intelligence and big data for SDGs and prosperous urban future in South Asia},
journal = {Environmental and Sustainability Indicators},
volume = {11},
pages = {100127},
year = {2021},
issn = {2665-9727},
doi = {https://doi.org/10.1016/j.indic.2021.100127},
url = {https://www.sciencedirect.com/science/article/pii/S2665972721000283},
author = {Md. Arfanuzzaman},
keywords = {Artificial intelligence, Big data, Climate resilience, Data infrastructure, South Asia, SDG, Technological readiness, Urban transformation},
abstract = {Artificial intelligence (AI) and big data solutions are currently being utilized to offer low cost and efficient solutions in solving pressing urban socio-economic and environmental problems globally. The study found big data and AI have the potentiality to solve the common urban problems in South Asia and upsurge the efficiency of urban industries, increase competitiveness and productivity of the human and natural resources, reduce the cost of urban service delivery, and build climate resilience. The study has assessed the current AI and big data initiatives and technologies in mitigating the urban development challenges and their potentiality for scaling up in South Asian cities. The study also examined the latest innovations in AI and big data solutions for SDG monitoring and implementation in South Asia and their implication for transformational change. The study suggested that South Asia can harness the maximum benefit of AI and big data technologies by building big data and associated IT infrastructure, advancing research and innovations with regional cooperation, enhancing technological readiness, and eliminating week enabling conditions.}
}
@article{GHASEMAGHAEI201938,
title = {Can big data improve firm decision quality? The role of data quality and data diagnosticity},
journal = {Decision Support Systems},
volume = {120},
pages = {38-49},
year = {2019},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619300466},
author = {Maryam Ghasemaghaei and Goran Calic},
keywords = {Big data utilization, Data quality, Decision quality, Data diagnosticity},
abstract = {Anecdotal evidence suggests that, despite the large variety of data, the huge volume of generated data, and the fast velocity of obtaining data (i.e., big data), quality of big data is far from perfect. Therefore, many firms defer collecting and integrating big data as they have concerns regarding the impact of utilizing big data on data diagnosticity (i.e., retrieval of valuable information from data) and firm decision making quality. In this study, we use the Organizational Learning Theory and Wang and Strong's data quality framework to explore the impact of processing big data on firm decision quality and the mediating role of data quality (DQ) and data diagnosticity on this relationship. We validate the proposed research model using survey data from 130 firms, obtained from data analysts and IT managers. Results confirm the critical role of DQ in increasing data diagnosticity and improving firm decision quality when processing big data; suggesting important implications for practice and theory. Findings also reveal that while big data utilization positively impacts contextual DQ, accessibility DQ, and representational DQ, interestingly, it negatively impacts intrinsic DQ. Furthermore, findings show that while intrinsic DQ, contextual DQ, and representational DQ significantly increase data diagnosticity, accessibility DQ does not influence it. Most importantly, the findings show that big data utilization does not significantly impact the quality of firm decisions and it is fully mediated through DQ and data diagnosticity. The results of this study contribute to practice by providing important guidelines for managers to improve firm decision quality through the use of big data.}
}
@article{CARRA2020300,
title = {Data-driven ICU management: Using Big Data and algorithms to improve outcomes},
journal = {Journal of Critical Care},
volume = {60},
pages = {300-304},
year = {2020},
issn = {0883-9441},
doi = {https://doi.org/10.1016/j.jcrc.2020.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0883944120306791},
author = {Giorgia Carra and Jorge I.F. Salluh and Fernando José {da Silva Ramos} and Geert Meyfroidt},
keywords = {Big data, Data mining, Machine learning, Predictive modeling, Intensive care unit},
abstract = {The digitalization of the Intensive Care Unit (ICU) led to an increasing amount of clinical data being collected at the bedside. The term “Big Data” can be used to refer to the analysis of these datasets that collect enormous amount of data of different origin and format. Complexity and variety define the value of Big Data. In fact, the retrospective analysis of these datasets allows to generate new knowledge, with consequent potential improvements in the clinical practice. Despite the promising start of Big Data analysis in medical research, which has seen a rising number of peer-reviewed articles, very limited applications have been used in ICU clinical practice. A close future effort should be done to validate the knowledge extracted from clinical Big Data and implement it in the clinic. In this article, we provide an introduction to Big Data in the ICU, from data collection and data analysis, to the main successful examples of prognostic, predictive and classification models based on ICU data. In addition, we focus on the main challenges that these models face to reach the bedside and effectively improve ICU care.}
}
@article{SHAMIM2021106777,
title = {Big data management capabilities in the hospitality sector: Service innovation and customer generated online quality ratings},
journal = {Computers in Human Behavior},
volume = {121},
pages = {106777},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.106777},
url = {https://www.sciencedirect.com/science/article/pii/S074756322100100X},
author = {Saqib Shamim and Yumei Yang and Najam Ul Zia and Mahmood Hussain Shah},
keywords = {Big data management, Dynamic capabilities, Service innovation, Knowledge creation, Customer generated online quality rating, Hospitality},
abstract = {Despite the wide usage of big data in tourism and the hospitality sector, little research has been done to understand the role of organizations’ capability of managing big data in value creation. This study bridges this gap by investigating how big data management capabilities lead to service innovation and high online quality ratings. Instead of treating big data management as a whole, we access big data management capabilities at the strategic and operational level. Using a sample of 202 hotels in Pakistan, we collected the primary data for big data capabilities, knowledge creation and service innovation; the secondary data about quality rating were collected from Booking.com. Structural equation modelling through SmartPLS was used for data analysis. The results indicated that big data management capabilities lead to high online quality ratings through the mediation of knowledge creation and service innovation. We contribute to the current literature by empirically testing how strategic level big data capabilities enable the firm to add value in innovativeness and positive online quality ratings through acquiring, contextualizing, experimenting and applying big data.}
}
@article{LEON2021100253,
title = {Enhancing Precision Medicine: A Big Data-Driven Approach for the Management of Genomic Data},
journal = {Big Data Research},
volume = {26},
pages = {100253},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100253},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000708},
author = {Ana León and Óscar Pastor},
keywords = {Big Data, Genomics, Computer science, Theory and methods},
abstract = {The management of the exponential growth of data that Next Generation Sequencing techniques produce has become a challenge for researchers that are forced to delve into an ocean of complex data in order to extract new insights to unravel the secrets of human diseases. Initially, this can be faced as a Big Data-related problem, but the genomic data have particular and relevant challenges that make them different from other Big Data working domains. Genomic data are much more heterogeneous; they are spread in hundreds of repositories, represented in multiple formats, and have different levels of quality. In addition, getting meaningful conclusions from genomic data requires considering all of the relevant surrounding knowledge that is under continuous evolution. In this scenario, the precise identification of what makes Genome Data Management so different is essential in order to provide effective Big Data-based solutions. Genomic projects require dealing with the technological problems associated with data management, nomenclature standards, and quality issues that only robust Information Systems that use Big Data techniques can provide. The main contribution of this paper is to present a Big Data-driven approach for managing genomic data, that is adapted to the particularities of the domain and to show its applicability to improve genetic diagnoses, which is the core of the development of accurate Precision Medicine.}
}
@article{GU2021132,
title = {SparkDQ: Efficient generic big data quality management on distributed data-parallel computation},
journal = {Journal of Parallel and Distributed Computing},
volume = {156},
pages = {132-147},
year = {2021},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0743731521001246},
author = {Rong Gu and Yang Qi and Tongyu Wu and Zhaokang Wang and Xiaolong Xu and Chunfeng Yuan and Yihua Huang},
keywords = {Parallel data quality algorithms, Distributed system, Data quality management system, Multi-tasks scheduling, Big data},
abstract = {In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.}
}
@article{TALHA2019916,
title = {Big Data: Trade-off between Data Quality and Data Security},
journal = {Procedia Computer Science},
volume = {151},
pages = {916-922},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.127},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305915},
author = {M. TALHA and A. ABOU {EL KALAM} and N. ELMARZOUQI},
keywords = {Big Data, Data Quality, Data Security, Trade-off between Quality, Security},
abstract = {The essence of an information system lies in the data; if it is not of good quality or not sufficiently protected, the consequences will undoubtedly be harmful. Quality and Security are two essential aspects that add value to data and their implementation has become a real need and must be adopted before any data exploitation. Due to the high volume of data, their diversity and their rapid generation, effective implementation of such systems requires well thought out mechanisms and strategies. This paper provides an overview of Data Quality and Data Security in a Big Data context. We want through this paper to highlight the conflicts that may exist during the implementation of data security and data quality management systems. Such a conflict makes the complexity greater and requires new adapted solutions.}
}
@article{HUANG2021101712,
title = {Analytics of location-based big data for smart cities: Opportunities, challenges, and future directions},
journal = {Computers, Environment and Urban Systems},
volume = {90},
pages = {101712},
year = {2021},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101712},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001198},
author = {Haosheng Huang and Xiaobai Angela Yao and Jukka M. Krisp and Bin Jiang},
keywords = {Location-based big data (LocBigData), Smart cities, Data analytics, State-of-the-art review, Research agenda, Geodata},
abstract = {The growing ubiquity of location/activity sensing technologies and location-based services (LBS) has led to a large volume and variety of location-based big data (LocBigData), such as location tracking or sensing data, social media data, and crowdsourced geographic information. The increasing availability of such LocBigData has created unprecedented opportunities for research on urban systems and human environments in general. In this article, we first review the common types of LocBigData: mobile phone network data, GPS data, Location-based social media data, LBS usage/log data, smart card travel data, beacon log data (WiFi or Bluetooth), and camera imagery data. Secondly, we describe the opportunities fueled by LocBigData for the realization of smart cities, mainly via answering questions ranging from “what happened” and “why did it happen” to “what's likely to happen in the future” and “what to do next”. Thirdly, pitfalls of dealing with LocBigData are summarized, such as high volume/velocity/variety; non-random sampling; messy and not clean data; and correlations rather than causal relationships. Finally, we review the state-of-the-art research trends in this field, and conclude the article with a list of open research challenges and a research agenda for LocBigData research to help achieve the vision of smart and sustainable cities.}
}
@article{MOSTEFAOUI2022,
title = {Big data architecture for connected vehicles: Feedback and application examples from an automotive group},
journal = {Future Generation Computer Systems},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001492},
author = {Ahmed Mostefaoui and Mohammed Amine Merzoug and Amir Haroun and Anthony Nassar and François Dessables},
keywords = {Connected vehicles, V2I communication, Automotive big data, Big data architecture, Groupe PSA},
abstract = {Nowadays, using their onboard built-in sensors and communication devices, connected vehicles (CVs) can perform numerous measurements (speed, temperature, fuel consumption, etc.) and transmit them, in a real-time fashion, to dedicated infrastructure, usually via 4G/5G wireless communications. This raises many opportunities to develop new innovative telematics services, including, among others, driver safety, customer experience, location-based services and infotainment. Indeed, it is expected that there will be roughly 2 billion connected cars by the end of 2025 on the world’s roadways, where each of which can produce up to 30 terabytes of data per day. Managing this big automotive data, in real and batch modes, imposes tight constraints on the underlying data management platform. To contribute to this research area, in this paper, we report on a real, in-production automotive big data platform; specifically, the one deployed by Groupe PSA (a French car manufacturer known also as Peugeot-Citroën). In particular, we present the technologies and open-source products used within the different components of this CV platform to gather, store, process, and leverage big automotive data. The proposed architecture is then assessed through realistic experiments, and the obtained results are reported and analyzed. Finally, we also provide examples of deployed automotive applications and reveal the implementation details of one of them (an eco-driving service).}
}
@article{YANG202223,
title = {Big data and reference intervals},
journal = {Clinica Chimica Acta},
volume = {527},
pages = {23-32},
year = {2022},
issn = {0009-8981},
doi = {https://doi.org/10.1016/j.cca.2022.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0009898122000018},
author = {Dan Yang and Zihan Su and Min Zhao},
keywords = {Indirect method, Reference interval, Data pre-processing, Verification, Big data},
abstract = {Although reference intervals (RIs) play an important role in clinical diagnosis, there remain significant differences with respect to race, gender, age and geographic location. Accordingly, the Clinical Laboratory Standards Institute (CLSI) EP28-A3c has recommended that clinical laboratories establish RIs appropriate to their subject population. Unfortunately, the traditional and direct approach to establish RIs relies on the recruitment of a sufficient number of healthy individuals of various age groups, collection and testing of large numbers of specimens and accurate data interpretation. The advent of the big data era has, however, created a unique opportunity to “mine” laboratory information. Unfortunately, this indirect method lacks standardization, consensus support and CLSI guidance. In this review we provide a historical perspective, comprehensively assess data processing and statistical methods, and post-verification analysis to validate this big data approach in establishing laboratory specific RIs.}
}
@article{BINDER2020307,
title = {Big Data Management Using Ontologies for CPQ Solutions},
journal = {Procedia Manufacturing},
volume = {52},
pages = {307-312},
year = {2020},
note = {System-Integrated Intelligence – Intelligent, Flexible and Connected Systems in Products and ProductionProceedings of the 5th International Conference on System-Integrated Intelligence (SysInt 2020), Bremen, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.11.051},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321958},
author = {Alexander Binder and Eva-Maria Iwer and Werner Quint},
keywords = {CPQ, Semantic Technologies, Ontologies, Ontology Matching, Data Quality},
abstract = {In recent years, due to a progressive complexity of handling and processing business data, proper big data management has become a challenge, especially for SMEs that have limited resources for investing in the requested business transformation process. As a solution, we suggest an ontology-based CPQ software approach, where we show how the implementation of semantic technologies and ontologies affects data integration processes. We also propose a method called "ontology-based data matching", which allows the semiautomatic generation of alignments used to formalize the coherence between ontologies. The proposed method will ensure consistency during integration, significantly improving the productivity of enterprises.}
}
@article{RHAHLA2021102896,
title = {Guidelines for GDPR compliance in Big Data systems},
journal = {Journal of Information Security and Applications},
volume = {61},
pages = {102896},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102896},
url = {https://www.sciencedirect.com/science/article/pii/S221421262100123X},
author = {Mouna Rhahla and Sahar Allegue and Takoua Abdellatif},
keywords = {The General Data Protection Regulation (GDPR), Big Data analytics, Privacy, Security},
abstract = {The implementation of the GDPR that aims at protecting European citizens’ privacy is still a real challenge. In particular, in Big Data systems where data are voluminous and heterogeneous, it is hard to track data evolution through its complex life cycle ranging from collection, ingestion, storage and analytics. In this context, from 2016 to 2021 research has been conducted and several security tools designed. However, they are either specific to particular applications or address partially the regulation articles. To identify the covered parts, the missed ones and the necessary metrics for comparing different works, we propose a framework for GDPR compliance. The framework identifies the main components for the regulation implementation by mapping requirements aligned with GDPR’s provisions to IT design requirements. Based on this framework, we compare the main GDPR solutions in the Big Data domain and we propose a guideline for GDPR verification and implementation in Big Data systems.}
}
@article{LI2021241,
title = {AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making},
journal = {Environmental Science & Policy},
volume = {125},
pages = {241-246},
year = {2021},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121002471},
author = {Victor O.K. Li and Jacqueline C.K. Lam and Jiahuan Cui},
abstract = {AI and big data technologies have been increasingly deployed to process complex, heterogeneous, high-resolution environmental data, and generate results at greater speeds and higher accuracies to facilitate environmental decision-making. However, current attempts to develop reliable AI and big data technologies for environmental decision-making are still inadequate. In this special issue, AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making, we attempt to address the following important questions: What are the conditions for AI and big data technologies to facilitate environmental decision-making? How can AI and big data be used to facilitate environmental decision-making? Do AI and big data serve those most at risk of environmental pollution? Who should own and govern AI and big data? This special issue brings together researchers in relevant fields of AI and environmental science to address these pertinent questions. First, we will review the existing works which attempt to address these four questions. Second, we summarize the significance and novelty of six articles included in our special issue in addressing these four questions. Finally, we highlight the important principles of AI for Social Good, which can help distinguish good from bad environmental decisions based on AI and big data technologies.}
}
@article{WILKIN2020100470,
title = {Big data prioritization in SCM decision-making: Its role and performance implications},
journal = {International Journal of Accounting Information Systems},
volume = {38},
pages = {100470},
year = {2020},
note = {2019 UW CISA Symposium},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2020.100470},
url = {https://www.sciencedirect.com/science/article/pii/S1467089520300385},
author = {Carla Wilkin and Aldónio Ferreira and Kristian Rotaru and Luigi Red Gaerlan},
keywords = {Big data, Big data availability, Big data prioritization, Supply chain management, Performance},
abstract = {Given exponential growth in the size of big data, its multi-channel sources and variability in quality that create challenges concerning cost-effective use, firms have invested significantly in databases and analytical tools to inform decision-making. In this regard, one means to avoid the costs associated with producing less than insightful reports and negative effects on performance through wasted resources is prioritizing data in terms of relevance and quality. The aim of this study is to investigate this approach by developing and testing a scale to evaluate Big Data Availability and the role of Big Data Prioritization for more effective use of big data in decision-making and performance. Focusing on the context of supply chain management (SCM), we validate this scale through a survey involving 84 managers. Findings support a positive association between Big Data Availability and its use in SCM decision-making, and suggest that Big Data Prioritization, as conceptualized in the study, has a positive impact on the use of big data in SCM decision-making and SCM performance. Through developing a scale to evaluate association between Big Data Availability and use in SCM decision-making, we make an empirical contribution to value generation from big data.}
}
@article{WEERASINGHE2022121222,
title = {Big data analytics for clinical decision-making: Understanding health sector perceptions of policy and practice},
journal = {Technological Forecasting and Social Change},
volume = {174},
pages = {121222},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121222},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521006557},
author = {Kasuni Weerasinghe and Shane L. Scahill and David J. Pauleen and Nazim Taskin},
keywords = {Analytics, Clinical decision-making, Big data, Healthcare, Social representation theory},
abstract = {The introduction and use of ‘big data and analytics’ is an on-going issue of discussion in health sectors globally. Healthcare systems of developed countries are trying to create more value and better healthcare through data and use of big data technologies. With an increasing number of articles identifying the value creation of big data and analytics for clinical decision-making, this paper examines how big data is applied, or not applied, in clinical practice. Using social representation theory as a theoretical foundation the paper explores people's perceptions of big data across all levels (policy making, planning, funding, and clinical care) of the New Zealand healthcare sector. The findings show that although adoption of big data technologies is planned for population health and health management, the potential of big data for clinical care has yet to be explored in the New Zealand context. The findings also highlight concern over data quality. The paper provides recommendations for policy and practice particularly around the need for engagement and participation of all levels to discuss data quality as well as big-data-based changes such as precision medicine and technology-assisted clinical decision-making tools. Future avenues of research are suggested.}
}
@article{MISHRA20216864,
title = {An efficient approach for manufacturing process using Big data analytics},
journal = {Materials Today: Proceedings},
volume = {47},
pages = {6864-6866},
year = {2021},
note = {International Conference on Advances in Design, Materials and Manufacturing},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.146},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321037226},
author = {Devendra Kumar Mishra and Arvind Kumar Upadhyay and Sanjiv Sharma},
keywords = {Manufacturing process, Bigdata, Structured data, Unstructured data},
abstract = {Manufacturing is a technique that produce finished goods after taking supplies, raw materials and ingredients. Manufacturing process is frequently used to produce food, chemicals and other things those have very important place in human’s life. This manufacturing process involve data for analysis and management of the process, but in current scenario the data that are generated by the process is increasing day by day. This huge amount of data in known as big data. Big data is difficult to handle by traditional data management tools. Data that are generated by the manufacturing process collected by the logs records and may be as structured or unstructured. Generally analysis is performed by structured data. Unstructured data also provide good insights in the manufacturing process if analysed in proper manner. This paper involved an efficient approach of big data analysis for manufacturing process.}
}
@article{GODOY2021100079,
title = {Transformations of trust in society: A systematic review of how access to big data in energy systems challenges Scandinavian culture},
journal = {Energy and AI},
volume = {5},
pages = {100079},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100079},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000331},
author = {Jaqueline de Godoy and Kathrin Otrel-Cass and Kristian Høyer Toft},
keywords = {Surveillance capitalism, Smart meters, Energy transition, Trust, Data Ethics, Big Data},
abstract = {In the era of information technology and big data, the extraction, commodification, and control of personal information is redefining how people relate and interact. However, the challenges that big data collection and analytics can introduce in trust-based societies, like those of Scandinavia, are not yet understood. For instance, in the energy sector, data generated through smart appliances, like smart metering devices, can have collateral implications for the end-users. In this paper, we present a systematic review of scientific articles indexed in Scopus to identify possible relationships between the practices of collecting, processing, analysing, and using people's data and people's responses to such practices. We contextualise this by looking at research about Scandinavian societies and link this to the academic literature on big data and trust, big data and smart meters, data ethics and the energy sector, surveillance capitalism, and subsequently performing a reflexive thematic analysis. We broadly situate our understanding of culture in this context on the interactions between cognitive norms, material culture, and energy practices. Our analysis identified a number of articles discussing problems and solutions to do with the practices of surveillance capitalism. We also found that research addresses these challenges in different ways. While some research focuses on technological amendments to address users’ privacy protection, only few examine the fundamental ethical questions that discuss how big data practices may change societies and increase their vulnerability. The literature suggests that even in highly trusting societies, like the ones found in Scandinavian countries, trust can be undermined and weakened.}
}
@article{ARDAGNA2018548,
title = {Context-aware data quality assessment for big data},
journal = {Future Generation Computer Systems},
volume = {89},
pages = {548-562},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17329151},
author = {Danilo Ardagna and Cinzia Cappiello and Walter Samá and Monica Vitali},
keywords = {Data quality, Big data, Context-awareness, Data profiling, DQ assessment},
abstract = {Big data changed the way in which we collect and analyze data. In particular, the amount of available information is constantly growing and organizations rely more and more on data analysis in order to achieve their competitive advantage. However, such amount of data can create a real value only if combined with quality: good decisions and actions are the results of correct, reliable and complete data. In such a scenario, methods and techniques for the Data Quality assessment can support the identification of suitable data to process. If for traditional database numerous assessment methods are proposed, in the Big Data scenario new algorithms have to be designed in order to deal with novel requirements related to variety, volume and velocity issues. In particular, in this paper we highlight that dealing with heterogeneous sources requires an adaptive approach able to trigger the suitable quality assessment methods on the basis of the data type and context in which data have to be used. Furthermore, we show that in some situations it is not possible to evaluate the quality of the entire dataset due to performance and time constraints. For this reason, we suggest to focus the Data Quality assessment only on a portion of the dataset and to take into account the consequent loss of accuracy by introducing a confidence factor as a measure of the reliability of the quality assessment procedure. We propose a methodology to build a Data Quality adapter module, which selects the best configuration for the Data Quality assessment based on the user main requirements: time minimization, confidence maximization, and budget minimization. Experiments are performed by considering real data gathered from a smart city case study.}
}
@article{PAL2020100869,
title = {Big data in biology: The hope and present-day challenges in it},
journal = {Gene Reports},
volume = {21},
pages = {100869},
year = {2020},
issn = {2452-0144},
doi = {https://doi.org/10.1016/j.genrep.2020.100869},
url = {https://www.sciencedirect.com/science/article/pii/S2452014420302831},
author = {Subhajit Pal and Sudip Mondal and Gourab Das and Sunirmal Khatua and Zhumur Ghosh},
keywords = {Big data, Cloud computing, Bioinformatics, High throughput data, MapReduce, Machine learning},
abstract = {The wave of new technologies has opened up the opportunity for cost-effective generation of high-throughput profiles of biological systems. This is generating tons of biological data. It is thus leading us towards the “big data” era which is creating a pressing need to bridge the gap between high-throughput technological development and our ability for managing, analyzing, and integrating the biological big data. To harness the maximum out of it, sufficient expertise needs to be developed for big data management and analysis. In this review, we discuss the challenges related to storage, transfer, access and analysis of unstructured and structured biological big data. Subsequently, it provides a comprehensive summary regarding the important strategies adopted for biological big data management which includes a discussion on all the recently used tools or software built for high throughput processing and analysis of biological big data. Finally it discusses the future perspectives of big data bioinformatics.}
}
@article{ULLAH2022103294,
title = {On the scalability of Big Data Cyber Security Analytics systems},
journal = {Journal of Network and Computer Applications},
volume = {198},
pages = {103294},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2021.103294},
url = {https://www.sciencedirect.com/science/article/pii/S1084804521002897},
author = {Faheem Ullah and M. Ali Babar},
keywords = {Big data, Cyber security, Adaptation, Scalability, Configuration parameter, Spark},
abstract = {Big Data Cyber Security Analytics (BDCA) systems use big data technologies (e.g., Apache Spark) to collect, store, and analyse a large volume of security event data for detecting cyber-attacks. The volume of digital data in general and security event data in specific is increasing exponentially. The velocity with which security event data is generated and fed into a BDCA system is unpredictable. Therefore, a BDCA system should be highly scalable to deal with the unpredictable increase/decrease in the velocity of security event data. However, there has been little effort to investigate the scalability of BDCA systems to identify and exploit the sources of scalability improvement. In this paper, we first investigate the scalability of a Spark-based BDCA system with default Spark settings. We then identify Spark configuration parameters (e.g., execution memory) that can significantly impact the scalability of a BDCA system. Based on the identified parameters, we finally propose a parameter-driven adaptation approach, SCALER, for optimizing a system's scalability. We have conducted a set of experiments by implementing a Spark-based BDCA system on a large-scale OpenStack cluster. We ran our experiments with four security datasets. We have found that (i) a BDCA system with default settings of Spark configuration parameters deviates from ideal scalability by 59.5% (ii) 9 out of 11 studied Spark configuration parameters significantly impact scalability and (iii) SCALER improves the BDCA system's scalability by 20.8% compared to the scalability with default Spark parameter setting. The findings of our study highlight the importance of exploring the parameter space of the underlying big data framework (e.g., Apache Spark) for scalable cyber security analytics.}
}
@article{ALAOUI2019803,
title = {The Impact of Big Data Quality on Sentiment Analysis Approaches},
journal = {Procedia Computer Science},
volume = {160},
pages = {803-810},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919317077},
author = {Imane El Alaoui and Youssef Gahi},
keywords = {Big Data Quality Metrics, Big Data Value Chain, Big Data, Big Social Data, Sentiment Analysis, Opinion Mining},
abstract = {Human beings share their good or bad opinions about subjects, products, and services through internet and social networks. The ability to effectively analyze this kind of information is now seen as a key competitive advantage to better inform decisions. In order to do so, organizations employ Sentiment Analysis (SA) techniques on these data. However, the usage of social media around the world is ever-increasing, which considerably accelerates massive data generation and makes traditional SA systems unable to deliver useful insights. Such volume of data can be efficiently analyzed using the combination of SA techniques and Big Data technologies. In fact, big data is not a luxury but an essential necessary to make valuable predictions. However, there are some challenges associated with big data such as quality that could highly affect the SA systems’ accuracy that use huge volume of data. Thus, the quality aspect should be addressed in order to build reliable and credible systems. For this, the goal of our research work is to consider Big Data Quality Metrics (BDQM) in SA that rely of big data. In this paper, we first highlight the most eloquent BDQM that should be considered throughout the Big Data Value Chain (BDVC) in any big data project. Then, we measure the impact of BDQM on a novel SA method accuracy in a real case study by giving simulation results.}
}
@article{MUHEIDAT202215,
title = {Emerging Concepts Using Blockchain and Big Data},
journal = {Procedia Computer Science},
volume = {198},
pages = {15-22},
year = {2022},
note = {12th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 11th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.206},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921024455},
author = {Fadi Muheidat and Dhaval Patel and Spandana Tammisetty and Lo’ai A. Tawalbeh and Mais Tawalbeh},
keywords = {Blockchain, Bigdata, Data analytics, Smart Cities applications, Healthcare},
abstract = {Blockchain and big data are the emerging technologies that are on the highest agendas of the firms. These are significantly expected to transform the ways in which the business as well as the firm run. These are on the verge of increasing the expectation of the distributed ledgers, which would keep the firms away from struggling challenges. The concept of big data and Blockchain has been used up by various other concepts that would help secure and interpret the information. The ideal solutions offered by these technologies shall address the challenges of big data management as well as for analytics. In addition to that, Blockchain provides its own consensus method, which is the primary means to create an audit trail. This enables users to verify all transactions. The audit trail is a means of verifying the correctness and integrity of every transaction, regardless of who owns the asset. The Blockchain can also verify that different parties of a transaction are following an agreement and not breaking the agreement. Moreover, there have been continuous arguments in the concept of Blockchain at which the bitcoin is fundamental and there are several popular blockchain approaches developed which would deliver performance, security as well as privacy. Apart from this, the use of Blockchain plays a major role in adding an extra data layer for the big data analytics process. Big data is considered secure which cannot be further forged with the network architecture. The current paper shall be discussing the emerging concepts that are using Blockchain as well as big data.}
}
@article{TANG2022100289,
title = {Big Data in Forecasting Research: A Literature Review},
journal = {Big Data Research},
volume = {27},
pages = {100289},
year = {2022},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100289},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621001064},
author = {Ling Tang and Jieyi Li and Hongchuan Du and Ling Li and Jun Wu and Shouyang Wang},
keywords = {Big data, Forecasting, Literature review, Prediction models, Information},
abstract = {With the boom in Internet techniques and computer science, a variety of big data have been introduced into forecasting research, bringing new knowledge and improving prediction models. This paper is the first attempt to conduct a literature review on full-scale big data in forecasting research. By source, big data in forecasting research fell into user-generated content data (from the users on social media in texts, photos, etc.), device-monitored data (by meteorological monitors, smart meters, GPS, etc.) and activity log data (for web searching/visiting, online/offline marketing, clinical treatments, laboratory experiments, etc.). Different data types, bearing distinctive information and characteristics, dominated different forecasting tasks, required different analysis technologies and improved different forecasting models. This survey provides an overall review of big data-based forecasting research, details what (regarding data types and sources), where (forecasting hotspots) and how (analysis and forecasting methods used) big data improved prediction, and offers insights into future prospects.}
}
@article{FUGINI2021100192,
title = {A Big Data Analytics Architecture for Smart Cities and Smart Companies},
journal = {Big Data Research},
volume = {24},
pages = {100192},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100192},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000095},
author = {Mariagrazia Fugini and Jacopo Finocchi and Paolo Locatelli},
keywords = {Big Data platforms, Unstructured data, Text analytics, Machine learning, Virtual enterprises, Smart communities},
abstract = {This paper presents the approach to Big Data Analytics (BDA) developed in the SIBDA (Sistema Innovativo Big Data Analytics) Project. The project aim is to study and develop innovative solutions in the field of BDA for three companies cooperating in a temporary association of enterprises. We discuss elements of Big Data tackled in the project, namely document processing, mass e-mail applications and Internet of Things sensor networks, to be integrated into a shared platform of common assets and services for the three cooperating companies. We comment about the “Big Data Journey” status in Italy reported by Osservatorio Politecnico di Milano. Then, the paper presents the SIBDA project approach and requirements, outlines the adopted architecture and provides implementation hints, along with some experiments and considerations on the use of the proposed architecture for Smart Cities and Smart Enterprises and Communities.}
}
@article{WEN2021295,
title = {Big data driven Internet of Things for credit evaluation and early warning in finance},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {295-307},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001977},
author = {Chunhui Wen and Jinhai Yang and Liu Gan and Yang Pan},
keywords = {Internet of Things finance, Credit evaluation and early warning, Factor analysis, Particle swarm optimization, Big data driven},
abstract = {The big data technology framework has been successfully used in the Internet of Things, and the financial industry also hopes to use the advanced technology of big data to integrate and improve internal and external data related to credit risks. Relying on more efficient machine learning algorithms to get a reasonable prediction of credit risk can reduce the self-generated losses of the Internet of Things finance and increase profits. This article uses distributed search engine technology to customize web crawlers to obtain the required bank card and transaction data from the multi-source heterogeneous data of the Internet of Things financial industry, design the corresponding Spark parallel algorithm to preprocess the data, and establish an inverted table and two Level index file provides data source for big data analysis platform. After the data source is determined, the Mutually Exclusive Collectively Exhaustive (MECE) analysis method is combined with the scores of many financial business experts in the industry to obtain a set of candidate indicators and quantification methods for the financial credit risk evaluation of the Internet of Things, and analyze the correlation of indicators and risk grading. The random forest algorithm in the big data machine learning library is used to select the feature of the candidate index set, and a multi-level spatial association rule algorithm based on the Hash structure is designed to mine the financial risk information of the Internet of Things, and build a credit risk assessment and intelligent early warning model. This paper selects 26 indicators of Internet of Things finance as the research objects, uses SPSS26.0 software to perform sample Kaiser–Meyer–Olkin (KMO) test and Bartlett sphere test on the original data, and describes the results of factor analysis in detail. The particle swarm algorithm is introduced into the parameter optimization of random forest, and the financial credit risk assessment model of the Internet of Things is established. The results show that this method can significantly reduce the probability of banks making the first and second error rates when evaluating the credit risk of financing the Internet of Things Finance. This is conducive to the smooth development of the Internet of Things financial business for banks, which enables banks to enhance their own profitability while effectively reducing losses due to incorrect credit provision.}
}
@article{SOUIFI2021857,
title = {From Big Data to Smart Data: Application to performance management},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {857-862},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.100},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321008491},
author = {Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar},
keywords = {Big Data, Smart Data, Performance Management},
abstract = {In the context of digitalization, some companies are considering a transition to Industry 4.0 to ensure greater flexibility, productivity and responsiveness. The implementation of a relevant performance management system is then a real necessity to measure the degree of achievement of these objectives. In the era of Industry 4.0, the potential access to large amounts of data, i.e. Big Data, poses new challenges to the design and implementation of these systems. With the exponential growth of data generated from different sources, there is a need for extensive exploitation of data for performance management. Given the large volume of data, the speed at which it is generated and the variety of data sources, the manufacturing sector is facing with the challenge of creating value from large data sets. This paper introduces some potential benefits of Big Data for business and in particular its role in performance management systems. However, the key idea is that Big Data are not always neither available nor necessary. Authors focus on the concept of smart data, the result of the transformation of Big Data, and define a set of necessary and sufficient conditions the data should satisfy to be considered as Smart. The paper presents some methods of smart data extraction. Such smart data will be used to feed the performance management system in order to obtain more accurate, timely and representative key performance indicators.}
}
@article{SHAHOUD2021100432,
title = {An extended Meta Learning Approach for Automating Model Selection in Big Data Environments using Microservice and Container Virtualizationz Technologies},
journal = {Internet of Things},
volume = {16},
pages = {100432},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100432},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000767},
author = {Shadi Shahoud and Moritz Winter and Hatem Khalloof and Clemens Duepmeier and Veit Hagenmeyer},
keywords = {Meta learning, Machine learning, Microservice, Web-based applications, Big data},
abstract = {For a given specific machine learning task, very often several machine learning algorithms and their right configurations are tested in a trial-and-error approach, until an adequate solution is found. This wastes human resources for constructing multiple models, requires a data analytics expert and is time-consuming. Meta learning addresses these problems and supports non-expert users by recommending a promising learning algorithm based on meta features computed from a given dataset. In the present paper, a new concept for enhancing the predictive performance of meta learning classification models by generating new meta examples is introduced. Our concept is realized and evaluated in a microservice-based meta learning framework. This framework makes use of a powerful Big Data software stack, container visualization, modern web technologies and a microservice architecture. In this demonstration and for evaluation purpose, time series model selection is taken as a use case for applying meta learning. It is shown that the proposed microservice-based meta learning framework introduces an excellent performance in assigning the adequate forecasting model for the chosen time series datasets. Moreover, our new concept for generating new meta examples enhances the predictive performance of the meta learner up to 16.77% and 27.07% in the case of using the original and encoded representation forms of meta features respectively. The recommendation of the most appropriate forecasting model results in a well acceptable low framework overhead demonstrating that the framework can provide an efficient approach to solve the problem of model selection in the context of Big Data.}
}
@article{KESKAR2022532,
title = {Perspective of anomaly detection in big data for data quality improvement},
journal = {Materials Today: Proceedings},
volume = {51},
pages = {532-537},
year = {2022},
note = {CMAE'21},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.05.597},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321042243},
author = {Vinaya Keskar and Jyoti Yadav and Ajay Kumar},
keywords = {Credit card, Validation, LUHN, Big data, Bank},
abstract = {The period of Big Data examination has started in many businesses inside created nations. With expanding headway of Internet technology, expanding measures of data are spilling into contemporary associations. Data are getting bigger and more muddled because of the nonstop age of data from numerous gadgets and sources. In this investigation, we have examined the banking area's inconsistencies because of big data technology, inconsistencies in credit card and afterwards the path how to remove these inconsistencies.}
}
@article{SHARMA20215515,
title = {A framework based on BWM for big data analytics (BDA) barriers in manufacturing supply chains},
journal = {Materials Today: Proceedings},
volume = {47},
pages = {5515-5519},
year = {2021},
note = {3rd International e-Conference on Frontiers in Mechanical Engineering and nanoTechnology},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.03.374},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321024263},
author = {Vikrant Sharma and Atul Kumar and Mukesh Kumar},
keywords = {Big data analytics, Barriers, Manufacturing supply chains, Best worst method (BWM)},
abstract = {Due to its potential utility, Big Data (BD) recently attracted researchers and practitioners in decision-making. Big Data analytics (BDA) becomes more common among manufacturing companies because it lets them gain insight and make decisions based on BD. Given the importance of both BD and BDA, this study aims to identify and analyse essential BDA adoption barriers in supply chains. This study explores the current knowledge base using a BWM (Best Worst Method) to discuss these barriers. Data were obtained from five Indian manufacturing companies. Research findings show that data-related barriers are most significant. The findings will help managers understand the exact nature of the challenges and possible advantages of the BDA and implement BDA policies for the growth and output of supply chain operations.}
}
@article{CISNEROSCABRERA2021114858,
title = {Experimenting with big data computing for scaling data quality-aware query processing},
journal = {Expert Systems with Applications},
volume = {178},
pages = {114858},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114858},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421002992},
author = {Sonia Cisneros-Cabrera and Anna-Valentini Michailidou and Sandra Sampaio and Pedro Sampaio and Anastasios Gounaris},
keywords = {Data quality-aware queries, Big data computing, Empirical evaluation},
abstract = {Combining query processing techniques with data quality management approaches enables enforcement of quality constraints, such as timeliness, accuracy and completeness, as part of ad-hoc query specification and execution, improving the quality of query results. Despite the emergence of novel data quality processing tools, there is a dearth of studies assessing performance and scalability in the execution of data quality assessment tasks during query processing. This paper reports on an empirical study aiming to investigate the extent to which a big data computing framework (Spark) can offer significant gains in performance and scalability when executing data quality querying tasks over a range of computational platforms including a single commodity multi-core machine and a cluster-based platform for a wide range of workloads. Our results show that substantial performance and scalability gains can be obtained by using optimized data science libraries combined with the parallel and distributed capabilities of big data computing. We also provide guidelines on choosing the appropriate computational infrastructure for executing DQ-aware queries.}
}
@article{WANG2022663,
title = {A novel oscillation identification method for grid-connected renewable energy based on big data technology},
journal = {Energy Reports},
volume = {8},
pages = {663-671},
year = {2022},
note = {2021 International Conference on New Energy and Power Engineering},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2022.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S2352484722002682},
author = {Jian Wang},
keywords = {Oscillation identification, Big data, Evidence theory, Support vector machine},
abstract = {With the development of big data technology, power system has entered the era of data analysis. With the help of the massive data provided by the wide area measurement system, the power system can be easily evaluated, and the abnormal operation status can be detected and positioned. As the increase of renewable energy permeability, more new abnormal operating status have appeared in the system. Aimed at the abnormal operation state in the development of new energy, this paper proposes an oscillation location scheme based on evidence theory and support vector machine, which makes up for the limitation of single oscillation location method. The result of location analysis of oscillation energy method, oscillation phase difference method and forced oscillation phase difference location method is fused by evidence theory.}
}
@article{GEORGIADIS2022105640,
title = {Towards a privacy impact assessment methodology to support the requirements of the general data protection regulation in a big data analytics context: A systematic literature review},
journal = {Computer Law & Security Review},
volume = {44},
pages = {105640},
year = {2022},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2021.105640},
url = {https://www.sciencedirect.com/science/article/pii/S0267364921001138},
author = {Georgios Georgiadis and Geert Poels},
keywords = {Big data analytics, Data protection, Data protection directive, General data protection regulation, Governance, Information security, Privacy, Privacy impact assessment, Systematic literature review},
abstract = {Big Data Analytics enables today's businesses and organisations to process and utilise the raw data that is generated on a daily basis. While Big Data Analytics has improved efficiency and created many opportunities, it has also increased the risk of personal data being compromised or breached. The General Data Protection Regulation (GDPR) mandates Data Protection Impact Assessment (DPIA) as a means of identifying appropriate controls to mitigate risks associated with the protection of personal data. However, little is currently known about how to conduct such a DPIA in a Big Data Analytics context. To this end, we conducted a systematic literature review with the aim of identifying privacy and data protection risks specific to the Big Data Analytics context that could negatively impact individuals' rights and freedoms when they occur. Based on a sample of 159 articles, we applied a thematic analysis to all identified risks which resulted in the definition of nine Privacy Touch Points that summarise the identified risks. The coverage of these Privacy Touch Points was then analysed for ten Privacy Impact Assessment (PIA) methodologies. The insights gained from our analysis will inform the next phase of our research, in which we aim to develop a comprehensive DPIA methodology that will enable data processors and data controllers to identify, analyse and mitigate privacy and data protection risks when storing and processing data involving Big Data Analytics.}
}
@article{YIN2021102514,
title = {Integrating remote sensing and geospatial big data for urban land use mapping: A review},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {103},
pages = {102514},
year = {2021},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2021.102514},
url = {https://www.sciencedirect.com/science/article/pii/S030324342100221X},
author = {Jiadi Yin and Jinwei Dong and Nicholas A.S. Hamm and Zhichao Li and Jianghao Wang and Hanfa Xing and Ping Fu},
keywords = {Integration methods, Urban functional zone classification, Urban management, Land use},
abstract = {Remote Sensing (RS) has been used in urban mapping for a long time; however, the complexity and diversity of urban functional patterns are difficult to be captured by RS only. Emerging Geospatial Big Data (GBD) are considered as the supplement to RS data, and help to contribute to our understanding of urban lands from physical aspects (i.e., urban land cover) to socioeconomic aspects (i.e., urban land use). Integrating RS and GBD could be an effective way to combine physical and socioeconomic aspects with great potential for high-quality urban land use classification. In this study, we reviewed the existing literature and focused on the state-of-the-art and perspective of the urban land use categorization by integrating RS and GBD. Specifically, the commonly used RS features (e.g., spectral, textural, temporal, and spatial features) and GBD features (e.g., spatial, temporal, semantic, and sequence features) were identified and analyzed in urban land use classification. The integration strategies for RS and GBD features were categorized into feature-level integration (FI) and decision-level integration (DI). To be more specific, the FI method integrates the RS and GBD features and classifies urban land use types using the integrated feature sets; the DI method processes RS and GBD independently and then merges the classification results based on decision rules. We also discussed other critical issues, including analysis unit setting, parcel segmentation, parcel labeling of land use types, and data integration. Our findings provide a retrospect of different features from RS and GBD, strategies of RS and GBD integration, and their pros and cons, which could help to define the framework for future urban land use mapping and better support urban planning, urban environment assessment, urban disaster monitoring and urban traffic analysis.}
}
@article{SHAMIM2020120315,
title = {Big data analytics capability and decision making performance in emerging market firms: The role of contractual and relational governance mechanisms},
journal = {Technological Forecasting and Social Change},
volume = {161},
pages = {120315},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120315},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520311410},
author = {Saqib Shamim and Jing Zeng and Zaheer Khan and Najam Ul Zia},
keywords = {Big data, Contractual governance, Relational governance, Big data analytics capability, Culture, Decision-making performance, Emerging markets},
abstract = {This study examines the role of big data contractual and relational governance in big data decision-making performance of firms based in China. It investigates the mediation of big data analytics (BDA) capability in the association of contractual and relational governance with decision-making performance. Furthermore, moderating role of data-driven culture in the relationship of BDA capability and decision-making performance is examined. Data are collected from 108 Chinese firms engaged in big data-related activities. Structural equation modeling is employed to test the hypotheses. This study contributes towards the literature on big data management and governance mechanisms, by establishing the relationship of decision-making performance with big data contractual and relational governance directly and through the mediation of BDA capabilities. It also contributes towards knowledge based dynamic capabilities (KBDCs) view of firms, arguing that dynamic capabilities such as BDA capabilities can be influenced through knowledge sources and activities. We add to the discussions on whether contractual and relational governance are alternatives or they complement each other, by establishing the moderating role of big data relational governance in the relationship of contractual governance and decision-making performance. Finally, we argue that social capital can enhance KBDCs through contractual and relational governance in big data context.}
}
@article{ROSADO2021102155,
title = {MARISMA-BiDa pattern: Integrated risk analysis for big data},
journal = {Computers & Security},
volume = {102},
pages = {102155},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.102155},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820304284},
author = {David G. Rosado and Julio Moreno and Luis E. Sánchez and Antonio Santos-Olmo and Manuel A. Serrano and Eduardo Fernández-Medina},
keywords = {Big data, Risk assessment, Risk analysis, Information security, Security standards},
abstract = {Data is one of the most important assets for all types of companies, which have undoubtedly grown their quantity and the ways of exploiting them. Big Data appears in this context as a set of technologies that manage data to obtain information that supports decision-making. These systems were not conceived to be secure, resulting in significant risks that must be controlled. Security risks in Big Data must be analyzed and managed in an appropriate manner to protect the system and secure the information and the data being handled. This paper proposes a risk analysis approach for Big Data environments, which is based on a security analysis methodology called MARISMA (Methodology for the Analysis of Risks on Information System), supported by a technological environment in the cloud (eMARISMA tool) already used by numerous clients. Both MARISMA and eMARISMA are specifically designed to be easily adapted to particular contexts, such as Big Data. Our proposal, called MARISMA-BiDa, is based on the main related standards, such as ISO/IEC 27,000 and 31,000, or the NIST Big Data reference architecture or ENISA and CSA recommendations for Big Data.}
}
@article{KWON2014387,
title = {Data quality management, data usage experience and acquisition intention of big data analytics},
journal = {International Journal of Information Management},
volume = {34},
number = {3},
pages = {387-394},
year = {2014},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2014.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0268401214000127},
author = {Ohbyung Kwon and Namyeon Lee and Bongsik Shin},
keywords = {Big data analytics, Resource-based view, Data quality management, IT capability, Data usage},
abstract = {Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics mainly from the theoretical perspectives of data quality management and data usage experience. Our empirical investigation reveals that a firm's intention for big data analytics can be positively affected by its competence in maintaining the quality of corporate data. Moreover, a firm's favorable experience (i.e., benefit perceptions) in utilizing external source data could encourage future acquisition of big data analytics. Surprisingly, a firm's favorable experience (i.e., benefit perceptions) in utilizing internal source data could hamper its adoption intention for big data analytics.}
}
@article{LIU2016134,
title = {Rethinking big data: A review on the data quality and usage issues},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {115},
pages = {134-142},
year = {2016},
note = {Theme issue 'State-of-the-art in photogrammetry, remote sensing and spatial information science'},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2015.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271615002567},
author = {Jianzheng Liu and Jie Li and Weifeng Li and Jiansheng Wu},
keywords = {Big data, Data quality and error, Data ethnics, Spatial information sciences},
abstract = {The recent explosive publications of big data studies have well documented the rise of big data and its ongoing prevalence. Different types of “big data” have emerged and have greatly enriched spatial information sciences and related fields in terms of breadth and granularity. Studies that were difficult to conduct in the past time due to data availability can now be carried out. However, big data brings lots of “big errors” in data quality and data usage, which cannot be used as a substitute for sound research design and solid theories. We indicated and summarized the problems faced by current big data studies with regard to data collection, processing and analysis: inauthentic data collection, information incompleteness and noise of big data, unrepresentativeness, consistency and reliability, and ethical issues. Cases of empirical studies are provided as evidences for each problem. We propose that big data research should closely follow good scientific practice to provide reliable and scientific “stories”, as well as explore and develop techniques and methods to mitigate or rectify those ‘big-errors’ brought by big data.}
}
@article{YANG2021100234,
title = {Risk Prediction of Renal Failure for Chronic Disease Population Based on Electronic Health Record Big Data},
journal = {Big Data Research},
volume = {25},
pages = {100234},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100234},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000514},
author = {Yujie Yang and Ye Li and Runge Chen and Jing Zheng and Yunpeng Cai and Giancarlo Fortino},
keywords = {Renal failure, Risk prediction, Electronic health record, Health big data, Machine learning},
abstract = {Renal failure is a fatal disease raising global concerns. Previous risk models for renal failure mostly rely on the diagnosis of chronic kidney disease, which lacks obvious clinical symptoms and thus is mostly undiagnosed, causing significant omission of high-risk patients. In this paper, we proposed a framework to predict the risk of renal failure directly from a big data repository of chronic disease population without prerequisite diagnosis of chronic kidney disease. The electronic health records of 42,256 patients with hypertension or diabetes in Shenzhen Health Information Big Data Platform were collected, with 398 suffered from renal failure during a 3-year follow-up. Five state-of-the-art machine learning methods are utilized to build risk prediction models of renal failure for chronic disease population. Extensive experimental results show that the proposed framework achieves quite well performance. Particularly, the XGBoost obtains the best performance with an area under receiving-operating-characteristics curve (AUC) of 0.9139. By analyzing the effect of risk factors, we identified that serum creatine, age, urine acid, systolic blood pressure, and blood urea nitrogen are the top five factors associated with renal failure risk. Compared with existing models, our model can be deployed into routine chronic disease management procedures and enable more preemptive, widely-covered screening of renal risks, which would in turn reduce the damage caused by the disease through timely intervention.}
}
@article{YAO20181,
title = {Big data quality prediction in the process industry: A distributed parallel modeling framework},
journal = {Journal of Process Control},
volume = {68},
pages = {1-13},
year = {2018},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2018.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959152418300660},
author = {Le Yao and Zhiqiang Ge},
keywords = {Distributed modeling, MapReduce framework, Parallel computing, Quality prediction, Big data analytics},
abstract = {With the ever increasing data collected from the process, the era of big data has arrived in the process industry. Therefore, the computational effort for data modeling and analytics in standalone modes has become increasingly demanding, particularly for large-scale processes. In this paper, a distributed parallel process modeling approach is presented based on a MapReduce framework for big data quality prediction. Firstly, the architecture for distributed parallel data modeling is formulated under the MapReduce framework. Secondly, a big data quality prediction scheme is developed based on the distributed parallel data modeling approach. As an example, the basic Semi-Supervised Probabilistic Principal Component Regression (SSPPCR) model is deployed to concurrently train a set of local models with split datasets. Meanwhile, Bayesian rule is utilized in a MapReduce way to integrate local models based on their predictive abilities. Two case studies demonstrate the effectiveness of the proposed method for big data quality prediction.}
}
@article{CORTEREAL2020103141,
title = {Leveraging internet of things and big data analytics initiatives in European and American firms: Is data quality a way to extract business value?},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103141},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308662},
author = {Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira},
keywords = {Big data analytics, Internet of things, Strategic management, Knowledge-based theory, Dynamics capability theory},
abstract = {Big data analytics (BDA) and the Internet of Things (IoT) tools are considered crucial investments for firms to distinguish themselves among competitors. Drawing on a strategic management perspective, this study proposes that BDA and IoT capabilities can create significant value in business processes if supported by a good level of data quality, which will lead to a better competitive advantage. Responses are collected from 618 European and American firms that use IoT and BDA applications. Partial least squares results reveal that better data quality is needed to unlock the value of IoT and BDA capabilities.}
}
@article{HORNG202222,
title = {Role of big data capabilities in enhancing competitive advantage and performance in the hospitality sector: Knowledge-based dynamic capabilities view},
journal = {Journal of Hospitality and Tourism Management},
volume = {51},
pages = {22-38},
year = {2022},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2022.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S1447677022000389},
author = {Jeou-Shyan Horng and Chih-Hsing Liu and Sheng-Fang Chou and Tai-Yi Yu and Da-Chian Hu},
keywords = {Knowledge-based dynamic capabilities view, Big data capabilities, Knowledge management, Sustainability marketing, Social media, Big data strategy},
abstract = {To address the unsolved problem of the mechanism underlying the effect of big data analytics capabilities on competitive advantage and performance, this study combined quantitative and qualitative methods to test the examined framework. The results of 257 questionnaires from hotel marketing managers and 19 semistructured interviews, confirm that big data analytics capabilities develop from big data strategies and knowledge management and enhance competitive advantage and performance through sustainability marketing. Moreover, social media enhance sustainability marketing and competitive advantage and performance. The original findings of the current research contribute to the development of big data, sustainability marketing, and social media.}
}
@article{ALI2021101600,
title = {Is big data used by cities? Understanding the nature and antecedents of big data use by municipalities},
journal = {Government Information Quarterly},
volume = {38},
number = {4},
pages = {101600},
year = {2021},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101600},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000368},
author = {Hamza Ali and Ryad Titah},
keywords = {Big data use, Big data use in municipalities, Smart cities, E-government, Municipal use of IT, Digital government},
abstract = {It is estimated that by 2050, 70% of the population will be urban (Nations Unies, 2014). This massive urbanization has created unprecedented challenges for cities and city managers which has led many of them to look for technological solutions to address them, including the use of Big Data, which is among the most considered technological support to help improve the overall operational and service delivery of cities. It is estimated that around 7 billion connected objects will soon be implemented in cities worldwide which will produce an unprecedented and massive amount of real-time data that will have to be managed, used, and analyzed effectively. If this massive amount of data is effectively managed and used, it can provide important benefits and produce real positive impacts on the functioning of cities. Nonetheless, despite these benefits, only a few cities are able to use and exploit big data, and some studies have shown that less than 0.5% of all the available data has been explored. The objective of this study is to understand the factors that influence cities to use big data and the nature of such use. Based on a field survey involving 106 municipalities, this study investigates the antecedents of big data use by cities and shows how different sets of antecedents influence three different types of big data use by cities.}
}
@article{GUPTA2021120986,
title = {Big data and firm marketing performance: Findings from knowledge-based view},
journal = {Technological Forecasting and Social Change},
volume = {171},
pages = {120986},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.120986},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521004182},
author = {Shivam Gupta and Théo Justy and Shampy Kamboj and Ajay Kumar and Eivind Kristoffersen},
keywords = {Big data analytics, Artificial intelligence, Marketing performance, Knowledge-based view},
abstract = {A universal trend in advanced manufacturing countries is defining Industry 4.0, industrialized internet and future factories as a recent wave, which may transform the production and its related services. Further, big data analytics has emerged as a game changer in the business world due to its uses for increasing accuracy in decision-making and enhancing performance of sustainable industry 4.0 applications. This study intends to emphasize on how to support Industry 4.0 with knowledge based view. For the same, a conceptual model is framed and presented with essential components that are required for a real world implementation. The study used qualitative analysis and was guided by a knowledge-based theoretical framework. Thematic analysis resulted in the identification of a number of emergent categories. Key findings highlight significant gaps in conventional decision-making systems and demonstrate how big data enhances firms’ strategic and operational decisions as well as facilitates informational access for improved marketing performance. The resulting proposed model can provide managers with a reference point for using big data to line up firms’ activities for more effective marketing efforts and presents a conceptual basis for further empirical studies in this area.}
}
@article{SINGH2021157,
title = {Big data, industry 4.0 and cyber-physical systems integration: A smart industry context},
journal = {Materials Today: Proceedings},
volume = {46},
pages = {157-162},
year = {2021},
note = {2nd International Conference on Manufacturing Material Science and Engineering},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.07.170},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320352639},
author = {Harpreet Singh},
keywords = {Agile management, Heterogeneity, Internet-of-things, Smart factory, Smart manufacturing},
abstract = {The advancements in the industries have paved the way for the distributed establishment of the big data volumes, cyber-physical systems, and industrie 4.0. The perspectives of modules are integrated with the shop-floor monitoring and controlled by computational paradigms, and digital computational spaces. The performance rises after introducing an intelligent and automated manufacturing industry into the next-generation industry. The scope of this paper is to address the state-of-the-art technologies and phases such as digital twins, big data analytics, artificial intelligence, and internet-of-things. The research challenges are examined with attention on data integrity, data quality, data privacy, data availability, data scalability, data transformation, legitimate and monitoring issues, and governance. Lastly, potential research issues that need considerable research efforts are summarized. We believe that this paper is presenting the research directions for researchers in the area of smart industry towards its integration for the advancements of the industrial sector, and agile management. Some surprising development as industry 4.0 integration with socio-technical systems was found in designing the architecture of vertical, horizontal, and end-to-end integration mechanisms.}
}
@article{BAZZAZABKENAR2021101517,
title = {Big data analytics meets social media: A systematic review of techniques, open issues, and future directions},
journal = {Telematics and Informatics},
volume = {57},
pages = {101517},
year = {2021},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2020.101517},
url = {https://www.sciencedirect.com/science/article/pii/S0736585320301763},
author = {Sepideh {Bazzaz Abkenar} and Mostafa {Haghi Kashani} and Ebrahim Mahdipour and Seyed Mahdi Jameii},
keywords = {Social networks, Big data, Content analysis, Sentiment analysis, Systematic literature review},
abstract = {Social Networking Services (SNSs) connect people worldwide, where they communicate through sharing contents, photos, videos, posting their first-hand opinions, comments, and following their friends. Social networks are characterized by velocity, volume, value, variety, and veracity, the 5 V’s of big data. Hence, big data analytic techniques and frameworks are commonly exploited in Social Network Analysis (SNA). By the ever-increasing growth of social networks, the analysis of social data, to describe and find communication patterns among users and understand their behaviors, has attracted much attention. In this paper, we demonstrate how big data analytics meets social media, and a comprehensive review is provided on big data analytic approaches in social networks to search published studies between 2013 and August 2020, with 74 identified papers. The findings of this paper are presented in terms of main journals/conferences, yearly distributions, and the distribution of studies among publishers. Furthermore, the big data analytic approaches are classified into two main categories: Content-oriented approaches and network-oriented approaches. The main ideas, evaluation parameters, tools, evaluation methods, advantages, and disadvantages are also discussed in detail. Finally, the open challenges and future directions that are worth further investigating are discussed.}
}
@article{CECH20211947,
title = {Benefiting from big data in natural products: importance of preserving foundational skills and prioritizing data quality},
journal = {Natural Product Reports},
volume = {38},
number = {11},
pages = {1947-1953},
year = {2021},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d1np00061f},
url = {https://www.sciencedirect.com/science/article/pii/S0265056822008856},
author = {Nadja B. Cech and Marnix H. Medema and Jon Clardy},
abstract = {ABSTRACT
Systematic, large-scale, studies at the genomic, metabolomic, and functional level have transformed the natural product sciences. Improvements in technology and reduction in cost for obtaining spectroscopic, chromatographic, and genomic data coupled with the creation of readily accessible curated and functionally annotated data sets have altered the practices of virtually all natural product research laboratories. Gone are the days when the natural products researchers were expected to devote themselves exclusively to the isolation, purification, and structure elucidation of small molecules. We now also engage with big data in taxonomic, genomic, proteomic, and/or metabolomic collections, and use these data to generate and test hypotheses. While the oft stated aim for the use of large-scale -omics data in the natural products sciences is to achieve a rapid increase in the rate of discovery of new drugs, this has not yet come to pass. At the same time, new technologies have provided unexpected opportunities for natural products chemists to ask and answer new and different questions. With this viewpoint, we discuss the evolution of big data as a part of natural products research and provide a few examples of how discoveries have been enabled by access to big data. We also draw attention to some of the limitations in our existing engagement with large datasets and consider what would be necessary to overcome them.}
}
@article{HAZEN201472,
title = {Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications},
journal = {International Journal of Production Economics},
volume = {154},
pages = {72-80},
year = {2014},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2014.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0925527314001339},
author = {Benjamin T. Hazen and Christopher A. Boone and Jeremy D. Ezell and L. Allison Jones-Farmer},
keywords = {Data quality, Statistical process control, Knowledge-based view, Organizational information processing view, Systems theory},
abstract = {Today׳s supply chain professionals are inundated with data, motivating new ways of thinking about how data are produced, organized, and analyzed. This has provided an impetus for organizations to adopt and perfect data analytic functions (e.g. data science, predictive analytics, and big data) in order to enhance supply chain processes and, ultimately, performance. However, management decisions informed by the use of these data analytic methods are only as good as the data on which they are based. In this paper, we introduce the data quality problem in the context of supply chain management (SCM) and propose methods for monitoring and controlling data quality. In addition to advocating for the importance of addressing data quality in supply chain research and practice, we also highlight interdisciplinary research topics based on complementary theory.}
}
@article{OUAFIQ2022102093,
title = {AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities},
journal = {Sustainable Energy Technologies and Assessments},
volume = {52},
pages = {102093},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102093},
url = {https://www.sciencedirect.com/science/article/pii/S221313882200145X},
author = {El Mehdi Ouafiq and Rachid Saadane and Abdellah Chehri and Seunggil Jeon},
keywords = {Smart Farming, Energy Harvesting Capabilities, IoT, Big Data, Agriculture 4.0, Water Management, Sustainability},
abstract = {The use of Internet of Things (IoT) networks offers great advantages over wired networks, especially due to their simple installation, low maintenance costs, and automatic configuration. IoT facilitates the integration of sensing and communication for various industries, including smart farming and precision agriculture. For several years, many researchers have strived to find new sources of energy that are always “cleaner” and more environmentally friendly. Energy harvesting technology is one of the most promising environment-friendly solutions that extend the lifetime of these IoT devices. In this paper, the state-of-art of IoT energy harvesting capabilities and communication technologies in smart agriculture is presented. In addition, this work proposes a comprehensive architecture that includes big data technologies, IoT components, and knowledge-based systems for innovative farm architecture. The solution answers some of the biggest challenges the agriculture industry faces, especially when handling small files in a big data environment without impacting the computation performance. The solution is built on top of a pre-defined big data architecture that includes an abstraction layer of the data lake that handles data quality following a data migration strategy to ensure the data's insights. Furthermore, in this paper, we compared several machine learning algorithms to find the most suitable smart farming analytics tools in terms of forecasting and predictions.}
}
@article{JIN202024,
title = {Big Data in food safety- A review},
journal = {Current Opinion in Food Science},
volume = {36},
pages = {24-32},
year = {2020},
note = {Food Safety},
issn = {2214-7993},
doi = {https://doi.org/10.1016/j.cofs.2020.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214799320301260},
author = {Cangyu Jin and Yamine Bouzembrak and Jiehong Zhou and Qiao Liang and Leonieke M. {van den Bulk} and Anand Gavai and Ningjing Liu and Lukas J. {van den Heuvel} and Wouter Hoenderdaal and Hans J.P. Marvin},
abstract = {The massive rise of Big Data generated from smartphones, social media, Internet of Things (IoT), and multimedia, has produced an overwhelming flow of data in either structured or unstructured format. Big Data technologies are being developed and implemented in the food supply chain that gather and analyse these data. Such technologies demand new approaches in data collection, storage, processing and knowledge extraction. In this article, an overview of the recent developments in Big Data applications in food safety are presented. This review shows that the use of Big Data in food safety remains in its infancy but it is influencing the entire food supply chain. Big Data analysis is used to provide predictive insights in several steps in the food supply chain, support supply chain actors in taking real time decisions, and design the monitoring and sampling strategies. Lastly, the main research challenges that require research efforts are introduced.}
}
@article{BENITEZHIDALGO2021107489,
title = {TITAN: A knowledge-based platform for Big Data workflow management},
journal = {Knowledge-Based Systems},
volume = {232},
pages = {107489},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107489},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121007516},
author = {Antonio Benítez-Hidalgo and Cristóbal Barba-González and José García-Nieto and Pedro Gutiérrez-Moncayo and Manuel Paneque and Antonio J. Nebro and María del Mar Roldán-García and José F. Aldana-Montes and Ismael Navas-Delgado},
keywords = {Big Data analytics, Semantics, Knowledge extraction},
abstract = {Modern applications of Big Data are transcending from being scalable solutions of data processing and analysis, to now provide advanced functionalities with the ability to exploit and understand the underpinning knowledge. This change is promoting the development of tools in the intersection of data processing, data analysis, knowledge extraction and management. In this paper, we propose TITAN, a software platform for managing all the life cycle of science workflows from deployment to execution in the context of Big Data applications. This platform is characterised by a design and operation mode driven by semantics at different levels: data sources, problem domain and workflow components. The proposed platform is developed upon an ontological framework of meta-data consistently managing processes and models and taking advantage of domain knowledge. TITAN comprises a well-grounded stack of Big Data technologies including Apache Kafka for inter-component communication, Apache Avro for data serialisation and Apache Spark for data analytics. A series of use cases are conducted for validation, which comprises workflow composition and semantic meta-data management in academic and real-world fields of human activity recognition and land use monitoring from satellite images.}
}
@article{GOKALP2022103585,
title = {A process assessment model for big data analytics},
journal = {Computer Standards & Interfaces},
volume = {80},
pages = {103585},
year = {2022},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2021.103585},
url = {https://www.sciencedirect.com/science/article/pii/S0920548921000805},
author = {Mert Onuralp Gökalp and Ebru Gökalp and Kerem Kayabay and Selin Gökalp and Altan Koçyiğit and P. Erhan Eren},
keywords = {Big data, Data analytics, Software development, Software process improvement, Software process assessment},
abstract = {Today, business success is essentially powered by data-centric software. Big data analytics (BDA) grasp the potential of generating valuable insights and empowering businesses to support their strategic decision-making. However, although organizations are aware of BDAs’ potential opportunities, they face challenges to satisfy the BDA-specific processes and integrate them into their daily software development lifecycle. Process capability/ maturity assessment models are used to assist organizations in assessing and realizing the value of emerging capabilities and technologies. However, as a result of the literature review and its analysis, it was observed that none of the existing studies in the BDA domain provides a complete, standardized, and objective capability maturity assessment model. To address this research gap, we focus on developing a BDA process capability assessment model grounded on the well-accepted ISO/IEC 330xx standard series. The proposed model comprises two main dimensions: process and capability. The process dimension covers six BDA-specific processes: business understanding, data understanding, data preparation, model building, evaluation, and deployment and use. The capability dimension has six levels, from not performed to innovating. We conducted case studies in two different organizations to validate the applicability and usability of the proposed model. The results indicate that the proposed model provides significant insights to improve the business value generated by BDA via determining the current capability levels of the organizations' BDA processes, deriving a gap analysis, and creating a comprehensive roadmap for continuous improvement in a standardized way.}
}
@article{YANG2021107550,
title = {Optimal timing of big data application in a two-period decision model with new product sales},
journal = {Computers & Industrial Engineering},
volume = {160},
pages = {107550},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107550},
url = {https://www.sciencedirect.com/science/article/pii/S036083522100454X},
author = {Lei Yang and Anqian Jiang and Jiahua Zhang},
keywords = {Supply chain management, Optimal strategy, Big data application, Two-period model, Social welfare},
abstract = {We study a firm's strategy in adopting big data technology to motivate consumer demand over two periods. In the first period, the firm designs a product to sell to the market and determines whether to apply big data to attract more consumers. In the second period, the firm designs a new product and determines whether to sell the old product and the new product simultaneously, where big data can also be applied in this period to stimulate more demands. We formulate this problem into four models considering whether the firm adopts big data in the first period and/or the second period, and whether the firm only sells the new product or sells both the old and new products in the second period. We find that the firm prefers to apply big data over both periods when the cost is low, only over the second period when the cost is median and will not apply big data when the cost is high. Interestingly, only applying big data over the first period also may bring the most profits with heterogeneous big data coefficients. Furthermore, applying big data in the second period is the better choice for the social welfare.}
}
@article{RAUT2021103368,
title = {Big data analytics: Implementation challenges in Indian manufacturing supply chains},
journal = {Computers in Industry},
volume = {125},
pages = {103368},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103368},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520306023},
author = {Rakesh D. Raut and Vinay Surendra Yadav and Naoufel Cheikhrouhou and Vaibhav S. Narwane and Balkrishna E. Narkhede},
keywords = {Big data analytics, DEMATEL, Indian manufacturing supply chains, Interpretive structural modeling, MICMAC analysis},
abstract = {Big Data Analytics (BDA) has attracted significant attention from both academicians and practitioners alike as it provides several ways to improve strategic, tactical and operational capabilities to eventually create a positive impact on the economic performance of organizations. In the present study, twelve significant barriers against BDA implementation are identified and assessed in the context of Indian manufacturing Supply Chains (SC). These barriers are modeled using an integrated two-stage approach, consisting of Interpretive Structural Modeling (ISM) in the first stage and Decision-Making Trial and Evaluation Laboratory (DEMATEL) in the second stage. The approach developed provides the interrelationships between the identified constructs and their intensities. Moreover, Fuzzy MICMAC technique is applied to analyze the high impact (i.e., high driving power) barriers. Results show that four constructs, namely lack of top management support, lack of financial support, lack of skills, and lack of techniques or procedures, are the most significant barriers. This study aids policy-makers in conceptualizing the mutual interaction of the barriers for developing policies and strategies to improve the penetration of BDA in manufacturing SC.}
}
@article{SILVA2019532,
title = {Risk Analysis of Using Big Data in Computer Sciences},
journal = {Procedia Computer Science},
volume = {160},
pages = {532-537},
year = {2019},
note = {The 10th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2019) / The 9th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919317521},
author = {Jesus Silva and Omar Bonerge {Pineda Lezama} and Ligia Romero and Darwin Solano and Claudia Fernández},
keywords = {Data management, data quality, decision making, data analysis},
abstract = {Today, as technologies mature and people are encouraged to contribute data to organizations’ databases, more transactions are being captured than ever before. Meanwhile, improvements in data storage technologies have made the cost of evaluating, selecting, and destroying legacy data considerably greater than simply letting it accumulate. On the one hand, the excess of stored data has considerably increased the opportunities to interrelate and analyze them, while the moderate enthusiasm generated by data warehousing and data mining in the 1990s has been replaced by a rampant euphoria about big data and data analytics. But, is this as wonderful as seems? This paper presents a risk analysis of Big Data and Big Data Analytics based on a review of quality factors.}
}
@article{ZHANG2022101626,
title = {Big data analytics, resource orchestration, and digital sustainability: A case study of smart city development},
journal = {Government Information Quarterly},
volume = {39},
number = {1},
pages = {101626},
year = {2022},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2021.101626},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X21000629},
author = {Dan Zhang and L.G. Pee and Shan L. Pan and Lili Cui},
keywords = {Smart city, Big data, Digital sustainability, Resource orchestration, Socio-technical issues},
abstract = {Smart cities are expected to improve the efficiency and effectiveness of urban management, including public services, public security, and environmental protection, and to ultimately achieve Sustainable Development Goal (SDG) 11 for making cities inclusive, safe, resilient, and sustainable. Big data have been identified as a key enabler in the development of smart cities. However, our understanding of how different data sources should be managed and integrated remains limited. By analyzing data applications in the development of a sustainable smart city, this case study identified three phases of development, each requiring a different approach to orchestrating diverse data sources. A framework identifying the phases, data-related issues, data orchestration and its interaction with other resources, focal capabilities, and development approaches is developed. This study benefits both researchers and practitioners by making theoretical contributions and by offering practical insights in the fields of smart cities and big data.}
}
@article{LIU2022103622,
title = {scenario modeling for government big data governance decision-making: Chinese experience with public safety services},
journal = {Information & Management},
volume = {59},
number = {3},
pages = {103622},
year = {2022},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2022.103622},
url = {https://www.sciencedirect.com/science/article/pii/S0378720622000349},
author = {Zhao-ge LIU and Xiang-yang LI and Xiao-han ZHU},
keywords = {Government big data governance, Scenario-based decision-making, Scenario modeling, Model-driven, Data link network, Public safety services},
abstract = {In the public safety service context, government big data governance (GBDG) is a challenging decision-making problem that encompasses uncertainties in the arenas of big data and its complex links. Modeling and collaborating the key scenario information required for GBDG decision-making can minimize system uncertainties. However, existing scenario-building methods are limited by their rigidity as they are employed in various application contexts and the associated high costs of modeling. In this paper, using a design science paradigm, a model-driven scenario modeling approach is proposed to achieve flexible scenario modeling for various applications through the transfer of generic domain knowledge. The key component of the proposed approach is a scenario meta-model that is built from existing literatures and practices by integrating qualitative, quantitative, and meta-modeling analysis. An instantiation mechanism of the scenario meta-model is also proposed to generate customized scenarios under Antecedent-Behavior-Consequence (ABC) theory. Two real-world safety service cases in Wuhan, China were evaluated to find that the proposed approach reduces GBDG decision-making uncertainties significantly by providing key information for GBDG problem identification, solution design, and solution value perception. This scenario-building approach can be further used to develop other GBDG systems for public safety services with reduced uncertainties and complete decision-making functions.}
}
@article{KOZIARA202184,
title = {Introduction to Big Data in trauma and orthopaedics},
journal = {Orthopaedics and Trauma},
volume = {35},
number = {2},
pages = {84-89},
year = {2021},
issn = {1877-1327},
doi = {https://doi.org/10.1016/j.mporth.2021.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S187713272100004X},
author = {Michal Koziara and Andrew Gaukroger and Caroline Hing and Will Eardley},
keywords = {Big Data, clinical database, GDPR, healthcare, orthopaedics, privacy},
abstract = {The enormous amount of data created daily within healthcare has become so complex, that it cannot be effectively handled by routine analytical methods. Such large data sets can be processed, looking for correlation not otherwise obvious in smaller patient samples. Further advances in terms of data processing as well as significant infrastructure and personnel investments are required to fully reap the benefits of Big Data, in terms of research and financial sense. However, despite its popularity and promise, Big Data in orthopaedics has attracted a number of criticisms, not only in terms of data input and processing, but particularly with regards to analysis of the output, which are explored within the article. Moreover, use of Big Data within healthcare carries the ethical question of privacy and consent.}
}
@incollection{SEBASTIANCOLEMAN20223,
title = {Chapter 1 - The Importance of Data Quality Management},
editor = {Laura Sebastian-Coleman},
booktitle = {Meeting the Challenges of Data Quality Management},
publisher = {Academic Press},
pages = {3-30},
year = {2022},
isbn = {978-0-12-821737-5},
doi = {https://doi.org/10.1016/B978-0-12-821737-5.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128217375000018},
author = {Laura Sebastian-Coleman},
keywords = {Quality management, process management, data quality management, data governance, costs of poor quality, big data, digital transformation, data privacy, data monetization},
abstract = {This chapter analyzes the role of data quality management in response to the rapid evolution of data in our world. It discusses the impact of poor-quality data on organizations, focusing on the costs and risks associated with poorly managed data. In many organizations, poor-quality data is tolerated to a degree that poor-quality products would not be. Data quality management reduces the costs and risks of poor-quality data and enables the benefits and opportunities of high-quality data, especially in an age of big data, digital transformation, and artificial intelligence.}
}
@article{LEI2021101570,
title = {Modelling and analysis of big data platform group adoption behaviour based on social network analysis},
journal = {Technology in Society},
volume = {65},
pages = {101570},
year = {2021},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2021.101570},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X21000452},
author = {Zhimei Lei and Yandan Chen and Ming K. Lim},
keywords = {Big data, Platforms, Technology adoption, Corporate group behaviour, Social network analysis},
abstract = {Due to the importance of big data technology in decision-making, production and service provision, enterprises have adopted various big data technologies and platforms to improve their operational efficiency. However, the number of enterprises that have adopted big data is not promising. The purpose of this study is to explore the current status of big data adoption by Chinese enterprises and to reveal the possible factors that hinder big data adoption from the group behaviour network perspective. Based on a real case survey of 54 big data platforms (BDPs), four types of networks—i.e., the enterprise-platform network, enterprise network, platform network and industry similarity and difference (ISD) network—are constructed and analysed on the basis of social network analysis (SNA). This study finds that among Chinese enterprises, the level and scope of big data adoption are generally low and are imbalanced among industries; the cognitive level and adoption behaviour of enterprises on BDPs are inconsistent, the compatibility of BDPs is different, and the density and distance-based cohesion of networks are weak; although the current big data adoption behaviours of Chinese enterprises have formed some structural features, core-periphery structures and maximal complete cliques are found, and the current network structure has little impact on individual enterprises and platforms; enterprises in the same industry prefer to adopt the same kind of big data technology or platform. Based on these findings, several strategies and suggestions to improve big data adoption are provided.}
}
@article{YOUSSEF2022102827,
title = {Cross-national differences in big data analytics adoption in the retail industry},
journal = {Journal of Retailing and Consumer Services},
volume = {64},
pages = {102827},
year = {2022},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2021.102827},
url = {https://www.sciencedirect.com/science/article/pii/S0969698921003933},
author = {Mayada Abd El-Aziz Youssef and Riyad Eid and Gomaa Agag},
keywords = {Big data analytics, Technology adoption, Diffusion of innovations model, Cross-national differences, Retail industry},
abstract = {Big data analytics (BDA) has emerged as a significant area of research for both researchers and practitioners in the retail industry, indicating the importance and influence of solving data-related problems in contemporary business organization. The present study utilised a quantitative-methods approach to investigate factors affecting retailers' adoption of BDA across three countries. A survey questionnaire was used to collect data from managers and decision-makers in the retail industry. Data of 2278 respondents were analysed through structural equation modelling. The findings revealed that security concerns, external support, top management support, and rational decision making culture have a greater effect on BDA adoption in developed countries UK than in UAE and Egypt. However, competition intensity and firm size have a greater effect on BDA adoption in UAE and Egypt than in UK. Finally, human variables (competence of information system's staff and staff's information system knowledge) have a greater effect on BDA adoption in Egypt than UK and UAE. The findings indicate that a “one-size-fits-all” approach is insufficient in capturing the heterogeneity of managers across countries. Implications for practice and theory were demonstrated.}
}
@article{LACAM2021100406,
title = {Big data and Smart data: two interdependent and synergistic digital policies within a virtuous data exploitation loop},
journal = {The Journal of High Technology Management Research},
volume = {32},
number = {1},
pages = {100406},
year = {2021},
issn = {1047-8310},
doi = {https://doi.org/10.1016/j.hitech.2021.100406},
url = {https://www.sciencedirect.com/science/article/pii/S1047831021000031},
author = {Jean-Sébastien Lacam and David Salvetat},
keywords = {Big data, Smart data, Volume, Velocity, Variety, Automotive distribution},
abstract = {This research examines for the first time the relationship between Big data and Smart data among French automotive distributors. Many low-tech firms engage in these data policies to improve their decisions and performance through the predictive capacities of their data. A discussion emerges in the literature according to which an effective policy lies in the conversion of a mass of raw data into so-called intelligent data. In order to understand better this digital transition, we question the transformation of data policies practiced in low-tech firms through the founding model of 3Vs (Volume, Variety and Velocity of data). First of all, this empirical study of 112 French automotive distributors develops the existing literature by proposing an original and detailed typology of the data policies practiced (Low data, Big data and Smart data). Secondly, after specifying the elements of the differences between the quantitative nature of Big data and the qualitative nature of Smart data, our results reveal and analyse for the first time the existence of their synergistic relationship. Companies transform their Big data approach into Smart data when they move from massive exploitation to intelligent exploitation of their data. The phenomenon is part of a high-end loop data exploitation. Initially, the exploitation of intelligent data can only be done by extracting a sample from a large raw data pool previously made by a Big data policy. Secondly, the organization's raw data pool is in turn enriched by the repayment of contributions made by the Smart data approach. Thus, this study develops three important ways. First off, we identify, detail and compare the current data policies of a traditional industry. Secondly, we reveal and explain the evolution of digital practices within organizations that now combine both quantitative and qualitative data exploitation. Finally, our results guide decision-makers towards the synergistic and the legitimate association of different forms of data management for better performance.}
}
@article{LEEPOST2019113135,
title = {Numerical, secondary Big Data quality issues, quality threshold establishment, & guidelines for journal policy development},
journal = {Decision Support Systems},
volume = {126},
pages = {113135},
year = {2019},
note = {Perspectives on Numerical Data Quality in IS Research},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2019.113135},
url = {https://www.sciencedirect.com/science/article/pii/S0167923619301642},
author = {Anita Lee-Post and Ram Pakath},
keywords = {Data quality, Big data, Secondary data, Numerical data, Quality threshold},
abstract = {An IS researcher may obtain Big Data from primary or secondary data sources. Sometimes, acquiring primary Big Data is infeasible due to availability, accessibility, cost, time, and/or complexity considerations. In this paper, we focus on Big Data-based IS research and discuss ways in which one may, post hoc, establish quality thresholds for numerical Big Data obtained from secondary sources. We also present guidelines for developing journal policies aimed at ensuring the veracity and verifiability of such data when used for research purposes.}
}
@article{DEEPA2022209,
title = {A survey on blockchain for big data: Approaches, opportunities, and future directions},
journal = {Future Generation Computer Systems},
volume = {131},
pages = {209-226},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000243},
author = {N. Deepa and Quoc-Viet Pham and Dinh C. Nguyen and Sweta Bhattacharya and B. Prabadevi and Thippa Reddy Gadekallu and Praveen Kumar Reddy Maddikunta and Fang Fang and Pubudu N. Pathirana},
keywords = {Blockchain, Big data, Vertical applications, Smart city, Smart healthcare, Smart transportation, Security},
abstract = {Big data has generated strong interest in various scientific and engineering domains over the last few years. Despite many advantages and applications, there are many challenges in big data to be tackled for better quality of service, e.g., big data analytics, big data management, and big data privacy and security. Blockchain with its decentralization and security nature has the great potential to improve big data services and applications. In this article, we provide a comprehensive survey on blockchain for big data, focusing on up-to-date approaches, opportunities, and future directions. First, we present a brief overview of blockchain and big data as well as the motivation behind their integration. Next, we survey various blockchain services for big data, including blockchain for secure big data acquisition, data storage, data analytics, and data privacy preservation. Then, we review the state-of-the-art studies on the use of blockchain for big data applications in different domains such as smart city, smart healthcare, smart transportation, and smart grid. For a better understanding, some representative blockchain-big data projects are also presented and analyzed. Finally, challenges and future directions are discussed to further drive research in this promising area.}
}
@article{KASTOUNI2020,
title = {Big data analytics in telecommunications: Governance, architecture and use cases},
journal = {Journal of King Saud University - Computer and Information Sciences},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S131915782030553X},
author = {Mohamed Zouheir Kastouni and Ayoub {Ait Lahcen}},
keywords = {Big data analytics, Big data project’s governance methodology, Big data architecture, Data governance methodology, Big data project’s team, Big data telecommunications use cases},
abstract = {With the upsurge of data traffic due to the change in customer behavior towards the use of telecommunications services, fostered by the current global health situation (mainly due to Covid-19), the telecommunications operators have a golden opportunity to create new sources of revenues using Big Data Analytics (BDA) solutions. Looking to setting up a BDA project, we faced several challenges, notably, in terms of choice of the technical solution from the plethora of the existing tools, and the choice of the governance methodologies for governing the project and the data. The majority of research documents related to the telecommunications industry have not addressed BDA project implementation from start to finish. The purpose of this study focuses on a BDA telecommunications project, namely, Project’s Governance, Architecture, Data Governance and the BDA Project’s Team. The last part of this study presents useful BDA use cases, in terms of applications enabling revenue creation and cost optimization. It appears that this work will facilitate the implementation of BDA projects, and enable telecommunications operators to have a better understanding about the fundamental aspects to be focused on. It is therefore, a study that will contribute positively toward such goal.}
}
@article{GHALLAB2020131,
title = {Detection outliers on internet of things using big data technology},
journal = {Egyptian Informatics Journal},
volume = {21},
number = {3},
pages = {131-138},
year = {2020},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2019.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110866519301616},
author = {Haitham Ghallab and Hanan Fahmy and Mona Nasr},
keywords = {Internet of things, IoT, Big data, Data quality, Outliers Detection, DBSCAN, RDDs},
abstract = {Internet of Things (IoT) is a fundamental concept of a new technology that will be promising and significant in various fields. IoT is a vision that allows things or objects equipped with sensors, actuators, and processors to talk and communicate with each other over the internet to achieve a meaningful goal. Unfortunately, one of the major challenges that affect IoT is data quality and uncertainty, as data volume increases noise, inconsistency and redundancy increases within data and causes paramount issues for IoT technologies. And since IoT is considered to be a massive quantity of heterogeneous networked embedded devices that generate big data, then it is very complex to compute and analyze such massive data. So this paper introduces a new model named NRDD-DBSCAN based on DBSCAN algorithm and using resilient distributed datasets (RDDs) to detect outliers that affect the data quality of IoT technologies. NRDD-DBSCAN has been applied on three different datasets of N-dimensions (2-D, 3-D, and 25-D) and the results were promising. Finally, comparisons have been made between NRDD-DBSCAN and previous models such as RDD-DBSCAN model and DBSCAN algorithm, and these comparisons proved that NRDD-DBSCAN solved the low dimensionality issue of RDD-DBSCAN model and also solved the fact that DBSCAN algorithm cannot handle IoT data. So the conclusion is that NRDD-DBSCAN proposed model can detect the outliers that exist in the datasets of N-dimensions by using resilient distributed datasets (RDDs), and NRDD-DBSCAN can enhance the quality of data exists in IoT applications and technologies.}
}
@article{BERGIER2021102864,
title = {Digital health, big data and smart technologies for the care of patients with systemic autoimmune diseases: Where do we stand?},
journal = {Autoimmunity Reviews},
volume = {20},
number = {8},
pages = {102864},
year = {2021},
issn = {1568-9972},
doi = {https://doi.org/10.1016/j.autrev.2021.102864},
url = {https://www.sciencedirect.com/science/article/pii/S1568997221001361},
author = {Hugo Bergier and Loïc Duron and Christelle Sordet and Lou Kawka and Aurélien Schlencker and François Chasset and Laurent Arnaud},
keywords = {Autoimmune diseases, Digital technology, Big data, Delivery of health care, Telemedicine},
abstract = {The past decade has seen tremendous development in digital health, including in innovative new technologies such as Electronic Health Records, telemedicine, virtual visits, wearable technology and sophisticated analytical tools such as artificial intelligence (AI) and machine learning for the deep-integration of big data. In the field of rare connective tissue diseases (rCTDs), these opportunities include increased access to scarce and remote expertise, improved patient monitoring, increased participation and therapeutic adherence, better patient outcomes and patient empowerment. In this review, we discuss opportunities and key-barriers to improve application of digital health technologies in the field of autoimmune diseases. We also describe what could be the fully digital pathway of rCTD patients. Smart technologies can be used to provide real-world evidence about the natural history of rCTDs, to determine real-life drug utilization, advanced efficacy and safety data for rare diseases and highlight significant unmet needs. Yet, digitalization remains one of the most challenging issues faced by rCTD patients, their physicians and healthcare systems. Digital health technologies offer enormous potential to improve autoimmune rCTD care but this potential has so far been largely unrealized due to those significant obstacles. The need for robust assessments of the efficacy, affordability and scalability of AI in the context of digital health is crucial to improve the care of patients with rare autoimmune diseases.}
}
@article{PICCAROZZI20221746,
title = {The role of Big Data in the business challenge of Covid-19: a systematic literature review in managerial studies},
journal = {Procedia Computer Science},
volume = {200},
pages = {1746-1755},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.375},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003842},
author = {Michela Piccarozzi and Barbara Aquilani},
keywords = {Big Data, Covid-19, systematic literature review, management},
abstract = {2020 was globally greatly affected by the Covid-19 pandemic caused by SARS-CoV-2, which is still today impacting and profoundly changing life globally for people but also for firms. In this context, the need for timely and accurate information has become vital in every area of business management. The spread of the Covid-19 global pandemic has generated an exponential increase and extraordinary volume of data. In this domain, Big Data is one of the digital innovation technologies that can support business organizations during these complex times. Based on these considerations, the aim of this paper is to analyze the managerial literature concerning the issue of Big Data in the management of the Covid-19 pandemic through a systematic literature review. The results show a fundamental role of Big Data in pandemic management for businesses. The paper also provides managerial and theoretical implications.}
}
@article{RIDZUAN2022685,
title = {Diagnostic analysis for outlier detection in big data analytics},
journal = {Procedia Computer Science},
volume = {197},
pages = {685-692},
year = {2022},
note = {Sixth Information Systems International Conference (ISICO 2021)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921024133},
author = {Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}},
keywords = {Big data, data quality, outlier, Sustainable Development Goals},
abstract = {Recently, Big Data analytics has been one of the most emerging topics in the business field. Data is collected, processed and analyzed to gain useful insight for their organization. Big Data analytics has the potential to improve the quality of life and help to achieve Sustainable Development Goals (SDG). To ensure that SDG goals are achieved, we must utilize existing data to meet those targets and ensure accountability. However, data quality is often left out when dealing with data. Any types of errors presented in the dataset should be properly addressed to ensure the analysis provided is accurate and truthful. In this paper, we have addressed the concept of data quality diagnosis to identify the outlier presented in the dataset. The cause of the outlier is further discussed to identify potential improvements that can be done to the dataset. In addition, recommendations to improve the quality of data and data collection systems are provided.}
}
@article{SFAXI2020101862,
title = {DECIDE: An Agile event-and-data driven design methodology for decisional Big Data projects},
journal = {Data & Knowledge Engineering},
volume = {130},
pages = {101862},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2020.101862},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X19303830},
author = {Lilia Sfaxi and Mohamed Mehdi Ben Aissa},
keywords = {Big Data, Methodology, Decisional systems, Agile, Data governance, Data quality},
abstract = {Decision making is the lifeblood of the enterprise — from the mundane to the strategically critical. However, the increasing deluge of data makes it more important than ever to understand and use it effectively in every context. Being “data driven” is more aspiration than reality in most organizations due to the complexity, volume, variability and velocity of data streams from every customer and employee interaction. The purpose of this paper is to provide a flexible and adaptable methodology for governing, managing and applying data throughout the enterprise, called DECIDE.}
}
@article{GOH2022,
title = {Are batch effects still relevant in the age of big data?},
journal = {Trends in Biotechnology},
year = {2022},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2022.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167779922000361},
author = {Wilson Wen Bin Goh and Chern Han Yong and Limsoon Wong},
keywords = {artificial intelligence, batch effect, machine learning, RNA sequencing, single cell},
abstract = {Batch effects (BEs) are technical biases that may confound analysis of high-throughput biotechnological data. BEs are complex and effective mitigation is highly context-dependent. In particular, the advent of high-resolution technologies such as single-cell RNA sequencing presents new challenges. We first cover how BE modeling differs between traditional datasets and the new data landscape. We also discuss new approaches for measuring and mitigating BEs, including whether a BE is significant enough to warrant correction. Even with the advent of machine learning and artificial intelligence, the increased complexity of next-generation biotechnological data means increased complexities in BE management. We forecast that BEs will not only remain relevant in the age of big data but will become even more important.}
}
@incollection{BIRKIN2020303,
title = {Big Data},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {303-311},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10616-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008102295510616X},
author = {Mark Birkin},
keywords = {Administrative data, Crowdsourcing, Data ethics, Data quality, Data sharing, Representation and bias, Social media, Spatial analysis, Value, Variety, Velocity, Volume, Volunteered geographical information},
abstract = {Big Data are deeply impactful for research in providing a diverse and rich array of novel sources for academic enquiry. Geographers are benefitting from varied data types including administrative data, social media, volunteered geographic information, and consumer data. The article presents examples and illustrations associated with each of these data types. An additional benefit of Big Data analytics is that it brings geographers into closer contact with real world partners and policy problems. Big Data also attract challenges including representational bias, variable data quality, difficulties of access and ownership, and ethical and legal restrictions in their use. Hence we conclude that Big Data are ripe for fruitful exploitation, but careful investments will be necessary if opportunities are to be realized to the full.}
}
@article{LI2021102064,
title = {Big data driven vehicle battery management method: A novel cyber-physical system perspective},
journal = {Journal of Energy Storage},
volume = {33},
pages = {102064},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2020.102064},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X20318971},
author = {Shuangqi Li and Pengfei Zhao},
keywords = {Electric vehicles, Battery energy storage, Cyber-physical battery management system, Big data, Deep learning},
abstract = {The establishment of an accurate battery model is of great significance to improve the reliability of electric vehicles (EVs). However, the battery is a complex electrochemical system with hardly observable and simulatable internal chemical reactions, and it is challenging to estimate the state of battery accurately. This paper proposes a novel flexible and reliable battery management method based on the battery big data platform and Cyber-Physical System (CPS) technology. First of all, to integrate the battery big data resources in the cloud, a Cyber-physical battery management framework is defined and served as the basic data platform for battery modeling issues. And to improve the quality of the collected battery data in the database, this work reports the first attempt to develop an adaptive data cleaning method for the cloud battery management issue. Furthermore, a deep learning algorithm-based feature extraction model, as well as a feature-oriented battery modeling method, is developed to mitigate the under-fitting problem and improve the accuracy of the cloud-based battery model. The actual operation data of electric buses is used to validate the proposed methodologies. The maximum data restoring error can be limited within 1.3% in the experiments, which indicates that the proposed data cleaning method is able to improve the cloud battery data quality effectively. Meanwhile, the maximum SoC estimation error in the proposed feature-oriented battery modeling method is within 2.47%, which highlights the effectiveness of the proposed method.}
}
@article{CAISSIE2022100925,
title = {Head and Neck Radiotherapy Patterns of Practice Variability Identified as a Challenge to Real-World Big Data: results from the Learning from Analysis of Multicentre Big Data Aggregation (LAMBDA) Consortium},
journal = {Advances in Radiation Oncology},
pages = {100925},
year = {2022},
issn = {2452-1094},
doi = {https://doi.org/10.1016/j.adro.2022.100925},
url = {https://www.sciencedirect.com/science/article/pii/S245210942200032X},
author = {Amanda Caissie and Michelle Mierzwa and Clifton David Fuller and Murali Rajaraman and Alex Lin and Andrew MacDonald and Richard Popple and Ying Xiao and Lisanne VanDijk and Peter Balter and Helen Fong and Heping Xu and Matthew Kovoor and Joonsang Lee and Arvind Rao and Mary Martel and Reid Thompson and Brandon Merz and John Yao and Charles Mayo},
abstract = {Purpose/Objective
Outside of randomized clinical trials, it is difficult to develop clinically relevant evidence-based recommendations for radiotherapy (RT) practice guidelines due to lack of comprehensive real-world data. To address this knowledge gap, we formed the Learning and Analytics from Multicenter Big Data Aggregation (LAMBDA) consortium to cooperatively implement RT data standardization, develop software solutions for data analysis and recommend clinical practice change based on real-world data analyzed. The first phase of this “Big Data” study aimed at characterizing variability in clinical practice patterns of dosimetric data for organs at risk (OAR), that would undermine subsequent use of large scale, electronically aggregated data to characterize associations with outcomes. Evidence from this study was used as the basis for practical recommendations to improve data quality.
Materials/Methods
Dosimetric details of patients with H&N cancer treated with RT between 2014 and 2019 were analyzed. Institutional patterns of practice were characterized including structure nomenclature, volumes and frequency of contouring. Dose volume histogram (DVH) distributions were characterized and compared to institutional constraints and literature values.
Results
Plans for 4664 patients treated to a mean plan dose of 64.4 ± 13.2 Gy in 32 ± 4 fractions were aggregated. Prior to implementation of TG263 guidelines in each institution, there was variability in OAR nomenclature across institutions and structures. With evidence from this study, we identified a targeted and practical set of recommendations aimed at improving the quality of real-world data.
Conclusion
Quantifying similarities and differences among institutions for OAR structures and DVH metrics is the launching point for next steps to investigate potential relationships between DVH parameters and patient outcomes.}
}
@article{ARDAGNA2021107215,
title = {Big Data Analytics-as-a-Service: Bridging the gap between security experts and data scientists},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107215},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107215},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002081},
author = {Claudio A. Ardagna and Valerio Bellandi and Ernesto Damiani and Michele Bezzi and Cedric Hebert},
keywords = {Artificial intelligence, Big Data Analytics, Machine learning, Security and privacy},
abstract = {We live in an interconnected and pervasive world where huge amount of data are collected every second. Fully exploiting data through advanced analytics, machine learning and artificial intelligence, becomes crucial for businesses, from micro to large enterprises, resulting in a key advantage (or shortcoming) in the global market competition, as well as in a strong market driver for business analytics solutions. This scenario is deeply changing the security landscape, introducing new risks and threats that affect security and privacy of systems, on one side, and safety of users, on the other side. Many domains that can benefit from novel solutions based on data analytics have stringent security requirements to fulfill. The Energy domain’s Smart Grid is a major example of systems at the crossroads of security and data-driven intelligence. The Smart Grid plays a crucial role in modern energy infrastructure. However, it must face two major challenges related to security: managing front-end intelligent devices such as power assets and smart meters securely, and protecting the huge amount of data received from these devices. Starting from these considerations, setting up proper analytics is a complex problem because security controls could have the undesired side effect of decreasing the accuracy of the analytics themselves. This is even more critical when the configuration of security controls is let to the security expert, who often has only basic skills in data science. In this paper, we propose a solution based on the concept of Model-Based Big Data Analytics-as-a-Service (MBDAaaS) that bridges the gap between security experts and data scientists. Our solution acts as a middleware allowing a security expert and a data scientist to collaborate to the deployment of an analytics addressing their needs.}
}
@article{SURBAKTI2020103146,
title = {Factors influencing effective use of big data: A research framework},
journal = {Information & Management},
volume = {57},
number = {1},
pages = {103146},
year = {2020},
note = {Big data and business analytics: A research agenda for realizing business value},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617308649},
author = {Feliks P. Sejahtera Surbakti and Wei Wang and Marta Indulska and Shazia Sadiq},
keywords = {Big data, Effective use, Factors, Framework},
abstract = {Information systems (IS) research has explored “effective use” in a variety of contexts. However, it is yet to specifically consider it in the context of the unique characteristics of big data. Yet, organizations have a high appetite for big data, and there is growing evidence that investments in big data solutions do not always lead to the derivation of intended value. Accordingly, there is a need for rigorous academic guidance on what factors enable effective use of big data. With this paper, we aim to guide IS researchers such that the expansion of the body of knowledge on the effective use of big data can proceed in a structured and systematic manner and can subsequently lead to empirically driven guidance for organizations. Namely, with this paper, we cast a wide net to understand and consolidate from literature the potential factors that can influence the effective use of big data, so they may be further studied. To do so, we first conduct a systematic literature review. Our review identifies 41 factors, which we categorize into 7 themes, namely data quality; data privacy and security and governance; perceived organizational benefit; process management; people aspects; systems, tools, and technologies; and organizational aspects. To explore the existence of these themes in practice, we then analyze 45 published case studies that document insights into how specific companies use big data successfully. Finally, we propose a framework for the study of effective use of big data as a basis for future research. Our contributions aim to guide researchers in establishing the relevance and relationships within the identified themes and factors and are a step toward developing a deeper understanding of effective use of big data.}
}
@article{RIDZUAN2019731,
title = {A Review on Data Cleansing Methods for Big Data},
journal = {Procedia Computer Science},
volume = {161},
pages = {731-738},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.177},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919318885},
author = {Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}},
keywords = {data cleansing, big data, data quality},
abstract = {Massive amounts of data are available for the organization which will influence their business decision. Data collected from the various resources are dirty and this will affect the accuracy of prediction result. Data cleansing offers a better data quality which will be a great help for the organization to make sure their data is ready for the analyzing phase. However, the amount of data collected by the organizations has been increasing every year, which is making most of the existing methods no longer suitable for big data. Data cleansing process mainly consists of identifying the errors, detecting the errors and corrects them. Despite the data need to be analyzed quickly, the data cleansing process is complex and time-consuming in order to make sure the cleansed data have a better quality of data. The importance of domain expert in data cleansing process is undeniable as verification and validation are the main concerns on the cleansed data. This paper reviews the data cleansing process, the challenge of data cleansing for big data and the available data cleansing methods.}
}
@article{LI2020100608,
title = {Network analysis of big data research in tourism},
journal = {Tourism Management Perspectives},
volume = {33},
pages = {100608},
year = {2020},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2019.100608},
url = {https://www.sciencedirect.com/science/article/pii/S2211973619301400},
author = {Xin Li and Rob Law},
keywords = {Big data, Tourism studies, Co-citation analysis, Network analysis, Research trends},
abstract = {This study aims to provide a comprehensive network analysis to understand the current state of big data research in tourism by investigating multi-disciplinary contributions relevant to big data. A comprehensive network analytical method, which includes co-citation, clustering and trend analysis, is applied to systematically analyse publications from 2008 to 2017. Two unique data sets from Web of Science are collected. The first data set focuses on big data research in tourism and hospitality. The second data set involves other disciplines, such as computer science, for a comparison with tourism. Results suggest that applications of social media and user-generated content are gaining momentum, whereas theory-based studies on big data in tourism remain limited. Tourism and other relevant domains have similar concerns with the challenges involved in big data, such as privacy, data quality and appropriate data use. This comparative network analysis has implications for future big data research in tourism.}
}
@article{BRINCH2021539,
title = {Firm-level capabilities towards big data value creation},
journal = {Journal of Business Research},
volume = {131},
pages = {539-548},
year = {2021},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.07.036},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320304859},
author = {Morten Brinch and Angappa Gunasekaran and Samuel {Fosso Wamba}},
keywords = {Big data, Supply chain management, Operations management, Value creation, Business analytics, Capabilities},
abstract = {Big data has played an increasingly important role in using data to improve business value. In response to several big data challenges, the purpose of this study is to identify firm-level capabilities required to create value from big data. The adjacent theories of business process management and IT business value underpinned the study, together with an in-depth case study that led to the identification of twenty-four types of capabilities related to IT, process, performance, human, strategic, and organizational practices. The findings confirmed the application of practices and capabilities of adjacent theories, as well as certain practices and attributes that were both changed and reinforced at the intersection of big data. As an outstanding additional support to the extant big data studies, this work empirically confirms and portrays hitherto unexplored capabilities of big data and set their roles, thus providing a holistic overview of firm-level capabilities that are required for big data value creation.}
}
@article{YE2021106293,
title = {Management of medical and health big data based on integrated learning-based health care system: A review and comparative analysis},
journal = {Computer Methods and Programs in Biomedicine},
volume = {209},
pages = {106293},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2021.106293},
url = {https://www.sciencedirect.com/science/article/pii/S0169260721003679},
author = {Yuguang Ye and Jianshe Shi and Daxin Zhu and Lianta Su and Jianlong Huang and Yifeng Huang},
keywords = {Integrated learning, Health care system, Elaboration Likelihood Machine, System design, Medical big data, Internet of Medical Things},
abstract = {Purpose
We present a Health Care System (HCS) based on integrated learning to achieve high-efficiency and high-precision integration of medical and health big data, and compared it with an internet-based integrated system.
Method
The method proposed in this paper adopts the Bagging integrated learning method and the Extreme Learning Machine (ELM) prediction model to obtain a high-precision strong learning model. In order to verify the integration efficiency of the system, we compare it with the Internet-based health big data integration system in terms of integration volume, integration efficiency, and storage space capacity.
Results
The HCS based on integrated learning relies on the Internet in terms of integration volume, integration efficiency, and storage space capacity. The amount of integration is proportional to the time and the integration time is between 170-450 ms, which is only half of the comparison system; whereby the storage space capacity reaches 8.3×28TB.
Conclusion
The experimental results show that the integrated learning-based HCS integrates medical and health big data with high integration volume and integration efficiency, and has high space storage capacity and concurrent data processing performance.}
}
@article{SELLAMI2020102732,
title = {On the use of big data frameworks for big service composition},
journal = {Journal of Network and Computer Applications},
volume = {166},
pages = {102732},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2020.102732},
url = {https://www.sciencedirect.com/science/article/pii/S108480452030206X},
author = {Mokhtar Sellami and Haithem Mezni and Mohand Said Hacid},
keywords = {Big data, Big service, Big service composition, Quality of big services, Fuzzy RCA, Spark},
abstract = {Over the last years, big data has emerged as a new paradigm for the processing and analysis of massive volumes of data. Big data processing has been combined with service and cloud computing, leading to a new class of services called “Big Services”. In this new model, services can be seen as an abstract layer that hides the complexity of the processed big data. To meet users' complex and heterogeneous needs in the era of big data, service reuse is a natural and efficient means that helps orchestrating available services' operations, to provide customer on-demand big services. However different from traditional Web service composition, composing big services refers to the reuse of, not only existing high-quality services, but also high-quality data sources, while taking into account their security constraints (e.g., data provenance, threat level and data leakage). Moreover, composing heterogeneous and large-scale data-centric services faces several challenges, apart from security risks, such as the big services' high execution time and the incompatibility between providers' policies across multiple domains and clouds. Aiming to solve the above issues, we propose a scalable approach for big service composition, which considers not only the quality of reused services (QoS), but also the quality of their consumed data sources (QoD). Since the correct representation of big services requirements is the first step towards an effective composition, we first propose a quality model for big services and we quantify the data breaches using L-Severity metrics. Then to facilitate processing and mining big services' related information during composition, we exploit the strong mathematical foundation of fuzzy Relational Concept Analysis (fuzzy RCA) to build the big services' repository as a lattice family. We also used fuzzy RCA to cluster services and data sources based on various criteria, including their quality levels, their domains, and the relationships between them. Finally, we define algorithms that parse the lattice family to select and compose high-quality and secure big services in a parallel fashion. The proposed method, which is implemented on top of Spark big data framework, is compared with two existing approaches, and experimental studies proved the effectiveness of our big service composition approach in terms of QoD-aware composition, scalability, and security breaches.}
}
@incollection{BUDNITZ2021665,
title = {Transport Modes and Big Data},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {665-670},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10601-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026717106013},
author = {Hannah D Budnitz and Emmanouil Tranos and Lee Chapman},
keywords = {Application programming interfaces, Automation, Big data, Crowd-sourced, Digitization, Geolocation, Information and communication technology, Intelligent transport systems, Internet of things, Location-based services, Mobility as a service, Real time information systems, Timestamp, Vehicle telematics},
abstract = {Transport modes and big data considers the characteristics of “Big Data” as described by the 5 “Vs,” Volume, Velocity, Variety, Veracity, and Value. It reviews the many sources of big data within the transport sector by modal group, and the sources of big data from the Information and Communication Technology (ICT) sector that may be applied to the understanding of transport. It briefly considers the uses, advantages, and disadvantages of these data sources, and the challenges to analyzing and interpreting them. It concludes that Big Data is necessary to prepare for the uncertain future of transport, but recognizes the challenges to applying it accurately and effectively to transport problems and policies.}
}
@article{LI2022101021,
title = {A review of industrial big data for decision making in intelligent manufacturing},
journal = {Engineering Science and Technology, an International Journal},
volume = {29},
pages = {101021},
year = {2022},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2215098621001336},
author = {Chunquan Li and Yaqiong Chen and Yuling Shang},
keywords = {Intelligent manufacturing, Artificial intelligence, Industrial big data, Big data-driven technology, Decision-making},
abstract = {Under the trend of economic globalization, intelligent manufacturing has attracted a lot of attention from academic and industry. Related enabling technologies make manufacturing industry more intelligent. As one of the key technologies in artificial intelligence, big data driven analysis improves the market competitiveness of manufacturing industry by mining the hidden knowledge value and potential ability of industrial big data, and helps enterprise leaders make wise decisions in various complex manufacturing environments. This paper provides a theoretical analysis basis for big data-driven technology to guide decision-making in intelligent manufacturing, fully demonstrating the practicability of big data-driven technology in the intelligent manufacturing industry, including key advantages and internal motivation. A conceptual framework of intelligent decision-making based on industrial big data-driven technology is proposed in this study, which provides valuable insights and thoughts for the severe challenges and future research directions in this field.}
}
@article{KONG2020123142,
title = {A systematic review of big data-based urban sustainability research: State-of-the-science and future directions},
journal = {Journal of Cleaner Production},
volume = {273},
pages = {123142},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123142},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620331875},
author = {Lingqiang Kong and Zhifeng Liu and Jianguo Wu},
keywords = {Big data, Social media data, Urban landscape sustainability, Smart city, Urban planning},
abstract = {The future of humanity depends increasingly on the performance of cities. Big data provide new and powerful ways of studying and improving coupled urban environmental, social, and economic systems to achieve urban sustainability. However, the term big data has been defined variably, and its urban applications have so far been sporadic in terms of research topic and location. A comprehensive review of big data-based urban environment, society, and sustainability (UESS) research is much needed. The aim of this study was to summarize the big data-based UESS research using a systematic review approach in combination with bibliometric and thematic analyses. The results showed that the numbers of publications and citations of related articles have been increasing exponentially in recent years. The most frequently used big data in UESS research are human behavior data, and the major analytical methods are of five types: classification, clustering, regression, association rules, and social network analysis. The major research topics of big data-based UESS research include urban mobility, urban land use and planning, environmental sustainability, public health and safety, social equity, tourism, resources and energy utilization, real estate, and retail, accommodation and catering. Big data benefit UESS research by proving a people-oriented perspective, timely and real-time information, and fine-resolution spatial dynamics. In addition, several obstacles were identified to applying big data in UESS research, which are related to data quality and acquisition, data storage and management, data security and privacy, data cleaning and preprocessing, and data analysis and information mining. To move forward, future research should integrate multiple big data sources, develop and utilize new methods such as deep learning and cloud computing, and expand the application fields to focus on the interactions between human activities and urban environments. This review can contribute to understanding the current situation of big data-based UESS research, and provide a reference for studies of this topic in the future.}
}
@article{LIU202053,
title = {Semantic-aware data quality assessment for image big data},
journal = {Future Generation Computer Systems},
volume = {102},
pages = {53-65},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.063},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19302304},
author = {Yu Liu and Yangtao Wang and Ke Zhou and Yujuan Yang and Yifei Liu},
keywords = {Semantic-aware, Quality assessment, Image big data, IDSTH, SHR},
abstract = {Data quality (DQ) assessment is essential for realizing the promise of big data by judging the value of data in advance. Relevance, an indispensable dimension of DQ, focusing on “fitness for requirement”, can arouse the user’s interest in exploiting the data source. It has two-level evaluations: (1) the amount of data that meets the user’s requirements; (2) the matching degree of these relevant data. However, there lack works of DQ assessment at dimension of relevance, especially for unstructured image data which focus on semantic similarity. When we try to evaluate semantic relevance between an image data source and a query (requirement), there are three challenges: (1) how to extract semantic information with generalization ability for all image data? (2) how to quantify relevance by fusing the quantity of relevant data and the degree of similarity comprehensively? (3) how to improve assessing efficiency of relevance in a big data scenario by design of an effective architecture? To overcome these challenges, we propose a semantic-aware data quality assessment (SDQA) architecture which includes off-line analysis and on-line assessment. In off-line analysis, for an image data source, we first transform all images into hash codes using our improved Deep Self-taught Hashing (IDSTH) algorithm which can extract semantic features with generalization ability, then construct a graph using hash codes and restricted Hamming distance, next use our designed Semantic Hash Ranking (SHR) algorithm to calculate the importance score (rank) for each node (image), which takes both the quantity of relevant images and the degree of semantic similarity into consideration, and finally rank all images in descending order of score. During on-line assessment, we first convert the user’s query into hash codes using IDSTH model, then retrieve matched images to collate their importance scores, and finally help the user determine whether the image data source is fit for his requirement. The results on public dataset and real-world dataset show effectiveness, superiority and on-line efficiency of our SDQA architecture.}
}
@article{LI2022121355,
title = {Evaluating the impact of big data analytics usage on the decision-making quality of organizations},
journal = {Technological Forecasting and Social Change},
volume = {175},
pages = {121355},
year = {2022},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121355},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521007861},
author = {Lei Li and Jiabao Lin and Ye Ouyang and Xin (Robert) Luo},
keywords = {Big data analytics usage, Data analytics capabilities, Decision-making quality, Agricultural firms},
abstract = {Big data initiatives are critical for transforming traditional organizational decision making into data-driven decision making. However, prior information systems research has not paid enough attention to the impact of big data analytics usage on decision-making quality. Drawing on the dynamic capability theory, this study investigated the impact of big data analytics usage on decision-making quality and tested the mediating effect of data analytics capabilities. We collected data from 240 agricultural firms in China. The empirical results showed that big data analytics usage had a positive impact on decision-making quality and that data analytics capabilities played a mediating role in the relationship between big data analytics usage and decision-making quality. Hence, firms should not only popularize big data analytics usage in their business activities but also take measures to improve their data analytics capabilities, which will improve their decision-making quality toward competitive advantages.}
}
@article{CORALLO2022102331,
title = {Model-based Big Data Analytics-as-a-Service framework in smart manufacturing: A case study},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {76},
pages = {102331},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102331},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000205},
author = {Angelo Corallo and Anna Maria Crespino and Mariangela Lazoi and Marianna Lezzi},
keywords = {Big Data Analytics, BDA, MBDAaaS framework, Smart manufacturing, Industry 4.0, Anomaly detection},
abstract = {Today, in a smart manufacturing environment based on the Industry 4.0 paradigm, people, technological infrastructure and machinery equipped with sensors can constantly generate and communicate a huge volume of data, also known as Big Data. The manufacturing industry takes advantage of Big Data and analytics evolution by improving its capability to bring out valuable information and knowledge from industrial processes, production systems and sensors. The adoption of model-based frameworks in the Big Data Analytics pipeline can better address user configuration requirements (e.g. type of analysis to perform, type of algorithm to be applied) and also provide more transparency and clearness on the execution of workflows and data processing. In the current state of art, an application of a model-based framework in a manufacturing scenario is missing. Therefore, in this study, by means of a case study research focused on data from sensors associated with Computer Numerical Control machines, the configuration and execution of a Big Data Analytics pipeline with a Model-based Big Data Analytics-as-a-Service framework is described. The case study provides to theoreticians and managerial experts useful evidence for managing real-time data analytics and deploying a workflow that addresses specific analytical goals, driven by user requirements and developer models, in a complex manufacturing domain.}
}
@article{NIKOLOV2021100440,
title = {Conceptualization and scalable execution of big data workflows using domain-specific languages and software containers},
journal = {Internet of Things},
volume = {16},
pages = {100440},
year = {2021},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2021.100440},
url = {https://www.sciencedirect.com/science/article/pii/S2542660521000834},
author = {Nikolay Nikolov and Yared Dejene Dessalk and Akif Quddus Khan and Ahmet Soylu and Mihhail Matskin and Amir H. Payberah and Dumitru Roman},
keywords = {Big data workflows, Internet of Things, Domain-specific languages, Software containers},
abstract = {Big Data processing, especially with the increasing proliferation of Internet of Things (IoT) technologies and convergence of IoT, edge and cloud computing technologies, involves handling massive and complex data sets on heterogeneous resources and incorporating different tools, frameworks, and processes to help organizations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, requires taking advantage of Cloud infrastructures’ elasticity for scalability. In this article, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies, message-oriented middleware (MOM), and a domain-specific language (DSL) to enable highly scalable workflow execution and abstract workflow definition. We demonstrate our system in a use case and a set of experiments that show the practical applicability of the proposed approach for the specification and scalable execution of Big Data workflows. Furthermore, we compare our proposed approach’s scalability with that of Argo Workflows – one of the most prominent tools in the area of Big Data workflows – and provide a qualitative evaluation of the proposed DSL and overall approach with respect to the existing literature.}
}
@article{SHEN2022102529,
title = {Personal big data pricing method based on differential privacy},
journal = {Computers & Security},
volume = {113},
pages = {102529},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102529},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821003539},
author = {Yuncheng Shen and Bing Guo and Yan Shen and Xuliang Duan and Xiangqian Dong and Hong Zhang and Chuanwu Zhang and Yuming Jiang},
keywords = {Personal big data, Data privacy, Privacy protection, Differential privacy, Positive pricing, Reverse pricing, Privacy budget, Privacy compensation},
abstract = {Personal big data can greatly promote social management, business applications, and personal services, and bring certain economic benefits to users. The difficulty with personal big data security and privacy protection lies in realizing the maximization of the value of personal big data and in striking a balance between data privacy protection and sharing on the premise of satisfying personal big data security and privacy protection. Thus, in this paper, we propose a personal big data pricing method based on differential privacy (PMDP). We design two different mechanisms of positive and reverse pricing to reasonbly price personal big data. We perform aggregate statistics on an open dataset and extensively evaluated its performance. The experimental results show that PMDP can provide reasonable pricing for personal big data and fair compensation to data owners, ensuring an arbitrage-free condition and finding a balance between privacy protection and data utility.}
}
@article{JOSE2022100081,
title = {Integrating big data and blockchain to manage energy smart grid - TOTEM framework},
journal = {Blockchain: Research and Applications},
pages = {100081},
year = {2022},
issn = {2096-7209},
doi = {https://doi.org/10.1016/j.bcra.2022.100081},
url = {https://www.sciencedirect.com/science/article/pii/S2096720922000227},
author = {Dhanya Therese Jose and Jørgen Holme and Antorweep Chakravorty and Chunming Rong},
keywords = {Blockchain, Big data analytic, Hyperledger fabric, Hadoop, MapReduce, Docker, PIVT},
abstract = {The demand for electricity is increasing exponentially day by day especially with the arrival of electric vehicles. In smart community neighborhood project, it demands electricity should be produced at the household or community level and sell or buy according to the demands. Since the actors can produce, sell and also buy according to the demands, thus the name prosumers. ICT solutions can contribute to this in several ways such as machine learning for analysing the household data for customer demand and peak hour for the usage of electricity, blockchain as a trustworthy platform for selling or buying, data hub, and ensure data security and privacy of prosumers. TOTEM: Token for controlled computation is a framework that allows the users to analyze the data without moving the data from the data owner's environment. It also ensures the data security and privacy of the data. Here in this article, we will show the importance of TOTEM architecture in the EnergiX project and how the extended version of TOTEM can be efficiently merged with the demands of the current and similar projects.}
}