,author,title,keywords,abstract,year,type_publication,doi
0,"Liu, He and Huang, Fupeng and Li, Han and Liu, Weiwei and Wang, Tongxun",A Big Data Framework for Electric Power Data Quality Assessment,Big Data;Data integrity;Power grids;History;Real-time systems;Sensors;data quality;electric power data;data quality assessment;big data;framework,"Since a low-quality data may influence the effectiveness and reliability of applications, data quality is required to be guaranteed. Data quality assessment is considered as the foundation of the promotion of data quality, so it is essential to access the data quality before any other data related activities. In the electric power industry, more and more electric power data is continuously accumulated, and many electric power applications have been developed based on these data. In China, the power grid has many special characteristic, traditional big data assessment frameworks cannot be directly applied. Therefore, a big data framework for electric power data quality assessment is proposed. Based on big data techniques, the framework can accumulate both the real-time data and the history data, provide an integrated computation environment for electric power big data assessment, and support the storage of different types of data.",2017,,10.1109/WISA.2017.29
1,"Taleb, Ikbal and Serhani, Mohamed Adel",Big Data Pre-Processing: Closing the Data Quality Enforcement Loop,Big Data;Optimization;Data models;Quality assessment;Big Data;Data Quality Evaluation;Data Quality Rules Discovery;Big Data Pre-Processing,"In the Big Data Era, data is the core for any governmental, institutional, and private organization. Efforts were geared towards extracting highly valuable insights that cannot happen if data is of poor quality. Therefore, data quality (DQ) is considered as a key element in Big data processing phase. In this stage, low quality data is not penetrated to the Big Data value chain. This paper, addresses the data quality rules discovery (DQR) after the evaluation of quality and prior to Big Data pre-processing. We propose a DQR discovery model to enhance and accurately target the pre-processing activities based on quality requirements. We defined, a set of pre-processing activities associated with data quality dimensions (DQD's) to automatize the DQR generation process. Rules optimization are applied on validated rules to avoid multi-passes pre-processing activities and eliminates duplicate rules. Conducted experiments showed an increased quality scores after applying the discovered and optimized DQR's on data.",2017,,10.1109/BigDataCongress.2017.73
2,"Loetpipatwanich, Sakda and Vichitthamaros, Preecha",Sakdas: A Python Package for Data Profiling and Data Quality Auditing,Data integrity;Pipelines;Data visualization;Big Data;Syntactics;Software;Python;Data Quality Management;Data Profiling;Data Quality Auditing;Python Package;Data Pipeline,"Data Profiling and data quality management become a more significant part of data engineering, which an essential part of ensuring that the system delivers quality information to users. In the last decade, data quality was considered to need more managing. Especially in the big data era that the data comes from many sources, many data types, and an enormous amount. Thus it makes the managing of data quality is more difficult and complicated. The traditional system was unable to respond as needed. The data quality managing software for big data was developed but often found in a high-priced, difficult to customize as needed, and mostly provide as GUI, which is challenging to integrate with other systems. From this problem, we have developed an opensource package for data quality managing. By using Python programming language, Which is a programming language that is widely used in the scientific and engineering field today. Because it is a programming language that is easy to read syntax, small, and has many additional packages to integrate. The software developed here is called “Sakdas” this package has been divided into three parts. The first part deals with data profiling provide a set of data analyses to generate a data profile, and this profile will help to define the data quality rules. The second part deals with data quality auditing that users can set their own data quality rules for data quality measurement. The final part deals with data visualizing that provides data profiling and data auditing report to improve the data quality. The results of the profiling and auditing services, the user can specify both the form of a report for self-review. Or in the form of JSON for use in post-process automation.",2020,,10.1109/IBDAP50342.2020.9245455
3,"Hongxun, Tian and Honggang, Wang and Kun, Zhou and Mingtai, Shi and Haosong, Li and Zhongping, Xu and Taifeng, Kang and Jin, Li and Yaqi, Cai",Data quality assessment for on-line monitoring and measuring system of power quality based on big data and data provenance theory,Data integrity;Power quality;Monitoring;Power measurement;Redundancy;Big Data;Business;power qualiy;data quality;big data;data provenance;data assessment,"Currently, on-line monitoring and measuring system of power quality has accumulated a huge amount of data. In the age of big data, those data integrated from various systems will face big data application problems. This paper proposes a data quality assessment system method for on-line monitoring and measuring system of power quality based on big data and data provenance to assess integrity, redundancy, accuracy, timeliness, intelligence and consistency of data set and single data. Specific assessment rule which conforms to the situation of on-line monitoring and measuring system of power quality will be devised to found data quality problems. Thus it will provide strong data support for big data application of power quality.",2018,,10.1109/ICCCBDA.2018.8386521
4,"Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida",Big Data Quality Assessment Model for Unstructured Data,Big Data;Data integrity;Data mining;Feature extraction;Data models;Measurement;Quality assessment;Big Data;Data Quality;Unstructured Data;Quality of Unstructured Big Data,"Big Data has gained an enormous momentum the past few years because of the tremendous volume of generated and processed Data from diverse application domains. Nowadays, it is estimated that 80% of all the generated data is unstructured. Evaluating the quality of Big data has been identified to be essential to guarantee data quality dimensions including for example completeness, and accuracy. Current initiatives for unstructured data quality evaluation are still under investigations. In this paper, we propose a quality evaluation model to handle quality of Unstructured Big Data (UBD). The later captures and discover first key properties of unstructured big data and its characteristics, provides some comprehensive mechanisms to sample, profile the UBD dataset and extract features and characteristics from heterogeneous data types in different formats. A Data Quality repository manage relationships between Data quality dimensions, quality Metrics, features extraction methods, mining methodologies, data types and data domains. An analysis of the samples provides a data profile of UBD. This profile is extended to a quality profile that contains the quality mapping with selected features for quality assessment. We developed an UBD quality assessment model that handles all the processes from the UBD profiling exploration to the Quality report. The model provides an initial blueprint for quality estimation of unstructured Big data. It also, states a set of quality characteristics and indicators that can be used to outline an initial data quality schema of UBD.",2018,,10.1109/INNOVATIONS.2018.8605945
5,"HongJu, Xiao and Fei, Wang and FenMei, Wang and XiuZhen, Wang",Some key problems of data management in army data engineering based on big data,Data engineering;Data analysis;Data integration;Big Data;Distributed databases;Data models;Uncertainty;army data engineering;data management;data integration;data analysis;representation of data analysis results;data quality,"This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.",2017,,10.1109/ICBDA.2017.8078796
6,"Becker, David and King, Trish Dunn and McMullen, Bill","Big data, big data quality problem",Big data;Biomedical imaging;Personnel;Complexity theory;Bioinformatics;Process control;Instruments;Big Data;Data Quality;Returns to Scale,"A USAF sponsored MITRE research team undertook four separate, domain-specific case studies about Big Data applications. Those case studies were initial investigations into the question of whether or not data quality issues encountered in Big Data collections are substantially different in cause, manifestation, or detection than those data quality issues encountered in more traditionally sized data collections. The study addresses several factors affecting Big Data Quality at multiple levels, including collection, processing, and storage. Though not unexpected, the key findings of this study reinforce that the primary factors affecting Big Data reside in the limitations and complexities involved with handling Big Data while maintaining its integrity. These concerns are of a higher magnitude than the provenance of the data, the processing, and the tools used to prepare, manipulate, and store the data. Data quality is extremely important for all data analytics problems. From the study's findings, the ""truth about Big Data"" is there are no fundamentally new DQ issues in Big Data analytics projects. Some DQ issues exhibit return-s-to-scale effects, and become more or less pronounced in Big Data analytics, though. Big Data Quality varies from one type of Big Data to another and from one Big Data technology to another.",2015,,10.1109/BigData.2015.7364064
7,"Wong, Ka Yee. and Wong, Raymond K.",Big Data Quality Prediction on Banking Applications: Extended Abstract,Big Data;Data models;Banking;Neural networks;Industries;Data integrity;Computational modeling;Big Data;Data Noise;Data Quality;Prediction and Machine Learning,"Big data has been transformed into knowledge by information systems to add value in businesses. Enterprises relying on it benefit from risk management to a certain extent. The value, however, depends on the quality of data. The quality needs to be verified before any use of the data. Specifically, measuring the quality by simulating the real life situation and even forecast it accurately turns into a hot topic. In recent years, there have been numerous researches on the measurement and assessment of data quality. These are yet to utilize a scientific computational method for the measurement and prediction. Current methods either fail to make an accurate prediction or do not consider the correlation and time sequence factors of the data. To address this, we design a model to extend machine learning technique to business applications predicting this. Firstly, we implement the model to detect data noises from a risk dataset according to an international data quality standard from banking industry and then estimate their impacts with Gaussian and Bayesian methods. Secondly, we direct sequential learning in multiple deep neural networks for the prediction with an attention mechanism. The model is experimented with various network methodologies to show the predictive power of machine learning technique and is evaluated by validation data to confirm the model effectiveness. The model is scalable to apply to any industries utilizing big data other than the banking industry.",2020,,10.1109/DSAA49011.2020.00119
8,"Abdallah, Mohammad",Big Data Quality Challenges,Big Data;Quality Measurement;Quality Model;Quality Assurance,"Big Data, is a growing technique these days. There are many uses of Big Data; Artificial Intelligence, Health Care, Business, and many more. For that reason, it becomes necessary to deal with this massive volume of data with caution and care in a term to make sure that the data used and produced is in high quality. Therefore, the Big Data quality is must, and its rules have to be satisfied. In this paper, the main Big Data Quality Factors, which need to be measured, is presented in the perspective of the data itself, the data management, data processing, and data users. This research highlights the quality factors that may be used later to create different Big Data quality models.",2019,,10.1109/ICBDCI.2019.8686099
9,"Taleb, Ikbal and Kassabi, Hadeel T. El and Serhani, Mohamed Adel and Dssouli, Rachida and Bouhaddioui, Chafik",Big Data Quality: A Quality Dimensions Evaluation,Big data;Measurement;Feature extraction;Quality assessment;Social network services;Companies;Context;Big Data;data quality dimensions;data quality evaluation;Big data sampling,"Data is the most valuable asset companies are proud of. When its quality degrades, the consequences are unpredictable, can lead to complete wrong insights. In Big Data context, evaluating the data quality is challenging, must be done prior to any Big data analytics by providing some data quality confidence. Given the huge data size, its fast generation, it requires mechanisms, strategies to evaluate, assess data quality in a fast, efficient way. However, checking the Quality of Big Data is a very costly process if it is applied on the entire data. In this paper, we propose an efficient data quality evaluation scheme by applying sampling strategies on Big data sets. The Sampling will reduce the data size to a representative population samples for fast quality evaluation. The evaluation targeted some data quality dimensions like completeness, consistency. The experimentations have been conducted on Sleep disorder's data set by applying Big data bootstrap sampling techniques. The results showed that the mean quality score of samples is representative for the original data, illustrate the importance of sampling to reduce computing costs when Big data quality evaluation is concerned. We applied the Quality results generated as quality proposals on the original data to increase its quality.",2016,,10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0122
10,"Rao, Dhana and Gudivada, Venkat N and Raghavan, Vijay V.",Data quality issues in big data,Databases;Big data;Measurement;Context;Cleaning;Biology;Computers;Data quality;big data;biological data;information quality,"Though the issues of data quality trace back their origin to the early days of computing, the recent emergence of Big Data has added more dimensions. Furthermore, given the range of Big Data applications, potential consequences of bad data quality can be for more disastrous and widespread. This paper provides a perspective on data quality issues in the Big Data context. it also discusses data integration issues that arise in biological databases and attendant data quality issues.",2015,,10.1109/BigData.2015.7364065
11,"Juneja, Ashish and Das, Nripendra Narayan",Big Data Quality Framework: Pre-Processing Data in Weather Monitoring Application,Big Data;Data integrity;Meteorology;Monitoring;Data mining;Organizations;Big Data;Big Data Quality;Data Quality;preprocessing;pre-processing,"Big Data has become an imminent part of all industries and business sectors today. All organizations in any sector like energy, banking, retail, hardware, networking, etc all generate huge quantum of heterogenous data which if mined, processed and analyzed accurately can reveal immensely useful patterns for business heads to apply to generate and grow their businesses. Big Data helps in acquiring, processing and analyzing large amounts of heterogeneous data to derive valuable results. Quality of information is affected by size, speed and format in which data is generated. Hence, Quality of Big Data is of great relevance and importance. We propose addressing various aspects of the raw data to improve its quality in the pre-processing stage, as the raw data may not usable as-is. We are exploring process like Cleansing to fix as much data as feasible, Noise filters to remove bad data, as well sub-processes for Integration and Filtering along with Data Transformation/Normalization. We evaluate and profile the Big Data during acquisition stage, which is adapted to expectations to avoid cost overheads later while also improving and leading to accurate data analysis. Hence, it is imperative to improve Data quality even it is absorbed and utilized in an industry's Big Data system. In this paper, we propose a Pre-Processing Framework to address quality of data in a weather monitoring and forecasting application that also takes into account global warming parameters and raises alerts/notifications to warn users and scientists in advance.",2019,,10.1109/COMITCon.2019.8862267
12,"Haryadi, Adiska Fardani and Hulstijn, Joris and Wahyudi, Agung and van der Voort, Haiko and Janssen, Marijn",Antecedents of big data quality: An empirical examination in financial service organizations,Big data;Industries;Insurance;Companies;Finance;big data;data quality;big data quality;antecedents;finance,"Big data has been acknowledged for its enormous potential. In contrast to the potential, in a recent survey more than half of financial service organizations reported that big data has not delivered the expected value. One of the main reasons for this is related to data quality. The objective of this research is to identify the antecedents of big data quality in financial institutions. This will help to understand how data quality from big data analysis can be improved. For this, a literature review was performed and data was collected using three case studies, followed by content analysis. The overall findings indicate that there are no fundamentally new data quality issues in big data projects. Nevertheless, the complexity of the issues is higher, which makes it harder to assess and attain data quality in big data projects compared to the traditional projects. Ten antecedents of big data quality were identified encompassing data, technology, people, process and procedure, organization, and external aspects.",2016,,10.1109/BigData.2016.7840595
13,"Li, Mingda and Wang, Hongzhi and Li, Jianzhong",Mining conditional functional dependency rules on big data,Big Data;Training;Data mining;Cleaning;Sampling methods;Heuristic algorithms;Fault tolerance;data mining;conditional functional dependency;big data;data quality,"Current Conditional Functional Dependency (CFD) discovery algorithms always need a well-prepared training dataset. This condition makes them difficult to apply on large and low-quality datasets. To handle the volume issue of big data, we develop the sampling algorithms to obtain a small representative training set. We design the fault-tolerant rule discovery and conflict-resolution algorithms to address the low-quality issue of big data. We also propose parameter selection strategy to ensure the effectiveness of CFD discovery algorithms. Experimental results demonstrate that our method can discover effective CFD rules on billion-tuple data within a reasonable period.",2020,,10.26599/BDMA.2019.9020019
14,"Zhang, Pengcheng and Xiong, Fang and Gao, Jerry and Wang, Jimin","Data quality in big data processing: Issues, solutions and open problems",Big Data;Data integrity;Data analysis;Social network services;Data preprocessing;Internet;Big Data;Big data processing;Data Quality;Recommendation system;Prediction system,"With the rapid development of social networks, Internet of things, Cloud computing as well as other technologies, big data age is arriving. The increasing number of data has brought great value to the public and enterprises. Meanwhile how to manage and use big data better has become the focus of all walks of life. The 4V characteristics of big data have brought a lot of issues to the big data processing. The key to big data processing is to solve data quality issue, and to ensure data quality is a prerequisite for the successful application of big data technique. In this paper, we use recommendation systems and prediction systems as typical big data applications, and try to find out the data quality issues during data collection, data preprocessing, data storage and data analysis stages of big data processing. According to the elaboration and analysis of the proposed issues, the corresponding solutions are also put forward. Finally, some open problems to be solved in the future are also raised.",2017,,10.1109/UIC-ATC.2017.8397554
15,"Arruda, Darlan and Madhavji, Nazim H.",QualiBD: A Tool for Modelling Quality Requirements for Big Data Applications,Tools;Data models;Containers;Software;Real-time systems;Big Data applications;Big Data Applications;Quality Requirements;Big Data Goal-oriented Requirements Language;Requirements Modelling Tool,"The development of Big Data applications is not well-explored, to our knowledge. Embracing Big Data in system building, questions arise as to how to elicit, specify, analyse, model, and document Big Data quality requirements. In our ongoing research, we explore a requirements modelling language for Big Data software applications. In this paper, we introduce QualiBD, a modelling tool that implements the proposed goal-oriented requirements language that facilitates the modelling of Big Data quality requirements.",2019,,10.1109/BigData47090.2019.9006294
16,"Juddoo, Suraj and George, Carlisle",Discovering Most Important Data Quality Dimensions Using Latent Semantic Analysis,Data integrity;Big Data;Semantics;Industries;Bibliographies;Libraries;Production;Big Data;data quality;health data;data quality dimensions;latent semantic analysis,"Big Data quality is a field which is emerging. Many authors nowadays agree that data quality is still very relevant, even for Big Data uses. However, there is a lack of frameworks or guidelines about how to carry out those big data quality initiatives. The starting point of any data quality work is to determine the properties of data quality, termed as data quality dimensions (DQDs). Even those dimensions lack precise rigour in terms of definition from existing literature. This current research aims to contribute towards identifying the most important DQDs for big data in the health industry. It is a continuation of a previous work, which already identified five most important DQDs, using a human judgement based technique known as inner hermeneutic cycle. To remove potential bias coming from the human judgement aspect, this research uses the same set of literature but applies a statistical technique known to extract knowledge from a set of documents known as latent semantic analysis. The results confirm only 2 similar most important DQDs, namely accuracy and completeness.",2018,,10.1109/ICABCD.2018.8465129
17,"Nobles, Alicia L. and Vilankar, Ketki and Wu, Hao and Barnes, Laura E.",Evaluation of data quality of multisite electronic health record data for secondary analysis,Documentation;Data warehouses;Medical services;Big data;Databases;Cleaning;Medical diagnostic imaging;electronic health records;data quality;big data;multiple data vendors;metrics,"Currently, a large amount of data is amassed in electronic health records (EHRs). However, EHR systems are largely information silos, that is, uses of these systems are often confined to management of patient information and analytics specific to a clinician's practice. A growing trend in healthcare is combining multiple databases to support epidemiological research. The College Health Surveillance Network is the first national data warehouse containing EHR data from 31 different student health centers. Each member university contributes to the data warehouse by uploading select EHR data including patient demographics, diagnoses, and procedures to a common server on a monthly basis. In this paper, we focus on the data quality dimensions from a subsample of the data comprised of over 5.7 million patient visits for approximately 980,000 patients with 4,465 unique diagnoses from 23 of those universities. We examine the data for measures of completeness, consistency, and availability for secondary use for epidemiological research. Additionally, clinical documentation practices and EHR vendor were evaluated as potential contributors to data quality. We found that overall about 70% of the data in the data warehouse is available for secondary use, and identified clinical documentation practices that are correlated to a reduction in data quality. This suggests that automated quality control and proactive clinical documentation support could reduce ad-hoc data cleaning needs resulting in greater data availability for secondary use.",2015,,10.1109/BigData.2015.7364060
18,"Kumar, Jitendra and Crow, Michael C. and Devarakonda, Ranjeet and Giansiracusa, Michael and Guntupally, Kavya and Olatt, Joseph V. and Price, Zach and Shanafield, Harold A. and Singh, Alka",Provenance–aware workflow for data quality management and improvement for large continuous scientific data streams,Data integrity;Instruments;Atmospheric measurements;Dictionaries;Big Data;Portals;scientific data workflows;data quality;provenance;atmospheric science,"Data quality assessment, management and improvement is an integral part of any big data intensive scientific research to ensure accurate, reliable, and reproducible scientific discoveries. The task of maintaining the quality of data, however, is non-trivial and poses a challenge for a program like the Department of Energy's Atmospheric Radiation Measurement (ARM) that collects data from hundreds of instruments across the world, and distributes thousands of streaming data products that are continuously produced in near-real-time for an archive 1.7 Petabyte in size and growing. In this paper, we present a computational data processing workflow to address the data quality issues via an easy and intuitive web-based portal that allows reporting of any quality issues for any site, facility or instruments at a granularity down to individual variables in the data files. This portal allows instrument specialists and scientists to provide corrective actions in the form of symbolic equations. A parallel processing framework applies the data improvement to a large volume of data in an efficient, parallel environment, while optimizing data transfer and file I/O operations; corrected files are then systematically versioned and archived. A provenance tracking module tracks and records any change made to the data during its entire life cycle which are communicated transparently to the scientific users. Developed in Python using open source technologies, this software architecture enables fast and efficient management and improvement of data in an operational data center environment.",2019,,10.1109/BigData47090.2019.9006358
19,"Han, Weiguo and Jochum, Matthew",A Machine Learning Approach for Data Quality Control of Earth Observation Data Management System,Data integrity;Machine learning;Earth;Satellites;Big Data;Process control;Monitoring;Big Data;Machine Learning;Earth Observation Data;Data management;Data Quality;Random Forest,"In the big data era, innovative technologies like cloud computing, artificial intelligence, and machine learning are increasingly utilized in the large-scale data management systems of many industry sectors to make them more scalable and intelligent. Applying them to automate and optimize earth observation data management is a hot topic. To improve data quality control mechanisms, a machine learning method in combination with built-in quality rules is presented in this paper to evolve processes around data quality and enhance management of earth observation data. The rules of quality check are set up to detect the common issues, including data completeness, data latency, bad data, and data duplication, and the machine learning model is trained, tested, and deployed to address these quality issues automatically and reduce manual efforts.",2020,,10.1109/IGARSS39084.2020.9323615
20,"Shrivastava, Shrey and Patel, Dhaval and Bhamidipaty, Anuradha and Gifford, Wesley M. and Siegel, Stuart A. and Ganapavarapu, Venkata Sitaramagiridharganesh and Kalagnanam, Jayant R.","DQA: Scalable, Automated and Interactive Data Quality Advisor",Data integrity;Machine learning;Pipelines;Cleaning;Libraries;Buildings;Data quality;machine learning;data cleaning;scalability;automation;data science,"Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today's world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.",2019,,10.1109/BigData47090.2019.9006187
21,"Khaleel, Majida Yaseen and Hamad, Murtadha M.",Data Quality Management for Big Data Applications,Distributed databases;Metadata;Big Data;Standards;File systems;Data integrity;Big Data;data quality;unstructured Data Distributed data file system;and statistical model.,"Currently, as a result of the continuous increase of data, one of the key issues is the development of systems and applications to deal with storage, management and processing of big numbers of data. These data are found in unstructured ways. Data management with traditional approaches is inappropriate because of the large and complex data sizes. Hadoop is a suitable solution for the continuous increase in data sizes. The important characteristics of the Hadoop are distributed processing, high storage space, and easy administration. Hadoop is better known for distributed file systems. In this paper, we have proposed techniques and algorithms that deal with big data including data collecting, data preprocessing, algorithms for data cleaning, A Technique for Converting Unstructured Data to Structured Data using metadata, distributed data file system (fragmentation algorithm) and Quality assurance algorithms by using the model is the statistical model to evaluate the highest educational institutions. We concluded that Metadata accelerates query response required and facilitates query execution, metadata will be content for reports, fields and descriptions. Total time access for three complex queries in distributed processing it is 00: 03: 00 per second while in nondistributed processing it is at 00: 15: 77 per second, average is approximately five minutes per second. Quality assurance note values (T-test) is 0.239 and values (T-dis) is 1.96, as a result of dealing with scientific sets and humanities sets. In the comparison law, it can be deduced that if the t-test is smaller than the t-dis; so there is no difference between the mean of the scientific and humanities samples, the values of C.V for both scientific is (8.585) and humanities sets is (7.427), using the law of homogeneity know whether any sets are more homogeneous whenever the value of a small C.V was more homogeneous however the humanity set is more homogeneity.",2019,,10.1109/DeSE.2019.00072
22,"Juddoo, Suraj",Overview of data quality challenges in the context of Big Data,Big data;Context;Quality management;Frequency measurement;Organizations;Big Data;Data quality;Data profiling;Data cleansing;data quality rules;dimensions;metrics,"Data quality management systems are thoroughly researched topics and have resulted in many tools and techniques developed by both academia and industry. However, the advent of Big Data might pose some serious questions pertaining to the applicability of existing data quality concepts. There is a debate concerning the importance of data quality for Big Data; one school of thought argues that high data quality methods are essential for deriving higher level analytics while another school of thought argues that data quality level will not be so important as the volume of Big Data would be used to produce patterns and some amount of dirty data will not mask the analytic results which might be derived. This paper aims to investigate various components and activities forming part of data quality management such as dimensions, metrics, data quality rules, data profiling and data cleansing. The result list existing challenges and future research areas associated with Big Data for data quality management.",2015,,10.1109/CCCS.2015.7374131
23,"Wenlu Yang and Da Silva, Alzennyr and Picard, Marie-Luce",Computing data quality indicators on Big Data streams using a CEP,Smart meters;Data visualization;Smart grids;Real-time systems;Image color analysis;Indexes;Data quality;Big Data;data stream;CEP;smart grids,"Big Data is often referred to as the 3Vs: Volume, Velocity and Variety. A 4th V (validity) was introduced to address the quality dimension. Poor data quality can be costly, lead to breaks in processes and invalidate the company's efforts on regulatory compliance. In order to process data streams in real time, a new technology called CEP (complex event processing) was developed. In France, the current deployment of smart meters will generate massive electricity consumption data. In this work, we developed a diagnostic approach to compute generic quality indicators of smart meter data streams on the fly. This solution is based on Tibco StreamBase CEP. Visualization tools were also developed in order to give a better understanding of the inter-relation between quality issues and geographical/temporal dimensions. According to the application purpose, two visualization methods can be loaded: (1) StreamBase LiveView is used to visualize quality indicators in real time; and (2) a Web application provides a posteriori and geographical analysis of the quality indicators which are plotted on a map within a color scale (lighter colors indicate good quality and darker colors indicate poor quality). In future works, new quality indicators could be added to the solution which can be applied in an operational context in order to monitor data quality from smart meters.",2015,,10.1109/IWCIM.2015.7347061
24,"Li, Congli and Yang, Bin and Chen, Xuezhen and Zhang, Enjie and Huang, Hongjun and Li, Da",Research on smart grid big data amp;#x2019;s curve mean clustering algorithm for edge-cloud collaborative application,Wireless communication;Low voltage;Data integrity;Urban areas;Clustering algorithms;Collaboration;Big Data;Smart grid big data;Data quality;Curve mean clustering Algorithm,"As the demand for smart grid construction increases, advanced power applications based on edge-cloud collaboration continue to increase. Among them, there are many data-driven artificial intelligence calculations and analyses, all of which are calculated and analyzed based on electric power big data. However, for the massive electric power big data, it is impossible to obtain more internally related information only by observing the data from the surface. To a certain extent, it directly affects the upper-level advanced applications. To solve this problem, this paper studies and proposes a curve-mean clustering algorithm for load big data, which is the most widely used load data in smart grid. By analyzing the advanced measurement infrastructure, the matrix low-rank property of load big data and the calculation of singular value, the curve mean clustering of load big data is realized, and the optimal determination method of cluster number is expounded. Experiments are conducted based on actual resident user load data and compared with the classic mean shift clustering algorithm. By calculating the average distance within the cluster, the average distance between clusters and the DI index, it is verified that the proposed method clustering is more accurate and the selection of cluster number is optimal. The research plays a very good role in basic analysis for improving the big data analysis capability and data quality of smart grid.",2021,,10.1109/ICWCSG53609.2021.00085
25,"Snineh, Sidi Mohamed and Bouattane, Omar and Youssfi, Mohamed and Daaif, Abdelaziz",Towards a multi-agents model for errors detection and correction in big data flows,Big Data;Multi-agent systems;Companies;Task analysis;Error correction;Data integrity;Real-time systems;Multi-agents;Big Data;Data quality;detection and correction of data errors,"The quality of data for decision-making will always be a major factor for companies that want to remain competitors. In addition, the era of Big Data has brought new challenges for the processing, management, storage of data and in particular the challenge represented by the veracity of these data which is one of the 5Vs that characterizes Big Data. This characteristic that defines the quality or reliability of the data and its sources must be verified in the future systems of each company. In this paper, we present an approach that helps to improve the quality of Big Data by the distributed execution of algorithms for detecting and correcting data errors. The idea is to have a multi-agents model for errors detection and correction in big data flow. This model linked to a repository specific to each company. This repository contains the most frequent errors, metadata, error types, error detection algorithms and error correction algorithms. Each agent of this model represents an algorithm and will be deployed in multiple instances when needed. The use of these agents will go through two steps. In the first step, the detection agents and error correction agents manage each flow entering the system in real time. In the second step, all the processed data flows in first step will be a dataset to which the error detection and correction agents are applied in batch in order to process other types of errors. Among architectures who allow this processing type, we have chosen Lambda architecture.",2019,,10.1109/ICDS47004.2019.8942297
26,"Catarci, Tiziana and Scannapieco, Monica and Console, Marco and Demetrescu, Camil",My (fair) big data,Big Data;Pipelines;Ontologies;Metadata;Google;Quality-driven policies;Big Data pipeline;ontology-based quality checking;Big Data confidentiality,"Policy making has the strict requirement to rely on quantitative and high quality information. This paper will address the data quality issue for policy making by showing how to deal with Big Data quality in the different steps of a processing pipeline, with a focus on the integration of Big Data sources with traditional sources. In this respect, a relevant role is played by metadata and in particular by ontologies. Integration systems relying on ontologies enable indeed a formal quality evaluation of inaccuracy, inconsistency and incompleteness of integrated data. The paper will finally describe data confidentiality as a Big Data quality dimension, showing the main issues to be faced for its assurance.",2017,,10.1109/BigData.2017.8258267
27,"Ezzine, Imane and Benhlima, Laila",A Study of Handling Missing Data Methods for Big Data,Data integrity;Big Data;Data models;Dictionaries;Machine learning algorithms;Machine learning;Measurement;data quality;missing data;big data;functional dependancy;master data;machine learning,"Improving data quality is not a recent field but in the context of big data this is a challenging area as there is a crucial need for data quality to, for example, increase the accuracy of big data analytics or avoid storing redundant data. Missing data is one of the major problem that faces the quality of data. There are several methods and approaches that have been used in relational databases to handle missing data most of which have been adapted to big data. This paper aims to provide an overview of some methods and approaches for handling missing data in big data contexts.",2018,,10.1109/CIST.2018.8596389
28,"Taleb, Ikbal and Dssouli, Rachida and Serhani, Mohamed Adel",Big Data Pre-processing: A Quality Framework,Big data;Data integration;Accuracy;Distributed databases;Data analysis;Business;Big Data;Data Quality;pre-processing,"With the abundance of raw data generated from various sources, Big Data has become a preeminent approach in acquiring, processing, and analyzing large amounts of heterogeneous data to derive valuable evidences. The size, speed, and formats in which data is generated and processed affect the overall quality of information. Therefore, Quality of Big Data (QBD) has become an important factor to ensure that the quality of data is maintained at all Big data processing phases. This paper addresses the QBD at the pre-processing phase, which includes sub-processes like cleansing, integration, filtering, and normalization. We propose a QBD model incorporating processes to support Data quality profile selection and adaptation. In addition, it tracks and registers on a data provenance repository the effect of every data transformation happened in the pre-processing phase. We evaluate the data quality selection module using large EEG dataset. The obtained results illustrate the importance of addressing QBD at an early phase of Big Data processing lifecycle since it significantly save on costs and perform accurate data analysis.",2015,,10.1109/BigDataCongress.2015.35
29,"Chenran, Xiong and Youde, Wu",The Geographic Environment Analysis of Regional Economic Development of Yunnan Province of China Based on the Big Data Technology,Transportation;Big data;Smart cities;analysis of big data;Yunnan province of China;regional economic development;geographic environment,"This paper uses the analysis methods of data mining, data quality and management, semantic engine and prediction in big data to analyze the geographic environment of the Yunnan's economic development from its special location and geographic elements, beholding that the geographic elements lay basic material conditions for its economic development of Yunnan province. Geographic factors on one hand encourage the economic development and on the other hand affect together the economic development of Yunnan province.",2015,,10.1109/ICITBS.2015.220
30,"Faroukhi, Abou Zakaria and El Alaoui, Imane and Gahi, Youssef and Amine, Aouatif",Big Data Value Chain: A Unified Approach for Integrated Data Quality and Security,Big Data;Security;Data integrity;Data models;Organizations;Decision making;Reliability;Big Data Value Chain;Data Management;Data Quality;Data Security;Process Integration;Orchestration,"Big Data has grown significantly in recent years. This growth has led organizations to adopt Big Data Value Chains (BDVC) as the appropriate framework for unlocking the value to make suitable decisions. Despite its promising opportunities, Big Data raises new concerns such as data quality and security that could radically impact the effectiveness of the BDVC. These two essential aspects have become an urgent need for any Big Data project to provide meaningful datasets and reliable insights. In this contribution, we highlight the importance of considering data quality and security requirements. Then, we propose a coherent, unified framework that extends BDVC with security and quality aspects. Through quality and security reports, the model can self-evaluate and arrange tasks according to orchestration and monitoring process, allowing the BDVC to evolve at the organization pace and to align strategically with its objectives as well as to federate a sustainable ecosystem.",2020,,10.1109/ICECOCS50124.2020.9314391
31,"Haug, Frank S.",Bad big data science,Big data;Metadata;Distributed databases;Stakeholders;Data science;Big Data;Data Quality;Data Warehousing;Distributed Database;Metadata,"As hardware and software technologies have improved, our definition of a “manageable amount of data” has increased in its scope dramatically. The term “big data” can be applied to any of several different projects and technologies sharing the ultimate goal of supporting analysis on these large, heterogeneous, and evolving data sets. The term “data science” refers to the statistical, technical, and domain-specific knowledge required to ensure that the analysis is done properly. Techniques for managing some common causes for bad data and invalid analysis have been used in other areas, such as data warehousing and distributed database. However, big data projects face special challenges when trying to combine big data and data science without producing inaccurate, misleading, or invalid results. This paper discusses potential causes for “bad big data science”, focusing primarily on the data quality of the input data, and suggests methods for minimizing them based on techniques originally developed for data warehousing and distributed database projects.",2016,,10.1109/BigData.2016.7840935
32,"Taleb, Ikbal and Serhani, Mohamed Adel and Dssouli, Rachida",Big Data Quality: A Survey,"Big Data;Data integrity;Quality management;Data visualization;Databases;Sensors;Social network services;Big Data, Data Quality, Quality Management framework, Quality of Big Data","With the advances in communication technologies and the high amount of data generated, collected, and stored, it becomes crucial to manage the quality of this data deluge in an efficient and cost-effective way. The storage, processing, privacy and analytics are the main keys challenging aspects of Big Data that require quality evaluation and monitoring. Quality has been recognized by the Big Data community as an essential facet of its maturity. Yet, it is a crucial practice that should be implemented at the earlier stages of its lifecycle and progressively applied across the other key processes. The earlier we incorporate quality the full benefit we can get from insights. In this paper, we first identify the key challenges that necessitates quality evaluation. We then survey, classify and discuss the most recent work on Big Data management. Consequently, we propose an across-the-board quality management framework describing the key quality evaluation practices to be conducted through the different Big Data stages. The framework can be used to leverage the quality management and to provide a roadmap for Data scientists to better understand quality practices and highlight the importance of managing the quality. We finally, conclude the paper and point to some future research directions on quality of Big Data.",2018,,10.1109/BigDataCongress.2018.00029
33,"Freitas, Patrícia Alves de and Reis, Everson Andrade dos and Michel, Wanderson Senra and Gronovicz, Mauro Edson and Rodrigues, Márcio Alexandre de Macedo","Information Governance, Big Data and Data Quality",Companies;Information management;Data handling;Data storage systems;Computer architecture;Information Governance;Big Data;Data Quality,"The value of information as a competitive differential has been taken into consideration in companies all over the world for some time already. In recent years, there has been heated debate about some terms originated from new concepts related to information, such as big data, due to the promise that such topic might revolutionise world trade. Hence, data and information governance and quality have been increasingly discussed in the business world.",2013,,10.1109/CSE.2013.168
34,"Lee, Doyoung",Big Data Quality Assurance Through Data Traceability: A Case Study of the National Standard Reference Data Program of Korea,Standards;Uncertainty;Big Data;Metrology;Reliability;Measurement uncertainty;Biomedical measurement;Big data;data quality;data traceability;metrology;standard reference data;uncertainty,"In the era of big data, the scientific and social demand for quality data is aggressive and urgent. This paper sheds light on the expanded role of metrology of verifying validated procedures of data production and developing adequate uncertainty evaluation methods to ensure the trustworthiness of data and information. In this regard, I explore the mechanism of the national standard reference data (SRD) program of Korea, which connects various scientific and social sectors to metrology by applying useful metrological concepts and methods to produce reliable data and convert such data into national standards. In particular, the changing interpretation of metrological key concepts, such as “measurement,” “traceability,” and “uncertainty,” will be explored and reconsidered from the perspective of data quality assurance. As a result, I suggest the concept of “data traceability” with “the matrix of data quality evaluation” according to the elements of a data production system and related evaluation criteria. To conclude, I suggest social and policy implications for the new role of metrology and standards for producing and disseminating reliable knowledge sources from big data.",2019,,10.1109/ACCESS.2019.2904286
35,"Micic, Natasha and Neagu, Daniel and Campean, Felician and Zadeh, Esmaeil Habib",Towards a Data Quality Framework for Heterogeneous Data,Data integrity;Measurement;Warranties;Metadata;Cleaning;Big Data;Heterogeneous Data Sets;Data Quality;Metadata;Data Cleaning;Data Quality Assessment,"Every industry has significant data output as a product of their working process, and with the recent advent of big data mining and integrated data warehousing it is the case for a robust methodology for assessing the quality for sustainable and consistent processing. In this paper a review is conducted on Data Quality (DQ) in multiple domains in order to propose connections between their methodologies. This critical review suggests that within the process of DQ assessment of heterogeneous data sets, not often are they treated as separate types of data in need of an alternate data quality assessment framework. We discuss the need for such a directed DQ framework and the opportunities that are foreseen in this research area and propose to address it through degrees of heterogeneity.",2017,,10.1109/iThings-GreenCom-CPSCom-SmartData.2017.28
36,"Blanquer, Ignacio and Meira, Wagner","EUBra-BIGSEA, A Cloud-Centric Big Data Scientific Research Platform","Big Data;Data analysis;Data privacy;Quality of service;Data models;Security;Biological system modeling;Cloud-Computing,-Big-Data,-Quality-of-Service,-Data-Analytics","This paper describes the achievements of project EUBra-BIGSEA, which has delivered programming models and data analytics tools for the development of distributed Big Data applications. As framework components, multiple data models are supported (e.g. data streams, multidimensional data, etc.) and efficient mechanisms to ensure privacy and security, on top of a QoS-aware layer for the smart and rapid provisioning of resources in a cloud-based environment.",2018,,10.1109/DSN-W.2018.00023
37,"Juddoo, Suraj and George, Carlisle",A Qualitative Assessment of Machine Learning Support for Detecting Data Completeness and Accuracy Issues to Improve Data Analytics in Big Data for the Healthcare Industry,Industries;Machine learning algorithms;Data integrity;Clustering algorithms;Medical services;Big Data;Tools;Big Data;Data Quality;Data Inaccuracy;Data incompleteness;Machine Learning,"Tackling Data Quality issues as part of Big Data can be challenging. For data cleansing activities, manual methods are not efficient due to the potentially very large amount of data.. This paper aims to qualitatively assess the possibilities for using machine learning in the process of detecting data incompleteness and inaccuracy, since these two data quality dimensions were found to be the most significant by a previous research study conducted by the authors. A review of existing literature concludes that there is no unique machine learning algorithm most suitable to deal with both incompleteness and inaccuracy of data. Various algorithms are selected from existing studies and applied against a representative big (healthcare) dataset. Following experiments, it was also discovered that the implementation of machine learning algorithms in this context encounters several challenges for Big Data quality activities. These challenges are related to the amount of data particualar machine learning algorithms can scale to and also to certain data type restrictions imposed by some machine learning algorithms. The study concludes that 1) data imputation works better with linear regression models, 2) clustering models are more efficient to detect outliers but fully automated systems may not be realistic in this context. Therefore, a certain level of human judgement is still needed.",2020,,10.1109/ELECOM49001.2020.9297009
38,"Zhou, Lixiao and Huang, Maohai",Challenges of Software Testing for Astronomical Big Data,Big Data;Software testing;Software;Astronomy;Data analysis;Distributed databases;software testing; big data; astronomical software,"Astronomy has been one of the first areas of science to embrace and learn from big data. The amount of data we have on our universe is doubling every year thanks to big telescopes and better light detectors. Most leading research is based on data from a handful of very expensive telescopes. Undoubtedly, the data quality is the key basis for the leading scientific findings. It is imperative that software testers understand that big data is about far more than simply data volume. This paper will analyze characteristics, types, methods, strategies, problems, challenges and propose some possible solutions of software testing for astronomical big data.",2017,,10.1109/BigDataCongress.2017.91
39,"Bhardwaj, Dave and Ormandjieva, Olga",Toward a Novel Measurement Framework for Big Data (MEGA),Solid modeling;Data integrity;Volume measurement;Pipelines;Big Data;Data models;Software;Big Data quality characteristics;The V’s;Big Data Quality Measurement Framework;Big Data Pipelines,"Big Data is quickly becoming a chief part of the decision-making process in both industry and academia. As more and more institutions begin relying on Big Data to make strategic decisions, the quality of the underlying data comes into question. The quality of Big Data isn’t always transparent and large-scale systems may even lack its visibility, which adversely affects the credibility of the Big Data systems. Continuous monitoring and measurement of data quality is therefore paramount in assessing whether the information can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses the need for Big Data quality measurement modeling and automation by proposing a novel conceptual quality measurement framework for Big Data (MEGA) with the purpose of assessing the underlying quality characteristics of Big Data (also known as the V’s of Big Data) at each step of the Big Data Pipelines. The theoretical quality measurement models for four of the Big Data V’s (Volume, Variety, Velocity, Veracity) are currently automated; the remaining 6 V’s (Vincularity, Validity, Value, Volatility, Valence and Vitality) will be tackled in our future work. The approach is illustrated on a case study.",2021,,10.1109/COMPSAC51774.2021.00235
40,"Hee, Kim",Is data quality enough for a clinical decision?: Apply machine learning and avoid bias,Data mining;Medical diagnostic imaging;Currencies;Tools;Medical services;Measurement;data quality;decision quality;healthcare;clinical data;EHRs;data quality assurance;stratified sampling;bias,"This paper provides a practical guideline for the assurance and (re-)usage of clinical data. It proposes a process which aims to provide a systematic data quality assurance even without involving a medical domain expert. Especially when (re-)using clinical data, data quality is an important topic because clinical data are not purposely collected. Therefore, data driven conclusions might be false, because a given dataset is not representative. These false data driven conclusions could even harm the life of patients. Thus, all researchers should adhere to some basic principles that can prevent false conclusions. Twelve empirical experiments were conducted in order to prove that my process is able to assure data quality with respect to the descriptive and predictive analysis. Descriptive results obtained by applying stratified sampling are conflicting in four out of nine population inputs. Sampling is carried based on the top ranked feature drawn by the Contextual Data Quality Assurance (CDQA). Between datasets these features are confirmed by the Mutual Data Quality Assurance (MDQA). Stratified sampled inputs improve predictive results compared to raw data. Both Area Under the Curve (AUC) scores and accuracy scores increase by three percent.",2017,,10.1109/BigData.2017.8258221
41,"Gan, Wenting",Design of Network Precision Marketing Based on Big Data Analysis Technology,Visualization;Data analysis;Databases;Data integrity;Semantics;Big Data;Internet;Big Data;E-commerce;Internet marketing;Marketing design,"In the process of big data processing, big data analysis is the core work content. After obtaining a large amount of data, we use related analysis technology to perform data processing and analysis to obtain knowledge. Its related content includes visual analysis, data mining, predictive analysis, semantic analysis and data quality management. How to obtain big data, classify and store according to data types, mine valuable information from big data, and effectively apply big data in the field of precision marketing are hot topics of research. On the basis of researching the key technologies of big data analysis, this paper uses Hadoop big data to analyze and store the massive online logs generated by users' mobile terminals, and calculates and builds user characteristic databases. We use relevant analysis technology to analyze the user's location information, browsing and usage habits, hobbies, and focus content. At the same time, a precise marketing model is established according to user behavior characteristics and attributes, thereby improving the marketing effect of the enterprise.",2020,,10.1109/ECIT50008.2020.00026
42,"Wang, Fengling and Wang, Han and Xue, Liang",Research on Data Security in Big Data Cloud Computing Environment,Cloud computing;Data security;Data integrity;Big Data;Maintenance engineering;Virtualization;Information technology;big data;cloud computing;data security;big data cloud computing;security policy,"In the big data cloud computing environment, data security issues have become a focus of attention. This paper delivers an overview of conceptions, characteristics and advanced technologies for big data cloud computing. Security issues of data quality and privacy control are elaborated pertaining to data access, data isolation, data integrity, data destruction, data transmission and data sharing. Eventually, a virtualization architecture and related strategies are proposed to against threats and enhance the data security in big data cloud environment.",2021,,10.1109/IAEAC50856.2021.9391025
43,"Guan, Zhibin and Ji, Tongkai and Qian, Xu and Ma, Yan and Hong, Xuehai",A Survey on Big Data Pre-processing,Big Data;Data integration;Data analysis;Dimensionality reduction;Feature extraction;Data integrity;big data processing;data pre-processing;data cleansing;dimension reduction;data quality,"In this paper, we briefly introduce some basic concepts and characteristics of big data. We are surrounded by massive amount of data but starving for knowledge. In the era of Big Data, how to quickly obtain high-quality and valuable information from massive amounts of data has become an important research direction. Hence, we focus our attention to the data pre-processing which is a sub-content of the data processing workflow. In this paper, the four phases of data pre-processing, including data cleansing, data integration, data reduction, and data transformation, have been discussed. And different approaches for a variety of purposes have been presented, which show current methods and techniques need to be further modified in order to improve the quality of data before data analysis.",2017,,10.1109/ACIT-CSII-BCD.2017.49
44,"Guntupally, Kavya and Devarakonda, Ranjeet and Kehoe, Kenneth",Spring Boot based REST API to Improve Data Quality Report Generation for Big Scientific Data: ARM Data Center Example,Springs;Databases;Service-oriented architecture;Data integrity;Tools;Servers;Big Data;auto configuration;CRUD;java framework;service-oriented architecture;REST;spring boot,"Web application technologies are growing rapidly with continuous innovation and improvements. This paper focuses on the popular Spring Boot [1] java-based framework for building web and enterprise applications and how it provides the flexibility for service-oriented architecture (SOA). One challenge with any Spring-based applications is its level of complexity with configurations. Spring Boot makes it easy to create and deploy stand-alone, production-grade Spring applications with very little Spring configuration. Example, if we consider Spring Model-View-Controller (MVC) framework [2], we need to configure dispatcher servlet, web jars, a view resolver, and component scan among other things. To solve this, Spring Boot provides several Auto Configuration options to setup the application with any needed dependencies. Another challenge is to identify the framework dependencies and associated library versions required to develop a web application. Spring Boot offers simpler dependency management by using a comprehensive, but flexible, framework and the associated libraries in one single dependency, which provides all the Spring related technology that you need for starter projects as compared to CRUD web applications. This framework provides a range of additional features that are common across many projects such as embedded server, security, metrics, health checks, and externalized configuration. Web applications are generally packaged as war and deployed to a web server, but Spring Boot application can be packaged either as war or jar file, which allows to run the application without the need to install and/or configure on the application server. In this paper, we discuss how Atmospheric Radiation Measurement (ARM) Data Center (ADC) at Oak Ridge National Laboratory, is using Spring Boot to create a SOA based REST [4] service API, that bridges the gap between frontend user interfaces and backend database. Using this REST service API, ARM scientists are now able to submit reports via a user form or a command line interface, which captures the same data quality or other important information about ARM data.",2018,,10.1109/BigData.2018.8621924
45,"Sadiq, Shazia and Papotti, Paolo",Big data quality - whose problem is it?,Big data;Cleaning;Computer science;Databases;Investment,"The increased reliance on data driven enterprise has seen an unprecedented investment in big data initiatives. Organizations averaged US$8M in investments in big data-related initiatives and programs in 2014, with 70% of large enterprises and 56% of small and medium enterprises (SMEs) having already deployed, or planning to deploy, big-data projects [1]. As companies intensify their efforts to get value from big data, the growth in the amount of data being managed continues at an exponential rate, leaving organizations with a massive footprint of unexplored, unfamiliar datasets. On February 8th, 2015, a group of global thought leaders from the database research community outlined the grand challenges in getting value from big data [2]. The key message was the need to develop the capacity to `understand how the quality of data affects the quality of the insight we derive from it'.",2016,,10.1109/ICDE.2016.7498367
46,"Rettig, Laura and Khayati, Mourad and Cudré-Mauroux, Philippe and Piórkowski, Michal",Online anomaly detection over Big Data streams,Entropy;Measurement;Correlation;Big data;Data structures;Yttrium;Sparks,"Data quality is a challenging problem in many real world application domains. While a lot of attention has been given to detect anomalies for data at rest, detecting anomalies for streaming applications still largely remains an open problem. For applications involving several data streams, the challenge of detecting anomalies has become harder over time, as data can dynamically evolve in subtle ways following changes in the underlying infrastructure. In this paper, we describe and empirically evaluate an online anomaly detection pipeline that satisfies two key conditions: generality and scalability. Our technique works on numerical data as well as on categorical data and makes no assumption on the underlying data distributions. We implement two metrics, relative entropy and Pearson correlation, to dynamically detect anomalies. The two metrics we use provide an efficient and effective detection of anomalies over high velocity streams of events. In the following, we describe the design and implementation of our approach in a Big Data scenario using state-of-the-art streaming components. Specifically, we build on Kafka queues and Spark Streaming for realizing our approach while satisfying the generality and scalability requirements given above. We show how a combination of the two metrics we put forward can be applied to detect several types of anomalies - like infrastructure failures, hardware misconfiguration or user-driven anomalies - in large-scale telecommunication networks. We also discuss the merits and limitations of the resulting architecture and empirically evaluate its scalability on a real deployment over live streams capturing events from millions of mobile devices.",2015,,10.1109/BigData.2015.7363865
47,"Yalaoui, Mehdi and Boukhedouma, Saida","A survey on data quality: principles, taxonomies and comparison of approaches",Data integrity;Taxonomy;Standards organizations;Decision making;Organizations;Big Data;Data models;Data Quality;Big Data;Quality Dimensions;Quality Metrics;Metamodel;Assessment process;Improvement,"Nowadays, data generation keeps increasing exponentially due to the emergence of the Internet of Things (IoT) and Big data technologies. The manipulation of such Big amount of data becomes more and more difficult because of its size and its variety. For better governance of organizations (decision making, data analysis, earnings increase …), data quality and data governance at present of Big data are two major pillars for the design of any system handling data within the organization. This explains the number of researches conducted as it constitutes a research subject with several gaps and opportunities. Many works were conducted to define and standardize Data Quality (DQ) and its dimensions, others were directed to design and propose data quality assessment and improvement models or frameworks. This work aims to recall the data quality principles starting by the needed background knowledge, then identify and compare the relevant taxonomies existing in the literature, next surveys and compares the available Data quality assessment and improvement approaches. After that, we propose a metamodel highlighting the main concepts of DQ assessment and we describe a generic process for DQ assessment and improvement. Finally, we evoke the main challenges in the field of DQ before and after the emergence of Big Data.",2021,,10.1109/ICISAT54145.2021.9678209
48,"Xie, Chunli and Gao, Jerry and Tao, Chuanqi",Big Data Validation Case Study,Big Data;Tools;Databases;Electronic mail;Reliability;Temperature measurement;Null value;big data quality;big data validation;data checklist;big data tool;case study,"With the advent of big data, data is being generated, collected, transformed, processed and analyzed at an unprecedented scale. Since data is created at a fast velocity and with a large variety, the quality of big data is far from perfect. Recent studies have shown that poor quality can bring serious erroneous data costs on the result of big data analysis. Data validation is an important process to recognize and improve data quality. In this paper, a case study that is relevant to big data quality is designed to study original big data quality, data quality dimension, data validation process and tools.",2017,,10.1109/BigDataService.2017.44
49,"Poon, Lex and Farshidi, Siamak and Li, Na and Zhao, Zhiming",Unsupervised Anomaly Detection in Data Quality Control,Data integrity;Decision making;Process control;Quality control;Organizations;Big Data;Data models;data quality control;data quality assessment;unsupervised learning;anomaly detection;automated data quality control,"Data is one of the most valuable assets of an organization and has a tremendous impact on its long-term success and decision-making processes. Typically, organizational data error and outlier detection processes perform manually and reactively, making them time-consuming and prone to human errors. Additionally, rich data types, unlabeled data, and increased volume have made such data more complex. Accordingly, an automated anomaly detection approach is required to improve data management and quality control processes. This study introduces an unsupervised anomaly detection approach based on models comparison, consensus learning, and a combination of rules of thumb with iterative hyper-parameter tuning to increase data quality. Furthermore, a domain expert is considered a human in the loop to evaluate and check the data quality and to judge the output of the unsupervised model. An experiment has been conducted to assess the proposed approach in the context of a case study. The experiment results confirm that the proposed approach can improve the quality of organizational data and facilitate anomaly detection processes.",2021,,10.1109/BigData52589.2021.9671672
50,"Zhang, Guobao",A data traceability method to improve data quality in a big data environment,Data Governance;Data Credibility;Data Traceability,"In the actual project of data sharing and data governance, the problems of data heterogeneity and low data quality have not been solved. Data quality detection based on syntax rules can effectively find data quality problems, but it had not improved the data quality, especially in a variety of heterogeneous data source environments. This paper designs a data governance model based on data traceability, and we can get data feedback and revision through this model. The proposed method considers the different ownership of data and may form a closed-loop data service chain including effective data validation, data tracking, and data revision, data release. The analysis of the case shows that the method is effective to improve the data quality and meet the requirements of data security also.",2020,,10.1109/DSC50466.2020.00051
51,"Priebe, Torsten and Markus, Stefan","Business information modeling: A methodology for data-intensive projects, data science and big data governance",Business;Data models;Terminology;Big data;Data warehouses;Wheels;Standards organizations;Data Modeling;Project Methodology;Data Governance;Metadata;Information Catalog,"This paper discusses an integrated methodology to structure and formalize business requirements in large data-intensive projects, e.g. data warehouses implementations, turning them into precise and unambiguous data definitions suitable to facilitate harmonization and assignment of data governance responsibilities. We place a business information model in the center - used end-to-end from analysis, design, development, testing to data quality checks by data stewards. In addition, we show that the approach is suitable beyond traditional data warehouse environments, applying it also to big data landscapes and data science initiatives - where business requirements analysis is often neglected. As proper tool support has turned out to be inevitable in many real-world settings, we also discuss software requirements and their implementation in the Accurity Glossary tool. The approach is evaluated based on a large banking data warehouse project the authors are currently involved in.",2015,,10.1109/BigData.2015.7363987
52,"Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal and Nujum, Alramzana",An Hybrid Approach to Quality Evaluation across Big Data Value Chain,Big data;Metadata;Measurement;Quality assessment;Quality of service;Unified modeling language;Big Data;Quality assessment;Metadata;Quality metrics;quality Metadata;Quality of process;Hybrid quality assessment,"While the potential benefits of Big Data adoption are significant, and some initial successes have already been realized, there remain many research and technical challenges that must be addressed to fully realize this potential. The Big Data processing, storage and analytics, of course, are major challenges that are most easily recognized. However, there are additional challenges related for instance to Big Data collection, integration, and quality enforcement. This paper proposes a hybrid approach to Big Data quality evaluation across the Big Data value chain. It consists of assessing first the quality of Big Data itself, which involve processes such as cleansing, filtering and approximation. Then, assessing the quality of process handling this Big Data, which involve for example processing and analytics process. We conduct a set of experiments to evaluate Quality of Data prior and after its pre-processing, and the Quality of the pre-processing and processing on a large dataset. Quality metrics have been measured to access three Big Data quality dimensions: accuracy, completeness, and consistency. The results proved that combination of data-driven and process-driven quality evaluation lead to improved quality enforcement across the Big Data value chain. Hence, we recorded high prediction accuracy and low processing time after we evaluate 6 well-known classification algorithms as part of processing and analytics phase of Big Data value chain.",2016,,10.1109/BigDataCongress.2016.65
53,"Fu, Qian and Easton, John M.",Understanding data quality: Ensuring data quality by design in the rail industry,Industries;Rails;Data models;Rail transportation;Systematics;Decision making;data quality;rail;quality by design;data quality schema,"The railways worldwide are increasingly looking to the integration of their data resources coupled with advanced analytics to enhance traffic management, to provide new insights on the health of infrastructure assets, to provide soft linkages to other transport modes, and ultimately to enable them to better serve their customers. As in many industrial sectors, over the past decade the rail industry has been investing heavily in sensing technologies that record every aspect of the operation of the railway network. However, as any data scientist knows, it does not matter how good an algorithm is, if you put rubbish in, you get rubbish out; and as the traditional industry model of working with data only within the system that it was collected by becomes increasingly fragile, the industry is discovering that it knows less than it thought about the data it is gathering. When coupled with legacy data resources of unknown accuracy, such as design diagrams for assets that in many cases are decades old, the rail industry now faces a crisis in which its data may become essentially worthless due to a poor understanding of the quality of its data. This paper reports the findings of the first phase of a three-phase systematic review of literature about how data quality can be managed and evaluated in the rail domain. It begins by discussing why data quality matters in a rail context, before going on to define the quality, introduce and expand the concept of a data quality schema.",2017,,10.1109/BigData.2017.8258380
54,"Benbernou, Salima and Ouziri, Mourad",Enhancing data quality by cleaning inconsistent big RDF data,Resource description framework;Ontologies;Sparks;Cleaning;Inference algorithms;Automobiles;Jacobian matrices;Big data;Data quality;Inconsistency;Logical inference;RDF data cleaning;Apache Spark ecosystems,"We address the problem of dealing with inconsistencies in fusion of big data sources using Resource Description Framework (RDF) and ontologies. We propose a scalable approach ensuring data quality for query answering over big RDF data in a distributed way on a Spark ecosystem. In so doing, the cleaning inconsistent big RDF data approach is built on the following steps (1) modeling consistency rules to detect the inconsistency triples even if it is implicitly hidden including inference and inconsistent rules (2) detecting inconsistency through rule evaluation based on Apache Spark framework to discover the minimally sub-set of inconsistent triples (3) cleaning the inconsistency through finding the best repair for consistent query answering.",2017,,10.1109/BigData.2017.8257913
55,"Shanmugam, Srinivasan and Seshadri, Gokul",Aspects of Data Cataloguing for Enterprise Data Platforms,Context;Business;Metadata;Big data;Electronic mail;Reliability;Indexes;Enterprise data;Data catalogue;Metadata of data;Data context;Data quality;Data governance;Data as a service,"As the adoption of enterprise Big Data platforms mature, the necessity to maintain systematic catalogue of data being processed and managed by these platforms becomes imperative. Enterprise data catalogues serve as centralized repositories of storing such metadata about data being handled by such platforms, enabling better data governance, security and control. Standardized approaches, methodologies and tools for data catalogues are being discussed and evolved. This paper introduces some key variables, attributes and indexes that need to be handled in such cataloguing solutions -- such as data contexts, data-system relationships, data quality, reliability, sensitivity and accessibility. The paper also discusses specific approaches on how each of these aspects can be adopted and applied to different enterprise contexts effectively.",2016,,10.1109/BigDataSecurity-HPSC-IDS.2016.52
56,"Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Kahn, Michael G",An Interactive Data Quality Test Approach for Constraint Discovery and Fault Detection,Fault detection;Data integrity;Data models;Semantics;Decision trees;Self-organizing feature maps;Inspection;Big Data;Data quality tests;Explainable learning;Interactive learning;Unsupervised learning,"Data quality tests validate heterogeneous data to detect violations of syntactic and semantic constraints. The specification of these constraints can be incomplete because domain experts typically specify them in an ad hoc manner. Existing automated test approaches can generate false alarms and do not explain the constraint violations while reporting faulty data records. In previous work, we proposed ADQuaTe, which is an automated data quality test approach that uses an unsupervised deep learning techni que (1) to discover constraints from big datasets that may have been missed by experts, and (2) to label as suspicious those records that violate the constraints. These records are grouped and explanations for constraint violations are presented to domain experts who determine whether or not the groups are actually faulty. This paper presents ADQuaTe2, which extends ADQuaTe to use an interactive learning technique that incorporates expert feedback to retrain the learning model and improve the accuracy of constraint discovery and fault detection. We evaluate the effectiveness of the approach on real-world datasets from a health data warehouse and a plant diagnosis database. We also use datasets with known faults from the UCI repository to evaluate the improvement in the accuracy of the approach after incorporating ground truth knowledge.",2019,,10.1109/BigData47090.2019.9006446
57,"Ciancarini, Paolo and Poggi, Francesco and Russo, Daniel",Big Data Quality: A Roadmap for Open Data,Big data;Metadata;Government;ISO Standards;Distributed databases;Open Data Quality;Information Modeling;E-Government;Big Data Knowledge Extraction,"Open Data (OD) is one of the most discussed issue of Big Data which raised the joint interest of public institutions, citizens and private companies since 2009. However, the massive amount of freely available data has not yet brought the expected effects: as of today, there is no application that has fully exploited the potential provided by large and distributed information sources in a non-trivial way, nor any service has substantially changed for the better the lives of people. The era of a new generation applications based on OD is far to come. In this context, we observe that OD quality is one of the major threats to achieving the goals of the OD movement. The starting point of this case study is the quality of the OD released by the five Constitutional offices of Italy. Our exploratory case study aims to assess the quality of such releases and the real implementations of OD. The outcome suggests the need of a drastic improvement in OD quality. Finally we highlight some key quality principles for OD, and propose a roadmap for further research.",2016,,10.1109/BigDataService.2016.37
58,"Khalfi, Besma and de Runz, Cyril and Faiz, Sami and Akdag, Herman",A New Methodology for Storing Consistent Fuzzy Geospatial Data in Big Data Environment,Fuzzy sets;Geospatial analysis;Big Data;Data models;NoSQL databases;Spatial databases;Fuzzy set theory;Spatial databases;data storage representations;schema and subschema;fuzzy set;imprecision;consistency;big data;NoSQL systems;JSON,"In this era of big data, as relational databases are inefficient, NoSQL databases are a workable solution for data storage. In this context, one of the key issues is the veracity and therefore the data quality. Indeed, as with classic data, geospatial big data are generally fuzzy even though they are stored as crisp data (perfect data). Hence, if data are geospatial and fuzzy, additional complexities appear because of the complex syntax and semantic features of such data. The NoSQL databases do not offer strict data consistency. Therefore, new challenges are needed to be overcome to develop efficient methods that simultaneously ensure the performance and the consistency in storing fuzzy geospatial big data. This paper presents a new methodology that tackles the storage issues and validates the fuzzy spatial entities' consistency in a document-based NoSQL system. Consequently, first, to better express the structure of fuzzy geospatial data in such a system, we present a logical model called Fuzzy GeoJSON schema. Second, for consistent storage, we implement a schema-driven pipeline based on the Fuzzy GeoJSON schema and semantic constraints.",2021,,10.1109/TBDATA.2017.2725904
59,"Gao, Jerry and Xie, Chunli and Tao, Chuanqi","Big Data Validation and Quality Assurance -- Issuses, Challenges, and Needs",Big data;Quality assurance;Organizations;Q-factor;Standards organizations;Quality assurance;big data quality assurance;big data validation;data validation,"With the fast advance of big data technology and analytics solutions, big data computing and service is becoming a very hot research and application subject in academic research, industry community, and government services. Nevertheless, there are increasing data quality problems resulting in erroneous data costs in enterprises and businesses. Current research seldom discusses how to effectively validate big data to ensure data quality. This paper provides informative discussions for big data validation and quality assurance, including the essential concepts, focuses, and validation process. Moreover, the paper presents a comparison among big data validation tools and several major players in industry are discussed. Furthermore, the primary issues, challenges, and needs are discussed.",2016,,10.1109/SOSE.2016.63
60,"Austin, Claire C.",A Path to Big Data Readiness,Big Data;Government;Standards organizations;Europe;Tools;Big Data;data quality;data checklist;data repository;open science;open government;data science,"""Big Data readiness"" begins at the source where data are first created and extends along a path through an organization to the outside world. This paper focuses on practical solutions to common problems experienced when integrating diverse datasets from disparate sources. Following the Introduction, Section 2 situates Big Data in the larger context of open government, open science, science integrity, and Standards, internationally and in Canada. Section 3 analyses the Big Data problem space, while Section 4 proposes a Big Data solution space. Section 5 proposes eight data checklist modules and suggests implementation strategies to effectively meet a variety of organizational needs. Section 6 summarizes conclusions and describes future work.",2018,,10.1109/BigData.2018.8622229
61,"O’Shea, Enda and Khan, Rafflesia and Breathnach, Ciara and Margaria, Tiziana",Towards Automatic Data Cleansing and Classification of Valid Historical Data An Incremental Approach Based on MDD,Java;Quality assurance;Semantics;Big Data;Tools;Syntactics;Cleaning;Data Collection;Data Analytics;Model-Driven Development;Historical Data;Data Parsing;Data Cleaning;Ethics;Data Assurance;Data Quality,"The project Death and Burial Data: Ireland 1864-1922 (DBDIrl) examines the relationship between historical death registration data and burial data to explore the history of power in Ireland from 1864 to 1922. Its core Big Data arises from historical records from a variety of heterogeneous sources, some aspects are pre-digitized and machine readable. A huge data set (over 4 million records in each source) and its slow manual enrichment (ca 7,000 records processed so far) pose issues of quality, scalability, and creates the need for a quality assurance technology that is accessible to non-programmers. An important goal for the researcher community is to produce a reusable, high-level quality assurance tool for the ingested data that is domain specific (historic data), highly portable across data sources, thus independent of storage technology.This paper outlines the step-wise design of the finer granular digital format, aimed for storage and digital archiving, and the design and test of two generations of the techniques, used in the first two data ingestion and cleaning phases.The first small scale phase was exploratory, based on metadata enrichment transcription to Excel, and conducted in parallel with the design of the final digital format and the discovery of all the domain-specific rules and constraints for the syntax and semantic validity of individual entries. Excel embedded quality checks or database-specific techniques are not adequate due to the technology independence requirement. This first phase produced a Java parser with an embedded data cleaning and evaluation classifier, continuously improved and refined as insights grew. The next, larger scale phase uses a bespoke Historian Web Application that embeds the Java validator from the parser, as well as a new Boolean classifier for valid and complete data assurance built using a Model-Driven Development technique that we also describe. This solution enforces property constraints directly at data capture time, removing the need for additional parsing and cleaning stages. The new classifier is built in an easy to use graphical technology, and the ADD-Lib tool it uses is a modern low-code development environment that auto-generates code in a large number of programming languages. It thus meets the technology independence requirement and historians are now able to produce new classifiers themselves without being able to program. We aim to infuse the project with computational and archival thinking in order to produce a robust data set that is FAIR compliant (Free Accessible Inter-operable and Re-useable).",2020,,10.1109/BigData50022.2020.9378148
62,"Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Marín-Tordera, Eva",Towards a Comprehensive Data LifeCycle Model for Big Data Environments,Data models;Adaptation models;Biological system modeling;Big Data;Organizations;Data acquisition;Computational modeling;Big Data;Data LifeCycle;Data Management;Data Organization;Data Complexity;Vs Challenges,"A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource, however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation. 2. Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity, and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.",2016,,
63,"Setiadi, Yazid and Uluwiyah, Ana",Improving data quality through big data: Case study on big data-mobile positioning data in Indonesia tourism statistics,1/f noise;Mobile communication;Big Data;Accuracy;Mobile Positioning Data;Tourism Statistics,"Big Data is a new concept that has become widely popularised in recent years. The revolutionized meaning of information communication technologies and Internet technologies refers to mobile communications which enable individuals to move and generate, transmit and receive different kinds of information. There are many communication options where users can search, interact and share information with other users such as website, social media, online communities blogs, and email called as Digital transformation. In Digital transformation era, over 95 % of travellers today use digital resources. Digital traveler can be as data source for official statistics. One of methods to capture the number of tourist can use Mobile Positioning Data (MPD). This method is considered able to improve the quality of survey data. This article will discuss further how to improve the quality of survey data through Big Data with case studies of MPD users in Indonesian Tourism Statistics.",2017,,10.1109/IWBIS.2017.8275101
64,"Geronazzo, Angela and Ziegler, Markus",QMLEx: Data Driven Digital Transformation in Marketing Analytics,Itemsets;Soft sensors;Digital transformation;Data integrity;Conferences;Big Data;Feature extraction;Digital transformation;entity linking;topic extraction;word embedding;pattern search;frequent itemset mining;data quality,"This paper presents a data driven approach to replace expert driven business processes. The QMLEx methodology combines NLP algorithms to improve data quality, ML techniques to perform the task and exploits external data sources to eliminate the need for the expert input. The methodology is applied to our internal process devoted to creating groups of products with similar features, one of the most relevant use case in marketing analytics.",2021,,10.1109/BigData52589.2021.9671890
65,"Yu, Wenjin and Liu, Yuehua and Dillon, Tharam and Rahayu, Wenny and Mostafa, Fahed",An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques,Big Data;Internet of Things;Intelligent sensors;Data analysis;Cloud computing;Smart manufacturing;Sensor phenomena and characterization;Big data;health state monitoring;Internet of Things (IoT);noisy data cleaning;real-time systems;sensor selection,"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &#x0026; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.",2022,,10.1109/JIOT.2021.3096637
66,"Ganapathi, Archana and Chen, Yanpei",Data quality: Experiences and lessons from operationalizing big data,Cleaning;Big data;Measurement;Business;Software;Instruments;Industries,"Data quality issues pose a significant barrier to operationalizing big data. They pertain to the meaning of the data, the consistency of that meaning, the human interpretation of results, and the contexts in which the results are used. Data quality issues arise after organizations have moved past clear-cut technical solutions to early bottlenecks in using data. Left unaddressed, such issues can and have led to high profile missteps, and raise doubts about the data-driven world view altogether. In this paper, we share real-world case studies of tackling data quality challenges across industry verticals. We present initial ideas on how to systematically address data quality issues via technology. The success of operationalizing big data will depend on the quality of data involved, and whether such data causes uncertainty and disruptions, or delivers genuine knowledge and value.",2016,,10.1109/BigData.2016.7840769
67,"Yang, Sha and Yu, Wei and Hu, Yahui and Wang, Kai and Wang, Jun and Li, Shijun",An Automatic Discovery Framework of Cross-Source Data Inconsistency for Web Big Data,Big data;Data models;Computers;Data mining;Industries;Algorithm design and analysis;Distributed databases;Web Big Data;Data Consistency;Web Data Management;Data Quality Assessment;Data Analysis,"The vigorous growth of big data has triggered both opportunities and challenges in business and industry. However, Web big data distributed in diverse sources with multiple data structures frequently conflict with each other, i.e. inconsistency in cross-source Web big data. In this paper, we propose a state-of-the-art architecture of auto-discovering inconsistency with Web big data. Our contributions include: (1) we classify the inconsistency features to formalize inconsistency data and establish an algebraic operation system, (2) we propose three algorithms to auto-discover inconsistency, including constraint-based, SDA-based and HPDM-based method and (3) we conduct experiments on real-world dataset to compare aforesaid schemes with Oracle-based inconsistency detection framework. The empirical results show that our methods outperform traditional framework both on accuracy and efficiency under Web big data.",2015,,10.1109/CBD.2015.22
68,"Zuo, Yiming and Vernica, Rares and Lei, Yang and Barcelo, Steven and Rogacs, Anita",A Big Data Platform for Surface Enhanced Raman Spectroscopy Data with an Application on Image-Based Sensor Quality Control,"Quality control;Servers;Cloud computing;Big Data;Visualization;Inspection;Databases;Big data platform, Surface Enhanced Raman Spectroscopy, sensor quality control","Surface-enhanced Raman spectroscopy (SERS) significantly enhances the Raman scattering by molecules, enabling detection and identification of small quantities of relevant bio-/chemical markers in a wide range of applications. In this paper, we present a big data platform with both a local client and cloud server built for acquiring, processing, visualizing and storing SERS sensor data. The local client controls the hardware (i.e., spectrometer and stage) to collect SERS spectra from HP designed sensors, and offers the options to analyze, visualize and save the spectra with meta-data records, including relevant experimental conditions. The cloud server contains remote databases and web interface for centralized data management to users from different locations. Here we describe how this platform was built and demonstrate its use for automated sensor quality control based on sensor images. Sensor quality control is a common practice, employed in sensor production to select high performing sensors. Image-based approach is a natural way to perform sensor quality control without destructing the sensors. Automating this process using the proposed platform can also reduce the time spent and achieve consistent result by avoiding human visual inspection.",2019,,10.1109/MIPR.2019.00093
69,"Du, Jinming",Research on the Construction of Educational Data Quality Model Based on Multiple Constraints Model,Training;Analytical models;Systematics;Databases;Data integrity;Biological system modeling;Education;Educational data;Data quality;Statistics,"With the development of Internet and information technology, data has become an important asset related to the development prospects of society and all walks of life. At present, there are many quality problems in the use of educational data, which has brought great obstacles to exerting the value of educational data. Only by using scientific statistical methods, obtaining real, objective, comprehensive, scientific and effective basic data, and carrying out systematic and comprehensive analysis on the obtained data, can we give full play to its command and decision-making role. The development of big data and artificial intelligence technology provides new ideas for the analysis and evaluation of educational data quality, and is committed to restoring the overall picture of the education system and promoting the change of regional educational ecology. In this paper, an educational data quality analysis model based on multiple constraint model is proposed, which classifies the data in the database, divides the information in the database into several different categories according to the data characteristics, and establishes a quality management system for educational data in universities, so as to effectively improve the quality of educational data.",2021,,10.1109/ICISCAE52414.2021.9590700
70,"Byabazaire, John and O’Hare, Gregory and Delaney, Declan",Using Trust as a Measure to Derive Data Quality in Data Shared IoT Deployments,Data integrity;Data models;Big Data;Biological system modeling;Measurement;Standards;Internet of Things;Data Quality;Internet of Things (IoT);Trust;Big Data Model;Machine learning,"Recent developments in Internet of Things have heightened the need for data sharing across application domains to foster innovation. As most of these IoT deployments are based on heterogeneous sensor types, there is increased scope for sharing erroneous, inaccurate or inconsistent data. This in turn may lead to inaccurate models built from this data. It is important to evaluate this data as it is collected to establish its quality. This paper presents an analysis of data quality as it is represented in Internet of Things (IoT) systems and some of the limitations of this representation. The paper then introduces the use of trust as a heuristic to drive data quality measurements. Trust is a well-established metric that has been used to determine the validity of a piece or source of data in crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework for representing data quality within the big data model. To demonstrate the application of a trust backed framework, we used data collected from a IoT deployment of sensors to measure air quality in which a low cost sensor was co-located with a gold reference sensor. Using data streams modeled based on a dataset from an IoT deployment, our initial results show that the framework's trust score are consistent with the accuracy measure of the machine learning models.",2020,,10.1109/ICCCN49398.2020.9209633
71,"Kim, Hee Young and Cho, June-Suh",Data Governance Framework for Big Data Implementation with a Case of Korea,Big Data;Government;Data privacy;Reliability;Social network services;Big data;Data governance;Data governance framework;Case analysis,"Big Data governance requires a data governance that can satisfy the needs for corporate governance, IT governance, and ITA/EA. While the existing data governance focuses on the processing of structured data, Big Data governance needs to be established in consideration of a broad sense of Big Data services including unstructured data. To achieve the goals of Big Data, strategies need to be established together with goals that are aligned with the vision and objective of an organization. In addition to the preparation of the IT infrastructure, a proper preparation of the components is required to effectively implement the strategy for Big Data services. We propose the Big Data Governance Framework in this paper. The Big Data governance framework presents criteria different from existing criteria at the data quality level. It focuses on timely, reliable, meaningful, and sufficient data services, focusing on what data attributes should be achieved based on the data attributes of Big Data services. In addition to the quality level of Big Data, the personal information protection strategy and the data disclosure/accountability strategy are also needed to achieve goals and to prevent problems. This paper performed case analysis based on the Big Data Governance Framework with the National Pension Service of South Korea. Big Data services in the public sector are an inevitable choice to improve the quality of people's life. Big Data governance and its framework are the essential components for the realization of Big Data service.",2017,,10.1109/BigDataCongress.2017.56
72,"Desai, Vinod and H A, Dinesha",A Hybrid Approach to Data Pre-processing Methods,Big Data;Data integrity;Error analysis;Data mining;Data analysis;Transforms;Time series analysis;Big Data;Data Pre-processing;Data Quality checks,"This is an era of big data, as data is growing exponentially and resources are running out of infrastructure, so it is required to accommodate all the data that gets generated. We collect data in enormous amounts to derive meaningful conclusions, perform effective data analytics and improve decision making. As we don't have enough infrastructures to support data storage for huge volumes, it is needed to clean the data in compulsion. It is a mandatory to carry out a step before doing anything with the data. We call it pre-processing of data and this is carried out in various steps. Pre-processing includes data cleaning, data integration, data filtering, and data transformation and so on. As such preprocessing is not limited to the number of steps or a number of methods or definitive methods. We must innovatively preprocess the data before it is being consumed for data analytics. It has become a responsibility for every data analyst or big data researcher to handpick data for his or her analytics. Considering all these techniques in mind we are proposing a hybrid technique to leverage various algorithms available to pre-process our data along with minor modifications such as at the run time, choosing an algorithm or technique wisely based on the data that we have.",2020,,10.1109/INOCON50539.2020.9298378
73,"Xiangwei, Kong",Evaluation of Flight Test Data Quality Based on Rough Set Theory,Data integrity;Rough sets;Feature extraction;Image color analysis;Shape;Packet loss;Data mining;Flight test;data quality;rough set;quality evaluation,"With the continuous development of flight test technology, the test system is filled with massive, multi-structured, and multi-dimensional data resources. The value of big data has been fully recognized by the society. How to tap the value of data has become an application in various research fields and industries. The most concerned issue of the field. Whether the data is rubbish or treasure, the most important question is whether the data to be analyzed and mined is of high quality. A low-quality data source will not only fail to reflect the value of the data, but may also run counter to the actual situation, which has side effects. In order to effectively evaluate the quality of flight test data, according to the characteristics of test data in flight test, a test data quality evaluation method based on rough set theory is proposed, standard test methods and test indicators for flight test data quality are proposed, and data quality evaluation is given. The method of rule extraction realizes the quality evaluation of flight test data.",2020,,10.1109/CISP-BMEI51763.2020.9263667
74,"Burkhardt, Andrew and Berryman, Sheila and Brio, Ashley and Ferkau, Susan and Hubner, Gloria and Lynch, Kevin and Mittman, Susan and Sonderer, Kathy",Measuring Manufacturing Test Data Analysis Quality,Data integrity;Manufacturing;Measurement;Decision making;Production facilities;Data models;manufacturing test;data quality;test data quality;cost of data quality,"Manufacturing test data volumes are constantly increasing. While there has been extensive focus in the literature on big data processing, less focus has existed on data quality, and considerably less focus has been placed specifically on manufacturing test data quality. This paper presents a fully automated test data quality measurement developed by the authors to facilitate analysis of manufacturing test operations, resulting in a single number used to compare manufacturing test data quality across programs and factories, and focusing effort cost-effectively. The automation enables program and factory users to see, understand, and improve their test data quality directly. Immediate improvements in test data quality speed manufacturing test operation analysis, reducing elapsed time and overall spend in test operations. Data quality has significant financial impacts to businesses [1]. While manufacturing cost models are well understood, data quality cost models are less well understood (see Eppler & Helfert [2] who review manufacturing cost models and create a taxonomy for data quality costs). Kim & Choi [3] discuss measuring data quality costs, and a rudimentary data quality cost calculation is described in [4]. Haug et al. [5] describe a classification of costs for poor data quality, and while they do not provide a cost calculation, they do define optimality for data quality. Laranjeiro et al. [6] have a recent survey of poor data quality classification. Ge & Helfert [7] extend the work in [2], and provide an updated review of data quality costs. Test data is specifically addressed in the context of data processing in [8]. Big data quality efforts are reviewed in [9, 10]. Data quality metrics are discussed in [11], and requirements for data quality metrics are identified in [12]. Data inconsistencies are detailed in [13], while categorical data inconsistencies are explained in [14]. In the current work, manufacturing test data quality is directly correlated to the speed of manufacturing test operations analysis. A measurement for manufacturing test data quality indicates the speed at which analysis can be performed, and increases in the test data quality score have precipitated increases in the speed of analysis, described herein.",2018,,10.1109/AUTEST.2018.8532518
75,"Yu, Bin and Zhang, Chen and Tang, ZhouHua and Sun, JiangYan",Verification method of data quality in science and technology cloud in Shaanxi province,Data integrity;Redundancy;Databases;Dynamic programming;Education;Remuneration;Organizations;Science and Technology Cloud;data quality;data redundancy;missing value processing,This paper analyzes and summarizes the data quality problems in the Shaanxi Science and Technology Resource Coordination Center “Science and Technology Cloud” project. These two major problems about scientific and technological information data quality are verified. One is data redundancy caused by organizations' name abbreviation and the other is partial scientific and technological information data missing. This paper designs and implements solutions to the problems in “Science and Technology Cloud” project. This paper extracts 15643 data from scientific and technical talent pool and scientific literature library. The experimental results verify the effectiveness and feasibility of the solution of data redundancy and data missing.,2018,,10.1109/ICBDA.2018.8367700
76,"Segooa, Mmatshuene Anna and Kalema, Billy Mathias",Improve Decision Making Towards Universities Performance Through Big Data Analytics,Big Data;Organizations;Decision making;Standards organizations;Learning management systems;Market research;Big Data;Big Data Analytics;universities decision making;data quality,"The technology Big Bang has seen organizations universities inclusive generating big volumes of data in various formats and at high speed than they used to do. Such data is referred to as Big Data. This voluminous data can be of great significance to organizations if better insights are drawn for management to improve decision making. However, to draw valued insight from Big Data, advanced forms of analytics need to be employed and such techniques are commonly known as Big Data analytics (BDA), This paper sough to report on analysis of factors influencing the leverage of BDA to improve performance in universities.",2018,,10.1109/ICABCD.2018.8465132
77,"Deng, Wei and Guo, Yixiu and Liu, Jie and Li, Yong and Liu, Dingguo and Zhu, Liang",A missing power data filling method based on improved random forest algorithm,Filling;Power systems;Random forests;Interpolation;Data models;Big Data;Data mining;Big data cleaning;missing data filling;data preprocessing;random forest;data quality,"Missing data filling is a key step in power big data preprocessing, which helps to improve the quality and the utilization of electric power data. Due to the limitations of the traditional methods of filling missing data, an improved random forest filling algorithm is proposed. As a result of the horizontal and vertical directions of the electric power data are based on the characteristics of time series. Therefore, the method of improved random forest filling missing data combines the methods of linear interpolation, matrix combination and matrix transposition to solve the problem of filling large amount of electric power missing data. The filling results show that the improved random forest filling algorithm is applicable to filling electric power data in various missing forms. What's more, the accuracy of the filling results is high and the stability of the model is strong, which is beneficial in improving the quality of electric power data.",2019,,10.23919/CJEE.2019.000025
78,"Ying, KangHui and Hu, WenYu and Chen, Jin Bo and Li, Guo Nong",Research on instance-level data cleaning technology,Electrical engineering;Data integrity;Big Data;Cleaning;Data mining;Artificial intelligence;Research and development;instance-level data cleaning;duplicate records cleaning;attribute cleaning,"Effectivedata analysis and data mining are based on data availability and data quality. Data cleaning is a commonly used technique to improve data quality. Instance-level data cleaning is an important part of data cleaning. The focus is on the comparison and analysis of the detection and cleaning methods of attributes and recorded values in the instance-level data cleaning technology, and the experimental analysis of the repeated record cleaning methods. This paper introduces the application field of data cleaning technology represented by the electrical engineering field combined with the application situation, and provides valuable selection suggestions for the characteristics of different data sets and the applicable instancelevel data cleaning technology. Summarizing and analyzing the existing detection and cleaning technology methods, it is concluded that instance-level data cleaning has a lot of research and development space in long text, unstructured data and specific fields. Finally, the challenges and development directions of the instance-level data cleaning technology are prospected.",2021,,10.1109/CAIBDA53561.2021.00057
79,"Alqarni, Mohammed A.",Benefits of SDN for Big data applications,Big Data applications;Optimization;Servers;Protocols;Multimedia communication;SDN;Network Virtualization;Software Defined Networks;Big data,"Big data applications depend on underlying networks that make the transfer of information possible. These networks may be real (conventional) or virtual (in case of services hosted in data centers). Either way, the responsibility of smooth execution of the application, despite increasing traffic volume, lies with the service provider. The service providers face many challenges with respect to providing a high quality of service. It is therefore in the best interest of the service providers that efficiency of the applications is increased. SDN has the potential to improve big data application performance. In this paper we have a look at the recent advancements in technology that helps improve big data applications using SDN and discuss our observations.",2017,,10.1109/HONET.2017.8102206
80,"Zillner, Sonja and Oberkampf, Heiner and Bretschneider, Claudia and Zaveri, Amrapali and Faix, Werner and Neururer, Sabrina",Towards a technology roadmap for big data applications in the healthcare domain,Medical services;Big data;Semantics;Data privacy;Standards;Biomedical imaging;Security;Big Data;technical requirements;data digitalization;semantic annotation;data integration;data privacy and security;data quality,"Big Data technologies can be used to improve the quality and efficiency of healthcare delivery. The highest impact of Big Data applications is expected when data from various healthcare areas, such as clinical, administrative, financial, or outcome data, can be integrated. However, as of today, the seamless access to the various healthcare data pools is only possible in a very constrained and limited manner. For enabling the seamless access several technical requirements, such as data digitalization, semantic annotation, data sharing, data privacy and security as well as data quality need to be addressed. In this paper, we introduce a detailed analysis of these technical requirements and show how the results of our analysis lead towards a technical roadmap for Big Data in the healthcare domain.",2014,,10.1109/IRI.2014.7051902
81,"Auer, Florian and Felderer, Michael",Addressing Data Quality Problems with Metamorphic Data Relations,"Big Data;Data integrity;Testing;Encyclopedias;Internet;Electronic publishing;metamorphic testing, data quality, big data, quality assessment, metamorphic data relations","In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.",2019,,10.1109/MET.2019.00019
82,"O'Leary, Daniel E.",Embedding AI and Crowdsourcing in the Big Data Lake,Crowdsourcing;Artificial intelligence;Big data;Data warehouses;Decision support systems;Databases;Business;Big Data Lake;data warehouses;artificial intelligence;crowdsourcing;data governance;master data management;intelligent systems,"Daniel E. O'Leary examines the notion of the Big Data Lake and contrasts it with decision support-based data warehouses. In addition, some of the risks of the emerging Lake concept that ultimately require data governance are analyzed. O'Leary investigates using different AI and crowdsourcing (human intelligence) applications in that lake in order to integrate disparate data sources, facilitate master data management and analyze data quality. Although data governance often is not seen as a technology issue, it is seen as a critical component of making the Big Data Lake ""work"".",2014,,10.1109/MIS.2014.82
83,"Colborne, Adrienne and Smit, Michael",Identifying and mitigating risks to the quality of open data in the post-truth era,Big Data;Meteorology;Portals;Voting;open data;post-truth;fake news;risk identification;risk mitigation;data quality assurance,"Big Data analysis often relies on open data, integrating it with large private data sets, using it as ground truth information, or providing it as part of the input to large simulations. Data can be released openly by governments to achieve various objectives: transparency, informing citizen engagement, or supporting private enterprise, to name a few. To the latter objective, Big Data analytics algorithms rely on high-quality, timely access to various data sources, including open data. Examples include retail analytics drawing on open demographic data and weather forecast systems drawing on open weather and climate data. In this paper, we describe the rise of post-truth in society, and the risks this poses to the quality, integrity, and authenticity of open data. We also discuss approaches to identifying, assessing, and mitigating these risks, and suggest future steps to manage this data quality concern.",2017,,10.1109/BigData.2017.8258218
84,"Mylavarapu, Goutam and Thomas, Johnson P and Viswanathan, K Ashwin",An Automated Big Data Accuracy Assessment Tool,Data integrity;Big Data;Tools;Data models;Standards;Machine learning;Couplings;Data accuracy;contextual accuracy;data quality;word embeddings;record linkage;k-nearest neighbors;logistic regression;decision trees,"Data analysis is the most important aspect of any business as it is critical to decision-making. Data quality assessment is a necessary function to be performed before data analysis, as the quality of data has high impact on the outcome of the analysis. Data quality is a multi-dimensional factor that affects the analysis in numerous ways. Among all the dimensions, accuracy is the most important and hardest dimension to assess. With the advent of big data, this problem becomes more complicated. There are only few studies that focus on data accuracy with minimal domain expert dependency. In this paper, we propose an extensive data accuracy assessment tool that uses machine learning to determine the accuracy of data. In addition, our model also addresses the intrinsic and contextual categories of data accuracy. Our model was developed on Apache Spark which serves as the big data environment for handling large datasets.",2019,,10.1109/ICBDA.2019.8713218
85,"Ku, Tai-Yeon and Park, Wan-Ki and Choi, Hoon",Mechanism of a big-data platform for residential heat energy consumption,Energy consumption;Temperature distribution;Water storage;Data integrity;Water heating;Solar energy;Big Data;energy management;energy big data;energy information collection,"Although the solar energy industry is becoming widespread, it is necessary to manage the charging and generating scheduling of solar power generation according to the ever-changing climate environment. In order to do this, a judgment criterion that can give timely charge / discharge instructions is needed and it needs to be actively performed. In this paper, we define a big-data platform for residential heat energy consumption. As a technology to secure thermal energy data of apartment houses, collect thermal energy data by dividing it into supply/equipment/usage. In order to secure standardized thermal energy data from the calorimeter installed. Equipped with data classification and processing, LP storage and management, data quality measurement and analysis functions. Develop a data adapter, from several multiunit dwellings with different calorimeter types. We will collect thermal energy data with an integrated big data system.",2021,,10.1109/ICTC52510.2021.9620761
86,"Li, Tao and He, Yihai and Zhu, Chunling",Big Data Oriented Macro-Quality Index Based on Customer Satisfaction Index and PLS-SEM for Manufacturing Industry,Indexes;Mathematical model;Data models;Manufacturing industries;Computational modeling;Big data;Customer satisfaction;macro-quality evaluation;big data;macro-quality index;customer satisfaction index;PLS-SEM,"The aim of this paper was to develop a novel macro-quality index driven by big quality data regarding the macro-quality management demands of manufacturing enterprises in Industry 4.0. Firstly, the connotation of big data of macro-quality management in manufacturing industry is expounded, which is the collection of the quality data in product lifecycle including the product quality data, process quality data and organizational ability data. Secondly, referring to the customer satisfaction index theory, a new big data oriented macro-quality index computation model based on the partial least square-structural equation modeling(PLS-SEM) theory is proposed, and the partial least square(PLS) is adopted to estimate the path parameters. Finally, a case study of macro-quality situation evaluation for the manufacturing industry of a city in China is presented. The final result shows that the proposed macro-quality index model is applicable and predictable.",2016,,10.1109/ICIICII.2016.0052
87,"Zan, Songting and Zhang, Xu",Medical Data Quality Assessment Model Based on Credibility Analysis,Data models;Data integrity;Big Data;Mathematical model;Analytical models;Analytic hierarchy process;Surgery;data quality evaluation;medical;credibility analysis;analytic hierarchy process,"The current main problem of medical data is low accuracy. Although existing data quality evaluation and audit can meet the needs, network security problem will be more serious in the future. As a result, the data quality evaluation model must include data credibility. This paper proposes the medical big data quality evaluation model based on credibility analysis and Analytic Hierarchy Process(AHP). Firstly, analyze the data credibility. After eliminating the unreliable data, calculate the data quality dimensions respectively. Then obtain the data quality evaluation result by integrating all dimensions with AHP. Through simulation, the data quality evaluation model can effectively identify the data that affects the credibility. The data quality evaluation results are improved after removing the untrusted data, besides the amount of data is reduced, which is applicable to the big data scenario.",2018,,10.1109/ITOEC.2018.8740576
88,"Wang, Jianwu and Tang, Yan and Nguyen, Mai and Altintas, Ilkay",A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning,Big data;Engines;Bayes methods;Partitioning algorithms;Accuracy;Algorithm design and analysis;Distributed databases;Big Data;Bayesian network;Distributed computing;Ensemble learning;Scientific workflow;Kepler;Hadoop,"In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.",2014,,10.1109/BDC.2014.10
89,"Hattawi, Waleed and Shaban, Sameeh and Shawabkah, Aon Al and Alzu’bi, Shadi",Recent Quality Models in BigData Applications,Analytical models;Area measurement;Government;Energy measurement;Medical services;Big Data;Data models;Big Data;Quality;Quality Models;Data Quality;Big Data Application;The 7v's Of Big Data,"In this time the big data became an important part of all areas, it can be used in multiple industrials such as banking, education, government, networking, energy, health care, etc. So, because of that the huge amount of data became have problems or unnecessary data, and so that comes from the difficulty of measure the quality of these data. In this research we show the quality characteristic that can be help to increase the efficiency of quality measurement process of BDA by comparing it with other quality model of BDA and applying it on the 7V's of big data.",2021,,10.1109/ICIT52682.2021.9491629
90,"Chengzan, Li and Yanfei, Hou and Jianhui, Li and Lili, Zhang",ScienceDB: A Public Multidisciplinary Research Data Repository for eScience,Metadata;Business;Data visualization;Data mining;Distributed databases;Collaboration;Computer architecture;research data repository;technical framework;multidiscipline;data recommendation;data collaboration;open data,"Research data repositories are necessary infrastructures that ensure the data generated for research are accessible, stable, reliable, and reusable. Based on years of accumulated data work experience, the Computer Network Information Center of the Chinese Academy of Sciences has built a multi-disciplinary data repository ScienceDB for research users and teams using its big data storage, analysis and computing environments. This paper firstly introduces the motivation to develop ScienceDB and gives a profile to it. Then the overall technical framework of ScienceDB is introduced, and the key technologies such as the support for multidiscipline extensibility, data collaboration and data recommendation are analyzed deeply. And then this paper presents the functions and features of ScienceDB's current version and discusses some issues such as its data policy, data quality assurance measures, and current application status. Finally, it summarizes and puts forward that it needs to carry out more in-depth research and practice of ScienceDB in order to meet the higher requirements of eScience in terms of thorough data association and fusion, data analysis and mining, data evaluation, and so on.",2017,,10.1109/eScience.2017.38
91,"Ludwig, Heiko",Managing Big Data Effectively - A Cloud Provider and a Cloud Consumer Perspective,Big data;Cloud computing;Organizations;Face;Conferences;Abstracts;Instruments,"Summary form only given. Instrumentation of processes and an organization's environment provides vast amounts of data that can be used to drivedecisions. Next to setting up data collection, supervising data quality, and applying proper methods of analysis, organizationsface the challenge to set up an infrastructure and architecture to do so efficiently and cost-effectively. Virtualized platformssuch as private or public clouds are the method of choice for deployment, in particular for data analyses not occurringconstantly. A cloud provider, either a commercial Cloud company or an IT organization within an enterprise, will like to set upa cloud platform such that clients can run big data workloads effectively on. Cloud customers would like to set up big dataapplications in a cost-effective and performant way on their platform.This keynote will walk through a few real life big data analysis scenarios from different industries and discuss thechallenges Cloud providers face making trade-offs. Understanding those challenges and solutions help cloud users choose theright match between their algorithm, big data system and cloud platform.",2014,,10.1109/EDOC.2014.21
92,"Ahnn, Jong Hoon",A Practical Approach to Scalable Big Data Computing for the Personalization of Services at Samsung,Adaptation models;Speech;Acoustics;Computational modeling;Engines;Big data;Speech recognition;Big Data;Cloud Computing;Hadoop;Speech Recognition;Scalability;Personalization,"We observe that the recent advances in big data computing have empowered the personalization of service including model-based services such as speech recognition, face recognition, and context-aware service. Various sources of user's logs can be utilized in remodeling, adapting, and personalizing pretrained models to improve the quality of service. We propose a system that can support store/retrieve data and process them in a scalable manner on top of Samsung' big data infrastructure. An automatic speech recognition (ASR) service such as Samsung's S-Voice, Apple's SIRI is one of the representative examples. Recently advances in ASR married with big data technologies drive more personalized services in many areas of services. A speaker adaptation is now a well-accepted technology that requires huge computation cost in creating a personalized acoustic model and corresponding language model over several billions of Samsung product users. We implement a personalized and scalable ASR system powered by the big data infrastructure which brings data-driven personalized opportunities to voice-enabled services such as voice-to-text transcriber, voice-enabled web search in a peta bytes scale. We verify the feasibility of speaker adaptation based on 107 testers' recordings and obtain about 10% of recognition accuracy. An optimal set of performance optimization is suggested to have the best performance such as workflow compaction, file compression, best file system selection among several distributed file systems.",2014,,10.1109/BDC.2014.11
93,"Ding, Junhua and Kang, Xiaojun and Hu, Xin-Hua and Gudivada, Venkat",Building a Deep Learning Classifier for Enhancing a Biomedical Big Data Service,Diffraction;Big Data;Machine learning;Morphology;Support vector machines;Three-dimensional displays;Noise measurement;machine learning;deep learning;big data;data quality;cross-validation,"Providing an easily accessible data service with high quality data is important for building big data applications. In this paper, we introduce a big data service for managing and accessing massive-scale biomedical image data. The service includes three major components: a NoSQL database for storing images and data analytics results, a client consisting of a group of query scripts for data access and management, and a data quality enhancement component for improving the performance of data analytics. Low-quality data can result in incorrect analytics results and may lead to no value even harmful conclusions. Therefore, it is important to provide an effective mechanism for ensuring data quality improvement in a big data service. We describe the implement ion of a deep learning classifier to automatically filter low quality data in datasets. To improve the effectiveness of data separation, the classifier is rigorously validated with synthetic data generated by a collection of scientific tools. Design of big data services with data quality improvement as an integral component, along with the best practices collected from this experimental study, will help researchers and practitioners to develop strategies for improving the quality of big data services, building big data applications, and designing machine learning classifiers.",2017,,10.1109/SCC.2017.25
94,"Qi, Cui and Mingyue, Sun and Na, Mi and Honggang, Wang and Yanhong, Jian and Jing, Zhu",Regional Electricity Sales Forecasting Research Based on Big Data Application Service Platform,Recurrent neural networks;Power supplies;Time series analysis;Predictive models;Big Data applications;Prediction algorithms;Data models;Recurrent neural network;Long Short-Term Memory neural network;regional monthly electricity sales;electricity sales forecast;big data application service platform,"Regional monthly electricity sales forecast is an important basis for regional power grid planning and construction, evaluation of regional economic development and operation, and protection of residents' lives. It is also an important work of regional power regulation and management, decision-making of power generation and purchase, improvement of power supply equipment utilization rate and deepening of power system reform. Based on the current situation of power supply enterprise information development, distribution network business status and characteristics, this paper analyzes the factors affecting electricity sales. According to the characteristics of annual changes in electricity sales and data quality factors, the recurrent neural network model is selected based on the big data application service platform. The long short term memory neural network model performs multi-step multivariate prediction on time series, and uses the attention mechanism to combine two independent models for prediction. Experiments conducted on the historical electricity sales data set of a power supply company show that compared with traditional machine learning methods, this method has advantages in accuracy and efficiency.",2020,,10.1109/ICECE51594.2020.9352886
95,"Serhani, Mohamed Adel and El Kassabi, Hadeel T. and Taleb, Ikbal",Quality Profile-Based Cloud Service Selection for Fulfilling Big Data Processing Requirements,Big Data;Quality of service;Task analysis;Cloud computing;Data models;Analytic hierarchy process;Mathematical model;Big Data Task profile;Cloud service selection;QoS,"Big data has emerged as promising technology to handle huge and special data. Processing Big data involves selecting the appropriate services and resources thanks to the variety of services offered by different Cloud providers. Such selection is difficult, especially if a set of Big data requirements should be met. In this paper, we propose a dynamic cloud service selection scheme that assess Big data requirements, dynamically map these to the most available cloud services, and then recommend the best match services that fulfill different Big data processing requests. Our selection is conducted in two stages: 1) relies on a Big data task profile that efficiently capture Big data task's requirements and map them to QoS parameters, and then classify cloud providers that best satisfy these requirements, 2) uses the list of selected providers from stage 1 to further select the appropriate Cloud services to fulfill the overall Big Data task requirements. We extend the Analytic Hierarchy Process (AHP) based ranking mechanism to cope with the problem of multi-criteria selection. We conduct a set of experiments using simulated cloud setup to evaluate our selection scheme as well as the extended AHP against other selection techniques. The results show that our selection approach outperforms the others and select efficiently the appropriate cloud services that guarantee Big data task's QoS requirements.",2017,,10.1109/SC2.2017.30
96,"Rathore, Purva and Shukla, Deepak",Analysis and performance improvement of K-means clustering in big data environment,Image recognition;Data visualization;Breast;Image segmentation;data mining;clustering;big data;performance improvement;implementation,"The big data environment is used to support the huge amount of data processing. In this environment tons (i.e. Giga bytes, Tera bytes) of data is processed. Therefore the various online applications where the huge data request are generated are treated using the big data i.e. facebook, google. In this presented work the big data environment is studied and investigated how the data is consumed using the big data and how the supporting tools are working with the Hadoop storage. Furthermore, for keen understanding and investigation, a cluster analysis technique more specifically the K-mean clustering algorithm is implemented through the Hadoop and MapReduce. The clustering is a part of big data analytics where the unlabelled data is processed and utilized to make groups of the data. In addition of that it is observed the traditional k-mean algorithm is not much suitably works with the Hadoop and MapReduce thus small amount of modification is performed on the data processing technique. In addition of that during cluster analysis various issues are found in traditional k-means i.e. fluctuating accuracy, outliers and empty cluster. Therefore a new clustering algorithm with modification on traditional approach of k-means clustering is proposed and implemented. That approach first enhances the data quality by removing the outlier points in datasets and then the bi-part method is used to perform the clustering. The proposed clustering technique implemented using the JAVA, Hadoop and MapReduce finally the performance of the proposed clustering approach is evaluated and compared with the traditional k-means clustering algorithm. The obtained performance shows the effective results and enhanced accuracy of cluster formation with the removal of the de-efficiency. Thus the proposed work is adoptable for the big data environment with improving the performance of clustering.",2015,,10.1109/ICCN.2015.9
97,"Hossen, Md Ismail and Goh, Michael and Hossen, Abid and Rahman, Md. Armanur",A Study on the Aspects of Quality of Big Data on Online Business and Recent Tools and Trends Towards Cleaning Dirty Data,Data integrity;Tools;Companies;Cleaning;Task analysis;Machine learning;Regulation;E-business;Big data;data quality;dirty data;machine learning,"The reliability, efficiency, and accuracy of e-business depend on the quality of data that is associated with a buyer, seller, brokers, e-business portals, admins, managers, decision-makers and so on. However, maintaining the quality of data in e-business is very challenging. It is because e-business data typically comes from different communication channels and sources. Integrating and managing the data quality of different sources is generally much troublesome than dealing with traditional business data. Even though there are several data cleaning methods and tools exist those methods and tools have some constraints. None of them directly working, particularly on e-business data that motivates to do research to highlight the aspects of big data quality related to e-business. Therefore, this research demonstrates the problems related to data quality related to online business, discusses the existing literature of data quality, the current tools and techniques that are being used for data quality and provides a research finding highlighting the weaknesses of current tools to address the problem of online business.",2020,,10.1109/ICSGRC49013.2020.9232648
98,"Adnan, Kiran and Akbar, Rehan and Wang, Khor Siak",Towards Improved Data Analytics Through Usability Enhancement of Unstructured Big Data,Bridges;Analytical models;Data analysis;Data integrity;Decision making;Data visualization;Big Data;Unstructured data;data usability;unstructured data analytics;pragmatic data quality,"A high volume of unstructured data is being generated from diverse and heterogeneous sources. The unstructured data analytics process is used to extract valuable insights from these unstructured data sources but unlocking useful and usable information is critical for analytics. Despite advancements in technologies, data preparation requires an inordinate amount of time in unstructured data manipulation into a usable form. Although several data manipulation and preparation techniques have been proposed for unstructured big data, relatively limited research has addressed the usability issues of unstructured data. This study identifies the usability issues of unstructured big data for the analytical process to bridge the identified gap. The usability enhancement model has been proposed for unstructured big data to facilitate the subjective and objective efficacy of unstructured big data for data preparation and manipulation activities. Moreover, concept mapping is an essential element to improve the usability of unstructured big data incorporated in the proposed model with usability rules. These rules reduce the usability gap between data availability and its usefulness for an intended purpose. The proposed research model will help to improve the efficiency of unstructured big data analytics.",2021,,10.1109/ICCOINS49721.2021.9497187
99,"Pan, Lingling and Liu, Jun and Li, Feng",Multi-dimensional Index Construction of Electric Power Multi-source Measurement Data considering Spatio-temporal Correlation,Big Data;Power grids;Indexes;Power measurement;Time measurement;Memory;power big data;multi-source heterogeneous data;spatio-temporal correlation;data storage;multi-dimensional index,"The operation of complex AC/DC power grid changes rapidly and dynamically, which objectively puts forward higher requirements for on-line analysis, and it is urgent to improve the basic data quality of power grid. Because of low quality and poor synchronization of the basic data of power grid, it is impossible to accurately map the actual operation of the power grid. At the same time, the cross-system data matching degree is low and the data correlation is poor, so it can not support the multi-scale data analysis for all kinds of applications. In this paper, the associated method of multi-source heterogeneous data in the power grid is studied. Combined with big data's access characteristics, big data storage, big data retrieval and artificial intelligence technology, the high-speed data storage and index architecture of power big data are constructed, and a multi-dimensional index reflecting the associated relationship of operating data is established from the dimensions of time, space, application, device and so on. It is easier to analyze multi-source data, to improve the basic data quality of power grid, which provides effective support for accurate data analysis and evaluation of power grid.",2019,,10.1109/IAEAC47372.2019.8997699
100,"Islam Sarker, Md Nazirul and Wu, Min and Chanthamith, Bouasone and Yusufzada, Shaheen and Li, Dan and Zhang, Jie",Big Data Driven Smart Agriculture: Pathway for Sustainable Development,Agriculture;Big Data;Production;Sensors;Systematics;Sociology;Statistics;big data;smart agriculture;data driven;precision agriculture;smart farming,"Increasing agricultural production is top most solution in the face of rapid population growth through digitalization of agriculture by using most developed technology like big data. There is a long debate on the application of big data in agriculture. This study is an attempt to explore the suitability of the big data technologies for increasing production and improving quality in agriculture. The study uses an extensive review of current research works and studies in agriculture for exploring the best and compatible practices which can help farmers at field level for increasing production and improving quality. This study reveals a number of available big data technologies and practices in agriculture for solving the current problems and challenges at field level. A conceptual model is developed for proper implementation of available big data technologies at farmer's field level. The study highlights data generation procedure, availability of technology, availability of hardware, software, data collection techniques, method of analysis and suitability of application of big data technologies for smart agriculture. The article explores that there are still some challenges exists in this field as a new domain in agriculture like privacy of data, data quality, availability, initial investment, infrastructure and related expertise. The study suggests that government initiatives, public-private partnership, openness of data, financial investment and regional basis research work are necessary for implementing the big data technologies in agriculture at large scale.",2019,,10.1109/ICAIBD.2019.8836982
101,"Palacio, Ana León and López, Óscar Pastor",From big data to smart data: A genomic information systems perspective,Genomics;Bioinformatics;Big Data;Data integrity;Diseases;Data models;Conceptual Modelling;Data Quality;Big Data;Smart Data;Genomics,"During the last two decades, data generated by Next Generation Sequencing Technologies have revolutionized our understanding of human biology and improved the study on how changes (variations) in the DNA are involved in the risk of suffering a certain disease. A huge amount of genomic data is publicly available and frequently used by the research community in order to extract meaningful and reliable gene-disease relationships. However, management of this exponential growth of data has become a challenge for biologists; under such a big data problem perspective, they are forced to delve into a lake of complex data spread in over thousand heterogeneous repositories, represented in multiple formats and with different levels of quality; but when data are used to solve a concrete problem only a small part of that “data lake” is really significant; this is what we call the “smart” data perspective. Using conceptual models and the principles of data quality management, adapted to the genomic domain, we propose a systematic approach to move from a big data to a smart data perspective. The aim of this approach is to populate an Information System with genomic data which must be accessible, informative and actionable enough to extract valuable knowledge.",2018,,10.1109/RCIS.2018.8406658
102,"Benbernou, Salima and Huang, Xin and Ouziri, Mourad",Semantic-Based and Entity-Resolution Fusion to Enhance Quality of Big RDF Data,Big Data;Resource description framework;Semantics;Joining processes;Erbium;Organizations;Data quality;big data fusion;inferences;entity resolution;query rewriting,"Within an organisation, the quality in big data is a cornerstone to operational, transactional processes and to the reliability of business analytics for decision making. In fact, as organizations are harnessing multi-sources data to rise the benefits of their business, the quality of data becomes important and crucial. This paper presents a new approach to query big data sources using Resource Description Framework (RDF) representation to ensure data quality by harvesting more relevant and complete query results. Our approach handles two important types of heterogeneity over multiple data sources: semantic heterogeneity and URI-based entity identification. It proposes (1) a semantic entity resolution method based on inference mechanism using rules to manage the misunderstanding of data, in real world entities (2) Data Quality enhancement using MapReduce-based query rewriting approach includes the entity resolution results to infer and adds implicit data into query results (3) a parallel combination of MapReduce jobs of saturation and query rewriting inferences to handle transitive and cyclic rules for a richer rules' expression language (4) experiments to assess the efficiency of the proposed approach over real big RDF data originating from insurance and synthetic data sets.",2021,,10.1109/TBDATA.2017.2710346
103,"Zhang, Zhenwei and Wu, Wenyan and Wu, Dongjie",A Multi-Mode Learning Behavior Real-time Data Acquisition Method Based on Data Quality,Measurement;Data integrity;Data acquisition;Learning (artificial intelligence);Interference;Data models;Real-time systems;multi-mode Data;Learning behavior;data quality;data acquisition,"With the rapid development of new technologies such as artificial intelligence, big data, and the Internet of Things, many researchers have probed into the study of learning analysis, trying to solve the problems of teaching by analyzing the learning behavior data from learning process. And in many learning behavior research, the sensor network usually consists of a host of mutually independent data sources, which can be used to monitor measured objects from multiple dimensions thereby obtaining the multi-source multi-modal sensory data. However, there still exist false negative readings, false positive readings and environmental interference, etc. Therefore, we propose a multi-source multimode sensory data acquisition method based on Date Quality(DQ). We first define the data quality in terms of four aspects-accuracy, integrity, consistency and instantaneity. Then, by the modeling there aspects respectively, we propose metrics to estimate the comprehensive data quality method of multi-source multi-mode sensory data. Finally, a data acquisition method is presented based on data quality, which selects a part of data sources for data transmission according to the given precision. This method aims at reducing the consumption of the sensory network on the premise of the data quality guarantee. An extensive experimental evaluation demonstrates the efficiency and effectiveness of the algorithm.",2021,,10.1109/ISCEIC53685.2021.00021
104,"Farooqi, M. Mashab and Ali Khattak, Hasan and Imran, Muhammad",Data Quality Techniques in the Internet of Things: Random Forest Regression,Data integrity;Data models;Internet of Things;Cleaning;Meteorology;Sensors;Predictive models;data quality;internet of things;big data;machine learning,"Internet of Things (IoTs) is one of the most promising fields in computer science. It consists of physical devices, automobiles, home appliances, embedded hardware, sensors and actuators which empowers these objects to interface and share information with other devices over the network. The data gathered from these devices is used to make intelligent decisions. If the data quality is poor, decisions are likely to be flawed. A little work has been carried out regarding data quality in the Internet of Things, but there is no scheme which is experimentally proved. In this paper we will identify data quality challenges in the Internet of Things domain and propose a model which ensure data quality standards provided by ISO 8000. We evaluated our model on the weather dataset and used the random forest prediction method to calculate the accuracy of our data. Results show that when compared with the baseline model the proposed system improves accuracy by 38.88%.",2018,,10.1109/ICET.2018.8603594
105,"Meng, Qianyu and Wang, Kun and He, Xiaoming and Guo, Minyi",QoE-driven big data management in pervasive edge computing environment,Big Data;Quality of experience;Edge computing;Quality of service;Computational modeling;Training;Streaming media;Quality-of-Experience (QoE); high-dimensional big data management; deep learning; pervasive edge,"In the age of big data, services in the pervasive edge environment are expected to offer end-users better Quality-of-Experience (QoE) than that in a normal edge environment. However, the combined impact of the storage, delivery, and sensors used in various types of edge devices in this environment is producing volumes of high-dimensional big data that are increasingly pervasive and redundant. Therefore, enhancing the QoE has become a major challenge in high-dimensional big data in the pervasive edge computing environment. In this paper, to achieve high QoE, we propose a QoE model for evaluating the qualities of services in the pervasive edge computing environment. The QoE is related to the accuracy of high-dimensional big data and the transmission rate of this accurate data. To realize high accuracy of high-dimensional big data and the transmission of accurate data through out the pervasive edge computing environment, in this study we focused on the following two aspects. First, we formulate the issue as a high-dimensional big data management problem and test different transmission rates to acquire the best QoE. Then, with respect to accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on deep learning, which is suitable for high-dimensional big data analysis in the pervasive edge computing environment. Our simulation results reveal that our proposed algorithm can achieve high QoE performance.",2018,,10.26599/BDMA.2018.9020020
106,"Liu, Lixin and Chen, Jun",The influences of deep-sea vision data quality on observational analysis,Scattering;Optical sensors;Optical imaging;Fish;Backscatter;deep-sea observation;vision data quality;automatic judging,"Deep-sea study by optical observation method is an interdisciplinary subject and faces plenty of difficulties. To find out the influences of vision data quality, characteristic of vision data for deep-sea observation is analyzed, and a deep-sea landing experiment has been implemented. Data quality analyzing based on real deep-sea vision data that collected in the in-situ observation platform is actualized. It is expected that the research on influence mechanism of deep-sea vision quality is beneficial to the detection of region of interest, the judging of animal existence, the classification of species, and the trajectories labeling. Further analyzing on unsupervised deep-sea vision data quality control is necessary.",2017,,10.1109/BigData.2017.8258543
107,"Luo, Xin and Chen, Minzhi and Wu, Hao and Liu, Zhigang and Yuan, Huaqiang and Zhou, MengChu",Adjusting Learning Depth in Nonnegative Latent Factorization of Tensors for Accurately Modeling Temporal Patterns in Dynamic QoS Data,Tensors;Big Data;Quality of service;Computational efficiency;Machine learning;Web services;Algorithm;big data;dynamics;high-dimensional and incomplete (HDI) data;machine learning;missing data estimation;multichannel data;nonnegative latent factorization of tensors (NLFT);temporal pattern;quality of service (QoS);web service,"A nonnegative latent factorization of tensors (NLFT) model precisely represents the temporal patterns hidden in multichannel data emerging from various applications. It often adopts a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. However, learning depth in this algorithm is not adjustable, resulting in frequent training fluctuation or poor model convergence caused by overshooting. To address this issue, this study carefully investigates the connections between the performance of an NLFT model and its learning depth via SLF-NMUT to present a joint learning-depth-adjusting scheme for it. Based on this scheme, a Depth-adjusted Multiplicative Update on tensor algorithm is innovatively proposed, thereby achieving a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Empirical studies on two industrial data sets demonstrate that compared with the state-of-the-art NLFT models, a DNL model achieves significant accuracy gain when performing missing data estimation on a high-dimensional and incomplete tensor with high efficiency. Note to Practitioners—Multichannel data are often encountered in various big-data-related applications. It is vital for a data analyzer to correctly capture the temporal patterns hidden in them for efficient knowledge acquisition and representation. This article focuses on analyzing temporal QoS data, which is a representative kind of multichannel data. To correctly extract their temporal patterns, an analyzer should correctly describe their nonnegativity. Such a purpose can be achieved by building a nonnegative latent factorization of tensors (NLFT) model relying on a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. But its learning depth is not adjustable, making an NLFT model frequently suffer from severe fluctuations in its training error or even fail to converge. To address this issue, this study carefully investigates the learning rules for an NLFT model’s decision parameters using an SLF-NMUT and proposes a joint learning-depth-adjusting scheme. This scheme manipulates the multiplicative terms in SLF-NMUT-based learning rules linearly and exponentially, thereby making the learning depth adjustable. Based on it, this study builds a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Compared with the existing NLFT models, a DNL model better represents multichannel data. It meets industrial needs well and can be used to achieve high performance in data analysis tasks like temporal-aware missing data estimation",2021,,10.1109/TASE.2020.3040400
108,"Bonner, Stephen and McGough, Andrew Stephen and Kureshi, Ibad and Brennan, John and Theodoropoulos, Georgios and Moss, Laura and Corsar, David and Antoniou, Grigoris",Data quality assessment and anomaly detection via map/reduce and linked data: A case study in the medical domain,Resource description framework;Medical diagnostic imaging;Medical services;Big data;Sensors;Biomedical monitoring;RDF;Medical Data;Map / Reduce;Joins,"Recent technological advances in modern healthcare have lead to the ability to collect a vast wealth of patient monitoring data. This data can be utilised for patient diagnosis but it also holds the potential for use within medical research. However, these datasets often contain errors which limit their value to medical research, with one study finding error rates ranging from 2.3%-26.9% in a selection of medical databases. Previous methods for automatically assessing data quality normally rely on threshold rules, which are often unable to correctly identify errors, as further complex domain knowledge is required. To combat this, a semantic web based framework has previously been developed to assess the quality of medical data. However, early work, based solely on traditional semantic web technologies, revealed they are either unable or inefficient at scaling to the vast volumes of medical data. In this paper we present a new method for storing and querying medical RDF datasets using Hadoop Map / Reduce. This approach exploits the inherent parallelism found within RDF datasets and queries, allowing us to scale with both dataset and system size. Unlike previous solutions, this framework uses highly optimised (SPARQL) joining strategies, intelligent data caching and the use of a super-query to enable the completion of eight distinct SPARQL lookups, comprising over eighty distinct joins, in only two Map / Reduce iterations. Results are presented comparing both the Jena and a previous Hadoop implementation demonstrating the superior performance of the new methodology. The new method is shown to be five times faster than Jena and twice as fast as the previous approach.",2015,,10.1109/BigData.2015.7363818
109,"Angryk, Rafal A. and Galarus, Douglas E.",The SMART approach to comprehensive quality assessment of site-based spatial-temporal data,Sensors;Quality control;Metadata;Meteorology;Interpolation;Quality assessment;Time series analysis;data quality;data stream processing;spatial-temporal data;quality control;interpolation,"There is a need for comprehensive solutions to address the challenges of spatio-temporal data quality assessment. Emphasis is often placed on the quality assessment of individual observations from sensors but not on the sensors themselves nor upon site metadata such as location and timestamps. The focus of this paper is on the development and evaluation of a representative and comprehensive, interpolation-based methodology for assessment of spatio-temporal data quality. We call our method the SMART method, short for Simple Mappings for the Approximation and Regression of Time series. When applied to a real-world, meteorological data set, we show that our method outperforms standard interpolators and we identify numerous problematic sites that otherwise would not have been flagged as bad. We further identify sites for which metadata is incorrect. We believe that there are many problems with real data sets like these and, in the absence of an approach like ours, these problems have largely gone unidentified. Our results bring into question the validity of provider-based quality control indicators. In addition to providing a comprehensive solution, our approach is novel for the simple but effective way that it accounts for spatial and temporal variation.",2016,,10.1109/BigData.2016.7840906
110,"Kläs, Michael and Putz, Wolfgang and Lutz, Tobias",Quality Evaluation for Big Data: A Scalable Assessment Approach and First Evaluation Results,Big data;Quality assessment;Metadata;Instruments;Ecosystems;Companies;big data quality assessment;quality measurement;velocity;volume;variety;SQA4BD;QUAMOCO;smart ecosystems;SPARK;HADOOP,"High-quality data is a prerequisite for most types of analysis provided by software systems. However, since data quality does not come for free, it has to be assessed and managed continuously. The increasing quantity, diversity, and velocity that characterize big data today make these tasks even more challenging. We identified challenges that are specific for big data quality assessments with particular emphasis on their usage in smart ecosystems and make a proposal for a scalable cross-organizational approach that addresses these challenges. We developed an initial prototype to investigate scalability in a multi-node test environment using big data technologies. Based on the observed horizontal scalability behavior, there is an indication that the proposed approach also allows dealing with increasing volumes of heterogeneous data.",2016,,10.1109/IWSM-Mensura.2016.026
111,"Lakshen, Guma Abdulkhader and Vraneš, Sanja and Janev, Valentina",Big data and quality: A literature review,Big data;Benchmark testing;Quality assessment;Databases;Software;Sparks;Engines;Big Data;Quality assessment;stream processing;survey;Big Data frameworks,"Big Data refers to data volumes in the range of Exabyte (1018) and beyond. Such volumes exceed the capacity of current on-line storage and processing systems. With characteristics like volume, velocity and variety big data throws challenges to the traditional IT establishments. Computer assisted innovation, real time data analytics, customer-centric business intelligence, industry wide decision making and transparency are possible advantages, to mention few, of Big Data. There are many issues with Big Data that warrant quality assessment methods. The issues are pertaining to storage and transport, management, and processing. This paper throws light into the present state of quality issues related to Big Data. It provides valuable insights that can be used to leverage Big Data science activities.",2016,,10.1109/TELFOR.2016.7818902
112,"Singh, Nitin and Lai, Kee-Hung and Vejvar, Markus and Cheng, T. C. E.","Big Data Technology: Challenges, Prospects, and Realities",Big Data;Data integrity;Data analysis;Software;Investment;Companies;Big data;business analytics;Hadoop;people;talent gap;implementation,"We attempt to demonstrate the value of big data to enterprises by interweaving the perceptions, challenges, and opportunities of big data for businesses. While enterprises are aware of the value of big data to their businesses, there are challenges of exploiting big data in terms of data quality and usage. Executives might lack knowledge on how applications are related to one another in the big data ecosystem and the business benefits to reap. We summarize the results of a research study that explores emerging business perceptions of big data. We examine the current practice in 20 large enterprises, each having an annual revenue of more than USD 0.5 billion and present our findings related to executive perceptions. We provide insights on how firms can develop their big data expertise along various dimensions and identify critical ideas to be further investigated to better understand the issues that practitioners and researchers might be equally grappling with.",2019,,10.1109/EMR.2019.2900208
113,"Grueneberg, K. and Calo, S. and Dewan, P. and Verma, D. and O’Gorman, Tristan",A Policy-based Approach for Measuring Data Quality,Data integrity;Standards;Measurement;Asset management;Data models;Complexity theory;Data Quality;Asset Management Systems;Policy based Data Management,"With the growing importance of data in all aspects of the functioning of an enterprise, having good quality of data is crucial in support of business processes. However, there do not exist good metrics to measure the quality of data that is available within an enterprise. While there are several data quality standards, their complexity and their required customization makes them difficult to use in real-world industrial scenarios. In this paper, we discuss the challenges encountered in measuring data quality within asset management systems. We propose a policy-based approach for measuring data quality, and show how such an approach can be customized and interpreted easily by practitioners in the field.",2019,,10.1109/BigData47090.2019.9006422
114,"Jiang, Wei and Ning, Xiuli and Xu, Yingcheng",Review on Big Data Fusion Methods of Quality Inspection for Consumer Goods,Big Data;Inspection;Data integration;Semantics;Hidden Markov models;Thesauri;Support vector machines;Big data of quality inspection;data fusion;named entity recognition;entity resolution;data conflict resolution,"Quality big data comes from a wide range, the problem is how to eliminate the structure barriers between various types of data from different sources, to achieve the effective integration for isolated or fragmented data, and then to mine the information, knowledge and wisdom according to the actual needs. It is urgent for the quality inspection departments to solve the problem of making a decision the first time and take preventive measures. This paper introduced the research status of named entity recognition, solid unified detection, data conflict resolution, data fusion method from the characteristics of quality inspection data. The existence problems are analyzed and the research direction is looking forward of the research on data fusion in this paper.",2018,,10.1109/CSCloud/EdgeCom.2018.00025
115,"Feng, Tao and Zhuang, Zhenyun and Pan, Yi and Ramachandra, Haricharan",A memory capacity model for high performing data-filtering applications in Samza framework,Containers;LinkedIn;Data models;Big data;Java;Measurement;Real-time systems;Apache Samza;capacity model;data filtering;performance,"Data quality is essential in big data paradigm as poor data can have serious consequences when dealing with large volumes of data. While it is trivial to spot poor data for small-scale and offline use cases, it is challenging to detect and fix data inconsistency in large-scale and online (real-time or near-real time) big data context. An example of such scenario is spotting and fixing poor data using Apache Samza, a stream processing framework that has been increasingly adopted to process near-real-time data at LinkedIn. To optimize the deployment of Samza processing and reduce business cost, in this work we propose a memory capacity model for Apache Samza to allow denser deployments of high performing data-filtering applications built on Samza. The model can be used to provision just-enough memory resource to applications by tightening the bounds on the memory allocations. We apply our memory capacity model on Linkedln's real use cases in production, which significantly increases the deployment density and saves business costs. We will share key learning in this paper.",2015,,10.1109/BigData.2015.7364058
116,"Mohammad, Banan and Alzyadat, Wael and Al-Fayoumi, Mohammad and EL Hawi, Ruba and Alhroob, Aysh",An Approach to Improve Data Quality from Big Data Aspect by Sensitive Cost and Time,Costs;Correlation;Statistical analysis;Data integrity;Volume measurement;Project management;Big Data;Big Data;Project Management;Sensitive Rule;Quality,"Big data is term of dataset with characteristic volume, value and veracity that lead to challenges unable proceed using traditional techniques to extract value, project management perspective is dynamic processing that utilizes the appropriate resources of organization in many phases by measuring in four-factor scope, time, cost and quality. In this research aim improve data quality from big data via project management scope depends on high trust which is getting high accuracy from confidence level in volume of data, confidence get with context and value of data which lead to determine accuracy deeply in it and finally choose from data depending on veracity of it, the experiment using three main factors time, cost and scope, strongest relation arranging between them start by project scope as strongest one then cost, product and finally time is weakest between them, in the final when select best quality use two sides generally from quality degree and be middle-quality interval and especially from relative distance with the strongest factor.",2020,,10.1109/ICICS49469.2020.239526
117,"Jiang, Yushan and Liu, Yongxin and Liu, Dahai and Song, Houbing",Applying Machine Learning to Aviation Big Data for Flight Delay Prediction,Atmospheric modeling;Neural networks;Machine learning;Predictive models;Big Data;Delays;Quantum cascade lasers;Flight Delay;Machine Learning;Aviation Data Analytics,"Flight delay has been a serious and widespread problem that needs to be solved. One promising solution is the flight delay prediction. Although big data analytics and machine learning have been applied successfully in many domains, their applications in aviation are limited. This paper presents a comprehensive study of flight delay spanning data pre-processing, data visualization and data mining, in which we develop several machine learning models to predict flight arrival delays. Two data sets were used, namely Airline On-Time Performance (AOTP) Data and Quality Controlled Local Climatological Data (QCLCD). This paper aims to recognize useful patterns of the flight delay from aviation data and perform accurate delay prediction. The best result for flight delay prediction (five classes) using machine learning models is 89.07% (Multilayer Perceptron). A Convolution neural network model is also built which is enlightened by the idea of pattern recognition and success of neural network method, showing a slightly better result with 89.32% prediction accuracy.",2020,,10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00114
118,"Wang, Jiye and Li, Yang and Guo, Jian and Cao, Junwei and Hua, Haochen and Xing, Chunxiao and Qi, Caijuan and Pi, Zhixian",Data Quality Analysis Framework and Evaluation Methods for Power System Operation with High Proportion of Renewable Energy Penetration,Data integrity;Renewable energy sources;Data models;Analytical models;Production;Monitoring;Clouds;renewable energy cloud;data quality analysis;AHP;evaluation metrics,"Global climate crisis in 21st century pushed countries to move towards energy transformation in generation and consumption. To achieve green and low-carbon energy transformation goals, it is necessary that a large number of renewable energy resources such as wind and solar to be consumed. Renewable energy with intermittent fluctuations in time dimension and agglomerations in spatial dimension increases the complexity of green energy consumption friendly. Therefore, comprehensive data and advanced predictive analysis methods are required to guarantee safety of operation and transactions for renewable energy plants and stations. We can even say that quality of renewable energy data determines the accuracy of prediction and analysis. Firstly, this article analyzes the operation and transaction characteristics of distributed renewable energy plants, and data quality analysis framework for distributed renewable energy operations and transactions was built on the new energy cloud platform. Data information were classified into model parameter and status instance, which are related to dispatching and energy power transaction businesses such as equipment model management, operation monitoring and security analysis, measurement statistics etc. The importance between them is determined according to pairwise comparison. Finally, analytic hierarchy process (AHP) theory was applied to calculate weights for data integrity, accuracy, consistency and timeliness, data quality assessment process and calculation methods were designed, and load series data was used to verify its correctness.",2020,,10.1109/ICIEA49774.2020.9102068
119,"Makkar, Himanshu and Toshniwal, Durga and Jangra, Shalini",Closed Itemset based Sensitive Pattern Hiding for Improved Data Utility and Scalability,Data privacy;Itemsets;Heuristic algorithms;Scalability;Big Data;Parallel processing;Sparks;Privacy Preserving Data Mining;Data Sanitisation;Reduced Sensitive itemset;Data Quality,"Frequent itemset mining is used to extract interesting associations and correlations between the itemsets present in transactional datasets. The frequently appearing patterns are used for various business decision making policies, for instance to increase co-purchase of products, price optimization, cross promotion etc. However, there are some sensitive patterns present in datasets that can reveal individual or organisation's specific confidential information that they would not prefer to be known since it can cause them huge social and monetary loss. Privacy Preserving Data Mining (PPDM) approaches are used to hide these sensitive patterns with maintaining the utility of the data. Heuristics-based PPDM approaches are widely adopted sensitive pattern hiding approaches due to their simplicity and lesser computational time as compared to the border-based and exact approaches. However, these approaches causes high side effects concerning the quality of datasets. In this paper, two heuristics-based algorithms, Removal of Closed Sensitive Itemsets with Maximum Support (MaxRCSI) and Removal of Closed Sensitive Itemsets with Minimum Support (MinRCSI), are proposed. In these algorithms, data sanitization is performed over closed sensitive itemsets to improve the utility of sanitized data. The proposed algorithms are parallelized on Spark parallel computing framework to deal with the massive amount of data i.e. big data. Experiments performed on real and synthetic datasets show that the proposed algorithms preserve the privacy of datasets with substantially better utility as compared to the traditional algorithms with less execution time.",2020,,10.1109/BigData50022.2020.9378401
120,"Hossen, J. and Jesmeen H, M. Z. and Sayeed, Shohel",Modifying Cleaning Method in Big Data Analytics Process using Random Forest Classifier,Forestry;Support vector machines;Data models;Cleaning;Feature extraction;Training;Big Data;Big data;Data Analytics;missing data;data cleaning;Machine learning;Random Forest;Gini Index,"Accurate data is a key success factor influencing the performance of data analytics results, especially for the detection and prediction purpose. Nowadays, Big Data analytics (BDA) is used to analyze the sheer volume of data available in an organization. These data quality must be maintained in order to obtain correct alert and valuable insights from the rapidly changing data of high volume, velocity, variety, veracity, and value. This paper aim is to modify existing framework of big data analytics by improving an important step in pre-processing (i.e. Data Cleaning). Initially, feature selection based on Random Forest is used to extract effective features. Then, two classifier algorithms (i.e. Random Forest classifier and Linear SVM classifier) are applied to train using the dataset to classify data quality and to develop an intelligent model. In evaluation, our experimental results show a consistent accuracy of Random Forest and Linear Regression around 90%. Using this approach, we expect to provide a set of cleaned data for further processing. Besides, analysts can benefit from this system in data analytical process in cleaning stage and conclude that the data is cleaned. Finally, a comparison is presented between available functions which are used to handle missing values with the developed system.",2018,,10.1109/ICCCE.2018.8539254
121,"Li, Mingxin and Wei, Heng and Liao, Hongxi",Mobile terminal quality of experience analysis based on big data,Indexes;Mobile communication;Mobile computing;Big data;Internet;Delays;big data;terminal;QoE;variable coefficient method,"In this paper, we proposes a method to analyze and evaluate the quality of experience (QoE) in mobile terminals using “big data”. The feature parameters of key quality indicator (KQI) are obtained from operators and quantized through the use of a scoring system. Then the scores of customer experience indicator (CEI) and QoE are calculated based on our proposed analytical model. In combination with the data of market operation, the terminal QoE evaluation scores contribute to offer effective suggestions on the promotion of mobile terminal.",2016,,10.1109/ISCIT.2016.7751629
122,"Wang, May D.",Biomedical Big Data Analytics for Patient-Centric and Outcome-Driven Precision Health,Biomedical imaging;Informatics;Bioinformatics;Big data;Cancer;Decision making,"Rapid advancements in biotechnologies such as -omic (genomics, proteomics, metabolomics, lipidomics etc.), next generation sequencing, bio-nanotechnologies, molecular imaging, and mobile sensors etc. accelerate the data explosion in biomedicine and health wellness. Multiple nations around the world have been seeking novel effective ways to make sense of ""big data"" for evidence-based, outcome-driven, and affordable 5P (Patient-centric, Predictive, Preventive, Personalized, and Precise) healthcare. My main research focus is on multi-modal and multi-scale (i.e. molecular, cellular, whole body, individual, and population) biomedical data analytics for discovery, development, and delivery, including translational bioinformatics in biomarker discovery for personalized care; imaging informatics in histopathology for clinical diagnosis decision support; bionanoinformatics for minimally-invasive image-guided surgery; critical care informatics in ICU for real-time evidence-based decision making; and chronic care informatics for patient-centric health. In this talk, first, I will highlight major challenges in biomedical and health informatics pipeline consisting of data quality control, information feature extraction, advanced knowledge modeling, decision making, and proper action taking through feedback. Second, I will present informatics methodological research in (i) data integrity and integration; (ii) case-based reasoning for individualized care; and (iii) streaming data analytics for real-time decision support using a few mobile health case studies (e.g. Sickle Cell Disease, asthma, pain management, rehabilitation, diabetes etc.). Last, there is big shortage of data scientists and engineers who are capable of handling Big Data. In addition, there is an urgent need to educate healthcare stakeholders (i.e. patients, physicians, payers, and hospitals) on how to tackle these grant challenges. I will discuss efforts such as patient-centric educational intervention, community-based crowd sourcing, and Biomedical Data Analytics MOOC development. Our research has been supported by NIH, NSF, Georgia Research Alliance, Georgia Cancer Coalition, Emory-Georgia Tech Cancer Nanotechnology Center, Children's Health Care of Atlanta, Atlanta Clinical and Translational Science Institute, and industrial partners such as Microsoft Research and HP.",2015,,10.1109/COMPSAC.2015.343
123,"Shrivastava, Shrey and Patel, Dhaval and Zhou, Nianjun and Iyengar, Arun and Bhamidipaty, Anuradha",DQLearn : A Toolkit for Structured Data Quality Learning,Data analysis;Automation;Data integrity;Optimization methods;Big Data;Solids;Task analysis,"Data Quality (DQ) has been one of the key focuses as Data Analytics and Artificial Intelligence (AI) fields continue to grow. Yet, data quality analysis has mostly been a disjointed, ad-hoc, and cumbersome process in the overall data analysis workflow. There have been ongoing attempts to formalize this process, but the solutions that have come out are not universally applicable. Most of the proposed solutions try to address the problem of data quality from a limited perspective and suc-cessfully address only a subset of all challenges. These solutions fail to translate to other domains due to a lack of structure. In this paper, we present DQLearn, a toolkit for structured data quality learning. We start by presenting the core principle on which we build our library and introduce the four components that provide a solid base to address the needs of the data quality problem. Then, we showcase our automation structure - ""Workflows"", and the two optimization techniques equipped with it, that help the users to structure their learning problem very easily. Next, we discuss four important scenarios of the DQ Workflows in the overall life-cycle. Finally, we demonstrate the utility of the proposed toolkit with public datasets and show benchmark results from optimization experiments.",2020,,10.1109/BigData50022.2020.9378296
124,"Libes, Don and Shin, Seungjun and Woo, Jungyub",Considerations and recommendations for data availability for data analytics for manufacturing,Data analysis;Manufacturing;Sensors;Encryption;NIST;Synchronization;big data;challenge problems;data analytics;data quality;requirements;smart manufacturing,"Data analytics is increasingly becoming recognized as a valuable set of tools and techniques for improving performance in the manufacturing enterprise. However, data analytics requires data and a lack of useful and usable data has become an impediment to research in data analytics. In this paper, we describe issues that would help aid data availability including data quality, reliability, efficiency, and formats specific to data analytics in manufacturing. To encourage data availability, we present recommendations and requirements to guide future data contributions. We also describe the need for data for challenge problems in data analytics. A better understanding of these needs, recommendations, and requirements may improve the ability of researchers and other practitioners to improve research and more rapidly deploy data analytics in manufacturing.",2015,,10.1109/BigData.2015.7363743
125,"Saha, Barna and Srivastava, Divesh",Data quality: The other face of Big Data,Information management;Data handling;Data storage systems;Databases;Maintenance engineering;Quality management;Cleaning,"In our Big Data era, data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Recent studies have shown that poor quality data is prevalent in large databases and on the Web. Since poor quality data can have serious consequences on the results of data analyses, the importance of veracity, the fourth `V' of big data is increasingly being recognized. In this tutorial, we highlight the substantial challenges that the first three `V's, volume, velocity and variety, bring to dealing with veracity in big data. Due to the sheer volume and velocity of data, one needs to understand and (possibly) repair erroneous data in a scalable and timely manner. With the variety of data, often from a diversity of sources, data quality rules cannot be specified a priori; one needs to let the “data to speak for itself” in order to discover the semantics of the data. This tutorial presents recent results that are relevant to big data quality management, focusing on the two major dimensions of (i) discovering quality issues from the data itself, and (ii) trading-off accuracy vs efficiency, and identifies a range of open problems for the community.",2014,,10.1109/ICDE.2014.6816764
126,"Lazar, Alina and Jin, Ling and Spurlock, C. Anna and Wu, Kesheng and Sim, Alex",Data quality challenges with missing values and mixed types in joint sequence analysis,Trajectory;Sequences;Time series analysis;Education;Employment;joint sequence analysis;optimal matching;missing values;time series clustering;data quality,"The goal of this paper is to investigate the impact of missing values in categorical time series sequences on common data analysis tasks. Being able to more effectively identify patterns in socio-demographic longitudinal data is an important component in a number of social science settings. However, performing fundamental analytical operations, such as clustering for grouping these data based on similarity patterns, is challenging due to the categorical and multi-dimensional nature of the data, and their corruption by missing and inconsistent values. To study these data quality issues, we employ longitudinal sequence data representations, a similarity measure designed for categorical and longitudinal data, together with state-of-the art clustering methodologies reliant on hierarchical algorithms. The key to quantifying the similarity and difference among data records is a distance metric. Given the categorical nature of our data, we employ an “edit” type distance using Optimal Matching (OM). Because each data record has multiple variables of different types, we investigate the impact of mixing these variables in a single similarity measure. Between variables with binary values and those with multiple nominal values, we find that the ability to overcome missing data problems is harder in the nominal domain versus the binary domain. Additionally, artificial clusters introduced by the alignment of leading missing values can be resolved by tuning the missing value substitution cost parameter.",2017,,10.1109/BigData.2017.8258222
127,"Fiore, Sandro and Elia, Donatello and Pires, Carlos Eduardo and Mestre, Demetrio Gomes and Cappiello, Cinzia and Vitali, Monica and Andrade, Nazareno and Braz, Tarciso and Lezzi, Daniele and Moraes, Regina and Basso, Tania and Kozievitch, Nádia P. and Fonseca, Keiko Verônica Ono and Antunes, Nuno and Vieira, Marco and Palazzo, Cosimo and Blanquer, Ignacio and Meira, Wagner and Aloisio, Giovanni",An Integrated Big and Fast Data Analytics Platform for Smart Urban Transportation Management,Urban areas;Big Data;Data analysis;Transportation;Cloud computing;Data mining;Europe;Big data;cloud computing;data analytics;data privacy;data quality;distributed environment;public transport management;smart city,"Smart urban transportation management can be considered as a multifaceted big data challenge. It strongly relies on the information collected into multiple, widespread, and heterogeneous data sources as well as on the ability to extract actionable insights from them. Besides data, full stack (from platform to services and applications) Information and Communications Technology (ICT) solutions need to be specifically adopted to address smart cities challenges. Smart urban transportation management is one of the key use cases addressed in the context of the EUBra-BIGSEA (Europe-Brazil Collaboration of Big Data Scientific Research through Cloud-Centric Applications) project. This paper specifically focuses on the City Administration Dashboard, a public transport analytics application that has been developed on top of the EUBra-BIGSEA platform and used by the Municipality stakeholders of Curitiba, Brazil, to tackle urban traffic data analysis and planning challenges. The solution proposed in this paper joins together a scalable big and fast data analytics platform, a flexible and dynamic cloud infrastructure, data quality and entity matching algorithms as well as security and privacy techniques. By exploiting an interoperable programming framework based on Python Application Programming Interface (API), it allows an easy, rapid and transparent development of smart cities applications.",2019,,10.1109/ACCESS.2019.2936941
128,"Bai, Zhongxian and Zhuo, Rongqing",Quality Management of Crowd Sensing Data Based on Machine Learning,Time series analysis;Temperature sensors;Mobile handsets;Temperature distribution;Databases;Data models;Machine Learning;Crowd Sensing;Big Data Analysis;Abnormal Data Detection Management;Clustering Method,"Recently, research on crowd sensing data quality management is a new subject area developed based on wireless sensor network related concepts. Crowd sensing data has also experienced many links in the process of network propagation, so it is inevitable that there are abnormal data in the database. Therefore, how to filter these unreliable data to get more real data is particularly important. This paper takes somatosensory temperature as an example, solves the problem of calculating the similarity of unequal long-term sequences by using DTW technology, and then clusters and compares the data in the database to find out the abnormal data. Thereby, the accuracy of the somatosensory temperature database is improved, and relevant users can obtain more accurate information. The experimental results show that when the minimum DTW value exceeds the threshold t = 0.7, the more the number of simulation sequences, the more stable the accuracy rate, and the faster the growth rate of the running time.",2020,,10.1109/CIBDA50819.2020.00049
129,"Zhou, Xiantian and Ordonez, Carlos",Programming Languages in Data Science: a Comparison from a Database Angle,Soft sensors;Redundancy;Relational databases;Big Data;Data science;Database systems;Task analysis,"In a typical Data Science project, the analyst uses many programming languages to explore and analyze big data coming from diverse data sources. A major challenge is managing and pre-processing so much data, with potentially inconsistent content, significant redundancy, in diverse formats, with varying data quality. Database systems research has tackled such problems for a long time, but mostly on relational databases. With such motivation in mind, this paper compares strengths and weaknesses of popular languages used nowadays from a database pespective: Python, R and SQL. We discuss the entire analytic pipeline, going from data integration, cleaning and pre-processing to model application and tuning. From a database systems perspective, we present a comprehensive survey of storage mechanisms, data processing algorithms, external algorithms, run-time memory management, consistency, optimizations and parallel processing. From a programming languages angle, we consider elegance, expressiveness, abstraction, composability, interactive behavior and automatic code optimization. We present a short experimental evaluation comparing the performance of the three languages on typical data exploration and pre-processing tasks. Our conclusion: there is no winner.",2021,,10.1109/BigData52589.2021.9672007
130,"El-Ghafar, Randa M. Abd and Gheith, Mervat H. and El-Bastawissy, Ali H. and Nasr, Eman S.",Record linkage approaches in big data: A state of art study,Couplings;Big Data;Programming;Data integration;Task analysis;Data models;Databases;Big Data;Big Data Integration;blocking;entity matching;entity resolution;Hadoop;machine learning;MapReduce;Record Linkage,"Record Linkage aims to find records in a dataset that represent the same real-world entity across many different data sources. It is a crucial task for data quality. With the evolution of Big Data, new difficulties appeared to deal mainly with the 5Vs of Big Data properties; i.e. Volume, Variety, Velocity, Value, and Veracity. Therefore Record Linkage in Big Data is more challenging. This paper investigates ways to apply Record Linkage algorithms that handle the Volume property of Big Data. Our investigation revealed four major issues. First, the techniques used to resolve the Volume property of Big Data mainly depend on partitioning the data into a number of blocks. The processing of those blocks is parallelly distributed among many executers. Second, MapReduce is the most famous programming model that is designed for parallel processing of Big Data. Third, a blocking key is usually used for partitioning the big dataset into smaller blocks; it is often created by the concatenation of the prefixes of chosen attributes. Partitioning using a blocking key may lead to unbalancing blocks, which is known as data skew, where data is not evenly distributed among blocks. An uneven distribution of data degrades the performance of the overall execution of the MapReduce model. Fourth, to the best of our knowledge, a small number of studies has been done so far to balance the load between data blocks in a MapReduce framework. Hence more work should be dedicated to balancing the load between the distributed blocks.",2017,,10.1109/ICENCO.2017.8289792
131,"Liu, He and Chen, Jiangqi and Huang, Fupeng and Li, Han",An Electric Power Sensor Data Oriented Data Cleaning Solution,Power systems;Cleaning;Clustering algorithms;Big Data;Sparks;Data acquisition;Algorithm design and analysis;electric power senser data;data cleaning;k-means clustering;outlier;Spark,"With the development of Smart Grid Technology, more and more electric power sensor data are utilized in various electric power systems. To guarantee the effectiveness of such systems, it is necessary to ensure the quality of electric power sensor data, especially when the scale of electric power sensor data is large. In the field of large-scale electric power sensor data cleaning, the computational efficiency and accuracy of data cleaning are two vital requirements. In order to satisfy these requirements, this paper presents an electric power sensor data oriented data cleaning solution, which is composed of a data cleaning framework and a data cleaning method. Based on Hadoop, the given framework is able to support large-scale electric power sensor data acquisition, storage and processing. Meanwhile, the proposed method which achieves outlier detection and reparation is implemented on the basis of a time-relevant k-means clustering algorithm in Spark. The feasibility and effectiveness of the proposed method is evaluated on a data set which originates from charging piles. Experimental results show that the proposed data cleaning method is able to improve the data quality of electric power sensor data by finding and repairing most outliers. For large-scale electric power sensor data, the proposed data cleaning method has high parallel performance and strong scalability.",2017,,10.1109/ISPAN-FCST-ISCC.2017.29
132,"Labeeb, Kashshaf and Chowdhury, Kuraish Bin Quader and Riha, Rabea Basri and Abedin, Mohammad Zoynul and Yesmin, Sarmila and Khan, Mohammad Nasfikur Rahman",Pre-Processing Data In Weather Monitoring Application By Using Big Data Quality Framework,Solid modeling;Oils;Organizations;Big Data;Solids;Monitoring;Meteorology;heterogeneous data;noise filter;convergence;climate change,"In this research, we are working with Big Data for obtaining, preparing and analyzing data-based information to make use of the data retrieved which will benefit any organization. It is a progressing part of all divisions of industry and business. All organizations in any field, for example, oil, money, fabricating hardware and so forth produce big data, which can show incredibly helpful designs to business directors to make and develop their organizations, when the information is gathered and analyzed accurately. It permits us to gather, store, and decipher immense measures of big data to produce useful outcomes. Data quality is affected by the information that is gathered to be analyzed as that data will make sure whether in the long run a specific method of conducting the ongoing process is useful or not. Consequently, the consistency of big data very important. Here, we propose that the various types of raw information should be analyzed to expand its precision in the pre-handling stage, as those pieces of information are not utilized later in the process. During investments, we break down and model the big data to decrease overhead expenses to create and add to a solid understanding of results to improve information consistency.",2020,,10.1109/WIECON-ECE52138.2020.9397990
133,"Saha, Ajitesh Kumar and Kumar, Ashwini and Tyagi, Vishu and Das, Sanjoy",Big Data and Internet of Things: A Survey,Big Data;Reliability;Cloud computing;Social networking (online);Sensors;Videos;Data integrity;Big Data;Big Data Analytics;Big Data Quality;Data Reliability;IOT;Hadoop;HDFS;Cloud Computing,"In this digital era, here the sum of data is generate with store has prolonged inside a less period of time. The data in this era generated high speed leads to many challenges. The size is primarily and periodically, just the measurement that bounces in the big data position. In this survey, we have attempt to give a broad explanation of big data that captures its other unique and important features. We have discussed 4V's model. Also, the latest technologies uses in big data, like Hadoop, HDFS, MapReduce and different type of methods used by big data. Finally various benefits of using big data analysis and include some feature of cloud computing.",2018,,10.1109/ICACCCN.2018.8748630
134,"Mejía-Lavalle, Manuel and Meusel, Winfrid and Tavira, Jonathan Villanueva and Cruz, Mirian Calderón",Effective Data Quality Diagnostic Schema for Big Data,Databases;Big Data;Measurement;Cleaning;Data mining,"Big Data environment is a computing area with a great growth. Today it is common that we hear about databases with huge volumes of information and also we hear about Data Mining and Business Intelligence projects related with these huge databases. However, in general, little attention has been given to the quality of the data. Here we propose and present innovative metrics and schema designed to perform a basic task related to the Data Quality issue, this is, the diagnostic. The preliminary results that we obtained when we apply our approaches to Big Data encourage us to continue this work.",2017,,10.1109/ICMEAE.2017.29
135,"Mystakidis, Aristeidis and Tjortjis, Christos",Big Data Mining for Smart Cities: Predicting Traffic Congestion using Classification,Machine learning algorithms;Smart cities;Big Data;Prediction algorithms;Data mining;Traffic congestion;Regression tree analysis;Data Mining;Big Data;Machine learning;Smart Cities;Prediction;Classification;Traffic Congestion,"This paper provides an analysis and proposes a methodology for predicting traffic congestion. Several machine learning algorithms and approaches are compared to select the most appropriate one. The methodology was implemented using Data Mining and Big Data techniques along with Python, SQL, and GIS technologies and was tested on data originating from one of the most problematic, regarding traffic congestion, streets in Thessaloniki, the 2nd most populated city in Greece. Evaluation and results have shown that data quality and size were the most critical factors towards algorithmic accuracy. Result comparison showed that Decision Trees were more accurate than Logistic Regression.",2020,,10.1109/IISA50023.2020.9284399
136,"Hossain, Md Monir and Sebestyen, Mark and Mayank, Dhruv and Ardakanian, Omid and Khazaei, Hamzeh",Large-scale Data-driven Segmentation of Banking Customers,Data integrity;Conferences;Time series analysis;Banking;Big Data;Anomaly detection;Business;customer segmentation;clustering;association rules mining;anomaly detection,"This paper presents a novel big data analytics framework for creating explainable personas for retail and business banking customers. These personas are essential to better tailor financial products and improve customer retention. This framework is comprised of several components including anomaly detection, binning and aggregation of contextual data, clustering of transaction time series, and mining association rules that map contextual data to cluster identifiers. Leveraging rich transaction and contextual data available from nearly 60,000 retail and 90,000 business customers of a financial institution, we empirically evaluate this framework and describe how the identified association rules can be used to explain and refine existing customer classes, and identify new customer classes and various data quality issues. We also analyze the performance of the proposed framework and show that it can easily scale to millions of banking customers.",2020,,10.1109/BigData50022.2020.9378483
137,"Darwish, Tasneem S. J. and Bakar, Kamalrulnizam Abu and Kaiwartya, Omprakash and Lloret, Jaime",TRADING: Traffic Aware Data Offloading for Big Data Enabled Intelligent Transportation System,Logic gates;Big Data;Quality of service;Delays;Real-time systems;Safety;Roads;Big data;gateway;intelligent transportation systems;VANET;vehicle-to-internet,"Todays' Intelligent Transportation System (ITS) applications majorly depend on either limited neighbouring traffic data or crowd sourced stale traffic data. Enabling big traffic data analytics in ITS environments is a step closer towards utilizing significant traffic patterns and trends for making more precise and intelligent decisions particularly in connected autonomous vehicular environments. Towards this end, this paper presents a Traffic Aware Data Offloading (TRADING) approach for big traffic data centric ITS applications in connected autonomous vehicular environments. Specifically, TRADING balances offloading data traffic among gateways focusing on vehicular traffic and network status in the vicinity of gateways. In addition, TRADING mitigates the effect of gateway advertisement overhead to liberate the transmission channels for traffic big data transmission. The performance of TRADING is comparatively evaluated in a realistic simulation environment by considering gateway access overhead, load distribution among gateways, data offloading delay, and data offloading success ratio. The comparative performance evaluation results show some significant developments towards enabling big traffic data centric ITS.",2020,,10.1109/TVT.2020.2991372
138,"Xu, Xuefang and Lei, Yaguo and Li, Zeda",An Incorrect Data Detection Method for Big Data Cleaning of Machinery Condition Monitoring,Big Data;Machinery;Feature extraction;Condition monitoring;Data integrity;Fault diagnosis;Cleaning;Condition-monitoring big data;data cleaning;data quality;incorrect data;local outlier factor (LOF),"The presence of incorrect data leads to the decrease of condition-monitoring big data quality. As a result, unreliable or misleading results are probably obtained by analyzing these poor-quality data. In this paper, to improve the data quality, an incorrect data detection method based on an improved local outlier factor (LOF) is proposed for data cleaning. First, a sliding window technique is used to divide data into different segments. These segments are considered as different objects and their attributes consist of time-domain statistical features extracted from each segment, such as mean, maximum and peak-to-peak value. Second, a kernel-based LOF (KLOF) is calculated using these attributes to evaluate the degree of each segment being incorrect data. Third, according to these KLOF values and a threshold value, incorrect data are detected. Finally, a simulation of vibration data generated by a defective rolling element bearing and three real cases concerning a fixed-axle gearbox, a wind turbine, and a planetary gearbox are used to verify the effectiveness of the proposed method, respectively. The results demonstrate that the proposed method is able to detect both missing segments and abnormal segments, which are two typical incorrect data, effectively, and thus is helpful for big data cleaning of machinery condition monitoring.",2020,,10.1109/TIE.2019.2903774
139,"Yin, Jianwei and Tang, Yan and Lo, Wei and Wu, Zhaohui",From Big Data to Great Services,Big data;Quality of service;Public transportation;Vehicles;Industries;Next generation networking;Complexity theory;Great Services;4P QoS,"Big Data is increasingly adopted by a wide range of service industries to improve the quality and value of their services, e.g., inventory that matches well the supply and demand, and pricing that reflects well the market needs. Customers benefit from higher quality of service enabled by Big Data. Service providers get higher profits from more precise control of costs and accurate knowledge of customer needs. In this paper, we define the next generation high quality services as Great Services, characterized by 4P Quality-of-Service (QoS) dimensions: Panorama, Penetration, Prediction and Personalization, which go much further than current services. The transformation of Big Data into Great Services would be difficult and expensive without methodical techniques and software tools. We call the intermediate step Deep Knowledge, which is generated by Big Data (with the 4V challenges - Volume, Velocity, Variety, and Veracity) and used in the creation of Great Services. Deep Knowledge is distinguished from traditional Big Data by 4C properties (Complexity, Cross-domain, Customization, and Convergence). In order to achieve the 4P QoS dimensions of Great Services, we need Deep Knowledge with 4C properties. In this paper, we describe an informal characterization of Great Services with 4P QoS dimensions with examples, and outline the techniques and tools that facilitate the transformation of Big Data into Deep Knowledge with 4C properties, and then the use of Deep Knowledge in Great Services.",2016,,10.1109/BigDataCongress.2016.28
140,"Guo, Peng and Yang, Guosheng and Wang, Wenhuan and Yan, Zhoutian and Zhang, Lie and Zhang, Hanfang",Relay Protection Data Integrity Check Method Based on Big Data Association Algorithm,Apriori algorithm;relay protection;defect data;integrity check,"Relay protection big data creates good conditions for the improvement of professional applications, and data integrity is an important aspect that reflects data quality. The association of relay protection big data is intense. This paper applies Apriori algorithm to mine data relevance and generate association rules. Based on this, the integrity of relay protection data is checked, and the incomplete data is predicted. Taking the relay protection defect data as an example, the paper explores the correlation among 251 items of the six dimensions of protection relay defect data such as type of protection, the severity of the defect, whether the protection is out of operation, the defect location, the cause of the defect, and the equipment manufacturer, completing the processing of incomplete data with great application results.",2019,,10.1109/EEI48997.2019.00115
141,"Khan, Abudul Wahid and Khan, Maseeh Ullah and Khan, Javed Ali and Ahmad, Arshad and Khan, Khalil and Zamir, Muhammad and Kim, Wonjoon and Ijaz, Muhammad Fazal",Analyzing and Evaluating Critical Challenges and Practices for Software Vendor Organizations to Secure Big Data on Cloud Computing: An AHP-Based Systematic Approach,Cloud computing;Security;Big Data;Software;Organizations;Social networking (online);STEM;Security challenges;big data;cloud computing;SLR;vendor;SPSS,"Recently, its becomes easy to track down the data due to its availability in a large number. Although for data management, processing, and obtainability, cloud computing is considered a well-known approach for organizational development on the internet. Despite many advantages, cloud computing has still numerous security challenges that can affect the big-data usage on cloud computing. To find the security issues/challenges that are faced by software vendors’ organizations we conducted a systematic literature review (SLR) through which we have find out 103 relevant research publications by developing a search string that is inspired by the research questions. This relevant data was comprised from different databases e.g. Google Scholar, IEEE Explore, ScienceDirect, ACM Digital Library, and SpringerLink. Furthermore, for the detailed literature review, we have accomplished all the steps in SLR, for example, development of SLR protocol, Initials and final assortment of the relevant data, data extraction, data quality assessment, and data synthesis. We identified fifteen (15) critical security challenges which are: data secrecy, geographical data location, unauthorized data access, lack of control, lack of data management, network-level issues, data integrity, data recovery, lack of trust, data sharing, data availability, asset issues, legal amenabilities, lack of quality, and lack of consistency. Furthermore, sixty four (64) standard practices are identified for these critical security challenges using the proposed SLR that could help vendor organizations to overcome the security challenges for big data. The findings of our research study demonstrate the resemblances and divergences in the identified security challenges in different periods, continents, databases, and methods. The proposed SLR will also support software vendor organizations for securing big data on the cloud computing platforms. This paper has the following content: in Section II, we have describe the Literature review; in Section III, research methodology is specified; in Section IV, the findings of the SLR and the analysis of result are discussed; in Section V, the limitations of this research are given; in Section VI, we discussed our conclusions and future work.",2021,,10.1109/ACCESS.2021.3100287
142,"Laranjeiro, Nuno and Soydemir, Seyma Nur and Bernardino, Jorge",A Survey on Data Quality: Classifying Poor Data,Standards organizations;Industries;Training;Companies;Decision making;Poor data quality;dirty data;poor data classification;data quality problems,"Data is part of our everyday life and an essential asset in numerous businesses and organizations. The quality of the data, i.e., the degree to which the data characteristics fulfill requirements, can have a tremendous impact on the businesses themselves, the companies, or even in human lives. In fact, research and industry reports show that huge amounts of capital are spent to improve the quality of the data being used in many systems, sometimes even only to understand the quality of the information in use. Considering the variety of dimensions, characteristics, business views, or simply the specificities of the systems being evaluated, understanding how to measure data quality can be an extremely difficult task. In this paper we survey the state of the art in classification of poor data, including the definition of dimensions and specific data problems, we identify frequently used dimensions and map data quality problems to the identified dimensions. The huge variety of terms and definitions found suggests that further standardization efforts are required. Also, data quality research on Big Data appears to be in its initial steps, leaving open space for further research.",2015,,10.1109/PRDC.2015.41
143,"Zhang, Mingming and Wo, Tianyu and Xie, Tao",A Platform Solution of Data-Quality Improvement for Internet-of-Vehicle Services,Interpolation;Task analysis;Inspection;Cloud computing;Roads;Data integrity;Conferences;Dependability;Interpolation;Sequence Matching;Data Quality;Big Data;Internet-of-Vehicles,"Interconnection and intelligence have become the latest trends of the new generation of vehicle and transportation technologies. Applications built upon platforms of cloud-centered vehicle networking, i.e., Internet-of-Vehicles (IoVs), have been increasingly developed and deployed to provide data-centric services (e.g., driving assistance). Because these services are often safety critical, assuring service dependability has become an important requirement. In this paper, we propose DQI, a platform-level solution of Data-Quality Improvement designed to assure service dependability for Internet-of-Vehicle services. As an example, DQI is deployed in CarStream, an industrial system of big data processing designed for chauffeured car services. Via CarStream, over 30,000 vehicles are organized in a virtual vehicle network by sharing vehicle-status data in a near real-time manner. Such data often have low-quality issues and compromise the dependability of data-centric services. DQI includes techniques of data-quality improvement, including detecting outliers, extracting frequent patterns, and interpolating sequences. DQI enhances the dependability of data-centric services in IoVs by addressing the common data-quality requirements at the platform level. Upper-level services can benefit from DQI for data-quality improvement and reduce the complexity of service logic. We evaluate DQI by using a three-year dataset of vehicles and real applications deployed in CarStream. The result shows that compared with existing approaches, DQI can effectively restore missing data and correct anomalies with more than 30.0% improvement in precision. By studying multiple real applications, we also show that this data-quality improvement can indeed enhance the dependability of IoV services.",2018,,10.1109/PERCOM.2018.8444581
144,"Yousfi, Aola and El Yazidi, Moulay Hafid and Zellou, Ahmed",HASSO: A Highly-Automated Source Selection and Ordering System Based on Data Quality Factors,Computer science;Data integrity;User interfaces;Big Data;Information systems;Source Selection;Source Ordering;Data Quality;Big Data Integration;Semantic Similarity,"Big data integration gives access to a large number of data sources through a unified user interface. Answers include high-quality data, medium-quality data, and low-quality data. Selecting a subset of high accurate and consistent data sources and ordering them appropriately is critical to obtain as many high-quality answers as possible right after querying few data sources. However, the process of selecting and ordering data sources can be quite complicated and can present several challenges. The main challenge faced during that process is identifying the most adequate data quality factors to consider. In this paper, we present HASSO, a Highly-Automated Source Selection and Ordering System based on data quality factors. To produce consistent and high accurate answers, HASSO identifies, for each data source, its domain, data consistency and data accuracy using the schema matches. To maximize the total number of complete and non-redundant answers returned right after querying a small number of data sources, HASSO orders data sources in terms of their data overlap and in a decreasing order of their overall coverage. Experimental results in real-world domains show that HASSO produces high-quality answers at high speed.",2020,,10.1109/ICACSIS51025.2020.9263243
145,"Earley, Seth",Presentation 1. Information governance in the age of big data,Organizations;Standards organizations;Information architecture;Fuels;Information management;Big data,"Organizations have understood the value of their structured data — mostly financial transactions — since the first mainframes were developed in the 40's. Data quality issues have always been a challenge and the increasing numbers of applications consuming and producing structured transactional data has grown exponentially. Unstructured information has been given less significance and strategic importance and therefore fewer resources and less attention on the part of leadership. All of that has changed and is changing faster than anyone imagined. Unstructured content is what humans produce. They create the documents: strategies, proposals, support documents, marketing content, white papers, engineering specifications, etc. that form the intelligence and core knowledge capital of the enterprise. Many organizations have left business units to fend for themselves and “go figure it out” with little guidance or support. These has led to terabytes of content that people cannot find their way through and that leaves the organization open to risks, liabilities and costs of e discovery. According to the Minnesota Journal of Law, Science & Technology, a gig of data costs $30,000 in e discovery costs. The cost of storage is ten cents. The problem is that the enterprise does not understand the hidden costs of not making data accessible and usable — lost time, lost IP, inefficiency, poor customer service that can lead to lost customers, slower growth, etc. As newer collaboration technologies are deployed, they expose the bad habits and sins of the past. Deploying a new search engine shows that the content is not curated. Standing up a new content management application like SharePoint reveals the haphazard shanty town of an information architecture with inconsistencies in models, terminology and applications. Today's landscape of marketing and customer experience technologies is complex and interconnected and requires those upstream knowledge processes that produce unstructured content as the fuel. Customer experience entails everything that happens before you purchase (marketing, education and outreach), when you purchase (e commerce with product content and data), and after you purchase (self-service systems and knowledge bases that support the call center). This is the customer lifecycle and at each step in the process systems and tools need to be harmonized as they gather information about the users using attribute models that are consistent and that serve the business and the customer. They take content and data as input and then output more data. One applications exhaust is another applications fuel. Organizations are also purchasing data streams to enrich their internal information sources. Social media is an enormous virtually untapped reservoir of data about customers and what they think about organizations. This can be mined for sentiment and to gauge marketing effectiveness. Increasingly, much of this is being placed into the hands of the marketing organization. In fact, a study by Gartner Group said that by 2017 the CMO will spend more money on IT than the CIO. What all of this means is that information, content and data governance need to be considered as part of a whole and not as separate initiatives. Elements of good information governance include: • Deployment and Operationalization • Alignment with User Needs • Business Value • Buy-In and Change Management • Sponsorship and Accountability Leads to the following outcomes: • Manages conflicts in business priorities (between initiatives, business units, drivers, etc) • Allows for ongoing input from various stakeholders and constituencies in order to evolve capabilities with the needs of the business • Prioritizes efforts and allocation of resources • Assigns roles and responsibilities with accountability to critical functions • Takes into consideration various levels of maturity in the organization — no one size fits all • Ensures that investments in systems, processes and tools are providing sufficient return to the business • Balances centralized standards with decentralized decision making • Aligns incentives to use a system with business goals This session will review governance concepts, discuss how they apply to various types of data and content and provide a framework for developing governance processes and structures.",2014,,10.1109/ITPRO.2014.7029280
146,"Ezzine, Imane and Benhlima, Laila",Technology against COVID-19 A Blockchain-based framework for Data Quality,COVID-19;Pandemics;Data integrity;Blockchain;Big Data;Statistics;Testing;Covid-19;Blockchain;Big Data;Data Quality;data governance,"The effects of COVID-19 have quickly spread around the world, testing the limits of the population and the public health sector. High demand on medical services are offset by disruptions in daily operations as hospitals struggle to function in the face of overcapacity, understaffing and information gaps. Faced with these problems, new technologies are being deployed to fight this pandemic and help medical staff governments to reduce its spread. Among these technologies, we find blockchains and Big Data which have been used in tracking, prediction applications and others. However, despite the help that these new technologies have provided, they remain limited if the data with which they are fed are not of good quality. In this paper, we highlight some benefits of using BIG Data and Blockchain to deal with this pandemic and some data quality issues that still present challenges to decision making. Finally we present a general Blockchain-based framework for data governance that aims to ensure a high level of data trust, security, and privacy.",2020,,10.1109/CiSt49399.2021.9357200
147,"Han, Weiguo and Jochum, Matthew",Practices and Experiences in High Volumes of Satellite Data Management,Satellites;Organizations;Research and development;Databases;Monitoring;Buildings;Data integrity;Satellite Data Management;Big Data;Data Quality;Central Data Repository,"High volumes of satellite data management within an organization is still challenging and daunting in the era of big data. The increasing information technology costs and limited budgets, growing satellite data needs, data availability across multiple teams and projects, strategic goals of organization, and expected project outcomes require better satellite data management mechanism and system to facilitate research and development activities. An organization level centralized satellite data repository is a practical solution to satisfy these requirements. This paper describes the best practices and experiences from building such a central satellite data repository within our organization, including data management strategy and policy, scalable and extensible system infrastructure, comprehensive data management system, and technical support and user assistance. These practices can be borrowed and applied in other organizations with similar requirements.",2019,,10.1109/IGARSS.2019.8900190
148,"Liu, Kai-Cheng and Kuo, Chuan-Wei and Liao, Wen-Chiuan and Wang, Pang-Chieh",Optimized Data de-Identification Using Multidimensional k-Anonymity,Data privacy;Privacy;Numerical models;Big Data;Measurement;Data models;Greedy algorithms;privacy preserving;k-anonymity;de-identification;data quality;information loss,"In the globalized knowledge economy, big data analytics have been widely applied in diverse areas. A critical issue in big data analysis on personal information is the possible leak of personal privacy. Therefore, it is necessary to have an anonymization-based de-identification method to avoid undesirable privacy leak. Such method can prevent published data form being traced back to personal privacy. Prior empirical researches have provided approaches to reduce privacy leak risk, e.g. Maximum Distance to Average Vector (MDAV), Condensation Approach and Differential Privacy. However, previous methods inevitably generate synthetic data of different sizes and is thus unsuitable for general use. To satisfy the need of general use, k-anonymity can be chosen as a privacy protection mechanism in the de-identification process to ensure the data not to be distorted, because k-anonymity is strong in both protecting privacy and preserving data authenticity. Accordingly, this study proposes an optimized multidimensional method for anonymizing data based on both the priority weight-adjusted method and the mean difference recommending tree method (MDR tree method). The results of this study reveal that this new method generate more reliable anonymous data and reduce the information loss rate.",2018,,10.1109/TrustCom/BigDataSE.2018.00235
149,"Tsoumakos, Dimitrios and Giannakopoulos, Ioannis",Content-Based Analytics: Moving beyond Data Size,Data models;Analytical models;Task analysis;Predictive models;Uncertainty;Biological system modeling;Numerical models;modeling;data quality;big data;Machine Learning;scheduling,"Efforts on Big Data technologies have been highly directed towards the amount of data a task can access or crunch. Yet, for content-driven decision making, it is not (only) about the size, but about the ""right"" data: The number of available datasets (a different type of volume) can reach astronomical sizes, making a thorough evaluation of each input prohibitively expensive. The problem is exacerbated as data sources regularly exhibit varying levels of uncertainty and velocity/churn. To date, there exists no efficient method to quantify the impact of numerous available datasets over different analytics tasks and workflows. This visionary work puts the spotlight on data content rather than size. It proposes a novel modeling, planning and processing research bundle that assesses data quality in terms of analytics performance. The main expected outcome is to provide efficient, continuous and intelligent management and execution of content-driven data analytics. Intelligent dataset selection can achieve massive gains on both accuracy and time required to reach a desired level of performance. This work introduces the notion of utilizing dataset similarity to infer operator behavior and, consequently, be able to build scalable, operator-agnostic performance models for Big Data tasks over different domains. We present an overview of the promising results from our initial work with numerical and graph data and respective operators. We then describe a reference architecture with specific areas of research that need to be tackled in order to provide a data-centric analytics ecosystem.",2020,,10.1109/BigDataService49289.2020.00013
150,"Wei, Li and Dawei, Wang and Lixia, Wang",Research on data Traceability Method Based on blockchain Technology,Industries;Technological innovation;Distributed databases;Blockchain;Data models;Internet;Safety;blockchain;data traceability;data quality;data security;data governance;energy Internet;huge data,"Energy Internet is a major innovation to deal with the environmental crisis and efficient energy management and use in the current society. The important condition to achieve this goal is to summarize, integrate, process and apply the data of various industries in the energy field, and then support the relevant management and decision-making. In this process, how to ensure the authenticity and credibility of data is one of the keys in the construction of energy Internet. Therefore, this paper will study the application scenarios of blockchain technology in data traceability. With the help of the natural characteristics of blockchain, such as decentralized, distributed storage, tamper proof, open and transparent, combined with relevant national standards and international theoretical models, based on the needs of energy Internet data integration and management, this paper will develop a data traceability method suitable for the energy industry, and build a covering energy Data life cycle model of Internet. Through the research of this paper, we can help all localities to establish data traceability mechanism in the energy Internet, to help users to accurately grasp where the data is created, what systems have been transferred, which users have carried out query and modification, and so on, so as to realize the monitoring and control of the whole process of data flow, which helps to improve the credibility of data, and also helps to ensure the safety and quality of data and promote the construction of energy Internet huge data application.",2020,,10.1109/ICBASE51474.2020.00017
151,"Micic, Natasha and Neagu, Daniel and Torgunov, Denis and Campean, Felician",Exploring Methods for Comparing Similarity of Dimensionally Inconsistent Multivariate Numerical Data,Measurement;Time series analysis;Feature extraction;Phylogeny;Mutual information;Big Data;Data mining;Similarity measures;Robinson Foulds;Variation of Information;Engineering data;Multivariate numerical data;Dimensional inconsistency,"When developing multivariate data classification and clustering methodologies for data mining, it is clear that most literature contributions only really consider data that contain consistently the same attributes. There are however many cases in current big data analytics applications where for same topic and even same source data sets there are differing attributes being measured, for a multitude of reasons (whether the specific design of an experiment or poor data quality and consistency). We define this class of data a dimensionally inconsistent multivariate data, a topic that can be considered a subclass of the Big Data Variety research. This paper explores some classification methodologies commonly used in multivariate classification and clustering tasks and considers how these traditional methodologies could be adapted to compare dimensionally inconsistent data sets. The study focuses on adapting two similarity measures: Robinson-Foulds tree distance metrics and Variation of Information; for comparing clustering of hierarchical cluster algorithms (such clusters are derived from the raw multivariate data). The results from experiments on engineering data highlight that adapting pairwise measures to exclude non-common attributes from the traditional distance metrics may not be the best method of classification. We suggest that more specialised metrics of similarity are required to address challenges presented by dimensionally inconsistent multivariate data, with specific applications for big engineering data analytics.",2018,,10.1109/HPCC/SmartCity/DSS.2018.00251
152,"Yu, Weiqing and Zhu, Wendong and Liu, Guangyi and Kan, Bowen and Zhao, Ting and Liu, He",Cluster-Based Best Match Scanning for Large-Scale Missing Data Imputation,Time complexity;Clustering algorithms;Algorithm design and analysis;Estimation;Big Data;Data mining;big data;cluster-based best match scanning;data imputation;k-NN,"High-quality data are the prerequisite for analyzing and using big data to guarantee the value of the data. Missing values in data is a common yet challenging problem in data analytics and data mining, especially in the era of big data. Amount of missing values directly affects the data quality. Therefore, it is critical to properly recover missing values in the dataset. This paper presents a new imputation algorithm called Cluster-based Best Match Scanning (CBMS) designed for Big Data. It is a modification of k-NN imputation. CBMS focuses on recovering continuous numeric missing values, and aims at balancing computational complexity and accuracy. As an imputation algorithm, it can potentially reduce the time complexity of k-NN from O(n^2*d) to O(n^1.5*d), and also reduce the space/memory usage, while perform no worse than k-NN imputation. On top of that CBMS is highly parallelizable.Simulation of CBMS is conducted on smart meter reading data. Data is manually divided into training set and testing set, and testing accuracy is evaluated by computing the mean absolute deviation. Comparison with linear interpolation and k-NN imputation is made to demonstrate the power and effectiveness of our proposed CBMS algorithm.",2017,,10.1109/BIGCOM.2017.48
153,"Tawakuli, Amal and Kaiser, Daniel and Engel, Thomas",Synchronized Preprocessing of Sensor Data,Deep learning;Cloud computing;Data preprocessing;Big Data;Synchronization;Reliability;Task analysis;Data Quality;Data Preprocessing;Sensor Data;Edge Computing;Data Management,"Sensor data whether collected for machine learning, deep learning or other applications must be preprocessed to fit input requirements or improve performance and accuracy. Data preparation is an expensive, resource consuming and complex phase often performed centrally on raw data for a specific application. The dataflow between the edge and the cloud can be enhanced in terms of efficiency, reliability and lineage by preprocessing the datasets closer to their data sources. We propose a dedicated data preprocessing framework that distributes preprocessing tasks between a cloud stage and two edge stages to create a dataflow with progressively improving quality. The framework handles heterogenous data and dynamic preprocessing plans simultaneously targeting diverse applications and use cases from different domains. Each stage autonomously executes sensor specific preprocessing plans in parallel while synchronizing the progressive execution and dynamic updates of the preprocessing plans with the other stages. Our approach minimizes the workload on central infrastructures and reduces the resources used for transferring raw data from the edge. We also demonstrate that preprocessing data can be sensor specific rather than application specific and thus can be performed prior to knowing a specific application.",2020,,10.1109/BigData50022.2020.9377900
154,"Feric, Zlatan and Agostini, Nicolas Bohm and Beene, Daniel and Signes-Pastor, Antonio J. and Halchenko, Yuliya and Watkins, Deborah and MacKenzie, Debra and Karagas, Margaret and Manjourides, Justin and Alshawabkeh, Akram and Kaeli, David",A Secure and Reusable Software Architecture for Supporting Online Data Harmonization,Dictionaries;Software architecture;Databases;Soft sensors;Data visualization;Big Data;Natural language processing,"Retrospective data harmonization across multiple research cohorts and studies is frequently done to increase statistical power, provide comparison analysis, and create a richer data source for data mining. However, when combining disparate data sources, harmonization projects face data management and analysis challenges. These include differences in the data dictionaries and variable definitions, privacy concerns surrounding health data representing sensitive populations, and lack of properly defined data models. With the availability of mature open-source web-based database technologies, developing a complete software architecture to overcome the challenges associated with the harmonization process can alleviate many roadblocks. By leveraging state-of-the-art software engineering and database principles, we can ensure data quality and enable cross-center online access and collaboration.This paper outlines a complete software architecture developed and customized using the Django web framework, leveraged to harmonize sensitive data collected from three NIH-support birth cohorts. We describe our framework and show how we successfully overcame challenges faced when harmonizing data from these cohorts. We discuss our efforts in data cleaning, data sharing, data transformation, data visualization, and analytics, while reflecting on what we have learned to date from these harmonized datasets.",2021,,10.1109/BigData52589.2021.9671538
155,"Xie, Huan and Tong, Xiaohua and Meng, Wen and Wang, Fang and Xu, Xiong",Multiple stratified sampling strategy for assessing the big remote sensing products,Remote sensing;Inspection;Sampling methods;Sociology;Big Data;Quality assessment;multiple stratified;spatial sampling;quality assessment;remote sensing products;big data,"The number and volume of remote sensing data and its derived products, which are regarded as typical “big data”, have grown exponentially. How to assess the quality of these big remote sensing products become a challenge. As an importance technique, spatial sampling is regarded to be necessary for the quality assessment of remote sensing derived products. This paper proposes an approach of multiple stratified spatial sampling for assessing the remote sensing products, with the aim of resolving the issue of the quality inspection of big remote sensing products. The proposed method improves the sampling accuracy without increasing the sampling size, and the whole procedure is repeatable and easily adopted for the quality inspection of remote sensing derived products.",2015,,10.1109/WHISPERS.2015.8075416
156,"Huang, Yu and Milani, Mostafa and Chiang, Fei","PACAS: Privacy-Aware, Data Cleaning-as-a-Service",Cleaning;Data privacy;Pricing;Data models;Semantics;Osteoarthritis;Maintenance engineering;data quality;data cleaning;data privacy,"Data cleaning consumes up to 80% of the data analysis pipeline. This is a significant overhead for organizations where data cleaning is still a manually driven process requiring domain expertise. Recent advances have fueled a new computing paradigm called Database-as-a-Service, where data management tasks are outsourced to large service providers. We propose a new Data Cleaning-as-a-Service model that allows a client to interact with a data cleaning provider who hosts curated, and sensitive data. We present PACAS: a Privacy-Aware data Cleaning-As-a-Service framework that facilitates communication between the client and the service provider via a data pricing scheme where clients issue queries, and the service provider returns clean answers for a price while protecting her data. We propose a practical privacy model in such interactive settings called (X,Y,L)-anonymity that extends existing data publishing techniques to consider the data semantics while protecting sensitive values. Our evaluation over real data shows that PACAS effectively safeguards semantically related sensitive values, and provides improved accuracy over existing privacy-aware cleaning techniques.",2018,,10.1109/BigData.2018.8622249
157,"Swami, Arun and Vasudevan, Sriram and Huyn, Joojay",Data Sentinel: A Declarative Production-Scale Data Validation Platform,Data integrity;Production;LinkedIn;Big Data;Business;Debugging;Schedules,"Many organizations process big data for important business operations and decisions. Hence, data quality greatly affects their success. Data quality problems continue to be widespread, costing US businesses an estimated $600 billion annually. To date, addressing data quality in production environments still poses many challenges: easily defining properties of high-quality data; validating production-scale data in a timely manner; debugging poor quality data; designing data quality solutions to be easy to use, understand, and operate; and designing data quality solutions to easily integrate with other systems. Current data validation solutions do not comprehensively address these challenges. To address data quality in production environments at LinkedIn, we developed Data Sentinel, a declarative production-scale data validation platform. In a simple and well-structured configuration, users declaratively specify the desired data checks. Then, Data Sentinel performs these data checks and writes the results to an easily understandable report. Furthermore, Data Sentinel provides well-defined schemas for the configuration and report. This makes it easy for other systems to interface or integrate with Data Sentinel. To make Data Sentinel even easier to use, understand, and operate in production environments, we provide Data Sentinel Service (DSS), a complementary system to help specify data checks, schedule, deploy, and tune data validation jobs, and understand data checking results. The contributions of this paper include the following: 1) Data Sentinel, a declarative production-scale data validation platform successfully deployed at LinkedIn 2) A generic design to build and deploy similar systems for production environments 3) Experiences and lessons learned that can benefit practitioners with similar objectives.",2020,,10.1109/ICDE48307.2020.00140
158,"Zhang, Xi and Zhu, Qixuan",Information-Centric Virtualization for Software-Defined Statistical QoS Provisioning Over 5G Multimedia Big Data Wireless Networks,Big Data;Quality of service;Wireless networks;5G mobile communication;Resource management;Wireless sensor networks;5G multimedia big data wireless networks;ICN;NFV;SDN;optimal transmit power;statistical delay-bounded QoS;effective capacity;relay selection,"The multimedia transmission represents a typical big data application in the fifth-generation (5G) wireless networks. However, supporting multimedia big data transmission over 5G wireless networks imposes many new and open challenges because multimedia big data services are both time-sensitive and bandwidth-intensive over time-varying wireless channels with constrained wireless resources. To overcome these difficulties, in this paper we propose the information-centric virtualization architectures for software-defined statistical delay-bounded quality of service (QoS) provisioning over 5G multimedia big data wireless networks. In particular, our proposed schemes integrate the three 5G-promising candidate techniques to guarantee the statistical delay-bounded QoS for multimedia big data transmissions: 1) information-centric network (ICN), to derive the optimal in-network caching locations for multimedia big data; 2) network functions virtualization (NFV), to abstract the PHY-layer infrastructures into several virtualized networks to derive the optimal multimedia data contents delivery paths; and 3) software-defined networks (SDNs), to dynamically reconfigure wireless resources allocation architectures through the SDN-control plane. Under our proposed architectures, to jointly optimize the implementations of NFV and SDN techniques under ICN architectures, we develop the three virtual network selection and transmit-power allocation schemes to: 1) maximize single user's effective capacity; 2) jointly optimize the aggregate effective capacity and allocation fairness over all users; and 3) coordinate non-cooperative gaming among all users, respectively. By simulations and numerical analyses, we show that our proposed architectures and schemes significantly outperform the other existing schemes in supporting the statistical delay-bounded QoS provisioning over the 5G multimedia big data wireless networks.",2019,,10.1109/JSAC.2019.2927088
159,"Sinha, Shweta and Seys, Marcia",HL7 Data Acquisition amp; Integration: Challenges and Best Practices,Medical services;Organizations;Standards organizations;Engines;Monitoring;Best practices;Interoperability;HL7;big data;integration;acquisition;master patient index;payer;interoperability;data quality,"Lack of interoperability between health data systems is a leading challenge for healthcare in the United States. This paper describes the challenges and lessons learned in the process of incorporating HL7 data and integration with Electronic Health Records and Health Information Exchanges from the perspective of a midsized Health Plan (Payer). As a Health Care Payer, Premera has a unique perspective regarding how health plans can provide the necessary data to complete the picture of care. This paper shares some of the best practices and focus areas for successful implementation of healthcare data integrations. This paper also focuses on integrating claims and clinical data using a master patient index as well as challenges faced in that process.Note that going forward `Health Plan' and `Payer' will be used interchangeably. Also, `Provider(s)', `Hospitals', `Healthcare Providers', `Clinics', `Provider Organizations' will be used interchangeably and in the context of this paper may mean the same.",2018,,10.1109/BigData.2018.8622349
160,"Mohammad, Banan and Alzyadat, Wael and Al-Fayoumi, Mohammad and EL Hawi, Ruba and AyshAlhroob",An Improve The Quality Of Data Considering Big Data Aspect Based On Sensitive Of Cost Time,Costs;Correlation;Data integrity;Volume measurement;Project management;Big Data;Time measurement;Big Data;Project Management;Sensitive Rule;Quality,"Big data is term of dataset with characteristic volume, value and veracity that lead to confrontation unable proceed using traditional techniques to extract value, project management perspective is dynamic processing that utilizes the suitable resources of organization in many stages by measuring in four-factor scope, time, cost and quality. In this research aim improve data quality from big data via project management scope consist on high trust which is bring high accuracy from confidence level in volume of data, confidence get with context and value of data which lead to determine accuracy deeply in it and finally select from data depending on veracity of it, the experiment using three main factors time, cost and scope, strongest relation organizing between them start by project scope as strongest one then cost, product and Last one time is weakest between them, in the final when select best quality use two sides mostly from quality degree and be center of quality interval and in particular from closest distance with the strongest factor.",2021,,10.1109/ICEET53442.2021.9659660
161,"Zhang, Meifan and Wang, Hongzhi and Li, Jianzhong and Gao, Hong",One-Pass Inconsistency Detection Algorithms for Big Data,Big Data;Data integrity;Detection algorithms;Databases;Time complexity;Hazards;Business;Inconsistency detection;big data;one-pass algorithm;data quality;denial constraint,"Data in the real world is often dirty. Inconsistency is an important kind of dirty data; before repairing inconsistency, we need to detect them first. The time complexities of the current inconsistency detection algorithms are super-linear to the size of data and not suitable for the big data. For the inconsistency detection of big data, we develop an algorithm that detects inconsistency within the one-pass scan of the data according to both the functional dependency (FD) and the conditional functional dependency (CFD) in our previous work. In this paper, we propose inconsistency detection algorithms in terms of FD, CFD, and Denial Constraint (DC). DCs are more expressive than FDs and CFDs. Developing the algorithm to detect the violation of DCs increases the applicability of our inconsistency detection algorithms. We compare the performance of our algorithm with the performance of implementing SQL queries in MySQL and BigQuery. The experimental results indicate the high efficiency of our algorithms.",2019,,10.1109/ACCESS.2019.2898707
162,"Radhakrishnan, Asha and Das, Sarasij",Quality Assessment of Smart Grid Data,Data integrity;Smart grids;Phasor measurement units;Big Data;Quality assessment;Big data;data quality;power system;smart grid,"Enormous amount of data gets generated in the Smart Grids (SGs) due to the large number of measuring devices, higher measurement rates and various types of sensors. Smart grid data contains important and critical information about the grid. Data driven applications are being developed for better planning, monitoring and operation of SGs. The outcome of data analytics heavily depends on the quality of SG data. However, not much work has been reported on the quality assessment of SG data. This paper addresses the objective assessment of SG data quality. Various dimensions of SG data quality are identified in this paper. Mathematical formulations are proposed to quantify the SG data quality. Proposed data quality metrics have been applied on the SCADA and PMU measurements collected from the Southern Regional Grid of India to demonstrate their effectiveness.",2018,,10.1109/NPSC.2018.8771733
163,"Aggarwal, Ankur",Identification of quality parameters associated with 3V's of Big Data,Big data;Organizations;Conferences;Social network services;Electronic mail;Databases;Big Data;Quality;Volume;Variety;Velocity,"Big Data approach uses an empirical process that does not lie on the understanding of underlying mechanisms, but lies on the observation of facts. Achieving high quality in Big Data is a critical issue for both the database researchers and practitioners. More explicit consideration must be given to data quality since data increasingly outlives the application for which it was initially designed. In this paper, identification of quality parameters is done which are compatible to the 3V's of big data which will further provide enhancement in achieving quality data to be stored in the repository. Good utilization of Big Data strengthens the performance and competitiveness of the firms by enabling better and faster results to its customer needs. In this paper GQM (Goal Question Metric) methodology is proposed to measure quality using metrics. It describes how to include data quality metrics to project, progress and maintain levels of quality in an organization. It helps to make a decision whether or not our current data satisfies our quality prospects.",2016,,
164,"Wang, Haiyan and Zhang, Han",User Requirements Based Service Identification for Big Data,Clustering algorithms;Big Data;Quality of service;Monitoring;Reliability;Optimization;Service Identification;User Requirements;PSO Algorithm;Quality of Experience (QoE),"Service identification meets with new challenges with overwhelming rise of categories and numbers of services in big data scenarios. Most of the current service identification approaches have paid little attention to the granularity of indicator for service identification, neither do they provide with any trustworthy monitoring mechanism during the process of service identification. To address the problems above, we propose a user requirements based service identification approach for big data (URBSI-BD). In the proposed URBSI-BD, we firstly cluster massive services with BIRCH clustering algorithm to obtain a number of service sets. We then employ PSO algorithm with MapReduce mechanism to achieve a fine-grained evaluation of indicator for service identification. Based on the integration, candidate services which can better meet with user requirements will be selected. Finally, we use Beth trust model on the quality of experience of users and set up a monitoring mechanism to better obtain required services. Simulation results and analysis demonstrate that the proposed approach has better performance in service identification compared with other current approaches in big data scenarios.",2017,,10.1109/ICWS.2017.11
165,"Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Gondalia, Shlok and Duggan, Jerry and Kahn, Michael G.",An Autocorrelation-based LSTM-Autoencoder for Anomaly Detection on Time-Series Data,NASA;Manuals;Big Data;Servers;Decision trees;Anomaly detection;Random forests;Anomaly detection;Autocorrelation;Data quality tests;Explainability;LSTM-Autoencoder;Time series,"Data quality significantly impacts the results of data analytics. Researchers have proposed machine learning based anomaly detection techniques to identify incorrect data. Existing approaches fail to (1) identify the underlying domain constraints violated by the anomalous data, and (2) generate explanations of these violations in a form comprehensible to domain experts. We propose IDEAL, which is an LSTM-Autoencoder based approach that detects anomalies in multivariate time-series data, generates domain constraints, and reports subsequences that violate the constraints as anomalies. We propose an automated autocorrelation-based windowing approach to adjust the network input size, thereby improving the correctness and performance of constraint discovery over manual and brute-force approaches. The anomalies are visualized in a manner comprehensible to domain experts in the form of decision trees extracted from a random forest classifier. Domain experts can then provide feedback to retrain the learning model and improve the accuracy of the process. We evaluate the effectiveness of IDEAL using datasets from Yahoo servers, NASA Shuttle, and Colorado State University Energy Institute. We demonstrate that IDEAL can detect previously known anomalies from these datasets. Using mutation analysis, we show that IDEAL can detect different types of injected faults. We also demonstrate that the accuracy improves after incorporating domain expert feedback.",2020,,10.1109/BigData50022.2020.9378192
166,"Hao, Jiao and Jinming, Chen and Yajuan, Guo",Data-driven lean Management for Distribution Network,Big Data;Maintenance engineering;Data models;Investment;Fault diagnosis;Poles and towers;data-driven;lean management;closed-loop;big data analysis,"This paper proposes a concept of “data-driven, lean-oriented and closed-loop” management for distribution network and explain how to implement this kind of management, as shown in fig.1 Firstly, a big data platform is constructed to integrate and combine the multi-source data. Secondly, big data analysis technologies such as data mining, machine learning and data visualization are applied to solve problems in distribution network production. For example, accurate location of the fault can be found with help of multisource information from different devices and systems. And we can also be aware of the risk points in distribution network through history data analysis. Finally, this Paper explains how to promote lean management of distribution network in the fields of asset, operation, maintenance and investment based on the big data platform and big data analysis methods. In addition, the feedback procedure sets up a bridge between application and data collecting, which further improve the data quality. Those management measure have been piloted in several cities in Jiangsu. The result proves that they can improve power supply reliability and reduce operating costs significantly. Two practical cases are given to show how they work.",2018,,10.1109/CICED.2018.8592556
167,"Ramzan, Shabana and Bajwa, Imran Sarwar and Ramzan, Bushra and Anwar, Waheed",Intelligent Data Engineering for Migration to NoSQL Based Secure Environments,Big Data;NoSQL databases;Transforms;Scalability;Servers;Tools;Relational databases;NoSQL;big data;data cleansing,"In an era of super computing, data is increasing exponentially requiring more proficiency from the available technologies of data storage, data processing, and analysis. Such continuous massive growth of structured and unstructured data is referred to as a “Big data”. The processing and storage of big data through a conventional technique is not possible. Due to improved proficiency of Big Data solution in handling data, such as NoSQL caused the developers in the previous decade to start preferring big data databases, such as Apache Cassandra, Oracle, and NoSQL. NoSQL is a modern database technology that is designed to provide scalability to support voluminous data, leading to the rise of NoSQL as the most viable database solution. These modern databases aim to overcome the limitations of relational databases such as unlimited scalability, high performance, data modeling, data distribution, and continuous availability. These days, the larger enterprises need to shift NoSQL databases due to their more flexible models. It is a great challenge for business organizations and enterprises to transform their existing databases to NoSQL databases considering heterogeneity and complexity in relational data. In addition, with the emergence of big data, data cleansing has become a great challenge. In this paper, we proposed an approach that has two modules: data transformation and data cleansing module. The first phase is the transformation of a relational database to Oracle NoSQL database through model transformation. The second phase provides data cleansing ability to improve data quality and prepare it for big data analytics. The experiments show the proposed approach successfully transforms the relational database to a big data database and improve data quality.",2019,,10.1109/ACCESS.2019.2916912
168,"Yao, Le and Ge, Zhiqiang",Scalable Semisupervised GMM for Big Data Quality Prediction in Multimode Processes,Data models;Big Data;Predictive models;Inference algorithms;Prediction algorithms;Semisupervised learning;Computational modeling;Big data;Gaussian mixture model (GMM);multimode process modeling;quality prediction;semisupervised modeling;stochastic variational inference (SVI),"In this paper, a novel variational inference semisupervised Gaussian mixture model (VI-S2GMM) model is first proposed for semisupervised predictive modeling in multimode processes. Parameters of Gaussian components are identified more accurately with extra unlabeled samples, which improve the prediction performance of the regression model. Since all labeled and unlabeled data samples are involved in each iteration of parameter updating, intractable computing problems occur when facing high-dimension datasets. To tackle this problem, a scalable stochastic VI-S2GMM (SVI-S2GMM) is further proposed. Through taking advantage of a stochastic gradient optimization algorithm to maximize the evidence of lower bound, the VI-based algorithm becomes scalable. In the SVI-S2GMM, only one or a minibatch of samples is randomly selected to update parameters in each iteration, which is more efficient than the VI-S2GMM. Since the whole dataset is divided and transferred to iterations batch by batch, the scalable SVI-S2GMM algorithm can easily handle the big data modeling issue. In this way, a large number of unlabeled data can be useful in the modeling, which will further benefit the prediction performance. The SVI-S2GMM is then exploited for the prediction of a quality-related key performance index. Two examples demonstrate the feasibility and effectiveness of the proposed algorithms.",2019,,10.1109/TIE.2018.2856200
169,"Dehui, Fu and Feng, Wang and Shuai, Yuan and Guangzhen, Wang and Mingxin, Shao",Fuzzy Comprehensive Evaluation Method for On-line Monitoring Data Quality of Substation Equipment,big data;data quality;subordination;fuzzy comprehensive evaluation;On-line monitoring,"This paper analyses the existing problems in on-line monitoring and data quality evaluation of substation equipment, and proposes a multi-dimensional fuzzy comprehensive evaluation method for on-line monitoring data quality of substation equipment. The evaluation index set of online monitoring data quality of substation equipment with 5 dimensions and 11 secondary indexes is established. The weight is determined by combining subjective and objective methods. The fuzzy transformation is completed based on membership function and a multi-dimensional fuzzy comprehensive evaluation model is established. Finally, the evaluation grade of online monitoring data of substation equipment is obtained. Finally, compared with other methods, the validity and accuracy of this method are verified.",2019,,10.1109/ICISCE48695.2019.00154
170,"Pankowska, Malgorzata",Service science facing Big Data,Big data;Quality of service;Monitoring;Decision making;Organizations;Data analysis;Big Data;information governance;service science;Service Level Management;service quality,"The Big Data is a modification of the traditional view of information organization, particularly view of the data warehouses and databases. Nowadays, business organizations must address a mix of structured, unstructured and streaming data that supports queries and reports. Business recognized the wealth of untapped information in open social media data. Therefore, the goal of this paper is to present the procedural approach on how to cope with massive data sets' management. The proposal included in this paper covers service science application.",2014,,10.1109/i-Society.2014.7009043
171,"Zeyong, Wang and Yutian, Hong and Zhongzheng, Tong",Risk Assessment Model and Experimental Analysis of Electric Power Production Based on Big Data,risk assessment;electric power;fuzzy comprehensive evaluation;Hadoop;index,"This paper studies the characteristics of big data of power, and aims at the data quality problems faced by power system. It puts forward an assessment method of power system data quality. Based on the characteristics of large power data, a series of indicators influencing the data are analyzed and hierarchically divided to determine the measurement standard of power production data during the process of risk management, namely, the risk index system. Then, the risk assessment model of power data is established by referring to the assessment model in other fields or the rules of deduction and induction in data mining. It can be used to evaluate the quality of power system data, and find a framework and solution suitable for large data quality assessment. Finally, the model is implemented on Hadoop platform, which proves that it takes into account the completeness of the index system, the objectivity of the assessment method and the rapidity of the calculation method.",2019,,10.1109/ICSGEA.2019.00028
172,"Zhang, Lu and Chen, Yanxia and Zhu, Jie and Pan, Mingyu and Sun, Zhou and Wang, Weixian",Data quality analysis and improved strategy research on operations management system for electric vehicles,Big data;Distributed databases;Decision support systems;Power industry;Electric vehicles;Systems operation;Maintenance engineering;electric vehicle;operations management system;big data;data quality;data selection and processing,"It is very important for Operations Management System (OMS) and big data analysis application to improve the data quality of Electric Vehicle (EV) charging service. This paper focuses on the charging transaction record data from the Beijing EV charging OMS, and analyzes error types and distributed locations of the abnormal data. Based on the mathematical logic among various kinds of operation data, the data selection rules and processing method are proposed, and the system improved scheme is given. Through the design and application of data selection and processing module, the abnormal data can be timely detected and corrected. It is also beneficial for the system operation and maintenance to improve the acquisition data quality. The comparative analysis results verify the feasibility and effectivity of the proposed scheme. This research is a necessary guarantee for the big data technology application.",2015,,10.1109/DRPT.2015.7432708
173,"Sattart, Farook and McQuay, Colter and Driessen, Peter F.",Marine mammal sound anomaly and quality detection using multitaper spectrogram and hydrophone big data,Spectrogram;Whales;Sonar equipment;Big Data;Acoustic distortion;Anomaly detection;Anomaly detection;Hydrophone Big data;Quality detection;Multitaper spectrogram;Marine mammal sound;Sperm whale,"This paper proposes a novel method for anomaly and quality detection of marine mammal sounds using multitaper spectrogram and hydrophone big data. The proposed method is aimed to automatically detect anomaly, such as high-frequency vessel noise, Doppler noise, in sperm whale (SPW) sound as well as the quality of the sound. A new signature function derived from a multi-taper spectrogram is able to detect the anomaly in the data and a new anomaly distortion measure can detect the sound quality into good/bad. The proposed method, is tested with 1905 minutes of data spanning a single year, and using a human operator's annotations. The experimental results reveal that the proposed multitaper spectrogram based approach is efficient in detecting anomaly as well as sperm whale sound quality for hydrophone big data and high detection accuracy (>85%) is achieved for raw input hydrophone data.",2017,,10.1109/PACRIM.2017.8121916
174,"Borrison, Reuben and Kloepper, Benjamin and Mullen, Jennifer",Data Preparation for Data Mining in Chemical Plants using Big Data,Data quality;Soft sensors;Big data,"Data preparation for data mining in industrial applications is a key success factor which requires considerable repeated efforts. Although the required activities need to be repeated in very similar fashion across many projects, details of their implementation differ and require both application understanding and experience. As a result, data preparation is done by data mining experts with a strong domain background and a good understanding of the characteristics of the data to be analyzed. Experts with these profiles usually have an engineering background and no strong expertise in distributed programming or big data technology. Unfortunately, the amount of data can be so large that distributed algorithms are required to allow for inspection of results and iteration of preparation steps. This contribution introduces an interactive data preparation workflow for signal data from chemical plants enabling domain experts without background in distributed computing and extensive programming experience to leverage the power of big data technologies.",2019,,10.1109/INDIN41052.2019.8972078
175,"Song, Shaoxu and Gao, Fei and Huang, Ruihong and Wang, Chaokun",Data Dependencies over Big Data: A Family Tree,Big Data;Phase frequency detectors;Lakes;Picture archiving and communication systems;Databases;Task analysis;Proposals;Integrity constraints;data dependencies,"Besides the conventional schema-oriented tasks, data dependencies are recently revisited for data quality applications, such as violation detection. To address the variety and veracity issues of big data, data dependencies have been extended as data quality rules to adapt to various data types, ranging from (1)categorical data with equality relationships to (2)heterogeneous data with similarity relationships, and (3)numerical data with order relationships. In this survey, we briefly review the recent proposals on data dependencies categorized into the aforesaid types of data. In addition to (a)the concepts of these data dependency notations, we investigate (b)the extension relationships between data dependencies, e.g., conditional functional dependencies (CFDs) extend the conventional functional dependencies (FDs). It forms a family tree of extensions, mostly rooted in FDs, helping us understand the expressive power of various data dependencies. Moreover, we summarize (c)the discovery of dependencies from data, since data dependencies are often unlikely to be manually specified in a traditional way, given the huge volume and high variety of big data. We further outline (d)the applications of the extended data dependencies, in particular in data quality practice. It guides users to select proper data dependencies with sufficient expressive power and reasonable discovery cost. Finally, we conclude with several directions of future studies on the emerging data.",2020,,10.1109/TKDE.2020.3046443
176,"Bruballa, Eva and Taboada, Manel and Cabrera, Eduardo and Rexachs, Dolores and Luque, Emilio",Simulation and Big Data: A Way to Discover Unusual Knowledge in Emergency Departments: Work-in-Progress Paper,Data models;Data mining;Computational modeling;Hospitals;Analytical models;Big data;Agent-Based Modeling and Simulation (ABMS);Big Data;Data Mining (DM);Decision Support Systems (DSS);Emergency Department (ED);Knowledge Discovery,"Here a work in progress is reported on within research that aims to obtain knowledge about variables which may influence a hospital emergency department's performance and quality of service. Knowledge discovery will be achieved through the analysis of intensive data generated by the simulation of any possible scenario in the real system. The challenge is to provide knowledge of critical, non-usual or extreme situations. Simulation is the only way to obtain information about these kinds of situations, as it is not possible to test such scenarios in the real system. We show how simulation of the real system through advanced computing is a source of big data, as it allows rapid and massive data generation. The potential of high performance computing makes it possible to generate a very large amount of data within a reasonable time, store this data, then process and analyze it to obtain knowledge. We describe the methodology proposed for this goal, which is based on the use of the simulator as a sensor of the real system, and so as the main source of data. The application of data mining techniques will open the doors to knowledge. To verify that the proposed methodology works, we propose a case study in which the aim is to obtain knowledge from a set of data already available, obtained from the simulation of a reduced set of scenarios of the real system.",2014,,10.1109/FiCloud.2014.65
177,"Tu, Shouzhong and Huang, Minlie",Scalable Functional Dependencies Discovery from Big Data,Distributed databases;Big data;Lattices;Algorithm design and analysis;Partitioning algorithms;Knowledge discovery;Functional dependencies;Discovering functional dependencies;Knowledge discovery;Big data,"Functional dependencies (FDs) represent potentially novel and interesting patterns existent in relational databases. The discovery of functional dependencies has a wide range of applications such as database design, knowledge discovery, data quality assessment, etc. There has been growing interest in the problem of functional dependencies discovery in the last ten years. However, existing functional dependencies discovery algorithms are mainly applied to centralized small data. It is far more challenging to discover functional dependencies from big data. In this paper, we propose an efficient functional dependencies discovery algorithm, for mining functional dependencies from distributed big data. We prune candidate FDs at each node by local fragmented data and batch verify candidate FDs in parallel. Load balance is taken into account when discovering functional dependencies. Experiments show that the proposed algorithm is effective on real dataset and synthetic dataset.",2016,,10.1109/BigMM.2016.63
178,"Elsahlamy, Ebtsam and Eshra, Abeer and Eshra, Nadia and El-Fishawy, Nawal",Empowering GIS with Big Data: A review of recent advances,Government;Data visualization;Production;Big Data;Agriculture;Mobile handsets;Servers;GIS;Big-Data;Geospatial;Agriculture;map-reduce,"In the past few decades, the use of geographic information systems (GIS) was efficient with servers that could handle the amount of data used. However, as geographical big data grows in size and complexity, storing, managing, processing, analyzing, visualizing, and confirming data quality becomes more difficult. Academia, industry, government, and other institutions are increasingly interested in this information. It's known as Big Data. Since that kind of data recently became massive, there was a need to develop methods to deal with big data and analyze it to keep pace with development. In this paper, we review the previous studies that involve both Big Data and GIS in different applications. Moreover, we focus on the field of agriculture, which is considered one of the most important sources of the economy. Produced results in this research area help decision-makers to make sound executive steps to reach better production.",2021,,10.1109/ICEEM52022.2021.9480634
179,"Ullah, Faheem and Ali Babar, M.",QuickAdapt: Scalable Adaptation for Big Data Cyber Security Analytics,"Big Data;Quality of service;Time factors;Feature extraction;Computer crime;Computer architecture;big data, cyber security, adaptation, accuracy","Big Data Cyber Security Analytics (BDCA) leverages big data technologies for collecting, storing, and analyzing a large volume of security events data to detect cyber-attacks. Accuracy and response time, being the most important quality concerns for BDCA, are impacted by changes in security events data. Whilst it is promising to adapt a BDCA system's architecture to the changes in security events data for optimizing accuracy and response time, it is important to consider large search space of architectural configurations. Searching a large space of configurations for potential adaptation incurs an overwhelming adaptation time, which may cancel the benefits of adaptation. We present an adaptation approach, QuickAdapt, to enable quick adaptation of a BDCA system. QuickAdapt uses descriptive statistics (e.g., mean and variance) of security events data and fuzzy rules to (re) compose a system with a set of components to ensure optimal accuracy and response time. We have evaluated QuickAdapt for a distributed BDCA system using four datasets. Our evaluation shows that on average QuickAdapt reduces adaptation time by 105× with a competitive adaptation accuracy of 70% as compared to an existing solution.",2019,,10.1109/ICECCS.2019.00016
180,"Molinari, Andrea and Nollo, Giandomenico",The quality concerns in health care Big Data,Big Data;Medical services;Data integrity;Biomedical monitoring;Business;Big Data;Analytics;healthcare quality;entity reconciliation,"Health information technology is showing an impressive growing interest towards Big Data. Big Data Analytics is expected to bring important achievements for building sophisticated models, methods and tools that are expected to improve healthcare services and citizen health and wellbeing. In spite of these expectations data quality and analytics methods are not getting the attention they deserve. In this short paper, we aimed to highlight the issues of data quality in the context of Big Data Healthcare Analytics. The common sources of errors, the consequence of these errors, and potential solutions that should be considered to mitigate errors and pitfalls are discussed in the healthcare context.",2020,,10.1109/MELECON48756.2020.9140534
181,"Han, Weiguo and Jochum, Matthew",Latency analysis of large volume satellite data transmissions,Satellites;Data communication;Real-time systems;Databases;Browsers;Big Data;Big Data;Satellite Data;Data Latency;Data Quality;NoSQL;MongoDB,"A wide array of time-sensitive satellite data is required in the research and development activities for natural hazard assessment, storms and weather prediction, hurricane tracking, disaster and emergency response, and so on. Identifying and analyzing the latencies of large volumes of real-time and near real-time satellite data is very useful and helpful for detecting transmission issues, managing IT resources, and configuring and optimizing data management systems. This paper introduces how to monitor and collect important timestamps of data transmissions, organize them in a NoSQL database, and explore data latency via a user-friendly dashboard. Taking Sentinel series satellite data as an example, data transmission issues are illustrated and investigated further. Latency analysis and explorations help data providers and managers improve data transmission and enhance data management.",2017,,10.1109/IGARSS.2017.8126976
182,"Liu, Wanting and Peng, Yonghong and Tobin, Desmond J",Integrated analytics of microarray big data reveals robust gene signature,Accuracy;Diseases;Big data;Robustness;Educational institutions;Bioinformatics;Malignant tumors;Microarray;integrated analytics;biomarkers,"The advance of high throughput biotechnology enables the generation of large amount of biomedical data. The microarray is increasingly a popular approach for the detection of genome-wide gene expression. Microarray data have thus increased significantly in public accessible database repositories, which provide valuable big data for scientific research. To deal with the challenge of microarray big data collected in different research labs using different experimental set-ups and on different bio-samples, this paper presents a primary study to evaluate the impact of two important factors (the origin of bio-samples and the quality of microarray data) on the integrated analytics of multiple microarray data. The aim is to enable the extraction of reliable and robust gene biomarkers from microarray big data. Our work showed that in order to enhance biomarker discovery from microarray big data (i) it is necessary to treat the microarray data differently in terms of their quality, (ii) it is recommended to stratifying (i.e., sub-group) the data according to the origin of bio-samples in the analytics.",2014,,10.1109/CIBD.2014.7011535
183,"Abboura, Asma and Sahrl, Soror and Ouziri, Mourad and Benbernou, Salima",CrowdMD: Crowdsourcing-based approach for deduplication,Big data;Databases;Labeling;Hybrid power systems;Crowdsourcing;Training;Cleaning;matching rules;deduplication;entity resolution;big data quality,"Matching dependencies (MDs) were recently introduced as quality rules for data cleaning and entity resolution. They are rules that specify what values should be considered duplicates, and have to be matched. Defining such quality rules on a database instance, is a very expensive and a time consuming process, and requires huge efforts to analyse the whole database. In this demo paper, we present CrowdMD, a hybrid machine-crowd system for generating MDs. It first asks the crowd to determine whether a given pair, from training sample pairs, match or not. Then, it uses data mining techniques to generate attributes constituting an MD. Using a Restaurant database, we will show how the crowders can help to generate MDs by labelling the training sample through the CrowdMD user interface and how MDs can be mined from this training set.",2015,,10.1109/BigData.2015.7364061
184,"Wigan, Marcus R. and Clarke, Roger",Big Data's Big Unintended Consequences,Information management;Data handling;Data storage systems;Government policies;Databases;Business;Legal aspects;Data privacy;policy;privacy;data;social impact;big data;private data commons,"Businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons. The Web extra at http://youtu.be/TvXoQhrrGzg is a video in which author Marcus Wigan expands on his article ""Big Data's Big Unintended Consequences"" and discusses how businesses and governments exploit big data without regard for issues of legality, data quality, disparate data meanings, and process quality. This often results in poor decisions, with individuals bearing the greatest risk. The threats harbored by big data extend far beyond the individual, however, and call for new legal structures, business processes, and concepts such as a Private Data Commons.",2013,,10.1109/MC.2013.195
185,"Yahia, Nesrine Ben and Hlel, Jihen and Colomo-Palacios, Ricardo",From Big Data to Deep Data to Support People Analytics for Employee Attrition Prediction,Big Data;Organizations;Radio frequency;Predictive models;Support vector machines;Data models;Analytical models;Deep people analytics;employee attrition;retention;prediction;interpretation;policies recommendation,"In the era of data science and big data analytics, people analytics help organizations and their human resources (HR) managers to reduce attrition by changing the way of attracting and retaining talent. In this context, employee attrition presents a critical problem and a big risk for organizations as it affects not only their productivity but also their planning continuity. In this context, the salient contributions of this research are as follows. Firstly, we propose a people analytics approach to predict employee attrition that shifts from a big data to a deep data context by focusing on data quality instead of its quantity. In fact, this deep data-driven approach is based on a mixed method to construct a relevant employee attrition model in order to identify key employee features influencing his/her attrition. In this method, we started thinking `big' by collecting most of the common features from the literature (an exploratory research) then we tried thinking `deep' by filtering and selecting the most important features using survey and feature selection algorithms (a quantitative method). Secondly, this attrition prediction approach is based on machine, deep and ensemble learning models and is experimented on a large-sized and a medium-sized simulated human resources datasets and then a real small-sized dataset from a total of 450 responses. Our approach achieves higher accuracy (0.96, 0.98 and 0.99 respectively) for the three datasets when compared previous solutions. Finally, while rewards and payments are generally considered as the most important keys to retention, our findings indicate that `business travel', which is less common in the literature, is the leading motivator for employees and must be considered within HR policies to retention.",2021,,10.1109/ACCESS.2021.3074559
186,"Tien, James M.",Big Data: Unleashing information,Information management;Data handling;Data storage systems;Educational institutions;Physics;Security;Cameras,"Summary form only given. At present, it is projected that about 4 zettabytes (or 10**21 bytes) of electronic data are being generated per year by everything from underground physics experiments to retail transactions to security cameras to global positioning systems. In the U. S., major research programs are being funded to deal with big data in all five economic sectors (i.e., services, manufacturing, construction, agriculture and mining) of the economy. Big Data is a term applied to data sets whose size is beyond the ability of available tools to undertake their acquisition, access, analytics and/or application in a reasonable amount of time. Whereas Tien (2003) forewarned about the data rich, information poor (DRIP) problems that have been pervasive since the advent of large-scale data collections or warehouses, the DRIP conundrum has been somewhat mitigated by the Big Data approach which has unleashed information in a manner that can support informed - yet, not necessarily defensible or knowledgeable - decisions or choices. Thus, by somewhat overcoming data quality issues with data quantity, data access restrictions with on-demand cloud computing, causative analysis with correlative data analytics, and model-driven with evidence-driven applications, appropriate actions can be undertaken with the obtained information. New acquisition, access, analytics and application technologies are being developed to further Big Data as it is being employed to help resolve the 14 grand challenges (identified by the National Academy of Engineering in 2008), underpin the 10 breakthrough technologies (compiled by the Massachusetts Institute of Technology in 2013) and support the Third Industrial Revolution of mass customization.",2013,,10.1109/ICSSSM.2013.6602615
187,"Peethambaran, Geetha and Naikodi, Chandrakant and Suresh, L",An Ensemble Learning Approach for Privacy–Quality–Efficiency Trade-Off in Data Analytics,Data privacy;Privacy;Support vector machines;Big Data;Classification algorithms;Data models;Data integrity;Privacy;Scalability;Big Data;Spark;Analytics;Privacy Preserving;Performance;Utility;UCI;Composite;Efficiency;Anonymization,"Privacy is an issue of concern in the electronic era where data has become a primary source of investment for businesses and organizations. The value generated from data is put to use in a number of ways for economic benefit. Customer profiling is one such instance, where data collected is used for targeted marketing, personalized purchase recommendations and customized product deliveries. In such applications, the risk of individual sensitive information disclosure always prevails, affecting the privacy of individuals involved. Hence privacy preserving analysis demands suppressing or transforming data before it is published for analysis, thus curbing data leak. Subsequently, data quality degrades, and operative analytics is affected. With Big data, algorithms that offer a reasonable qualityprivacy trade off need enhancements in terms of efficiency and scalability. In this paper, the work proposed uses a privacy based composite classifier model to analyze the accuracy of classification. The diverse characteristics of algorithms in the composite classifier are found to balance the classification accuracy that is likely to get affected by privacy model. Further, the model's performance with respect to execution time is then evaluated using the parallel computing framework Spark.",2020,,10.1109/ICOSEC49089.2020.9215250
188,"Alzyadat, Wael and AlHroob, Aysh and Almukahel, Ikhlas Hassan and Muhairat, Mohammad and Abdallah, Mohammad and Althunibat, Ahmad","Big Data, Classification, Clustering and Generate Rules: An inevitably intertwined for Prediction",Databases;Data integrity;Big Data;Data warehouses;Data mining;Information technology;Standards;classification;marketing;association rules;Big Data;k-mean;prediction;preprocess,"Big Data filed is an unsettled standard comparing with a traditional database, data mining, or data warehouse. Stability measure aims to acquire the quality dataset which encourages to use of preprocessing data method to handle instability that miniaturization missing data. Therefore, to increase the data quality in order to achieve an accurate prediction, significant rules are used to provide value and meaningful data. Through, three measures by support, confidence, and the lift to acquire frequently rules. These rules are used to conduct the objective extracting pattern, to estimate each browsing customer's likelihood of making a purchase, and to choose meaningful patterns from the discovered association rules.",2021,,10.1109/ICIT52682.2021.9491733
189,"Hampson, Gary and Hargreaves, Neil and Jakubowicz, Helmut and Williams, Gareth and Hatton, Les","Open Collaboration, Data Quality, and COVID-19",Industries;Pathogens;Pandemics;Data integrity;Collaboration;Data models;Numerical models,"The flavor of this ""Impact"" department is somewhat different. In a pandemic, everybody has to come together. In April 2020, a call went out in the United Kingdom for groups to informally form and collaborate to study this brutal pathogen in whatever way they could. The five authors of this article, old friends from the geophysical industry with decades of experience in numerical modeling and big data, formed such a group.",2021,,10.1109/MS.2021.3056642
190,"Wen, Hongsheng and Chen, Zhiqiang and Gu, Jianping and Zhu, Qiangqiang",Big Data Analysis on Radiographic Image Quality,Detectors;Image edge detection;Radiography;Standards;Image quality;X-ray imaging;Indexes;image quality;in-service;radiographic product;routine data;quality control,"Mass data generated from in-service radiographic product contain assignable information on Image Quality (IQ). Analyzing data from routine work might supplement the time-consuming Image Quality Assurance Test Procedure (IQATP) to evaluate IQ and to know product type performance on site, which can also locate risks and give manufacturer directions for the further actions as well. This article illustrates methodologies of extracting IQ information from mass data and visual quality track, analysis, control, and risk mitigation in Big Data environments.",2016,,10.1109/CCBD.2016.073
191,"Ramaswamy, Lakshmish and Lawson, Victor and Gogineni, Siva Venkat",Towards a Quality-centric Big Data Architecture for Federated Sensor Services,Feeds;Clouds;Wireless sensor networks;Computer architecture;Fluid flow measurement;Markup languages;Data models;Internet of Things;Federated Sensor Clouds;Data Quality;Sensor Virtualization,"As the Internet of Things (IoT) paradigm gains popularity, the next few years will likely witness 'servitization' of domain sensing functionalities. We envision a cloud-based eco-system in which high quality data from large numbers of independently-managed sensors is shared or even traded in real-time. Such an eco-system will necessarily have multiple stakeholders such as sensor data providers, domain applications that utilize sensor data (data consumers), and cloud infrastructure providers who may collaborate as well as compete. While there has been considerable research on wireless sensor networks, the challenges involved in building cloud-based platforms for hosting sensor services are largely unexplored. In this paper, we present our vision for data quality (DQ)-centric big data infrastructure for federated sensor service clouds. We first motivate our work by providing real-world examples. We outline the key features that federated sensor service clouds need to possess. This paper proposes a big data architecture in which DQ is pervasive throughout the platform. Our architecture includes a markup language called SDQ-ML for describing sensor services as well as for domain applications to express their sensor feed requirements. The paper explores the advantages and limitations of current big data technologies in building various components of the platform. We also outline our initial ideas towards addressing the limitations.",2013,,10.1109/BigData.Congress.2013.21
192,"Jiang, Ying and Zhang, Na and Fang, Ying",The Analysis and Design of Ship Monitoring System Based on Hybrid Replication Technology,Distributed databases;Database systems;Synchronization;Business;Monitoring;Marine vehicles;Distributed database;advanced replication;materialized view;data conflict,"As the core of informatization, data has a huge significance to the development of information-based enterprises. Data replication technology is an important approach to solve the problem of enterprise data sharing based on distributed database system. It plays a crucial role in promoting business integration of enterprises and institutions, improving data quality, enhancing data sharing and improving the application level of back-end big data analysis [1]. It is necessary to do research for making a good data management of the distributed database application system, synchronizing the data to the data, preventing data conflicting and being able to synchronize or asynchronous replication. Combining with the database design model of the ship monitoring and control system, this paper mainly described how to complete the construction of distributed database system using Oracle, based on advanced replication technology named as the combination of multi-agent replication and materialized views hybrid replication technology.",2019,,10.1109/ICITBS.2019.00118
193,"Albertoni, Riccardo and De Martino, Monica and Quarati, Alfonso",Linked Thesauri Quality Assessment and Documentation for Big Data Discovery,Thesauri;Metadata;Vocabulary;Measurement;Quality assessment;Big Data;quality;linked data;thesauri;AHP;metadata;DQV,"Thesauri are knowledge systems which may ease Big Data access, fostering their integration and re-use. Currently several Linked Data thesauri covering multi-disciplines are available. They provide a semantic foundation to effectively support cross-organization and cross-disciplinary management and usage of Big Data. Thesauri effectiveness is affected by their quality. Diverse quality measures are available taking into account different facets. However, an overall measure is needed to compare several thesauri and to identify those more qualified for a proper reuse. In this paper, we propose a Multi Criteria Decision Making based methodology for the documentation of the quality assessment of linked thesauri as a whole. We present a proof of concept of the Analytic Hierarchy Process adoption to the set of Linked Data thesauri for the Environment deployed in LusTRE. We discuss the step-by-step practice to document the overall quality measurements, generated by the quality assessment, with the W3C promoted Data Quality Vocabulary.",2017,,10.1109/HPCS.2017.16
194,"Shi, Weiwei and Zhu, Yongxin and Zhang, Jinkui and Tao, Xiang and Sheng, Gehao and Lian, Yong and Wang, Guoxing and Chen, Yufeng",Improving Power Grid Monitoring Data Quality: An Efficient Machine Learning Framework for Missing Data Prediction,Support vector machines;Data models;Predictive models;Training;Data mining;Feature extraction;Power grids;missing data prediction;machine learning;support vector machine (SVM);power transformer,"Big data techniques has been applied to power grid for the evaluation and prediction of grid conditions. However, the raw data quality rarely can meet the requirement of precise data analytics since raw data set usually contains samples with missing data to which the common data mining models are sensitive. Though classic interpolation or neural network methods can been used to fill the gaps of missing data, their predicted data often fail to fit the rules of power grid conditions. This paper presents a machine learning framework (OR_MLF) to improve the prediction accuracy for datasets with missing data points, which mainly combines preprocessing, optimizing support vector machine (OSVM) and refining SVM (RSVM). On top of the OSVM engine, the scheme introduces dedicated data training strategies. First, the original data originating from data generation facilities is preprocessed through standardization. Traditional SVM is then trained to obtain a preliminary prediction model. Next, the optimized SVM predictors are achieved with new training data set, which is extracted based on the preliminary prediction model. Finally, the missing data prediction result depending on OSVM is selectively inputted into the traditional SVM and the refined SVM is lastly accomplished. We test the OR_MLF framework on missing data prediction of power transformers in power grid system. The experimental results show that the predictors based on the proposed framework achieve lower mean square error than traditional ones. Therefore, the framework OR_MLF would be a good candidate to predict the missing data in power grid system.",2015,,10.1109/HPCC-CSS-ICESS.2015.16
195,"Hildebrandt, Kai and Panse, Fabian and Wilcke, Niklas and Ritter, Norbert",Large-Scale Data Pollution with Apache Spark,Big data;Pollution;Databases;Generators;Prototypes;Gold;Standards;Data quality;duplicate detection;data pollution;Apache Spark,"Because of the increasing volume of autonomously collected data objects, duplicate detection is an important challenge in today's data management. To evaluate the efficiency of duplicate detection algorithms with respect to big data, large test data sets are required. Existing test data generation tools, however, are either not able to produce large test data sets or are domain-dependent which limits their usefulness to a few cases. In this paper, we describe a new framework that can be used to pollute a clean, homogeneous and large data set from an arbitrary domain with duplicates, errors and inhomogeneities. To prove its concept, we implemented a prototype which is built upon the cluster computing framework Apache Spark and evaluate its performance in several experiments.",2020,,10.1109/TBDATA.2016.2637378
196,"Wrembel, Robert",Still Open Problems in Data Warehouse and Data Lake Research: extended abstract,Social networking (online);Soft sensors;Transforms;Data warehouses;Big Data applications;Data models;Security;data integration;data warehouse;data lake;big data;extract transform load;data processing workflow;data processing pipeline;data quality;ETL optimization;data source evolution;metadata,"During recent years, we observe a widespread of new data sources, especially all types of social media and IoT devices, which produce huge data volumes, whose content ranges from fully structured to totally unstructured. All these types of data are commonly referred to as big data. They are typically described by the three most important characteristics, called 3V [1], namely: an extremely large volume, a variety of data models and structures (data representations), as well as a high velocity at which data are generated. We argue that out of these three Vs, the most challenging is variety [2]. Such data need to be integrated and transformed into a common representation, which is suitable for analysis, in a similar manner as traditional (mainly table-like) data.",2021,,10.1109/SNAMS53716.2021.9732098
197,"Subhashini, R. and Akila, G",Valence arousal similarity based recommendation services,Web services;Collaboration;Big data;Quality of service;Recommender systems;Scalability;Web Service;Big Data;Recommender System;MapReduce;Hadoop,"Web Services play a vital role in e-commerce and e-business applications. A WS (Web Service) application is interoperable and can work on any platform i.e.; platform independent, large scale distributed systems can be established easily. A Recommender System is a precious tool for providing appropriate recommendations to all users in a Hotel Reservation Website. User based, Top k and profile based approaches are used in collaborative filtering algorithm which does not provide personalized results to the users and inefficiency and scalability problem also occurs due to the increase in the size of large datasets. To address the above mentioned challenges, a Valence-Arousal Similarity based Recommendation Services, called VAS based RS, is proposed. Our proposed mechanism aims to presents a personalized service recommendation list and recommending the most suitable service to the end users. Moreover, it classifies the positive and negative preferences of the users from their reviews to improve the prediction accuracy. For improve its efficiency and scalability in big data environment, VAS based RS is implemented using collaborative filtering algorithm on MapReduce parallel processing paradigm in Hadoop, a widely-adopted distributed computing platform.",2015,,10.1109/ICCPCT.2015.7159309
198,"Kaplunovich, Alex and Yesha, Yelena","Consolidating billions of Taxi rides with AWS EMR and Spark in the Cloud : Tuning, Analytics and Best Practices",Sparks;Public transportation;Servers;Tuning;Big Data;Structured Query Language;Tools;Analytics;Spark;EMR;Cloud;BigData;Best Practices;Parquet;AWS;Optimization;Tuning,"Saving nature using Big Data Analytics is a very noble goal. Using New York taxi rides data, we decided to learn how many rides could be consolidated. It was a journey we would like to share. First, we had to choose the platform for calculation between Amazon Athena, Serverless Microservices, SQL or NoSql databases, Hadoop and Spark. Then, we had to find an optimal solution for the platform using assorted tuning and optimization techniques. Although the problem seems to be straight forward, it turned out that the solution is quite challenging because of the input size, data quality, calculation complexities and numerous EMR/Spark tuning options. We have been using New York taxi data from 2009 to 2017 to quantify the rides that can be joined together. The taxi rides were consolidated based on pickup location, pickup time and drop-off location. We have been calculating the percentage of taxi rides that can be joined. The benchmark originally set was rides within five minutes with a pickup and drop-off locations within half a kilometer. Then we started experimenting with different times and locations. We have been using parquet format, parallel Scala collections, compression, filtering, new column introduction, tuning parameters, I/O overhead tuning, bucketing, timeouts and partitioning. Over 1.2 billion rides were processed using Amazon EMR with Spark. We have been optimizing calculation time and processing price. Spark has hundreds of parameters, EMR has over fifty instances to choose from. It was challenging to process our data within reasonable time. We were able to find the optimal Spark queries (plans), tested different types of joins and compared their performances. Also, we were able to compare I/O and in-memory operations during partitioning and large files manipulation (the input file sizes were hundreds of Gigabytes). The results were amazing - we could consolidate around thirty five percent of total rides, saving tons of gas and improving environment and traffic in New York City.",2018,,10.1109/BigData.2018.8622378
199,"Mylavarapu, Goutam and Viswanathan, K. Ashwin and Thomas, Johnson P.",Assessing Context-Aware Data Consistency,Feature extraction;Data analysis;Data integrity;Data models;Machine learning algorithms;Context modeling;Task analysis;Data analysis;data context;data quality;data consistency;machine learning;word embeddings;approximate dependencies;mutual information;apache hadoop;apache spark,"Data analysis is a demanding task that involves extracting deep insights hidden in data. Many businesses enforce data analysis irrespective of the domain, as it is crucial in minimizing developmental risks. Raw data cannot be used to perform any analysis as poor-quality data leads to erroneous decision-making. This makes data quality assessment a necessary function before data analysis. Data quality is a multi-dimensional factor that affects the analysis in multiple ways. Among all the dimensions, consistency is one of the most critical dimensions to assess. Context of data plays an important role in consistency assessment, as the records are inherently related within a dataset. Existing studies are computationally expensive and do not consider the context of data. In this paper, we propose a comprehensive context-aware data consistency assessment tool that uses machine learning to evaluate the consistency of data. Our model was developed on Apache Hadoop and Apache Spark to support big data, as well as to boost some computationally intensive algorithms.",2019,,10.1109/AICCSA47632.2019.9035250
200,"He, Tieke and Chen, Shenghao and Hao, Lian and Liu, Jia",Quality Driven Judicial Data Governance,Data integrity;Big Data;Decision making;Organizations;Standards organizations;data quality;judicial data governance;quality measurement,"With the development of Smart Court 3.0, the amount of judicial data that can be stored and processed by the computer is increasing rapidly. People gradually realize that judicial data contains tremendous social and business value. However, we need stronger ability to handle with and apply massive, multi-source and heterogeneous judicial data. A complete data governance system should be built in order to make full use of the value of data assets. In such a data governance system, data quality control is one of the key steps of data governance, and also the bottleneck of data service development, because data quality determines the upper limit of data application. This paper proposes a judicial data quality measurement framework by analyzing some judicial business data, followed by a data governance method driven by it.",2019,,10.1109/QRS-C.2019.00026
201,"Müller, Daniel and Te, Yiea-Funk and Jain, Pratiksha",Improving data quality through high precision gender categorization,Organizations;Patents;Databases;Systematics;Pragmatics;data quality improvement;hot deck imputation;record completion;gender name mapping;patenting,"First name to gender mappings have been widely recognized as a critical tool to complete, study and validate data records in a range of different areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 6 million people, provided by a car insurance. We then study how naming conventions have changed over time and how they differ by nationality. Second, we build a probabilistic first name to gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in a two label and three label setting and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies' outcomes and find that our mapping produces high precision results. We validate that the additional information of nationality and year of birth improve the recall scores of name to gender mappings. Therefore, it constitutes an efficient process to improve data quality of organizations' records, whenever the attribute gender is missing or unreliable.",2017,,10.1109/BigData.2017.8258223
202,"Li, Xiaoyong and Yuan, Jie and Ma, Huadong and Yao, Wenbin",Fast and Parallel Trust Computing Scheme Based on Big Data Analysis for Collaboration Cloud Service,Cloud computing;Collaboration;Monitoring;Security;Big Data;Quality of service;Computer architecture;Cloud computing;service behavior monitoring;trust computing;big data analysis,"Providing high trustworthy service is the most fundamental task for any cloud computing platform. Users are willing to deliver their computing tasks and the most sensitive data to cloud data centers, which is based on the trust relationship established between users and cloud service providers. However, with the development of collaboration cloud computing, how to provider fast response for a large number of users' service requests becomes a challenging problem. In order to quickly provide highly trustworthy services, the service platform must efficiently and quickly reply tens of millions of service requests, and automatically match-make tens of thousands of service resources. In this context, lightweight and fast (high-speed, low-overhead) trust computing schemes become the fundamental demand for implementing a trustworthy and collaborative cloud service. In this paper, we propose an innovative and parallel trust computing scheme based on big data analysis for the trustworthy cloud service environment. First, a distributed and modular perceiving architecture for large-scale virtual machines' service behavior is proposed relying on distributed monitoring agents. Then, an adaptive, lightweight, and parallel trust computing scheme is proposed for big monitored data. To the best of our knowledge, this paper is the first to use a blocked and parallel computing mechanism, the speed of trust calculation is greatly accelerated, which makes this trust computing scheme very suitable for a large-scale cloud computing environment. Performance analysis and experimental results verify feasibility and effectiveness of the proposed scheme.",2018,,10.1109/TIFS.2018.2806925
203,"Huang, Zhichuan and Xie, Tiantian and Zhu, Ting and Wang, Jianwu and Zhang, Qingquan",Application-driven sensing data reconstruction and selection based on correlation mining and dynamic feedback,,"As sensors spread across almost every industry, the Internet of Things (IoT) is going to trigger an era of big data. However, the abundance of available sensing data causes new challenges when building IoT applications. One main challenge is how to select proper data from large amount of sensing data for learning useful information efficiently. Existing approaches require developers to manage data for each specific application, which is very time consuming since the developers may not have enough knowledge about the dynamic changing data quality of different sensors. In this paper, we propose a data management middleware to learn the correlations between time series sensor data without prior knowledge. The learned correlation is then applied to select the useful sensor and reconstruct the incorrect data. To generalize the correlation models for each application, we utilize the dynamic feedback from the application to update the data selection and reconstruction. We evaluate our data management middleware in smart grids. The evaluation results show that our middleware can achieve better application performance with the help of dynamic feedback, data reconstruction and data selection.",2016,,10.1109/BigData.2016.7840737
204,"ur Rehman, Shafiq and Hark, Andre and Gruhn, Volker",A framework to handle big data for cyber-physical systems,Big Data;Cyber-physical systems;Safety;Real-time systems;Sensors;Big data;cyber-physical system (CPS);security;real-time;standardization;infrastructure;data quality,"The use of big data for cyber-physical systems (CPS) is gaining more importance due to the ever-increasing amount of collectable data. Due to the decreasing cost of sensors and the growth of embedded systems, which are increasingly used in the industries as well as in the private sectors, new methods are needed to evaluate and process the collected data. Therefore, in this paper we proposed a framework to handle big data for cyber-physical systems. The framework considered the possible solutions that would be standardization, cloud computing, online and data stream learning, a methodology to process data and multi-agent systems for CPS. Furthermore, we examine the security challenges and big data issues of cyber-physical systems.",2017,,10.1109/IEMCON.2017.8117153
205,"Ju, Xingang and Lian, Feiyu and Zhang, Yuan",Data Cleaning Optimization for Grain Big Data Processing using Task Merging,grain big data;data cleaning;task merging;hadoop;mapReduce,"Data quality has exerted important influence over the application of grain big data, so data cleaning is a necessary and important work. In MapReduce frame, we can use parallel technique to execute data cleaning in high scalability mode, but due to the lack of effective design there are amounts of computing redundancy in the process of data cleaning, which results in lower performance. In this research, we found some tasks often are carried out multiple times on same input files, or require same operation results in the process of data cleaning. For this problem, we proposed a new optimization technique that is based on task merge. By merging simple or redundancy computations on same input files, the number of the loop computation in MapReduce can be reduced greatly. The experiment shows, by this means, the overall system runtime is significantly reduced, which proves that the process of data cleaning is optimized. In this paper, we optimized several modules of data cleaning such as entity identification, inconsistent data restoration, and missing value filling. Experimental results show that the proposed method in this paper can increase efficiency for grain big data cleaning.",2019,,10.1109/ICISCE48695.2019.00053
206,"Hasan, Forat Falih and Bakar, Muhamad Shahbani Abu",Data Transformation from SQL to NoSQL MongoDB Based on R Programming Language,Structured Query Language;Computer languages;Data analysis;NoSQL databases;Data integrity;Scalability;Transforms;Big Data;Data Transformation;SABR Algorithm;NoSQL;ETL,"Owing to their high availability and scalability, NoSQL databases are becoming more popular for Big data applications in web analytics and supporting large websites. Moreover, each NoSQL system has its API which does not support industry standards like SQL and JDBC, integrating these systems with other enterprise and reporting software takes more time. The main requirements of Big data and data analytics are transforming the data from SQL databases to NoSQL data structures to represent the data. In this work, we presented a method to transform the data from different types of SQL databases to the desired NoSQL database based on the R programming language. The proposed work is based on the R environment used to handle the data from the source system to the target databases and meet the data quality requirements in data transformation. The results confirmed that the development provided a good solution for the data transformation from SQL to NoSQL by taking into account the data quality requirements.",2021,,10.1109/ISMSIT52890.2021.9604548
207,"Yu, Wenjin and Dillon, Tharam and Mostafa, Fahed and Rahayu, Wenny and Liu, Yuehua",Implementation of Industrial Cyber Physical System: Challenges and Solutions,Big Data;Manufacturing;Industries;Sensors;Data integrity;Real-time systems;Cloud computing;Cyber-Physical System;Internet of Things;Industry 4.0;cloud computing;big data ecosystem;data quality,"The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data & AI Innovation Awards.",2019,,10.1109/ICPHYS.2019.8780271
208,"Iyengar, Arun and Patel, Dhaval and Shrivastava, Shrey and Zhou, Nianjun and Bhamidipaty, Anuradha",Real-Time Data Quality Analysis,Data integrity;Measurement;Prediction algorithms;Anomaly detection;Machine learning algorithms;Task analysis;Interpolation;data quality;data analytics;real time data analytics,Data quality is critically important for big data and machine learning applications. Data quality systems can analyze data sets for quality and detection of potential errors. They can also provide remediation to fix problems encountered in analyzing data sets. This paper discusses key features that of data quality analysis systems. We also present new algorithms for efficiently maintaining updated data quality metrics on changing data sets. Our algorithms consider anomalies in data regions in determining how much different regions of data contribute to overall data metrics. We also make intelligent choices of which data metrics to update and how frequently to do so in order to limit the overhead for data quality metric updates.,2020,,10.1109/CogMI50398.2020.00022
209,"Lawson, Victor J. and Ramaswamy, Lakshmish",TAU-FIVE: A Multi-tiered Architecture for Data Quality and Energy-Sustainability in Sensor Networks,Measurement;Feeds;Computer architecture;Wireless sensor networks;Energy efficiency;Clouds;Data models;Data quality;cloud computing;energy model;applications to sensing;green networks,"Current research on wireless sensor networks ""WSNs"" in the Internet of Things ""IoT"" has focused on performance, scalability and energy efficiency. Innovations in these areas have many challenges due to the increasing volume of smart device data streams in the internet of Everything ""IoE"". Data feeds from future IoE systems such as the internet of vehicles, smart homes and smart-cities will need real time consolidation. This merger of technologies will require innovative big data algorithms and architectures that authenticate the data streams. A primary concern is in dynamically quantifying the data quality ""DQ"" of the streams while constructing real-time metrics to assess the energy efficiency ""EE"" of these IoE devices. In order to define the relationship between sensor stream DQ and EE, we propose our multi-tiered cloud-service architecture TAU-FIVE. The technical contributions of our framework includes data quality and energy efficiency models based on 7 DQ attributes and multiple reprogrammable smart sensors that dynamically modify and regulate the DQ and EE of a WSN. Our research maintains that WSN's can balance sustainability with quality of service by creating real-time metrics that merge energy usage with data stream integrity. This equilibrium will impact energy awareness in the IoT as the multitude of batch device data streams are integrated with the variety of social and professional networks and evolve into the IoE.",2016,,10.1109/DCOSS.2016.42
210,"Ding, Xiaoou and Wang, Hongzhi and Gao, Yitong and Li, Jianzhong and Gao, Hong",Efficient currency determination algorithms for dynamic data,Heuristic algorithms;Remuneration;Real-time systems;Databases;Big Data;data quality management; data currency; dynamic determining,"Data quality is an important aspect in data application and management, and currency is one of the major dimensions influencing its quality. In real applications, datasets timestamps are often incomplete and unavailable, or even absent. With the increasing requirements to update real-time data, existing methods can fail to adequately determine the currency of entities. In consideration of the velocity of big data, we propose a series of efficient algorithms for determining the currency of dynamic datasets, which we divide into two steps. In the preprocessing step, to better determine data currency and accelerate dataset updating, we propose the use of a topological graph of the processing order of the entity attributes. Then, we construct an Entity Query B-Tree (EQB-Tree) structure and an Entity Storage Dynamic Linked List (ES-DLL) to improve the querying and updating processes of both the data currency graph and currency scores. In the currency determination step, we propose definitions of the currency score and currency information for tuples referring to the same entity and use examples to discuss methods and algorithms for their computation. Based on our experimental results with both real and synthetic data, we verify that our methods can efficiently update data in the correct order of currency.",2017,,10.23919/TST.2017.7914196
211,"Srivastava, Divesh",Towards High-Quality Big Data: Lessons from FIT,,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Data are being generated, collected, and analyzed today at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. As the use of big data has grown, so too have concerns that poor-quality data, prevalent in large data sets, can have serious adverse consequences on data-driven decision making. Responsible data science thus requires a recognition of the importance of veracity, the fourth ""V"" of big data. In this talk, we first present a vision of high-quality big data and highlight the substantial challenges that the first three V’s, volume, velocity, and variety, bring to dealing with veracity in big data. We then present the FIT Family of adaptive, data-driven statistical tools that we have designed, developed, and deployed at AT&T for continuous data quality monitoring of a large and diverse collection of continuously evolving data. These tools monitor data movement to discover missing, partial, duplicated, and delayed data; identify changes in the content of spatiotemporal streams; and pinpoint anomaly hotspots based on persistence, pervasiveness, and priority. We conclude with lessons from FIT relevant to big data quality that are cause for optimism.",2020,,10.1109/BigData50022.2020.9378181
212,"Vieira, Vanessa and Pedrosa, Isabel and Soares, Bruno Horta",Big data amp; analytics: An approach using audit experts' interviews,Big Data;Interviews;Market research;Organizations;Surges;Software;Big Data;organizations;information;Audit;technology,"Big Data is one of the great trends in the short and medium term in organizations. There is a growing concern in provid solutions to address this trend, to methodical analysis of data and to do better decisions. The amount of data becomes less relevant when there is efficiency in Analytics. Internal auditors need to be in compliance with technology evolution. This research main objective is to understand how are internal auditors perceiving Big Data & Analytics' and which are the opportunities and difficulties pointed to address that challenge. To achieve this main goal, semi-structured interviews were conducted focused on internal auditors group. Those interviews intend to analyze and classify respondents' contributions in order to provide more insights for the present research. As a result, the main opportunities listed were greater information security and greater efficiency in data processing. Pointed obstacles were data quality, security and users' training.",2017,,10.23919/CISTI.2017.7976069
213,"Chen, Chengling and Su, Zhou and Li, Weiwei and Wang, Yuntao",Big Data Driven Computing Offloading Scheme with Driverless Vehicles Assistance,"Task analysis;Computational modeling;Automobiles;Big Data;Quality of service;Privacy;Bandwidth;Vehicular ad-hoc networks, reverse auction, computing offloading, driverless vehicles","In the era of big data, edge computing is emerged as a promising paradigm to alleviate the pressure on the backbone network and facilitate vehicular services on the road. As edge nodes deployed for vehicular applications, roadside units (RSUs) need to undertake a large number of local computing tasks. However, due to the uncertainty of the vehicular network topology, static RSU deployments are subject to short-term overload and cannot handle various delay-sensitive computing tasks concurrently. To address the problem, we propose a big data driven computing offloading scheme to dispatch idle driverless vehicles to enhance the capacities of RSUs dynamically. First, we present a trust assessment model to evaluate the credibility of driverless vehicles. Then, a multi-attribute reverse auction is applied to maximize the utilities of RSUs and driverless vehicles. In addition, a secure forwarding method is developed to protect the privacy of computing tasks.",2019,,10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00084
214,"Mecati, Mariachiara and Vetrò, Antonio and Torchiano, Marco",Detecting Discrimination Risk in Automated Decision-Making Systems with Balance Measures on Input Data,Training;Ethics;Decision making;Big Data;Software;Software reliability;Software measurement;Data quality;Data bias;Data ethics;Algorithm fairness;Automated decision-making,"Bias in the data used to train decision-making systems is a relevant socio-technical issue that emerged in recent years, and it still lacks a commonly accepted solution. Indeed, the ""bias in-bias out"" problem represents one of the most significant risks of discrimination, which encompasses technical fields, as well as ethical and social perspectives. We contribute to the current studies of the issue by proposing a data quality measurement approach combined with risk management, both defined in ISO/IEC standards. For this purpose, we investigate imbalance in a given dataset as a potential risk factor for detecting discrimination in the classification outcome: specifically, we aim to evaluate whether it is possible to identify the risk of bias in a classification output by measuring the level of (im)balance in the input data. We select four balance measures (the Gini, Shannon, Simpson, and Imbalance ratio indexes) and we test their capability to identify discriminatory classification outputs by applying such measures to protected attributes in the training set. The results of this analysis show that the proposed approach is suitable for the goal highlighted above: the balance measures properly detect unfairness of software output, even though the choice of the index has a relevant impact on the detection of discriminatory outcomes, therefore further work is required to test more in-depth the reliability of the balance measures as risk indicators. We believe that our approach for assessing the risk of discrimination should encourage to take more conscious and appropriate actions, as well as to prevent adverse effects caused by the ""bias in-bias out"" problem.",2021,,10.1109/BigData52589.2021.9671443
215,"Kalan, Reza Shokri and Ünalir, Murat Osman",Leveraging big data technology for small and medium-sized enterprises (SMEs),Organizations;Parallel processing;Computer architecture;Security;Privacy;big data;data analytic;data quality;SMEs;business intelligence;cloud computing,"Wisdom aligns with technology is the key factor for sustainable business development. By increasing amount of public and private data, organizations need to find new solutions to manage data and information which lead to knowledge, better decision making, and value. In the big data-bang, smart organization surfing on-line technology and start planning big data strategy. However, many organizations do not yet have a big data strategy. A challenge facing SMEs is that they may not have the same capacity as large companies to analysis new data sets. Also, traditional data processing tools are not capable for SMEs decision making because of volume, velocity and variety if data. For address this problem we need new leveraging technology, tools and talent. SMEs which have risen to leveraging the value of big data are using advantage of cloud computing and open-source software to realize various goals. The main goal of this investment is about value as a new concept in a big data era. In this study, we focus on emerging trends and future requirement: technology and tools for SMEs.",2016,,10.1109/ICCKE.2016.7802106
216,"Liu, Yunshu and Chen, Xuanyu and Chen, Cailian and Guan, Xingping",Traffic big data analysis supporting vehicular network access recommendation,Vehicles;Vehicular ad hoc networks;Roads;Big data;Quality of service;Internet;Real-time systems,"With the explosive growth of Internet of Vehicles (IoV), it is undoubted that vehicular demands for real-time Internet access would get a surge in the near future. Therefore, it is foreseeable that the cars within the IoV will generate enormous data. On the one hand, the huge volume of data mean we could get much information (e.g., vehicle's condition and real-time traffic distribution) through the big data analysis. On the other hand, the huge volume of data will overload the cellular network since the cellular infrastructure still represents the dominant access methods for ubiquitous connections. The vehicular ad hoc network (VANET) offloading is a promising solution to alleviate the conflict between the limited capacity of cellular network and big data collection. In a vehicular heterogeneous network formed by cellular network and VANET, an efficient network selection is crucial to ensure vehicles' quality of service. To address this issue, we develop an intelligent network recommendation system supported by traffic big data analysis. Firstly, the traffic model for network recommendation is built through big data analysis. Secondly, vehicles are recommended to access an appropriate network by employing the analytic framework which takes traffic status, user preferences, service applications and network conditions into account. Furthermore an Android application is developed, which enables individual vehicle to access network automatically based on the access recommender. Finally, extensive simulation results show that our proposal can effectively select the optimum network for vehicles, and network resource is fully utilized at the same time.",2016,,10.1109/ICC.2016.7510775
217,"Qi, Wenting and Chelmis, Charalampos",Improving Algorithmic Decision–Making in the Presence of Untrustworthy Training Data,Data integrity;Conferences;Supervised learning;Training data;Machine learning;Big Data;Data models;counterfactual explanations;data quality;data science;supervised learning,"Although data quality is of paramount importance in algorithmic decision–making, most existing methods for supervised classification use training data without ever questioning their fidelity. At the same time, counterfactual explanation approaches widely used for post–hoc explanation of algorithmic decisions may result in unrealistic recommendations when left unconstrained. This work highlights a significant research problem, and introduces a novel framework to improve supervised classification in the presence of untrustworthy data, while offering actionable suggestions when an undesirable decision has been made (e.g., loan application rejection). Evaluation results spanning datasets from different domains demonstrate the superiority of the proposed approach, and its comparative advantage as the percentage of mislabeled instances increases.",2021,,10.1109/BigData52589.2021.9671677
218,"Papageorgiou, Apostolos and Zahn, Manuel and Kovacs, Ernö",Auto-configuration System and Algorithms for Big Data-Enabled Internet-of-Things Platforms,Logic gates;Big data;Measurement;Heuristic algorithms;Complexity theory;Standards;Optimization;M2M;IoT;configuration;gateway;autonomic;self-management,"Internet of Things (IoT) platforms that handle Big Data might perform poorly or not according to the goals of their operator (in terms of costs, database utilization, data quality, energy-efficiency, throughput) if they are not configured properly. The latter configuration refers mainly to system parameters of the data-collecting gateways, e.g., polling intervals, capture intervals, encryption schemes, used protocols etc. However, re-configuring the platform appropriately upon changes of the system context or the operator targets is currently not taking place. This happens because of the complexity or unawareness of the synergies between system configurations and various aspects of the Big Data-handling IoT platform, but also because of the human resources that an efficient re-configuration would require. This paper presents an auto-configuration solution based on interpretable configuration suggestions, focusing on the algorithms for computing the mentioned suggested configurations. Five such algorithms are contributed, while a thorough evaluation reveals which of these algorithms should be used in different operation scenarios in order to achieve high fulfillment of the operator's targets.",2014,,10.1109/BigData.Congress.2014.78
219,"Xu, Xiaolong and Liu, Xinxin and Liu, Xiaoxiao and Sun, Yanfei",Truth finder algorithm based on entity attributes for data conflict solution,Algorithm design and analysis;Reliability;Internet;Telecommunications;Big Data;Data models;truth finder;data reliability;entity attribute;data conflict,"The Internet now is a large-scale platform with big data. Finding truth from a huge dataset has attracted extensive attention, which can maintain the quality of data collected by users and provide users with accurate and efficient data. However, current truth finder algorithms are unsatisfying, because of their low accuracy and complication. This paper proposes a truth finder algorithm based on entity attributes (TFAEA). Based on the iterative computation of source reliability and fact accuracy, TFAEA considers the interactive degree among facts and the degree of dependence among sources, to simplify the typical truth finder algorithms. In order to improve the accuracy of them, TFAEA combines the one-way text similarity and the factual conflict to calculate the mutual support degree among facts. Furthermore, TFAEA utilizes the symmetric saturation of data sources to calculate the degree of dependence among sources. The experimental results show that TFAEA is not only more stable, but also more accurate than the typical truth finder algorithms.",2017,,10.21629/JSEE.2017.03.21
220,"Zhang, Xupeng and Liang, Du",Construction of Elevator Inspection Quality Evaluation System Based on Big Data,Elevators;Inspection;Big Data;Safety;Market research;Testing;Monitoring;Elevator;inspection;big data;quality evaluation,"Elevator inspection information has typical big data characteristics. This paper points out that the elevator inspection data introduces the method of elevator inspection big data analysis. Taking elevator inspection as an example, it lists several kinds of big data analysis methods for inspection data, including the risk points describing the basic information of the elevator, the scanning inspection process and the inspection quality. Based on frequency analysis of active factors, outlier test, quality assessment, correlation analysis. Using big data technology, it can make statistical analysis on the data obtained by elevator inspection, make the inspection situation more intuitive, help the management organization to understand the overall elevator quality and elevator inspection, and build an elevator inspection quality evaluation system to make the work more transparent and management more precise. Find more accurate questions, deeper supervision, and more scientific government decisions.",2019,,10.1109/ICEIEC.2019.8784484
221,Liwei Zheng,SNSQ ontology: A domain ontology for SNSs data quality,Ontologies;Social network services;Artificial neural networks;Synchronization;Maintenance engineering;ontology;social network;data quality assessment,"the advent of online social networks has been one of the most exciting events in this decade. Many popular online social networks such as Twitter, Wechat, Weibo, LinkedIn, and Facebook have become increasingly popular. The consequences of the poor quality of data in a social network are often experienced in everyday life. This paper gives a domain ontology model, SNSQ Ontology, for data quality in the area of social networks. It could be a knowledge base for the quality assessment of the rich and linkage data in the social network. High-quality data would be relevant in the data searching, analyzing and mining. Based on the SNSQ Ontology the strategy for data quality assessment and repair is given. And the co-influence among the four quality dimensions, completeness, consistency, currency, and accuracy, are discussed to guarantee an effective assessment process.",2017,,10.1109/ICCCBDA.2017.7951876
222,"Cherubini, Giovanni and Jelitto, Jens and Venkatesan, Vinodh",Cognitive Storage for Big Data,Energy efficiency;Performance evaluation;Storage area networks;Big data;Quality of service;Context modeling;Data storage aystems;mass storage;big data;storage management;value of information;repositories;autonomous systems;content analysis;indexing,"Storage system efficiency can be significantly improved by determining the value of data. A key concept is cognitive storage, or optimizing storage systems by better comprehending the relevance of data to user needs and preferences. The Web extra at https://youtu.be/P-ZxlTLwzTI is a video of authors Giovanni Cherubini and Vinodh Venkatesan of IBM Research--Zurich discussing the concepts, applications, and benefits of cognitive storage for big data.",2016,,10.1109/MC.2016.117
223,"Xing, Xiaobo",Financial Big Data Reconciliation Method,Costs;Data integrity;Education;Distributed databases;Big Data;Maintenance engineering;Real-time systems;financial big data;full data reconciliation;incremental data reconciliation;quasi real time data reconciliation,"For data errors in distributed financial system caused by multi-system interaction, asynchronous processing and system bug, this paper proposes offline and quasi real-time data reconciliation methods based on the combination of Alibaba big data processing platform and accounting theory. In offline data reconciliation, Full data reconciliation and hour level incremental data reconciliation are introduced. And in quasi real-time data reconciliation, single system and distributed multi-system reconciliation models are introduced. These data reconciliation methods are then verified against 7 million pieces of daily data of the distributed loan system in a financial company. Results show that these methods can complete the financial big data processing, discover the data quality problems timely, and minimize the financial system capital loss.",2021,,10.1109/ISAIEE55071.2021.00071
224,"Huang, Haiyan and Wei, Bizhong and Dai, Jian and Ke, Wenlong",Data Preprocessing Method For The Analysis Of Incomplete Data On Students In Poverty,Filtering;Databases;Data integrity;Redundancy;Data preprocessing;Feature extraction;Stability analysis;data mining;data preprocessing;feature selection,"Data mining is the focus of big data applications in various fields. Data pre-processing is a crucial step in the data mining process. With the development of the information society and the application of databases, the educational data has seen explosive growth, and the data on poor students has become informative. However, the actual student financial aid management system collects the data on poor students which generally has problems such as missing values, attributes redundancy, and noise. To solve this problem, we proposed a novel method called DPBP to preprocess data. The proposed DPBP approach consists of four stages: the preparation of data, the scoping of characteristics, the combination of characteristics, and the filtering of missing number. Firstly, we prepare the dataset by extracting data. Next, the characteristic range is limited by choosing experimental results of feature selection algorithm. Then, third stage performs feature combination to obtain the feature decomposition sets. Finally, based on accuracy and missing number, we gain the optimal dataset. Series of experiments result show that our proposed method significantly improves the data quality and stability.",2020,,10.1109/CIS52066.2020.00060
225,"Manogaran, Gunasekaran and Nguyen, Tu N.",Displacement-Aware Service Endowment Scheme for Improving Intelligent Transportation Systems Data Exchange,Big Data;Quality of service;Delays;Data models;Vehicular ad hoc networks;Vehicle-to-everything;Optimization;ITS;mobility prediction;time synchronized data exchanges;data offloading;V2X data exchange.,"Intelligent Transportation Systems (ITS) is a smart-transportation system for road-side assistance and data exchange support by integrating cloud and wireless networks. ITS facilitates vehicle-to-vehicle and vehicle-to-anything (V2X) data exchanges for satisfying user demands. The rate of big data granting to the vehicular users is interrupted by the fundamental attributes such as mobility and link instability of the vehicles. To address the issues in vehicular data exchange big data, this article introduces displacement-aware service endowment scheme with the benefits of data offloading. Displacement-aware big data endowment ensures responsive availability of vehicle request information despite unfavorable location and density factors. The time congruency in V2V and V2X data exchanges are adopted for minimizing data exchange dropouts. In the data offloading phase, extraneous information and big data responses are detained based on data exchange relevance to improve congestion free big data endowment. The distinct methods work in a co-operative manner to improve big data quality of fast configuring smart vehicles to provide reliable big data in smart city environments.",2021,,10.1109/TITS.2021.3078753
226,"Meng, Qianyu and Wang, Kun and Liu, Bo and Miyazaki, Toshiaki and He, Xiaoming",QoE-Based Big Data Analysis with Deep Learning in Pervasive Edge Environment,Big Data;Quality of experience;Training;Tensile stress;Machine learning;Data models;Quality of service,"In the age of big data, the services in pervasive edge environment are expected to offer end-users better Quality of Experience (QoE) than that in a normal edge environment. Nevertheless, various types of edge devices with storage, delivery, and sensing are coming into our environment and produce the high-dimensional big data accompanied by a volume of pervasive big data increasingly with a lot of redundancy. Therefore, the satisfaction of QoE becomes the primary challenge in high dimensional big data on the basis of pervasive edge environment. In this paper, we first propose a QoE model to evaluate the quality of service in pervasive edge environment. The value of QoE does not only include the accurate data, but also the transmission rate. Then, on the basis of the accuracy, we propose a Tensor-Fast Convolutional Neural Network (TF-CNN) algorithm based on Deep Learning, which is suitable for pervasive edge environment with high-dimensional big data analysis. Simulation results reveal that our proposals could achieve high QoE performance.",2018,,10.1109/ICC.2018.8422106
227,"Ding, Xiaoou and Wang, Hongzhi and Su, Jiaxuan and Wang, Muxian and Li, Jianzhong and Gao, Hong",Leveraging Currency for Repairing Inconsistent and Incomplete Data,Currencies;Maintenance engineering;Cleaning;Urban areas;Remuneration;Databases;Companies;Data cleaning;data quality management;currency determining;temporal data repairing,"Data quality plays a key role in big data management today. With the explosive growth of data from a variety of sources, the quality of data is faced with multiple problems. Motivated by this, we study the multiple data cleaning on incompleteness and inconsistency with currency reasoning and determination in this paper. We introduce a 4-step framework, named ${\sf Imp3C}$Imp3C, for errors detection and quality improvement in incomplete and inconsistent data without timestamps. We achieve an integrated currency determining method to compute the currency orders among tuples, according to currency constraints. Thus, the inconsistent data and missing values are repaired effectively considering the temporal impact. For both effectiveness and efficiency consideration, we carry out inconsistency repair ahead of incompleteness repair. A currency-related consistency distance metric is defined to measure the similarity between dirty tuples and clean ones more accurately. In addition, currency orders are treated as an important feature in the missing imputation training process. The solution algorithms are introduced in detail with case studies. A thorough experiment on three real-life datasets verifies our method ${\sf Imp3C}$Imp3C improves the performance of data repairing with multiple quality problems. ${\sf Imp3C}$Imp3C outperforms the existing advanced methods, especially in the datasets with complex currency orders.",2022,,10.1109/TKDE.2020.2992456
228,"Lawson, Victor and Ramaswamy, Lakshmish",Data Quality and Energy Management Tradeoffs in Sensor Service Clouds,Feeds;Big data;Computer architecture;Clouds;Wireless sensor networks;Software;Conferences;data quality;cloud service;energy management;sensor network,"Cloud-based sensor data collection services are becoming an essential part of the Internet of Things (IoT). As the consumer demand grows for these services, the data quality (DQ) of the stream becomes an increasingly vital issue. Of particular interest is the inherent tradeoff between the DQ and the energy consumption of the sensor. Unfortunately, there has been very little research on the management of this tradeoff that allows data consumers to receive high quality data while simultaneously conserving energy. Our work seeks to explore this tradeoff in detail by combining DQ services for the data stream consumer with customizable energy efficient ""EE"" throttling algorithms for the data feed producers. These energy management services provide cost reduction rewards for consumers who would otherwise make poor DQ/EE decisions. Our primary contributions include cloud-based services for monitoring the tradeoff, an architecture that adjusts to DQ needs and a producer/consumer data stream best matching cloud service. We envision that our services architecture will reward energy efficiency decisions and profoundly affect consumer choices.",2015,,10.1109/BigDataCongress.2015.124
229,"Tang, Nan",Big RDF data cleaning,Resource description framework;Cleaning;Ontologies;Data mining;Knowledge based systems;Conferences;Databases,"Without a shadow of a doubt, data cleaning has played an important part in the history of data management and data analytics. Possessing high quality data has been proven to be crucial for businesses to do data driven decision making, especially within the information age and the era of big data. Resource Description Framework (RDF) is a standard model for data interchange on the semantic web. However, it is known that RDF data is dirty, since many of them are automatically extracted from the web. In this paper, we will first revisit data quality problems appeared in RDF data. Although many efforts have been put to clean RDF data, unfortunately, most of them are based on laborious manual evaluation. We will also describe possible solutions that shed lights on (semi-)automatically cleaning (big) RDF data.",2015,,10.1109/ICDEW.2015.7129549
230,"Chouhan, Ashish and Prabhune, Ajinkya and Prabhuraj, Paneesh and Chaudhari, Hitesh",DWreck: A Data Wrecker Framework for Generating Unclean Datasets,Generators;Data integrity;XML;Cleaning;Tools;Databases;Pipelines;data generators;data quality dimensions;data cleaning;microservice architectures;data management,"In this paper, we present DWreck, a data wrecker framework for generating unclean datasets by counterproductively applying different data quality dimensions. In a typical data-analysis pipeline, data cleaning is the most cost-intensive, laborious, and time-consuming step. Unclean dataset or partially cleaned dataset can lead to incorrect training of machine learning models and result in wrong conclusions. Generally, data-scientists examine null, missing, or duplicate values, and the dataset is cleaned by removing the entire record or imputing the values. However, deleting the records, or imputing the values cannot be termed as comprehensive cleaning, as these cleaning techniques may result in a reduction in the population of data, and increased error in estimation due to biased values. For systematically cleaning an unclean dataset, it is necessary to comply with the data quality dimensions such as completeness, validity, consistency, accuracy, and conformity. The errors described as violations of expectations for completeness, accuracy, timeliness, consistency and other dimensions of data quality often impede the successful completion of information processing streams and consequently degrade the dependent business processes. Therefore, educating a data-scientist for comprehensively cleaning a raw-dataset acquired for analysis is an incremental learning process. Moreover, for extensive training on cleaning a dataset on different quality dimensions, it is necessary to provide a variety of datasets that are unclean on various data quality dimensions. Hence, in this paper, we present DWreck, a data wrecker framework for generating unclean datasets by counterproductively applying different data quality dimensions. The DWreck framework is designed on the principles of microservices architecture pattern. For allowing function-specific extensibility, the DWreck comprises four groups of microservices: (a) Dataset Profiling, (b) Data type Processing, (c) Counterproductive Dimensions, and (d) Miscellaneous. The orchestrator coordinates the different microservices in a complex workflow that is further split into three sub-workflows to generate an unclean (wrecked) dataset as an output. Finally, we evaluate the DWreck framework on twenty seed-datasets to generate corresponding wrecked datasets.",2020,,10.1109/BigDataService49289.2020.00020
231,"Xia, Hong and Zhang, YongKang and Wang, Han and Chen, YanPing and Wang, ZhongMin",Crowdsourcing Answer Integration Algorithm For Big Data Environment,Crowdsourcing;Computational modeling;Scalability;Big Data;Quality assessment;Time factors;Task analysis;Crowdsourcing;quality assessment;answer integration;MapReduce,"Crowdsourcing is an emerging distributed computing model that is widely used. Aiming at the uneven quality of crowdsourcing answers due to different workers' capabilities and attitudes, it is necessary to effectively study the hotspot issue of crowdsourcing answer integration. A crowdsourced answer integration algorithm based on “filter-evaluate-vote” is proposed. This algorithm is implemented using MapReduce parallel programming model in the Hadoop platform, and experiments are performed on multiple data sets. The results show that the proposed algorithm can be effective. It improves the accuracy of crowdsourced answers, and has high computing performance and horizontal scalability, which is suitable for answer integration in a big data environment.",2020,,10.1109/NaNA51271.2020.00064
232,"Guo, Weisi",Partially Explainable Big Data Driven Deep Reinforcement Learning for Green 5G UAV,5G mobile communication;Batteries;Wireless communication;Big Data;Machine learning;Optimization;big data;machine learning;deep reinforcement learning;radio resource management;UAV;energy efficiency;XAI,"UAV enabled terrestrial wireless networks enables targeted user-centric service provisioning to en-richen both deep urban coverage and target various rural challenge areas. However, UAVs have to balance the energy consumption of flight with the benefits of wireless capacity delivery via a high dimensional optimisation problem. Classic reinforcement learning (RL) cannot meet this challenge and here, we propose to use deep reinforcement learning (DRL) to optimise both aggregate and minimum service provisioning. In order to achieve a trusted autonomy, the DRL agents have to be able to explain its actions for transparent human-machine interrogation. We design a Double Dueling Deep Q-learning Neural Network (DDDQN) with Prioritised Experience Replay (PER) and fixed Q-targets to achieve stable performance and avoid over-fitting, offering performance gains over naive DQN algorithms. We then use a big data driven case study and found that UAVs battery size determines the nature of its autonomous mission, ranging from an efficient exploiter of one hotspot (100% reward gain) to a stochastic explorer of many hotspots (60-150% reward gain). Using a variety of telecom and social media data, we infer driving Quality-of-Experience (QoE) and Quality-of-Service (QoS) metrics that are in contention with UAV power and communication constraints. Our greener UAVs (30-40% energy saved) address both quantitative QoS and qualitative QoE issues. Partial interpretability in the reinforcement learning is achieved using data features extracted in the hidden layers, offering an initial step for explainable AI (XAI) connecting machine intelligence with human expertise.",2020,,10.1109/ICC40277.2020.9149151
233,"Yan, Peipei and Li, Feng and Xiang, Zhiwei and Li, Mingxuan and Fan, Shuming",Research and application of power data management key technology,Visualization;Data security;Process control;Collaboration;Big Data;Power grids;Resource management;Electric power data;Data management;Data sharing;Data security,"With the intensified application of power information systems and the advent of the “big data” era, higher requirements are put forward for power data resource management and power data security. Electric power companies have carried out research on key technologies for data management, established a three-level management system at the provincial, prefectural and county levels, built a panoramic view of data resources, a data operation management platform, a data negative list sharing mechanism, and a data security protection mechanism, which were applied to all aspects of data management and data governance. Through the support of business processes, data standards, data quality, etc., it has effectively improved the management efficiency of power data, improved the company's data management level, promoted business collaboration and efficiency.",2021,,10.1109/ICIBA52610.2021.9687865
234,"Byabazaire, John and O'Hare, Gregory and Delaney, Declan",Data Quality and Trust : A Perception from Shared Data in IoT,Data integrity;Data models;Big Data;Biological system modeling;Ecosystems;Standards,"Internet of Things devices and data sources areseeing increased use in various application areas. The pro-liferation of cheaper sensor hardware has allowed for widerscale data collection deployments. With increased numbers ofdeployed sensors and the use of heterogeneous sensor typesthere is increased scope for collecting erroneous, inaccurate orinconsistent data. This in turn may lead to inaccurate modelsbuilt from this data. It is important to evaluate this data asit is collected to determine its validity. This paper presents ananalysis of data quality as it is represented in Internet of Things(IoT) systems and some of the limitations of this representation. The paper discusses the use of trust as a heuristic to drive dataquality measurements. Trust is a well-established metric that hasbeen used to determine the validity of a piece or source of datain crowd sourced or other unreliable data collection techniques. The analysis extends to detail an appropriate framework forrepresenting data quality effectively within the big data modeland why a trust backed framework is important especially inheterogeneously sourced IoT data streams.",2020,,10.1109/ICCWorkshops49005.2020.9145071
235,"Mavrogiorgou, Argyro and Kleftakis, Spyridon and Mavrogiorgos, Konstantinos and Zafeiropoulos, Nikolaos and Menychtas, Andreas and Kiourtis, Athanasios and Maglogiannis, Ilias and Kyriazis, Dimosthenis",beHEALTHIER: A Microservices Platform for Analyzing and Exploiting Healthcare Data,Machine learning algorithms;Data analysis;Software architecture;Medical services;Computer architecture;Big Data;Service-oriented architecture;Healthcare;Electronic Health Records;Data Collection;Data Analysis;Health Policies;Microservices,"The era of big data is surrounded by plenty of challenges, concerning aspects related to data quality, data management, and data analysis. Plenty of these challenges are met in several domains, such as the healthcare domain, where the corresponding healthcare platforms not only have to deal with managing and/or analyzing a tremendous quantity of health data, but also have to accomplish these actions in the most efficient and secure way possible. Towards this direction, medical institutions are paying attention to the replacement of traditional approaches such as the Monolithic and Service Oriented Architecture (SOA), which deal with many difficulties for handling the increasing amount of healthcare data. This paper presents a platform for overcoming these issues, by adopting the Microservice Architecture (MSA), being able to efficiently manage and analyze these vast amounts of data. More specifically, the proposed platform, namely beHEALTHIER, offers the ability to construct health policies out of data of collective knowledge, by utilizing a newly proposed kind of electronic health records (i.e., eXtended Health Records (XHRs)) and their corresponding networks, through the efficient analysis and management of ingested healthcare data. In order to achieve that, beHEALTHIER is architected based upon four (4) discrete and interacting pillars, namely the Data, the Information, the Knowledge and the Actions pillars. Since the proposed platform is based on MSA, it fully utilizes MSA's benefits, achieving fast response times and efficient mechanisms for healthcare data collection, processing, and analysis.",2021,,10.1109/CBMS52027.2021.00078
236,"Tekieh, Mohammad Hossein and Raahemi, Bijan",Importance of data mining in healthcare: A survey,Data mining;Diseases;Insurance;Data analysis;Organizations;Sociology;data mining;health data analysis;data quality;predictive modelling;health big data;data mining applications,"In this survey, we collect the related information that demonstrate the importance of data mining in healthcare. As the amount of collected health data is increasing significantly every day, it is believed that a strong analysis tool that is capable of handling and analyzing large health data is essential. Analyzing the health datasets gathered by electronic health record (EHR) systems, insurance claims, health surveys, and other sources, using data mining techniques is very complex and is faced with very specific challenges, including data quality and privacy issues. However, the applications of data mining in healthcare, advantages of data mining techniques over traditional methods, special characteristics of health data, and new health condition mysteries have made data mining very necessary for health data analysis.",2015,,10.1145/2808797.2809367
237,"Clarke, Roger",Quality Assurance for Security Applications of Big Data,Big data;Q-factor;Sociology;Statistics;Security;Reliability;Quality assurance;risk assessment;risk management;information quality;data semantics;data scrubbing;decision quality;transparency,"The quality of inferences drawn from data, big or small, is heavily dependent on the quality of the data and the quality of the processes applied to it. Big data analytics is emerging from laboratories and being applied to intelligence and security needs. To achieve confidence in the outcomes of these applications, a quality assurance framework is needed. This paper outlines the challenges, and draws attention to the consequences of misconceived and misapplied projects. It presents key aspects of the necessary risk assessment and risk management approaches, and suggests opportunities for research.",2016,,10.1109/EISIC.2016.010
238,"Gu, Liqiu and Wang, Kun and Liu, Xiulong and Guo, Song and Liu, Bo",A reliable task assignment strategy for spatial crowdsourcing in big data environment,Crowdsourcing;Big Data;Reliability theory;Sensors;Computational modeling;Measurement;Big data;crowdsourcing;task assignment,"With the ubiquitous deployment of the mobile devices with increasingly better communication and computation capabilities, an emerging model called spatial crowdsourcing is proposed to solve the problem of unstructured big data by publishing location-based tasks to participating workers. However, massive spatial data generated by spatial crowdsourcing entails a critical challenge that the system has to guarantee quality control of crowdsourcing. This paper first studies a practical problem of task assignment, namely reliability aware spatial crowdsourcing (RA-SC), which takes the constrained tasks and numerous dynamic workers into consideration. Specifically, the worker confidence is introduced to reflect the completion reliability of the assigned task. Our RA-SC problem is to perform task assignments such that the reliability under budget constraints is maximized. Then, we reveal the typical property of the proposed problem, and design an effective strategy to achieve a high reliability of the task assignment. Besides the theoretical analysis, extensive experimental results also demonstrate that the proposed strategy is stable and effective for spatial crowdsourcing.",2017,,10.1109/ICC.2017.7996546
239,Yuan Gao and Hong Ao and Kang Wang and Weigui Zhou and Yi Li,The diagnosis of wired network malfunctions based on big data and traffic prediction: An overview,Big data;Quality of service;Market research;Monitoring;Satellites;Delays;Prediction methods;Network Diagnosis;Big Data;Traffic Prediction;Large Scale Network;Complex Network,"The increasing demand on higher transmission speed and shorter delay in wired networks becomes critical in recent communication networks. However, the capacity of transmission link is limited by the method of transmission. In this paper, aiming at the situation of large scale networks, an overview of the network optimization based on big data and traffic prediction is given in our proposed work. In wired networks, how to make full use of the transmission bandwidth and provide more reliable QoS is in great demand. Based on the network topology in our facility, we make a summary of current diagnosis method of the network and then propose the future possible way to solve the network malfunction based on big data through network log and complex monitors, then we make an overview of the diagnosis method based on traffic prediction, which could effectively make full use of bandwidth and avoid collision of the network.",2015,,10.1109/ICCSNT.2015.7490949
240,"Moon, Aekyeung and Woo Son, Seung and Jung, Jiuk and Jeong Song, Yun",Understanding Bit-Error Trade-off of Transform-based Lossy Compression on Electrocardiogram Signals,Performance evaluation;Measurement uncertainty;Transforms;Medical services;Electrocardiography;Big Data;Monitoring;Transform coding;Lossy compression;IoT;Health care;R-peak;data fidelity,"The growing demand for recording longer ECG signals to improve the effectiveness of IoT-enabled remote clinical healthcare is contributing large amounts of ECG data. While lossy compression techniques have shown potential in significantly lowering the amount of data, investigation on how to trade-off between data reduction and data fidelity on ECG data received relatively less attention. This paper gives insight into the power of lossy compression to ECG signals by balancing between data quality and compression ratio. We evaluate the performance of transformed-based lossy compressions on the ECG datasets collected from the Biosemi ActiveTwo devices. Our experimental results indicate that ECG data exhibit high energy compaction property through transformations like DCT and DWT, thus could improve compression ratios significantly without hurting data fidelity much. More importantly, we evaluate the effect of lossy compression on ECG signals by validating the R-peak in the QRS complex. Our method can obtain low error rates measured in PRD (as low as 0.3) and PSNR (up to 67) using only 5% of the transform coefficients. Therefore, R-peaks in the reconstructed ECG signals are almost identical to ones in the original signals, thus facilitating extended ECG monitoring.",2020,,10.1109/BigData50022.2020.9378343
241,"Zhao, Cong and Yang, Shusen and McCann, Julie A.",On the Data Quality in Privacy-Preserving Mobile Crowdsensing Systems with Untruthful Reporting,Sensors;Data integrity;Data privacy;Task analysis;Mobile handsets;Roads;Monitoring;Mobile crowdsensing systems;privacy preservation;data quality;untruthful reporting,"The proliferation of mobile smart devices with ever improving sensing capacities means that human-centric Mobile Crowdsensing Systems (MCSs) can economically provide a large scale and flexible sensing solution. The use of personal mobile devices is a sensitive issue, therefore it is mandatory for practical MCSs to preserve private information (the user's true identity, precise location, etc.) while collecting the required sensing data. However, well intentioned privacy protection techniques also conceal autonomous, or even malicious, behaviors of device owners (termed as self-interested), where the objectivity and accuracy of crowdsensing data can therefore be severely threatened. The issue of data quality due to untruthful reporting in privacy-preserving MCSs has been yet to produce solutions. Bringing together game theory, algorithmic mechanism design, and truth discovery, we develop a mechanism to guarantee and enhance the quality of crowdsensing data without jeopardizing the privacy of MCS participants. Together with solid theoretical justifications, we evaluate the performance of our proposal with extensive real-world MCS trace-driven simulations. Experimental results demonstrate the effectiveness of our mechanism on both enhancing the quality of the crowdsensing data and eliminating the motivation of MCS participants, even when their privacy is well protected, to report untruthfully.",2021,,10.1109/TMC.2019.2943468
242,"Neves, Ricardo A. and Cruvinel, Paulo E.",Ontology for Structuring a Digital Databases for Decision Making in Grain Production,Cloud computing;Databases;Semantics;Decision making;Production;Ontologies;Big Data;Ontology;Agriculture;Digital Database;Cloud Computing;Big Data;Decision Making,"This paper presents an ontology for the structuring of digital databases with the objective of acting in a cloud environment and meeting big data sources in the agricultural context of grain production. Its conception is structured in three stages: the first stage presents an ontological architecture aimed at public and private cloud environments, the second stage deals with a semantic model at process level, and a pseudocode for ontological application is elaborated in the third stage, considering the technologies applied to the cloud. This work combines advanced features to support decision making from Data Lake storage solutions, semantic treatment of big data, as well as the presentation of strategies based on machine learning and data quality analysis to obtain data and metadata organized for application in a decision model. The configuration of the ontology presented meets the diversity of big data projects in the grain production context, the characteristics of which are based on interoperability in the use of heterogeneous data and its integration, elasticity of computational resources, and high availability of cloud access.",2021,,10.1109/ICSC50631.2021.00071
243,"Liu, Hong and Sang, Zhenhua and Karali, Sameer",Approximate Quality Assessment with Sampling Approaches,"Data integrity;Quality assessment;Writing;Gaussian distribution;Big Data;Sampling methods;Time-frequency analysis;Data Quality, Quality Assessment, Sampling","Data is useful to the extent that it can be quickly analyzed to reveal valuable information. With high-quality data, we can increase revenue, reduce cost, and reduce risk. On the other hand, the consequences of poor-quality data can be severe. It has been estimated that poor quality customer data costs U.S. businesses $611 billion annually in postage, printing, and staff overhead. These issues make data quality assessment a necessary and critical step in any data-related systems. Big data brings new challenges to data quality assessment due to the scale of data, streaming data, and different forms of data. Therefore, we proposed a sampling-based approximate quality assessment model on large data. Sampling large datasets can make all quality assessment processes cheaper and more feasible because of data reduction. The protocol of this work: First, the sample size is determined for estimating a large dataset. Next, sampling techniques are applied to collect samples. Then, these samples are used to estimate the quality of the large dataset. The objective of quality assessment in this work is to evaluate the completeness, accuracy, and timeliness of data and to return fast and approximate scores. Using different sample sizes and different sampling methods, we obtained 72 sets of data and compared them. These results show that the proposed approach is efficient and provides some insight into the quality assessment with samples.",2019,,10.1109/CSCI49370.2019.00244
244,"Dong, Yongquan and Dragut, Eduard C. and Meng, Weiyi",Normalization of Duplicate Records from Multiple Sources,Data integration;Standards;Task analysis;Databases;Google;Data mining;Terminology;Record normalization;data quality;data fusion;web data integration;deep web,"Data consolidation is a challenging issue in data integration. The usefulness of data increases when it is linked and fused with other data from numerous (Web) sources. The promise of Big Data hinges upon addressing several big data integration challenges, such as record linkage at scale, real-time data fusion, and integrating Deep Web. Although much work has been conducted on these problems, there is limited work on creating a uniform, standard record from a group of records corresponding to the same real-world entity. We refer to this task as record normalization. Such a record representation, coined normalized record, is important for both front-end and back-end applications. In this paper, we formalize the record normalization problem, present in-depth analysis of normalization granularity levels (e.g., record, field, and value-component) and of normalization forms (e.g., typical versus complete). We propose a comprehensive framework for computing the normalized record. The proposed framework includes a suit of record normalization methods, from naive ones, which use only the information gathered from records themselves, to complex strategies, which globally mine a group of duplicate records before selecting a value for an attribute of a normalized record. We conducted extensive empirical studies with all the proposed methods. We indicate the weaknesses and strengths of each of them and recommend the ones to be used in practice.",2019,,10.1109/TKDE.2018.2844176
245,"Liu, Jie and Cao, Yijia and Li, Yong and Guo, Yixiu and Deng, Wei",A big data cleaning method based on improved CLOF and Random Forest for distribution network,Cleaning;Distribution networks;Big Data;Prediction algorithms;Clustering algorithms;Data models;Anomaly detection;Data cleaning;Outliers detection;missing data imputation;LOF;DBSCAN;Random Forest,"In order to improve the data quality, the big data cleaning method of distribution network was studied in this paper. First, the Local Outlier Factor (LOF) algorithm based on DBSCAN clustering was used to detect outliers. However, due to the difficulty in determining the LOF threshold, a method of dynamically calculating the threshold based on the transformer districts and time was proposed. Besides, the LOF algorithm combines the statistical distribution method to reduce the ""misjudgment rate"". Aiming at the diversity and complexity of data missing forms in power big data, this paper improved the Random Forest imputation algorithm, which can be applied to various forms of missing data, especially the blocked missing data and even some horizontal or vertical data completely missing. The data in this paper were from real data of 44 transformer districts of a certain 10kV line in distribution network. Experimental results showed that outlier detection was accurate and suitable for any shape and multidimensional power big data. The improved Random Forest imputation algorithm was suitable for all missing forms, with higher imputation accuracy and better model stability. By comparing the network loss prediction between the data using this data cleaning method and the data removing outliers and missing values, it was found that the accuracy of network loss prediction had been improved by nearly 4 percentage points using the data cleaning method mentioned in this paper. Additionally, as the proportion of bad data increased, the difference between the prediction accuracy of cleaned data and that of uncleaned data was greater.",2020,,10.17775/CSEEJPES.2020.04080
246,"Wijerathna, Nadeesha and Matsubara, Masaki and Morishima, Atsuyuki",Finding Evidences by Crowdsourcing,Task analysis;Crowdsourcing;Libraries;Media;Uniform resource locators;Web pages;Big Data;Evidence;Assumption;Crowdsourcing,"Crowdsourcing is a promising tool involving multiple people in completing tasks that are difficult to complete by an individual, a small team or a computer. Ensuring the quality of the results is also one of the primary problems in crowdsourcing. One of the major approaches to improve the data quality to aggregate answers from more than one workers. This study explores a different approach - we ask workers to prove facts. We devise a general framework for collecting and ranking evidence-based proofs. The experiments results show that the proposed framework works and how diverse the collected proofs are. Our results clearly indicate that the crowd-based approach to prove facts is promising.",2018,,10.1109/BigData.2018.8622185
247,"Ahmad, Awais and Paul, Anand and Rathore, M. Mazhar and Rho, Seungmin",Big Data Analytical Architecture Using Divide-and-Conquer Approach in Machine-to-Machine Communication,Servers;Satellites;Big data;Computer architecture;Algorithm design and analysis;Decision making;Feature extraction;Big Data;divide-and-conquer;machine ID;efficiency,"Machine-to-Machine (M2M) technology unremittingly motivates any time-place-objects connectivity of the devices in and around the world. Every day, a rapid growth of large M2M networks and digital storage technology, lead to a massive heterogeneous data depository, in which the M2M data are captured and warehoused in the diverse database frameworks as a magnitude of heterogeneous data sources. Hence, the M2M that handles Big Data might perform poorly or not according to the goals of their operator due to massive heterogeneous data sources may face various incompatibilities, such as data quality, processing and computational efficiency, analysis and feature extraction applications. Therefore, to address the aforementioned constraints, this paper presents a Big Data Analytical architecture based on Divide-and-Conquer approach. The designed system architecture exploits divide-and-conquer approach, where big data sets are first transformed into a several data blocks that can be quickly processed, then it classifies and reorganizes these data blocks from the same source. In addition, the data blocks are aggregated in a sequential manner based on a machine ID, and equally partitions the data using filtration and load balancing algorithms. The feasibility and efficiency of the proposed system architecture are implemented on Hadoop single node setup. The results show that the proposed system architecture efficiently extract various features (such as River) from the massive volume of satellite data.",2015,,10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.330
248,"Yuan, Fang and Hong, Xianbin and Yuan, Cheng and Fei, Xiang and Guan, Sheng-Uei and Liu, Dawei and Wang, Wei",Keywords-oriented Data Augmentation for Chinese,Deep learning;Data integrity;Text categorization;Sampling methods;Natural language processing;Noise measurement;Task analysis;Data Augmentation;Chinese;Classification,"In natural language processing tasks, data is very important, but data collection is not cheap. Large volume data can well serve a series of tasks, especially for deep learning tasks. Data augmentation methods are solutions to data problems, which can work well on rising data quality and quantity, such as generating text without meaning changing and expanding the diversity of data distribution. A user-friendly method of the data augmentation is to sample words in a text then augmenting them. The sampling method is often implemented by a random probability. Although the performance of this solution has been proved over the past few years, random sampling is not the best choice for the data augmentation as it has a chance of randomly introducing some noise into initial data, like stop words. The generated data could interfere with the subsequent tasks and drop the accuracy of the tasks' solutions. Hence, this paper aims to introduce a novel data augmentation method that could avoid involving such noisy data. The strategy is keywords-oriented data augmentation for Chinese (KDA). The KDA proposed in this paper indicates a method of extracting keywords based on category labels, and an augmenting method based on the keywords. In contrast to randomness, the proposed technique firstly selects the key information data, then expands the selected data. The experimental section is compared with another two typical data augmentation techniques on three Chinese data sets for text classification tasks. The result shows that the KDA technique has a better performance in the data augmentation task than the compared two.",2020,,10.1109/ICCC51575.2020.9345133
249,"Pu, Dong-Mei and Gao, Da-Qi and Yuan, Yu-Bo",A dynamic data correction algorithm based on polynomial smooth support vector machine,Support vector machines;Heuristic algorithms;Data analysis;Aerodynamics;Cybernetics;Big data;Machine learning algorithms;Data analysis;Data correction;Support vector machine;Data Mining,"Data quality plays an important role in modern intelligent information system and is crucial to any data analysis task. Many imperfection-handling techniques avoid overfitting or simply remove offending portions of the data. Data correction can help to retain and recover as much information as possible from the original data resources. In this paper, we proposed a novel technique based on polynomial smooth support vector machine. The quadratic polynomial and the first degree of polynomial as the support vector machine smooth functions are investigated. At the same time, the function was used as smooth function to calculate compensation values. In order to show the procedures of our algorithm, some necessary steps need to be considered. Firstly, the original data are normalized, so as to eliminate experimental effects of dimensional problems. Secondly, the three different kinds of smooth functions need to be analysed mathematically. The difference measure are calculated to make sure the results of correction through different data correction models. The results of given noised data sets can show that the proposed the data correction method based on polynomial smooth support vector machine is effectiveness.",2016,,10.1109/ICMLC.2016.7872993
250,"Zheng, Ningxin and Chen, Quan and Chen, Chen and Guo, Minyi",CLIBE: Precise Cluster-Level I/O Bandwidth Enforcement in Distributed File System,Bandwidth;Quality of service;Big Data applications;Throughput;Writing;Reliability;Distributed file system;I/O bandwidth enforcement;HDFS,"A distributed file system (DFS) is a core component to implement big data applications. On the one hand, a DFS is capable of managing a large volume of data with desirable properties that strike the balance between high availability, reliability, and so on. On the other hand, a DFS relies on underlying storage systems (e.g., hard drives, solid state drives, etc.) and suffer from slow read/write operations. In big data era, large-scale data processing applications start to leverage the in-memory processing to improve the performance by reducing the inhibitive cost of I/O operations. However, it is still inevitable to read input data from or write outputs to the storage system. Slow I/O operations are often the main bottleneck of emerging big data applications. In particular, while these applications often use DFSs to store their results for the high availability and reliability, the unmanaged I/O bandwidth contention results in the QoS violation of high priority applications when multiple applications share the same DFS. To enable I/O management and allocation on big-data platforms, we propose a Cluster-Level I/O Bandwidth Enforcement (CLIBE) approach that consists of a cluster-level I/O bandwidth quota manager, multiple node-level I/O bandwidth controllers, and a feedback-based quota reallocator. The quota manager splits and distributes the I/O bandwidth quota of an application to the active nodes that are serving this application. The bandwidth controller on a node ensures that the I/O bandwidth used by an application would not exceed its bandwidth quota on the node. For an application affected by slow or overloaded nodes, the quota reallocator reallocates the idle I/O bandwidth on underloaded nodes to this application to guarantee its throughput. Our experiment on a real-system cluster shows that CLIBE is able to precisely control the I/O bandwidth used by an application at the cluster level, with the deviation smaller than 2.51%.",2018,,10.1109/HPCC/SmartCity/DSS.2018.00048
251,"Canbek, Gürol and Sagiroglu, Seref and Taskaya Temizel, Tugba",New Techniques in Profiling Big Datasets for Machine Learning with a Concise Review of Android Mobile Malware Datasets,Malware;Big Data;Machine learning;Mobile applications;Genomics;Bioinformatics;Aerospace electronics;data profiling;data quality;big data;malware detection;mobile malware;machine learning;classification;Android;feature engineering,"As the volume, variety, velocity aspects of big data are increasing, the other aspects such as veracity, value, variability, and venue could not be interpreted easily by data owners or researchers. The aspects are also unclear if the data is to be used in machine learning studies such as classification or clustering. This study proposes four techniques with fourteen criteria to systematically profile the datasets collected from different resources to distinguish from one another and see their strong and weak aspects. The proposed approach is demonstrated in five Android mobile malware datasets in the literature and in security industry namely Android Malware Genome Project, Drebin, Android Malware Dataset, Android Botnet, and Virus Total 2018. The results have shown that the proposed profiling methods reveal remarkable insight about the datasets comparatively and directs researchers to achieve big but more visible, qualitative, and internalized datasets.",2018,,10.1109/IBIGDELFT.2018.8625275
252,"Krawczyk, Bartosz and Wozniak, Michal",Weighted Naïve Bayes Classifier with Forgetting for Drifting Data Streams,Training;Adaptation models;Data mining;Memory management;Detectors;Data models;Probability;machine learning;data stream;concept drift;big data;incremental learning;forgetting,"Mining massive data streams in real-time is one of the contemporary challenges for machine learning systems. Such a domain encompass many of difficulties hidden beneath the term of Big Data. We deal with massive, incoming information that must be processed on-the-fly, with lowest possible response delay. We are forced to take into account time, memory and quality constraints. Our models must be able to quickly process large collection of data and swiftly adapt themselves to occurring changes (shifts and drifts) in data streams. In this paper, we propose a novel version of simple, yet effective Naïve Bayes classifier for mining streams. We add a weighting module, that automatically assigns an importance factor to each object extracted from the stream. The higher the weight, the bigger influence given object exerts on the classifier training procedure. We assume, that our model works in the non-stationary environment with the presence of concept drift phenomenon. To allow our classifier to quickly adapt its properties to evolving data, we imbue it with forgetting principle implemented as weight decay. With each passing iteration, the level of importance of previous objects is decreased until they are discarded from the data collection. We propose an efficient sigmoidal function for modeling the forgetting rate. Experimental analysis, carried out on a number of large data streams with concept drift prove that our weighted Naïve Bayes classifier displays highly satisfactory performance in comparison with state-of-the-art stream classifiers.",2015,,10.1109/SMC.2015.375
253,"Zeng, Hui and Zhao, Xiaoyong and Wang, Lei",Multivariate Time Series Anomaly Detection On Improved HTM Model,Performance evaluation;Job shop scheduling;Computational modeling;Time series analysis;Big Data;Predictive models;Prediction algorithms;Multivariate time series;Anomaly detection;Hierarchical temporal memory;Industrial bigdata,"In recent years, industrial big data has attracted much attention as the key technical support of “Intelligent Manufacturing” and “Industrial Internet”. And as the dependence of intelligent manufacturing on digitalization continues to increase, data quality problems caused by device and system failures, harsh environment, improper scheduling and management, duplication or missing of data fields, etc., have more significant impacts on industrial processes. Therefore, the anomaly detection of industrial big data is particularly important. Among the methods onto time series data for anomaly detection, HTM(Hierarchical Temporal Memory) algorithm performs well in the unsupervised univariate time series data anomaly detection, but the capability of original HTM model for detecting multivariate time series anomaly data is insufficient. However, the multivariate data anomaly detection is common in industry and the performance requirements for data anomaly detection are relatively high. Thus, this paper proposes an improved HTM algorithm model - MSP-HTM(Multiple Spatial Poolers HTM) model. The MSP-HTM model respectively encode the value of different dimensions at the same time, and then put the result from encoder into spatial pooler respectively, finally the temporal memory layer merge result from spatial poolers, and predict future data. Experiments show that the MSP-HTM model can improve performance by processing the multivariate time series data in parallel and improve the effect of data anomaly detection.",2021,,10.1109/CEI52496.2021.9574505
254,"Wang, Xiaofeng and Jiang, Yong and Zhan, Gaofeng and Zhao, Tong",Quality Analysis and Evaluation Method for Multisource Aggregation Data based on Structural Equation Model,Analytical models;Adaptation models;Numerical analysis;Data integrity;Computational modeling;Urban areas;Data aggregation;Data Aggregation;Data Analysis;Quality Evaluation;Structural Equation Model,"In the era of big data, how to evaluate the data quality of multi-source aggregation data is very important. The reason is that uneven data quality will directly lead to inaccurate or ambiguous data in the database, and indirectly lead to the deviation of subsequent data mining and decision-making. In this paper, structural equation model(SEM) is introduced to explore the effectiveness of various data quality evaluation indicators in data aggregation and finding out internal relationship between them. A new quality evaluation method of multi-source aggregation data is proposed, based on the regression's significance analysis and factor loads of each observation index in the SEM model. The case analysis shows that the proposed method is feasible and can be used to evaluate the quality of multi-source aggregation data adaptively for a long time.",2020,,10.1109/ICMCCE51767.2020.00280
255,"Jain, Shashwat and Khandelwal, Manish and Katkar, Ashutosh and Nygate, Joseph",Applying big data technologies to manage QoS in an SDN,Delays;Correlation;Quality of service;Jitter;Big data;Ports (Computers),"Managing QoS in a telecommunications network is a complex process. Effective network design and sizing in conjunction with load balancing, access control and traffic prioritization need to be orchestrated to optimize CAPEX investment, maximize network utilization and ensure that performance metrics and SLAs are met. This work shows how big data analytics were used to improve the management of QoS in an SDN by performing multi-dimensional analysis of Key Performance Indicators (KPIs) and applying machine learning algorithms to discover new correlations, perform root cause analysis and predict traffic congestion.",2016,,10.1109/CNSM.2016.7818437
256,"Feng, Xinyi",Power Data Quality Optimization and Evaluation Based on BPNN,Data integrity;Petri nets;Computer architecture;Aerospace electronics;Data processing;Data models;Cleaning;data space;data quality;business flow;petri net;BPNN,"With the continuous improvement of the information technology and communications of Smart Grid, the electric power big data environment has been formed. The data shows diversity and multi-source characteristics. How to ensure the quality of power data in the computer organization under the condition of heterogeneity is the premise of making relevant decisions. This paper firstly gives the definition of Data Space of power enterprises, analyzes the factors affecting the quality of data in the computer environment, and gives the relevant architecture of processing power data in the data space. Secondly, based on business flow and Petri net in the computer environment, this paper constructs the data flow and quality control model of the front and back platforms. The former represents the data flow in the power business and abstracts it to form Petri net computer information flow, so that the data can achieve the effect of cleaning while flowing in the business process. Finally, an evaluation index system is built and back-propagation neural network (BPNN) is used to determine the weight, a case study is given to verify the effectiveness of the proposed method.",2021,,10.1109/AIAM54119.2021.00106
257,"Rocha, Lais M. A. and Bessa, Aline and Chirigati, Fernando and OFriel, Eugene and Moro, Mirella M. and Freire, Juliana",Understanding Spatio-Temporal Urban Processes,Urban areas;Correlation;Spatial resolution;Data analysis;Mathematical model;Public transportation;Standards;data quality;data profiling;urban data,"Increasingly, decisions are based on insights and conclusions derived from the results of data analysis. Thus, determining the validity of these results is of paramount importance. In this paper, we take a step towards helping users identify potential issues in spatio-temporal data and thus gain trust in the results they derived from these data. We focus on processes that are captured by relationships among datasets that serve as the data exhaust for different components of urban environments. In this scenario, debugging data involves two important challenges: the inherent complexity of spatio-temporal data, and the number of possible relationships. We propose a framework for profiling spatio-temporal relationships that automatically identifies data slices that present a significant deviation from what is expected, and thus, helps focus a user's attention on slices of the data that may have quality issues and/or that may affect the conclusions derived from the analysis' results. We describe the profiling methodology and how it derives relationships, identifies candidate deviations, assesses their statistical significance, and measures their magnitude. We also present a series of cases studies using real datasets from New York City which demonstrate the usefulness of spatio-temporal profiling to build trust on data analysis' results.",2019,,10.1109/BigData47090.2019.9006289
258,"Zhang, Huaxin and Liu, Yu and Wang, Zituo and Li, Tiansong and Cao, Keyin",Research on Film Data Preprocessing and Visualization,Motion pictures;Data visualization;Visualization;Prediction algorithms;Market research;Arrays;Electronic commerce;recommended algorithm;Dataset;data processing;data visualization,"Data is the core of information, and good data quality is a prerequisite for many data analysis. Data cleaning is to increase the fault tolerance rate by correcting the error value of detected data. This paper aims to solve the problem of data set processing and visualization in the recommendation algorithm, so as to better apply in the field of recommendation algorithm. The recommendation algorithm and data sets Movielens and IMDB are analyzed theoretically. First, data set A was processed from data reading and movie score calculation; Again, the IMDB is processed in four steps to make it more suitable for the recommendation algorithm field; Finally, the plot function is used to visualize the key information. experiment shows: The data set sorted out by the above methods can effectively improve the quality and availability of data and provide relevant basis for better application in the algorithm.",2020,,10.1109/ICIBA50161.2020.9276830
259,"Yoo, Yeisol and Yoo, Jin Soung",RFID Data Warehousing and OLAP with Hive,RFID data warehousing;OLAP;Hive;Cloud computing,"Radio Frequency Identification (RFID) technology is used in many applications for monitoring object movement. The use of RFID in supply chain management systems enables to track the movement of products from suppliers to warehouses, store backrooms, and eventually points of sale. The vast amount of data resulting from the proliferation of RFID readers and tags poses challenges for data management and analytics. RFID data warehousing can enhance data quality and consistency, and give great potential benefits for Online Analytical Processing (OLAP) applications. Traditional data warehouses are built primarily on relational database management systems. However, the size of RFID data being collected and analyzed in the industry for business intelligence is growing rapidly, making traditional warehousing solutions prohibitively expensive. Hive is an open-source data warehousing solution built on top of Hadoop which is a popular Big Data computing framework. This paper presents alternative RFID data warehouse designs which can handle a large amount of RFID data and support a variety of OLAP queries. The proposed approaches are implemented on Hive and evaluated for query performance in cloud computing environment.",2019,,10.1109/IUCC/DSCI/SmartCNS.2019.00105
260,"Bantug, Derek and Franklin, Paul and Boone, Ted",Product Reliability and Databases: Lessons Learned,Databases;Tools;Measurement;Maintenance engineering;Reliability;Engines;Arrays;Databases;data cube;big data;lessons learned,"This experience paper describes some lessons learned using “big data.” Managers want to make data driven decisions, and many companies spend substantial effort and resources to develop collection methods, record facts, and store records in large data warehouses. Additional resources take this collection of data and produce reports, which are then used to support decision making. We work with customer premises equipment, and our databases track nearly 50 million serial numbers. As reliability engineers, we use this data as the basis of analysis to assess field performance of the equipment the databases track. In 2016, we began to use a standard tool to serve as a definitive repository and an engine to do preliminary postprocessing. This database uses data dimensions, where each dimension is an array. It is convenient to think of the lefthand column as a set of labels and the cells to the right as measures (either collected data or computed metrics) for each label. The advantage of creating dimensions is that-rather than working with individual data items and the relationships between them-a dimension preprocesses data into a set that has relationships with other sets. This models the “real world” more closely. A cube is just a set of one or more dimensions. Using a cube allows complex questions to be asked and answered in ways that relational databases do not. We have been using this data structure to support analysis of customer premises equipment, typically set top boxes, modems, and similar equipment that is leased by the provider to customers at their residences and businesses. Tools that support cubes offer several advantages. It is possible to do analyses in a cube that are difficult in a relational database that does not support the “logical ecology” that a cube does By moving up and down the data hierarchy, it is possible to see relationships on the screen, and outputs can be saved to other more powerful post processing tools for more detailed analysis Cubes support faster and less error prone analysis This paper describes these points and illustrate them with a simple example. Our objective is to illustrate the concepts rather than work through a detailed problem. Our work to date suggests that it is critical to manage data quality in a broad sense so that the resulting reports and analysis are trustworthy. We have had a generally positive experience with this technology and found that it benefits the business by allowing processes to be modeled. This refines our understanding of the meaning of the various process metrics, and in some cases, we have been able to recommend changes to policy.",2018,,10.1109/RAM.2018.8463091
261,"Xue, Lian",Competency Evaluation System of English Teaching Post based on K-Means Clustering Algorithm,Electrical engineering;Data integrity;Education;Clustering algorithms;Big Data;Parallel processing;Data warehouses;English Teaching;Post Competency;Clustering Algorithm;System Assessment,"With the rapid development of big data, user data information is increasingly perfect, data warehouse integration is more reasonable, and data quality is constantly improving, so the value of data is increasing. Based on parallel processing of K-means clustering algorithm, this paper extracts ability constraint information, integrates K-means clustering algorithm, clusters and integrates various index parameters of post competency. From the final experimental results, this method improves the execution efficiency and accuracy compared with other methods, and can be used in practice.",2021,,10.1109/AEECA52519.2021.9574197
262,"Xiao, Yunlong and Gu, Yang and Wang, Jiwei and Wu, Tong",A Collaborative Multi-modality Selection Method Based on Data Utility Assessment,Data integrity;Machine learning;Data models;Training;Task analysis;Mathematical model;Gesture recognition;data selection;multimodal;data utility;data quality assessment,"Multimodal fusion is more and more widely used in the field of machine learning, but it faces a prominent problem in practical application: data utility is not stable. The data of different modalities may be missing and noisy randomly, which will interfere the machine learning model of multi-modal fusion. Most of the existing multi-modal fusion methods neglect data utility problems or only adopt simple data denoising methods to improve data utility. To solve the problem of unstable data utility, we propose a data selection method based on the evaluation of data utility. By training a special machine learning model, the optimal modal combination is predicted according to the quality evaluation of multi-modal data samples to accomplish the dynamic selection of data modalities. The experimental results show that the proposed method can effectively improve the accuracy of multi-modal recognition under low data utility.",2019,,10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00120
263,"Jie, Lu and Zheng, Su and Qi, Wang and Xiya, Chen",Analysis of Employment Status and Countermeasures of Biology Graduates in Local Normal Universities Based on Big Data Technology—Take the Graduates of Guangxi Normal University From 20l6 to 2020 as an Example,Training;Technological innovation;Employment;Big Data;Prediction algorithms;Biology;Planning;data analysis;majors concerning biology;graduates;employment quality;local normal universities,"The data of annual reports on the employment quality f local normal universities from 2016 to 2020 were captured from the network with the help of information technology. The data related to basic situation and employment destination of biology graduates in Guangxi Normal University were analyzed and collected by data analytic technology such as Data Mining Algorithms, Data Quality Master Data Management and Predictive Analytic Capabilities. In view of the problems of single employment structure, insufficient employment skills, vague employment planning and low achievement of Innovation and Entrepreneurship, measures such as the employment policy interpretation, the talent training programs adjustment, the employment guidance services system improvement and teaching mode innovation are needed in the purpose of promoting employment, providing useful reference to the further progress of teaching reform and optimizing talent training modes and methods for graduates of biology.",2021,,10.1109/ICAIE53562.2021.00127
264,"Demchenko, Yuri and Cushing, Reggie and Los, Wouter and Grosso, Paola and de Laat, Cees and Gommans, Leon",Open Data Market Architecture and Functional Components,Economics;Cloud computing;Data models;Contracts;Big Data;Computer architecture;Open Data Market;Data Marketplace;Trusted Data Market;Industrial Data Space;Data Economics;STREAM Data Properties,"This paper discusses the principles of organisation and infrastructure components of Open Data Markets (ODM) that would facilitate secure and trusted data exchange between data market participants, and other cooperating organisations. The paper provides a definition of the data properties as economic goods and identifies the generic characteristics of ODM as a Service. This is followed by a detailed description of the generic data market infrastructure that can be provisioned on demand for a group of cooperating parties. The proposed data market infrastructure and its operation are employing blockchain technologies for securing data provenance and providing a basis for data monetisation. Suggestions for trust management and data quality assurance are discussed.",2019,,10.1109/HPCS48598.2019.9188195
265,"Park, Hyunseop and Ko, Hyunwoong and Lee, Yung-Tsun T. and Cho, Hyunbo and Witherell, Paul",A Framework for Identifying and Prioritizing Data Analytics opportunities in Additive Manufacturing,Decision making;Solid modeling;Mechanical variables measurement;Electric variables measurement;Shape measurement;Data analysis;Analytical models;Data analytics;opportunity identification and prioritization;architecture;additive manufacturing,"Many industries, including manufacturing, are adopting data analytics (DA) in making decisions to improve quality, cost, and on-time delivery. In recent years, more research and development efforts have applied DA to additive manufacturing (AM) decision-making problems such as part design and process planning. Though there are many AM decision-making problems, not all benefit greatly from DA. This may be due to insufficient AM data, unreliable data quality, or the fact that DA is not cost effective when it is applied to some AM problems. This paper proposes a framework to investigate DA opportunities in a manufacturing operation, specifically AM. The proposed framework identifies and prioritizes AM potential opportunities where DA can make impact. The proposed framework is presented in a five-tier architecture, including value, decision-making, data analytics, data, and data source tiers. A case study is developed to illustrate how the proposed framework identifies DA opportunities in AM.",2019,,10.1109/BigData47090.2019.9006489
266,"Zhang, Heng and Liu, Guohua and Zhao, Wenfeng and Ni, Mengfei",Incomplete relation revision method based on template,Algorithm design and analysis;Cleaning;Maintenance engineering;Feature extraction;Sorting;Classification algorithms;data quality;template;NFA;extensional B tree,"Data quality is the core problem in the field of big data application. In practical applications, data are often derived from multi-source databases, which can cause named conflict and similar duplicate record problems. This paper proposed a method to solve the named conflict problem and similar duplicate record based on a given template. In order to find the relationship between the template data and the data to be unified, we segmented the data that to be unified, then built an extensional B tree based on these data. At the same time, we construct a NFA for each template value. By using these NFA, we can get the language pattern for each column in the template relation. Finally, we search each template value in the extensional B tree, if the template value can be found and the corresponding data to be unified can be accepted by the NFA based on the template value, we can use the template value to replace the data that need to be recovered. Then, the data can be consolidated and integrated to ensure the consistency and integrity of the data.",2017,,10.1109/ICSAI.2017.8248534
267,"Zada, Muhammad Sadiq Hassan and Yuan, Bo and Anjum, Ashiq and Azad, Muhammad Ajmal and Khan, Wajahat Ali and Reiff-Marganiec, Stephan",Large-scale Data Integration Using Graph Probabilistic Dependencies (GPDs),Probabilistic logic;Uncertainty;Data integration;Data integrity;Redundancy;Scalability;Erbium;data integration;information retrieval;NoSQL databases;graph probabilistic dependencies;data science,"The diversity and proliferation of Knowledge bases have made data integration one of the key challenges in the data science domain. The imperfect representations of entities, particularly in graphs, add additional challenges in data integration. Graph dependencies (GDs) were investigated in existing studies for the integration and maintenance of data quality on graphs. However, the majority of graphs contain plenty of duplicates with high diversity. Consequently, the existence of dependencies over these graphs becomes highly uncertain. In this paper, we proposed graph probabilistic dependencies (GPDs) to address the issue of uncertainty over these large-scale graphs with a novel class of dependencies for graphs. GPDs can provide a probabilistic explanation for dealing with uncertainty while discovering dependencies over graphs. Furthermore, a case study is provided to verify the correctness of the data integration process based on GPDs. Preliminary results demonstrated the effectiveness of GPDs in terms of reducing redundancies and inconsistencies over the benchmark datasets.",2020,,10.1109/BDCAT50828.2020.00028
268,"Casale, Giuliano and Ardagna, Danilo and Artac, Matej and Barbier, Franck and Di Nitto, Elisabetta and Henry, Alexis and Iuhasz, Gabriel and Joubert, Christophe and Merseguer, Jose and Munteanu, Victor Ion and Perez, Juan Fernando and Petcu, Dana and Rossi, Matteo and Sheridan, Craig and Spais, Ilias and Vladuic, Daniel",DICE: Quality-Driven Development of Data-Intensive Cloud Applications,Unified modeling language;Big data;Data models;Computational modeling;Analytical models;Reliability;Software;Big Data;quality assurance;model-driven engineering,"Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.",2015,,10.1109/MiSE.2015.21
269,"Lawson, Victor J. and Banerjee, Madhushri",Measuring the Impact of an IoT Temperature Sensor Framework for Tracking Contagious Diseases,Temperature sensors;COVID-19;Temperature measurement;Temperature distribution;Data analysis;Correlation;Databases;Data Quality;Data Analytics;Information Quality;Big Data;Data Integration,"Due to the COVID-19 pandemic, much computer science research has been dedicated to utilizing sensor readings for medical purposes. Throughout this period, the need for virus symptom tracking has become a promising area for remotely deployed sensor networks and platforms. Our research goal is to prove that the temperature readings from these sensor network platforms can be statistically linked to public record, medical case study data. The expected outcome of our project is to prove the correlation between sensor network tracking of remote human temperature data and medical records for COVID cases. The results of this study will prove that tracking human temperature can assist in tracking disease outbreaks in various populations. Our framework platform is comprised of four main modules: (1) Temperature Collection, (2) Internal Data Validation (3) Internal-External data merger, (4) Data Analytics. The temperature data are collected from internal databases, mobile sensing devices and medical health professionals. After collection, the internal data are validated by our software, TAU-FIVE, a multi-tier data quality validation system, then merged with external data sources into a data analytic based data warehouse. The data mart queries are designed to compare the location and date of temperature sensor data with known data sets from government officials. Once blended into a fully operational data warehouse, these data marts produce high quality data analysis linking remotely sensed human temperature readings to sources of disease outbreaks.",2021,,10.1109/ICITISEE53823.2021.9655853
270,"Li, Song and Ning, Sun and Yezhou, Yao and Jingjing, Tian and Wenxue, Zhang and Liang, Chi",Application of Data Mining Technology in the Recall of Defective Automobile Products in China ——A Typical Case of the Construction of Digital China,Automobiles;Safety;Big Data;Government;Personnel;Data mining;automobile recall;data mining;information cluster,"According to multisource quality safety data of defective automobile products, key quality safety factors of defective automobile products are extracted, a defect information indicator system for automobile products is systematically constructed and a correlated graph is established between quality safety factors. Based on the optimization and correlation of the quality safety factor indicator system, Big Data technology is used to design a data structure for multisource quality safety information cluster, develop a data platform for the defect information analysis of automobile products and achieve information clustering and correlation analysis based on multisource quality safety data, providing technical support for the recall management of defective automobile products.",2019,,10.1109/IICSPI48186.2019.9095877
271,"Buyya, Rajkumar and Murray, Derek",HPGC Keynotes,Cloud computing;Computational modeling;Big data;Quality of service;Distributed processing;Conferences;Educational institutions,These keynote speeches discuss the following: Market-oriented cloud computing and Big Data applications; and Low-latency distributed analytics in Naiad.,2014,,10.1109/IPDPSW.2014.234
272,"Amato, Flora and Moscato, Francesco",Automatic Cloud Services Composition for Big Data Management,Cloud computing;Computer architecture;Semantics;Big data;Quality of service;Meteorology;NIST;Cloud Computing;Orchestration;Formal Semantics;Availability,"Every-Day lives are becoming increasingly instrumented by electronic devices and any kind of computer-based (distributed) service. As a result, organizations need to analyse an enormous amounts of data in order to increase their incomings or to improve their services. Anyway, setting-up a private infrastructure to execute analytics over Big Data is still expensive. The exploitation of Cloud infrastructure in Big Data management is appealing because of costs reductions and potentiality of storage, network and computing resources. The Cloud can consistently reduce the cost of analysis of data from different sources, opening analytics to big storages in a multi-cloud environment. Anyway, creating and executing this kind of service is very complex since different resources have to be provisioned and coordinated depending on users' needs. Orchestration is a solution to this problem, but it requires proper languages and methodologies for automatic composition and execution. In this work we propose a methodology for composition of services used for analyses of different Big Data sources: in particular an Orchestration language is reported able to describe composite services and resources in a multi-cloud environment.",2016,,10.1109/WAINA.2016.169
273,"Kong, Linghe and Zhang, Daqiang and He, Zongjian and Xiang, Qiao and Wan, Jiafu and Tao, Meixia",Embracing big data with compressive sensing: a green approach in industrial wireless networks,Big data;Compressed sensing;Wireless sensor networks;Wireless communication;Redundancy;Industrial plants;Green design,"New-generation industries heavily rely on big data to improve their efficiency. Such big data are commonly collected by smart nodes and transmitted to the cloud via wireless. Due to the limited size of smart node, the shortage of energy is always a critical issue, and the wireless data transmission is extremely a big power consumer. Aiming to reduce the energy consumption in wireless, this article introduces a potential breach from data redundancy. If redundant data are no longer collected, a large amount of wireless transmissions can be cancelled and their energy saved. Motivated by this breach, this article proposes a compressive-sensing-based collection framework to minimize the amount of collection while guaranteeing data quality. This framework is verified by experiments and extensive real-trace-driven simulations.",2016,,10.1109/MCOM.2016.7588229
274,"Mi, Jun and Wang, Kun and Li, Peng and Guo, Song and Sun, Yanfei",Software-Defined Green 5G System for Big Data,Monitoring;5G mobile communication;Green products;Big Data;Renewable energy sources;Quality of service;Computer architecture,"The 5G system has been recognized as the most promising technology to provide high-quality network services. As a huge number of networking and computing equipments that generate big data are integrated into the 5G system, energy efficiency becomes the major challenge in building a green 5G system. In this article, we propose a software-defined green 5G system for big data, which consists of three planes: the control plane, the data plane and the energy plane. The data plane contains networking and computing equipments, which can be powered by both traditional grid and renewable energy sources in the energy plane. The control plane monitors the system status and configures the corresponding equipments to achieve energy efficiency and quality-of-service. Furthermore, to reduce the overhead of this software- defined architecture, we investigate a FRS to eliminate redundant system monitoring information. To integrate features in software-defined architecture, we propose an AIFS to mine latent rules among features. Simulation results indicate that our proposals achieve higher efficiency in the green 5G system.",2018,,10.1109/MCOM.2017.1700048
275,"Bergès, Corinne and Bird, Jim and Shroff, Mehul D. and Rongen, René and Smith, Chris",Data analytics and machine learning: root-cause problem-solving approach to prevent yield loss and quality issues in semiconductor industry for automotive applications,Solid modeling;Electronics industry;Predictive models;Reliability engineering;Manufacturing;Safety;Problem-solving;Automotive semiconductor industry;root-cause problem-solving;failure prevention;data analytics;machine learning;big data,"Quality requirements in the semiconductor industry for automotive products are increasing rapidly with the movement to autonomous vehicles and higher levels of safety. It is no longer possible to express maximum failure requirements in parts per million (ppm). Individual failing parts observed in the field and reported by customers can trigger a significant quality response. ‘Zero-defect’ (ZD) is no longer considered a utopian ideal, but a required potentially reachable goal for semiconductor manufacturers. Projects and studies that include artificial intelligence and big data, are seen as key drivers to reach a ZD level of quality. Competing objectives targeted in any industrial project, such as quality improvement and gross margin, must also be considered. Initial projects in machine learning (ML), focusing on yield-loss issues, are being deployed within the manufacturing sites. These projects interconnect typical internal data collected from the manufacturing and assembly lines with engineering, qualification and reliability data. For a specific case study of unexpected abnormally high variability on some parameters, this paper presents a problem-solving approach in a big-data environment. Models implemented and results obtained towards root-cause problem solving for this issue, are discussed. This overall approach may be replicated in other ML projects.",2021,,10.1109/IPFA53173.2021.9617238
276,"Lu, Jianfei and Li, Suxiu and Zhang, Xinsheng",A Study on the Business Data Evaluation Method of the Power Grid Value-Added Service,Systematics;Data analysis;Data integrity;Storage management;Government;Data models;Power grids;Date Evaluation Method;Power Grid;Value- Added Service,"This research focuses on the data application basic technology for value-added services. In this research, the data quality and data value evaluation methods are studied. The data quality management system from data collection, storage, management and application is formed. Quality and power marketing data quality are analyzed and data value evaluation methods are established. As big data, artificial intelligence and other technologies continue to make breakthroughs, the value of data will become more and more important. Based on the State Grid’s full-service data, the full-scale data analysis across the professional, cross-business, and cross-system will promote the company’s power grid lean and intelligent management level, and will provide more value-added services for the company, government and society.",2021,,10.1109/ICPEE54380.2021.9662594
277,"Khokhlov, Igor and Reznik, Leon",What is the Value of Data Value in Practical Security Applications,Data quality;data value;security evaluation;privacy protection,"Data value (DV) is a novel concept that is introduced as one of the Big Data phenomenon features. While continuing an investigation of the DV ontology and its relationship with the data quality (DQ) on the conceptual level, this paper researches possible applications and use of the DV in the practical design of security and privacy protection systems and tools. We present a novel approach to DV evaluation that maps DQ metrics into DV value. Developed methods allow DV and DQ use in a wide range of application domains. To demonstrate DQ and DV concept employment in real tasks we present two real-life scenarios. The first use case demonstrates the DV use in crowdsensing application design. It shows up how DV can be calculated by integrating various metrics characterizing data application functionality, accuracy, and security. The second one incorporates the privacy consideration into DV calculus by exploring the relationship between privacy, DQ, and DV in the defense against web-site fingerprinting in The Onion Router (TOR) networks. These examples demonstrate how our methods of the DV and DQ evaluation may be employed in the design of real systems with security and privacy consideration.",2020,,10.1109/SSS47320.2020.9174457
278,"Hussain, Bilal and Du, Qinghe and Ren, Pinyi",Deep Learning-Based Big Data-Assisted Anomaly Detection in Cellular Networks,Computer architecture;Microprocessors;Big Data;Quality of service;Anomaly detection;Cellular networks;Quality of experience,"5G is envisioned to have an artificial intelligence (AI)-empowerment to efficiently plan, manage and optimize the extremely complex network by leveraging colossal amount of data (big data) generated at different levels of the network architecture. Cell outages and congestion pose serious threat to the network management. Sleeping cell is a special case of cell outage in which the cell provides inferior services to its users. This peculiar behavior of the cell is particularly challenging to detect as it disguises itself from the network monitoring entity. Inadequate accuracy and high false alarms are two major constraints of state-of-the-art approaches for the anomaly-sleeping cell and surge in user traffic activity that may lead to congestion-detection in cellular networks. This implies squandering of scarce resources which ultimately results in increased operational expenditure (OPEX) while disrupting network's quality of service (QoS) and user's quality of experience (QoE). Inspired from the prominent success of deep learning (DL) technology in machine learning domain, this is the first study that applies DL for the detection of abovementioned anomalies. We utilized, and did a comprehensive study of, L-layer deep feedforward neural network fueled by real call detail record (CDR) dataset (big data) and achieved 94.6% accuracy with 1.7% false positive rate (FPR), that are remarkable improvements and overcome the limitations of the previous studies. The preliminary results elucidate the feasibility and preeminence of our proposed anomaly detection framework.",2018,,10.1109/GLOCOM.2018.8647366
279,"Ba, Huafeng and Gao, Xiaoming and Zhang, Xiaofeng and He, Zhenyu",Protecting Data Privacy from Being Inferred from High Dimensional Correlated Data,Privacy;Data privacy;Algorithm design and analysis;Educational institutions;Equations;Computer science;Data analysis;privacy preserving data mining;data publishing;algorithm,"In the era of big data, privacy becomes a challenging issue which already attracts a good number of research efforts. In the literature, most of existing privacy preserving algorithms focus on protecting users' privacy from being disclosed by making the set of designated semi-id features indiscriminate. However, how to automatically determine the appropriate semi-id features from high-dimensional correlated data is seldom studied. Therefore, in this paper we first theoretically study the problem and propose the IPFS algorithm to find all possible features forming the candidate semi-id feature set which can infer users' privacy. Then, the KIPFS algorithm is proposed to find the key features from the candidate semi-id feature set. By anonymizing the key feature set, called as key inferring privacy features (KIPFS), users' privacy is protected. To evaluate the effectiveness and the efficacy of the proposed approach, two state-of-the-art algorithms, i.e., K-anonymity and t-closeness, applied on the designated semi-id feature set are chose as the baseline algorithms and their revised versions are applied on the KIPFS for the performance comparison. The promising results showed that by anonymizing the identified KIPFS, both aforementioned algorithms can achieve better performance than the original ones in terms of efficiency and data quality.",2014,,10.1109/WI-IAT.2014.139
280,"Fernández-Cerero, Damian and Fernández-Montes, Alejandro and Kolodziej, Joanna and Lefèvre, Laurent",Quality of Cloud Services Determined by the Dynamic Management of Scheduling Models for Complex Heterogeneous Workloads,Cloud computing;Task analysis;Processor scheduling;Dynamic scheduling;Big Data;Job shop scheduling;Servers;Big Data;Quality of Big Data;Scheduling;Cloud scheduling;Dynamic cloud scheduling,"The quality of services in Cloud Computing (CC) depends on the scheduling strategies selected for processing of the complex workloads in the physical cloud clusters. Using the scheduler of the single type does not guarantee of the optimal mapping of jobs onto cloud resources, especially in the case of the processing of the big data workloads. In this paper, we compare the performances of the cloud schedulers for various combinations of the cloud workloads with different characteristics. We define several scenarios where the proper types of schedulers can be selected from a list of scheduling models implemented in the system, and used to schedule the concrete workloads based on the workloads' parameters and the feedback on the efficiency of the schedulers. The presented work is the first step in the development and implementation of an automatic intelligent scheduler selection system. In our simple experimental analysis, we confirm the usefulness of such a system in today's data-intensive cloud computing.",2018,,10.1109/QUATIC.2018.00039
281,"Xinmei, Liang and Luqin",Research on Web Service Selection Based on Parallel Skyline Algorithm,Sparks;Web services;Clustering algorithms;Big Data;Quality of service;Computer science;Parallel processing;Skyline;big data;Spark;Hadoop;parallelization,"With the continuous development of the Internet, there are many web services with the same functional attributes but different functional attributes. It is urgent to find a web service that can satisfy itself quickly and efficiently from the massive web service data. This paper improves the traditional Skyline algorithm, divides the web service data set into regions, greatly reduces the data points without dominance, and saves memory usage. The improved Skyline algorithm can significantly improve the speed of Web service selection. However, the improved Skyline algorithm will still have insufficient computing resources when processing massive Web service data, resulting in a significant decrease in computing speed and even computer jam. In view of the above situation, this paper will parallelize the improved Skyline algorithm and parallelize the improved Skyline algorithm through the Spark platform. Experiments show that the parallelized Skyline algorithm can better handle massive Web service data.",2019,,10.1109/ICEIEC.2019.8784671
282,"Zong, Wei and Wu, Feng and Jiang, Zhengrui",A Markov-Based Update Policy for Constantly Changing Database Systems,Database systems;Data models;Organizations;Markov processes;Big data;Data quality;data timeliness;enterprise resource planning (ERP);Markov decision process;update policy,"In order to maximize the value of an organization's data assets, it is important to keep data in its databases up-to-date. In the era of big data, however, constantly changing data sources make it a challenging task to assure data timeliness in enterprise systems. For instance, due to the high frequency of purchase transactions, purchase data stored in an enterprise resource planning system can easily become outdated, affecting the accuracy of inventory data and the quality of inventory replenishment decisions. Despite the importance of data timeliness, updating a database as soon as new data arrives is typically not optimal because of high update cost. Therefore, a critical problem in this context is to determine the optimal update policy for database systems. In this study, we develop a Markov decision process model, solved via dynamic programming, to derive the optimal update policy that minimizes the sum of data staleness cost and update cost. Based on real-world enterprise data, we conduct experiments to evaluate the performance of the proposed update policy in relation to benchmark policies analyzed in the prior literature. The experimental results show that the proposed update policy outperforms fixed interval update policies and can lead to significant cost savings.",2017,,10.1109/TEM.2017.2648516
283,"Hassanein, Hossam S. and Oteafy, Sharief M. A.",Big Sensed Data Challenges in the Internet of Things,Sensors;Calibration;Internet of Things;Data integration;Conferences;Interoperability;Standards;Internet of Things;Big Sensed Data;Next Generation Networks;Quality of Data;Quality of Information,"Internet of Things (IoT) systems are inherently built on data gathered from heterogeneous sources. In the quest to gather more data for better analytics, many IoT systems are instigating significant challenges. First, the sheer volume and velocity of data generated by IoT systems are burdening our networking infrastructure, especially at the edge. The mobility and intermittent connectivity of edge IoT nodes are further hampering real-time access and reporting of IoT data. As we attempt to synergize IoT systems to leverage resource discovery and remedy some of these challenges, the rising challenges of Quality of Information (QoI) and Quality of Resource (QoR) calibration, render many IoT interoperability attempts far-fetched. We survey a number of challenges in realizing IoT interoperability, and advocate for a uniform view of data management in IoT systems. We delve into three planes that encompass Big Sensed Data (BSD) research directions, presenting a building block for future research efforts in IoT data management.",2017,,10.1109/DCOSS.2017.35
284,"Mao, Xu and Su, Fei",Standards compliance testing on generic data of telecom operators,C# languages;Data quality;Telecom operators;Generic data;Standards compliance testing,"The big data is bringing new opportunities to the world. Every traditional telecom operator is exploring new ways to increase revenues and profits from the explosive growth of data traffic, but few have demonstrated the data quality needed for data applications. Standards compliance testing on generic data of telecom operators is the key means to improve competitive quality of comprehensive telecom information services. With analysis of current contradiction between data supply and data demand of telecom operators, this paper defines the generic data of telecom operators and its standards compliance testing, presents the testing procedures and the testing applications, and points out a series of testing research directions.",2016,,10.1109/ISCIT.2016.7751640
285,"Xing, Xin and Dong, Bin and Ajo-Franklin, Jonathan and Wu, Kesheng",Automated Parallel Data Processing Engine with Application to Large-Scale Feature Extraction,Arrays;Sensors;Kernel;Data analysis;Engines;Feature extraction;ArrayUDF;distributed acoustic sensing;local similarity,"As new scientific instruments generate ever more data, we need to parallelize advanced data analysis algorithms such as machine learning to harness the available computing power. The success of commercial Big Data systems demonstrated that it is possible to automatically parallelize many algorithms. However, these Big Data tools have trouble handling the complex analysis operations from scientific applications. To overcome this difficulty, we have started to build an automated parallel data processing engine for science, known as ARRAYUDF. This paper provides an overview of this data processing engine, and a use case involving a feature extraction task from a large-scale seismic recording technology, called distributed acoustic sensing (DAS). The key challenge associated with DAS data sets is that they are vast in volume and noisy in data quality. The existing methods used by the DAS team for extracting useful signals like traveling seismic waves are complex and very time-consuming. Our parallel data processing engine reduces the job execution time from 10s of hours to 10s of seconds, and achieves 95% parallelization efficiency. ARRAYUDF could be used to implement more advanced data processing algorithms including machine learning, and could work with many more applications.",2018,,10.1109/MLHPC.2018.8638638
286,"Franklin, Paul",Solving Problems with Rapid Data Discovery,Analytical models;Decision making;Measurement uncertainty;Process control;Random access memory;Organizations;Reliability engineering;Data cubes;Metrics;Decision support,"Summary &#x0026; ConclusionsThis paper describes an approach to extracting reliability data from transaction data and performing analysis on it. Organizations typically collect significant amounts of data that could be used for reliability analysis but is not.Data science is a field that offers reliability engineers insights when faced with analyzing so-called ""big data."" One subset of data science that can be helpful in this regard is the idea of using data cubes as the basis for analysis. Data cubes use many techniques, such as slicing, aggregation, drill-downs, and pivots [1]. These concepts are widely implemented, and most engineers use them, even if they do not explicitly name them. For example, drill-down would occur when only the data from a particular equipment model is examined; aggregation reverses this. Slicing occurs with all but two dimensions (defined below) are held constant. Pivots occur when data is ""rotated"" by changing the way rows and columns are selected and displayed.This paper will report on the results of using data cubes to support and drive the culture of reliability engineering:&#x2022;Rapidly model large datasets to confirm or deny reliability and process measures&#x2022;Drive data quality improvements&#x2022;Build confidence in the way business rules are modeled&#x2022;Develop metrics&#x2022;Support decision making in the face of uncertaintyThe paper describes the work done and offers some recommendations for implementation.",2021,,10.1109/RAMS48097.2021.9605783
287,"Bronselaer, Antoon and De Mol, Robin and De Tré, Guy",A Measure-Theoretic Foundation for Data Quality,Current measurement;Urban areas;Context;Big Data;Uncertainty;Decision making;Data quality;fuzzy measure;uncertainty modeling,"In this paper, a novel framework for data quality measurement is proposed by adopting a measure-theoretic treatment of the problem. Instead of considering a specific setting in which quality must be assessed, our approach departs more formally from the concept of measurement. The basic assumption of the framework is that the highest possible quality can be described by means of a set of predicates. Quality of data is then measured by evaluating those predicates and by combining their evaluations. This combination is based on a capacity function (i.e., a fuzzy measure) that models for each combination of predicates the capacity with respect to the quality of the data. It is shown that expression of quality on an ordinal scale entails a high degree of interpretation and a compact representation of the measurement function. Within this purely ordinal framework for measurement, it is shown that reasoning about quality beyond the ordinal level naturally originates from the uncertainty about predicate evaluation. It is discussed how the proposed framework is positioned with respect to other approaches with particular attention to aggregation of measurements. The practical usability of the framework is discussed for several well known dimensions of data quality and demonstrated in a use-case study about clinical trials.",2018,,10.1109/TFUZZ.2017.2686807
288,"He, Ying and Yu, F. Richard and Zhao, Nan and Leung, Victor C. M. and Yin, Hongxi",Software-Defined Networks with Mobile Edge Computing and Caching for Smart Cities: A Big Data Deep Reinforcement Learning Approach,Smart cities;Streaming media;Cloud computing;Big Data;Quality of service;Mobile communication;Urban areas,"Recent advances in networking, caching, and computing have significant impacts on the developments of smart cities. Nevertheless, these important enabling technologies have traditionally been studied separately in the existing works on smart cities. In this article, we propose an integrated framework that can enable dynamic orchestration of networking, caching, and computing resources to improve the performance of applications for smart cities. Then we present a novel big data deep reinforcement learning approach. Simulation results with different system parameters are presented to show the effectiveness of the proposed scheme.",2017,,10.1109/MCOM.2017.1700246
289,"Kong, Weichang and Qiao, Fei and Wu, Qidi",Real Manufacturing Oriented Data Process Techniques with Domain Knowledge,Conferences;Big Data;Manufacturing processes;Manufacturing industries;Data analysis;Data integrity,"In the field of manufacturing industry, it is difficult to make full use of the research results for production optimization and/or management due to the low quality of real workshop data. Typical quality problems of the real workshop data include: data conflict, missing recessive data, and false error identification. The conventional data analysis methods cannot handle most such issues because they fail to consider professional insights into and domain knowledge about the data. The real production data from an actual semiconductor manufacturing workshop are adopted as the objective data in this paper. A series of data process techniques with domain knowledge are proposed to solve those data quality problems according to specific flaws of the data respectively. The work in this paper has the potential to be further extended and applied to other big data applications beyond the manufacturing industry.",2018,,10.1109/SMC.2018.00532
290,"Togneri, Rodrigo and Camponogara, Glauber and Soininen, Juha-Pekka and Kamienski, Carlos",Foundations of Data Quality Assurance for IoT-based Smart Applications,Irrigation;Quality assurance;Data integrity;Signal processing;Feature extraction;Robustness;Object recognition;Data quality;internet of things;smart applications;precision irrigation,"Most current scientific and industrial efforts in IoT are geared towards building integrated platforms to finally realize its potential in commercial scale applications. The IoT and Big Data contemporary context brings a number of challenges, such as providing quality assurance (defined by availability and veracity) for sensor data. Traditional signal processing approaches are no longer sufficient, requiring combined approaches in both architectural and analytical layers. This paper proposes a discussion on the adequate foundations of a new general approach aimed at increasing robustness and antifragility of IoT-based smart applications. In addition, it shows results of preliminary experiments with real data in the context of precision irrigation using multivariate methods to identify relevant situations, such as sensor failures and the mismatch of contextual sensor information due to different spatial granularities capture. Our results provide initial indications of the adequacy of the proposed framework.",2019,,10.1109/LATINCOM48065.2019.8937930
291,"Gong, Xiaowen and Shroff, Ness B.",Truthful Mobile Crowdsensing for Strategic Users With Private Data Quality,Task analysis;Transmitters;Signal to noise ratio;Data integrity;Sensors;IEEE transactions;Big Data;Crowdsensing;truthful incentive mechanism;data quality,"Mobile crowdsensing has found a variety of applications (e.g., spectrum sensing, environmental monitoring) by leveraging the “wisdom” of a potentially large crowd of mobile users. An important metric of a crowdsensing task is data accuracy, which relies on the data quality of the participating users' data (e.g., users' received SNRs for measuring a transmitter's transmit signal strength). However, the quality of a user can be its private information (which, e.g., may depend on the user's location) that it can manipulate to its own advantage, which can mislead the crowdsensing requester about the knowledge of the data's accuracy. This issue is exacerbated by the fact that the user can also manipulate its effort made in the crowdsensing task, which is a hidden action that could result in the requester having incorrect knowledge of the data's accuracy. In this paper, we devise truthful crowdsensing mechanisms for Quality and Effort Elicitation (QEE), which incentivize strategic users to truthfully reveal their private quality and truthfully make efforts as desired by the requester. The QEE mechanisms achieve the truthful design by overcoming the intricate dependency of a user's data on its private quality and hidden effort. Under the QEE mechanisms, we show that the crowdsensing requester's optimal (RO) effort assignment assigns effort only to the best user that has the smallest “virtual valuation”, which depends on the user's quality and the quality's distribution. We also show that, as the number of users increases, the performance gap between the RO effort assignment and the socially optimal effort assignment decreases, and converges to 0 asymptotically. We further discuss some extensions of the QEE mechanisms. Simulation results demonstrate the truthfulness of the QEE mechanisms and the system efficiency of the RO effort assignment.",2019,,10.1109/TNET.2019.2934026
292,"Cao, Wei and Wu, Zhengwei and Wang, Dong and Li, Jian and Wu, Haishan",Automatic user identification method across heterogeneous mobility data sources,Trajectory;Urban areas;Buildings;Noise measurement;Mobile communication;Navigation;Education,"With the ubiquity of location based services and applications, large volume of mobility data has been generated routinely, usually from heterogeneous data sources, such as different GPS-embedded devices, mobile apps or location based service providers. In this paper, we investigate efficient ways of identifying users across such heterogeneous data sources. We present a MapReduce-based framework called Automatic User Identification (AUI) which is easy to deploy and can scale to very large data set. Our framework is based on a novel similarity measure called the signal based similarity (SIG) which measures the similarity of users' trajectories gathered from different data sources, typically with very different sampling rates and noise patterns. We conduct extensive experimental evaluations, which show that our framework outperforms the existing methods significantly. Our study on one hand provides an effective approach for the mobility data integration problem on large scale data sets, i.e., combining the mobility data sets from different sources in order to enhance the data quality. On the other hand, our study provides an in-depth investigation for the widely studied human mobility uniqueness problem under heterogeneous data sources.",2016,,10.1109/ICDE.2016.7498306
293,"Georgieva, P. and Nikolova, E. and Orozova, D.",Data Cleaning Techniques in Detecting Tendencies in Software Engineering,Training;Data integrity;Tools;Strategic planning;Cleaning;Monitoring;Software engineering;Big Data Analytics;Data quality;Data cleaning;Software engineering,"The world of software engineering is dynamically changing over the last decade. Providing adequate university education is one of the key goals of the academic community for ensuring advanced and up-to-date students' training. One direction in achieving this goal is to constantly monitor the trends in the Information Technology (IT) sector. A reliable source of information is the data from the annual survey on technology and programming languages, as well as on preferred learning methods and ways to enhance competencies, conducted amongst Stack Overflow users since 2011. In processing the data from the survey, the authors have faced several problems that have provoked interest in a more general data problem - data quality and data cleaning.This paper looks into data quality, tools for data cleaning and the characteristics of high-quality data. A classification of data problems is proposed in the context of analyzing the information about software developers. Additionally, the proposed process of data cleaning in illustrated with data for 2018 and 2019.",2020,,10.23919/MIPRO48935.2020.9245416
294,"Luo, Xin and Zhou, MengChu and Li, Shuai and Xia, YunNi and You, Zhu-Hong and Zhu, QingSheng and Leung, Hareton",Incorporation of Efficient Second-Order Solvers Into Latent Factor Models for Accurate Prediction of Missing QoS Data,Quality of service;Predictive models;Optimization;Computational modeling;Mathematical model;Data models;Web services;Big data;latent factor model;missing data prediction;quality-of-service (QoS);second-order solver;service computing sparse matrices;Web service,"Generating highly accurate predictions for missing quality-of-service (QoS) data is an important issue. Latent factor (LF)-based QoS-predictors have proven to be effective in dealing with it. However, they are based on first-order solvers that cannot well address their target problem that is inherently bilinear and nonconvex, thereby leaving a significant opportunity for accuracy improvement. This paper proposes to incorporate an efficient second-order solver into them to raise their accuracy. To do so, we adopt the principle of Hessian-free optimization and successfully avoid the direct manipulation of a Hessian matrix, by employing the efficiently obtainable product between its Gauss-Newton approximation and an arbitrary vector. Thus, the second-order information is innovatively integrated into them. Experimental results on two industrial QoS datasets indicate that compared with the state-of-the-art predictors, the newly proposed one achieves significantly higher prediction accuracy at the expense of affordable computational burden. Hence, it is especially suitable for industrial applications requiring high prediction accuracy of unknown QoS data.",2018,,10.1109/TCYB.2017.2685521
295,"Cai, Xumin and Aydin, Berkay and Maydeo, Saurabh and Ji, Anli and Angryk, Rafal",Local Outlier Detection for Multi-type Spatio-temporal Trajectories,Machine learning;Big Data;Feature extraction;Spatial databases;Trajectory;Task analysis;Anomaly detection,"Outlier detection has become one of the core tasks in spatio-temporal data mining. It plays an essential role in data quality improvement for the machine learning models and recognizing the anomalous patterns, which may remarkably deviate from expected patterns among the trajectory datasets. In this work, we propose a clustering-based technique to detect local outliers in trajectory datasets by utilizing spatial and temporal attributes of moving objects. This local outlier detection involves three phases. In the first phase, we apply a temporal partition procedure to divide the raw trajectory into multiple trajectory segments and extract trajectory features from spatial and temporal attributes for each trajectory segment. Then, we generate template features of trajectory segments by applying a clustering schema in the second phase. Finally, we use the abnormal score - a novel dissimilarity measure, which quantifies the disparity among the query and template trajectory segments in terms of trajectory features and hence determines the local outliers based on the distribution of abnormal score. To demonstrate the effectiveness of our method, we conduct three case studies on the real-life spatio-temporal trajectory datasets from the solar astroinformatics domain (i.e., solar active regions, coronal mass ejections, polarity inversion lines (PIL)). Our experimental results show that our local outlier detection approach can effectively discover the erroneous reports from the reporting module and abnormal phenomenon in various spatio-temporal trajectory datasets.",2020,,10.1109/BigData50022.2020.9377801
296,"Wang, Jianmin and Song, Shaoxu and Zhu, Xiaochen and Lin, Xuemin and Sun, Jiaguang",Efficient Recovery of Missing Events,Business;Petri nets;Engineering drawings;Indexes;Routing;Sun;Data mining;Data repairing;event data processing;petri net,"For various entering and transmission issues raised by human or system, missing events often occur in event data, which record execution logs of business processes. Without recovering the missing events, applications such as provenance analysis or complex event processing built upon event data are not reliable. Following the minimum change discipline in improving data quality, it is also rational to find a recovery that minimally differs from the original data. Existing recovery approaches fall short of efficiency owing to enumerating and searching over all of the possible sequences of events. In this paper, we study the efficient techniques for recovering missing events. According to our theoretical results, the recovery problem appears to be NP-hard. Nevertheless, advanced indexing, pruning techniques are developed to further improve the recovery efficiency. The experimental results demonstrate that our minimum recovery approach achieves high accuracy, and significantly outperforms the state-of-the-art technique for up to five orders of magnitudes improvement in time performance.",2016,,10.1109/TKDE.2016.2594785
297,"Gao, Jian and Zhen, Yan and Bai, Huifeng and Huo, Chao and Wang, Dongshan and Zhang, Ganghong",Research and Analysis Validation of Data Fusion Technology Based on Edge Computing,Data integration;Power grids;Kalman filters;Monitoring;Big Data;Data models;Distributed databases;smart grid;multi-source data;edge computing;filtering algorithm,"Based on the smart grid as the research background, this paper responded to the massive multi-source data processing requirements of the smart grid, and combined with distributed computing to provide the edge of the solution, aiming at the existing data of electric power equipment state monitoring data in noise and redundant data problems. A distributed kalman filter algorithm based on edge of computing was put forward. In this algorithm, event decision strategy was added to the data processing and transmission process of edge computing terminal to control the communication times between nodes and terminals in an event-driven way. Meanwhile, redundant data and data interfered by noise were reduced through the processing of the algorithm, so as to ensure the data quality and improve the fusion efficiency. Finally, the effectiveness of the method was verified by the analysis of compression efficiency and data fusion time.",2019,,10.1109/ICCC47050.2019.9064032
298,"Tayeb, Shahab and Pirouz, Matin and Cozzens, Brittany and Huang, Richard and Jay, Maxwell and Khembunjong, Kyle and Paliskara, Sahan and Zhan, Felix and Zhang, Mark and Zhan, Justin and Latifi, Shahram",Toward data quality analytics in signature verification using a convolutional neural network,Neural networks;Image recognition;Feature extraction;Forgery;Machine learning;convolutional neural network;handwriting;deep learning;signature authentication;signature verification;machine learning;image classifier,"Many studies have been conducted on Handwritten Signature Verification. Researchers have taken many different approaches to accurately identify valid signatures from skilled forgeries, which closely resemble the real signature. The purpose of this paper is to suggest a method for validating written signatures on bank checks. This model uses a convolutional neural network (CNN) to analyze pixels from a signature image to recognize abnormalities. We believe the feature extraction capabilities of a CNN can optimize processing time and feature analysis of signature verification. Unique characteristics from signatures can be accurately and rapidly analyzed with multiple layers of receptive fields and hidden layers. Our method was able to correctly detect the validity of the inputted signature approximately 83 percent of the time. We tested our method using the SIGCOMP 2011 dataset. The main contribution of this method is to detect and decrease fraud committed, especially in the banking industry. Future uses of signature verification could include legal documents and the justice system.",2017,,10.1109/BigData.2017.8258225
299,"Lu, Xin and Wang, Yu and Yuan, Jiao and Wang, Xun and Fu, Kun and Yang, Ke",A Parallel Adaptive DBSCAN Algorithm Based on k-Dimensional Tree Partition,Machine learning algorithms;Merging;Clustering algorithms;Data structures;Partitioning algorithms;Sparks;Data mining;clustering analysis;data partition;k-dimensional tree;adaptive computing;Spark framework,"The existing parallel DBSCAN (density based spatial clustering of applications with noise) algorithm needs to determine the parameter settings manually, and the datasets will be repeatedly accessed in the process of data partitioning and data merging, which reduces the efficiency of the algorithm excuting. Therefore, this paper proposes a parallel adaptive DBSCAN algorithm based on k-dimensional tree partition. It divides the dataset into several balanced data partitions by using k-dimensional tree, and carries out parallel computing in spark distributed computing framework, thus increasing the concurrent processing ability of the algorithm program and improving the I/O access speed. In addition, the improved adaptive DBSCAN parameter method is applied to each data partition for clustering analysis to obtain local clusters, which solves the random problem of manual setting parameters in the clustering process, and ensures the data quality of clustering mining. At the same time of creating local clusters, this algorithm also puts the mapping relationship between data points and adjacent points into the HashMap data structure of the master node, and uses it to merge local clusters into whole clusters, which can reduce the time cost of data merging. The experimental results show that the proposed algorithm can save about 18% running time compared with RDD-DBSCAN algorithm without reducing the clustering quality. With the increase of the number of cluster nodes, the running efficiency of the algorithm can be further improved, so it is suitable for processing massive data clustering analysis.",2020,,10.1109/MLBDBI51377.2020.00053
300,"Qiao, Lin and Chen, Shuo and Bo, Jue and Liu, Sai and Ma, Guiwei and Wang, Haixin and Yang, Junyou",Wind power generation forecasting and data quality improvement based on big data with multiple temporal-spatual scale,Wind speed;Support vector machines;Biological neural networks;Indexes;Wind turbines;Mathematical model,"Wind energy is one of the renewable energy sources with a large number of installations in the world. The accuracy of power generation prediction using wind speed data severely challenges the regulation and safe operation of power system. Since there are many time points in the dispatching strategy of power system, which is related to the area condition. It is of great significance for power grid dispatching to be able to timely and accurately predict the generation capacity of wind turbines in a certain period. Due to the randomness and intermittency of wind speed, the accuracy of data quality will be influenced greatly. In this paper, a neural network algorithm based on combination of back propagation (BP) and Newton interpolation mathematical function method is proposed to effectively process wind speed data, so as to predict power generation. BP neural network is a kind of multi-layer feedforward neural network including a hidden layer, which can solve the learning problem of hidden layer connection weight in a multi-layer network. From the perspective of space scale, this paper studies different wind speed data at different heights in the same area. Research results show: compared with the traditional support vector machine method, the accuracy with the proposed method is improved by 3.1%.",2019,,10.1109/ICEI.2019.00104
301,"Saberi, Morteza and Hussain, Omar Khadeer and Chang, Elizabeth",Quality Management of Workers in an In-House Crowdsourcing-Based Framework for Deduplication of Organizations’ Databases,Crowdsourcing;Databases;Task analysis;Object recognition;Monitoring;Error analysis;Big Data;Quality management;quality control;data quality;duplicate detection;in-house crowdsourcing,"While organizations in the current era of big data are generating massive volumes of data, they also need to ensure that its quality is maintained for it to be useful in decision-making purposes. The problem of dirty data plagues every organization. One aspect of dirty data is the presence of duplicate data records that negatively impact the organization's operations in many ways. Many existing approaches attempt to address this problem by using traditional data cleansing methods. In this paper, we address this problem by using an in-house crowdsourcing-based framework, namely, DedupCrowd. One of the main obstacles of crowdsourcing-based approaches is to monitor the performance of the crowd, by which the integrity of the whole process is maintained. In this paper, a statistical quality control-based technique is proposed to regulate the performance of the crowd. We apply our proposed framework in the context of a contact center, where the Customer Service Representatives are used as the crowd to assist in the process of deduplicating detection. By using comprehensive working examples, we show how the different modules of the DedupCrowd work not only to monitor the performance of the crowd but also to assist in duplicate detection.",2019,,10.1109/ACCESS.2019.2924979
302,"Marone, Reine Marie and Camara, Fodé and Ndiaye, Samba",A large-scale filter method for feature selection based on spark,Feature extraction;Mutual information;Sparks;Algorithm design and analysis;Redundancy;Big Data;Classification algorithms;feature selection;filter method;parallel computing;apache spark;mRMR;SVM,"Recently, enormous volumes of data are generated in information systems. That's why data mining area is facing new challenges of transforming this “big data” into useful knowledge. In fact, “big data” relies low density of information (low data quality) and data redundancy, which negatively affect the data mining process. Therefore, when the number of variables describing the data is high, features selection methods are crucial for selecting relevant data. Features selection is the process of identifying the most relevant variables and removing those are redundant and irrelevant. In this paper, we propose a parallel, scalable feature selection algorithm based on mRMR (Max-Relevance and Min-Redundancy) in Spark, an in-memory parallel computing framework specialized in computation for large distributed datasets. Our experiments using real-world data of high dimensionality demonstrated that our proposition scale well and efficiently with large datasets.",2017,,10.1109/ISCMI.2017.8279590
303,"Zhu, Hong and Bayley, Ian and Younas, M. and Lightfoot, David and Yousef, Basel and Liu, Dongmei",Big SaaS: The Next Step beyond Big Data,Software as a service;Checkpointing;Fault tolerance;Fault tolerant systems;Ontologies;Computer architecture,"Software-as-a-Service (SaaS) is a model of cloud computing in which software functions are delivered to the users as services. The past few years have witnessed its global flourishing. In the foreseeable future, SaaS applications will integrate with the Internet of Things, Mobile Computing, Big Data, Wireless Sensor Networks, and many other computing and communication technologies to deliver customizable intelligent services to a vast population. This will give rise to an era of what we call Big SaaS systems of unprecedented complexity and scale. They will have huge numbers of tenants/users interrelated in complex ways. The code will be complex too and require Big Data but provide great value to the customer. With these benefits come great societal risks, however, and there are other drawbacks and challenges. For example, it is difficult to ensure the quality of data and metadata obtained from crowd sourcing and to maintain the integrity of conceptual model. Big SaaS applications will also need to evolve continuously. This paper will discuss how to address these challenges at all stages of the software lifecycle.",2015,,10.1109/CLOUD.2015.167
304,"Loh, Ji Meng and Dasu, Tamraparni",Effect of Data Repair on Mining Network Streams,Maintenance engineering;Data mining;Cleaning;Measurement;Time series analysis;Context;Information management;Data glitches;Big Data;missing values;outliers;network analysis;Earth Mover Distance,"Data quality issues have special implications in network data. Data glitches are propagated rapidly along pathways dictated by the hierarchy and topology of the network. In this paper, we use temporal data from a vast data network to study data glitches and their effect on network monitoring tasks such as anomaly detection. We demonstrate the consequences of cleaning the data, and develop targeted and customized cleaning strategies by exploiting the network hierarchy.",2012,,10.1109/ICDMW.2012.125
305,"Shioiri, Satoshi and Sato, Yoshiyuki and Horaguchi, Yuta and Muraoka, Hiroaki and Nihei, Mariko",Quali-Informatics in the Society with Yotta Scale Data,Productivity;Circuits and systems;Big Data;Acceleration;Information technology;data quality;human judgments,"Accumulation of information is essential for human knowledge production, and information technology has accelerated the speed of data accumulation. The increase in quantity of information with high speed does not promise high-quality knowledge production and possibly does cause problems. One big problem is lack of storage for such big data. Another critical problem in information usage is information overload, that is, deterioration of productivity by too much information. Decision accuracy decrease with amount of information beyond a certain point while it increases at the beginning. We introduce an approach for solution of these problems with an example of research along the approach.",2021,,10.1109/ISCAS51556.2021.9401161
306,"Lu, Jian and Li, Wei and Wang, Qingren and Zhang, Yiwen",Research on Data Quality Control of Crowdsourcing Annotation: A Survey,Crowdsourcing;Training;Annotations;Big Data;Tools;Inference algorithms;Task analysis;label;truth inference;learning model;crowdsourcing,"It is well known that many intelligent and computer-hard tasks cannot be effectively addressed by existing machine-based approaches, so that it is nature to think of utilizing the intelligence of human being. With the popularization of crowdsourcing concepts as well as the development of crowdsourcing platforms, as a new way of human intelligence to participate in machine computing, crowdsourcing annotation helps more and more supervised-learning-based approaches easily obtain enormous labeled data with relatively low cost. However, because of the diversity of the crowd employed by crowdsourcing platforms, how to control qualities of labels coming from the crowd plays a key role in crowdsourcing annotation. In this survey, we first present basic concepts and definitions of crowdsourcing annotation. Then, we review existing ground truth inference algorithms and learning models. After that, the advantages and distinctions among these algorithms and learning models as well as the levels of study progresses will be reported. And finally, we summarize realworld datasets widely utilized in the field of crowdsourcing annotation as well as available open source tools.",2020,,10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00044
307,"Wang, Qiyao and Wang, Haiyan and Gupta, Chetan and Serita, Susumu",Regularized Operating Envelope with Interpretability and Implementability Constraints,Genetic algorithms;Machine learning;Oils;Optimization;Search problems;Big Data;Quality assessment;Operating envelope;Genetic algorithm;Penalty approach;Generalization,"Operating envelope is an important concept in industrial operations. Accurate identification for operating envelope can be extremely beneficial to stakeholders as it provides a set of operational parameters that optimizes some key performance indicators (KPI) such as product quality, operational safety, equipment efficiency, environmental impact, etc. Given the importance, data-driven approaches for computing the operating envelope are gaining popularity. These approaches typically use classifiers such as support vector machines, to set the operating envelope by learning the boundary in the operational parameter spaces between the manually assigned `large KPI' and `small KPI' groups. One challenge to these approaches is that the assignment to these groups is often ad-hoc and hence arbitrary. However, a bigger challenge with these approaches is that they don't take into account two key features that are needed to operationalize operating envelopes: (i) interpretability of the envelope by the operator and (ii) implementability of the envelope from a practical standpoint. In this work, we propose a new definition for operating envelope which directly targets the expected magnitude of KPI (i.e., no need to arbitrarily bin the data instances into groups) and accounts for the interpretability and the implementability. We then propose a regularized `GA +penalty' algorithm that outputs an envelope where the user can tradeoff between bias and variance. The validity of our proposed algorithm is demonstrated by two sets of simulation studies and an application to a real-world challenge in the mining processes of a flotation plant.",2019,,10.1109/BigData47090.2019.9005484
308,"Brahim, Mohamed Ben and Menouar, Hamid",Optimizing V2X Data Collection and Storage for a Better Cost and Quality Trade-off,Sensors;Containers;Roads;Data integrity;Vehicle dynamics;Data mining;Memory;V2X wireless communication;vehicular edge computing;data storage;data quality;dimension reduction;data sampling,"Future vehicles will be equipped with advanced communication capabilities and a multitude of sensing devices. Vehicle-to-vehicle and to Infrastructure (V2X) is one of these future technologies. V2X-technology-enabled vehicles are expected to become a great source of big data. This data, if gathered in the right time and processed in the right way, can enable an interesting number of existing and new applications. This can be a challenging task, taken into account the considerable size of the data that will be gathered. One of the challenges is to find a good balance between the number of data to filter out and the quality of the end data. This contribution tackles this specific challenge, by studying data storage cost reduction and evaluating its impact on the data quality. The proposed solution compares three approaches of treating the collected data at the road-side unit after taking out unnecessary information details. This solution has been tested and validated through simulations that show promising results.",2017,,10.1109/ICAdLT.2017.8547038
309,"Zhang, Lichen",Specifying and Modeling Cloud Cyber Physical Systems Based on AADL,Cloud computing;Computational modeling;Unified modeling language;Analytical models;Computer architecture;Software;Object oriented modeling;cloud;CPS;big data;AADL;specification,"In cyber physical systems(CPS), the physical world and the information world are merged to form a new structure that combines both hardware and software, and become the core technology system that supports and leads the transformation of a new generation of industries. With the rapid development of network technology, the data generated has also increased rapidly, which means that today's information society has entered the era of big data, and the technology of adapting to the cloud platform has gradually matured. The cloud computing platform provides flexible and relatively inexpensive storage space and computing resources for the development of big data technology. This also provides basic support for the development of big data driven CPS based on the cloud platform. In this paper, we specify and model cloud cyber physical systems based on AADL, which can specify, model, and analyze cloud cyber physical systems, finally implement cyber physical systems on cloud platforms, provide availability analysis, reliability analysis. data quality analysis, real-time performance analysis, security analysis and resource consumption analysis.",2018,,10.1109/DCABES.2018.00017
310,Guang Wei and Hailong Yang and Zhongzhi Luan and Depei Qian,iDPL: A scalable and flexible inter-continental testbed for data placement research and experiment,Tools;Synchronization;Data visualization;Protocols;Distributed databases;Computers;Software,"In this paper, we propose the China-US international data placement laboratory (iDPL) based on an inter-continental testbed for data placement research. iDPL is able to support various data placement research due to its scalability and flexibility in deploying the experiments in the real network environment. The core design of iDPL leverages reliable workflow management and lightweight I/O protocol to allow complex experiment setup and on-the-fly experiment deployment. It is also extensible to plugin different network profiling tools such as iperf. We expect the powerful measurement capability of iDPL promotes research study on the intelligent data placement policies which adapt to the uncertainty of the wide-area network and guarantee the quality of service (QoS) of the big data applications. As a case study, we setup a set of data placement experiments to measure the end-to-end network performance constantly among several sites between China and US using different data placement tools. The experiments have been running for more than one year, and its measurement data is public available (http://mickey.buaa.edu.cn:8080/). We believe the measurement data is valuable for both network and big data researchers to understand the performance disparity between the raw network and the actual data placement, which provides useful insights to design big data applications with performance awareness. We encourage more researchers to deploy their own data placement experiments on iDPL, expediting the research direction of intelligent data placement with real network environment.",2017,,10.1109/ISCC.2017.8024681
311,"Soe, Thin Thin and Min, Myat Myat",Speeding up Incomplete Data Analysis using Matrix-Represented Approximations,Rough sets;Data analysis;Approximation algorithms;Big Data;Data integrity;Tools;rough set;incomplete data;missing values;approximations;matrix,"The veracity related with data quality such as incomplete, imprecise and inconsistent data creates a major challenge to data mining and data analysis. Rough set theory provides a special tool for handling the imprecise and incomplete data in information systems. However, the existing rough set based incomplete data analysis methods may not be able to handle large amount of data within the acceptable time. This paper focuses on speeding up the incomplete data analysis. The computation of the lower and upper approximations is a vital step for improving the performance of rough set based data analysis process. In this paper, the lower and upper approximations are characterized as matrix-represented approximations. The resulting approximations are exploited as inputs for data analysis method LERS (Learning from Examples based on Rough Set) used with LEM2 (Learning from Examples Module, Version2) rule induction algorithm. Then, this paper provides a set of experiments on missing datasets with different missing percent. The experimental results on incomplete or missing datasets from UCI Machine Learning Repository show that the proposed system effectively reduces the computational time in comparison with the existing system.",2018,,10.1109/SNPD.2018.8441160
312,Fan Xiaojiang and Zheng Liwei and Liu Jianbin,Measurement for social network data currency and trustworthiness,Facsimile;social network service;data quality;currency;trustworthiness,"Along with the explosive growth of the information in Social Network Service, the research of the quality of data has become a new hot point in related research field. High quality social data can more effectively support data mining, knowledge discovery, and can provide reliable and efficient data for users. Based on the measure problems of data quality, this paper discussed the measurement of two important dimensions of data quality: currency and trustworthiness. Computing models for currency measurement of data with or without time stamp are given. And based on the currency values, a trustworthiness measurement method is also given.",2017,,10.1109/ICCCBDA.2017.7951874
313,"Fähnrich, Cindy and Schapranow, Matthieu-P. and Plattner, Hasso",Towards integrating the detection of genetic variants into an in-memory database,Genomics;Bioinformatics;Databases;Sequential analysis;Runtime;Biological cells;Instruction sets;Genome Data Analysis;Variant Calling;Single Nucleotide Polymorphism;In-Memory Database Technology;Next-Generation Sequencing,"Next-generation sequencing enables whole genome sequencing within a few hours at a minimum of cost, entailing advanced medical applications such as personalized treatments. However, this recent technology imposes new challenges to alignment and variant calling as subsequent analysis steps. Compared to former sequencing, both must deal with an increasing amount of data to process at a significantly lower data quality - and are currently not capable of that. In this work, we focus on addressing these challenges for identifying Single Nucleotide Polymorphisms, i.e. SNP calling, in genome data as one subtask of variant calling. We propose the application of a column-store in-memory database for efficient data processing and apply the statistical model that is provided by the Genome Analysis Toolkit's UnifiedGenotyper. Comparisons with the UnifiedGenotyper show that our approach can exploit all computational resources available and accelerates SNP calling up to a factor of 22x.",2014,,10.1109/BigData.2014.7004389
314,"Lv, Yirong and Sun, Bin and Luo, Qingyi and Wang, Jing and Yu, Zhibin and Qian, Xuehai",CounterMiner: Mining Big Performance Data from Hardware Counters,Hardware;Time series analysis;Program processors;Cloud computing;Microarchitecture;Data mining;Benchmark testing;Performance;big data;computer architecture;performance counters;data mining,"Modern processors typically provide a small number of hardware performance counters to capture a large number of microarchitecture events. These counters can easily generate a huge amount (e.g., GB or TB per day) of data, which we call big performance data in cloud computing platforms with more than thousands of servers and millions of complex workloads running in a ""24/7/365"" manner. The big performance data provides a precious foundation for root cause analysis of performance bottlenecks, architecture and compiler optimization, and many more. However, it is challenging to extract value from the big performance data due to: 1) the many unperceivable errors (e.g., outliers and missing values); and 2) the difficulty of obtaining insights, e.g., relating events to performance. In this paper, we propose CounterMiner, a rigorous methodology that enables the measurement and understanding of big performance data by using data mining and machine learning techniques. It includes three novel components: 1) using data cleaning to improve data quality by replacing outliers and filling in missing values; 2) iteratively quantifying, ranking, and pruning events based on their importance with respect to performance; 3) quantifying interaction intensity between two events by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from the Spark version of HiBench) to evaluate CounterMiner. The experimental results show that CounterMiner reduces the average error from 28.3% to 7.7% when multiplexing 10 events on 4 hardware counters. We also conduct a real-world case study, showing that identifying important configuration parameters of Spark programs by event importance is much faster than directly ranking the importance of these parameters.",2018,,10.1109/MICRO.2018.00056
315,"Kumar, Abhishek and Braud, Tristan and Tarkoma, Sasu and Hui, Pan",Trustworthy AI in the Age of Pervasive Computing and Big Data,Artificial intelligence;Ethics;Data privacy;Biological system modeling;Training;Pervasive computing;Robustness;Artificial Intelligence;Pervasive Computing;Ethics;Data Fusion;Transparency;Privacy;Fairness;Accountability;Federated Learning,"The era of pervasive computing has resulted in countless devices that continuously monitor users and their environment, generating an abundance of user behavioural data. Such data may support improving the quality of service, but may also lead to adverse usages such as surveillance and advertisement. In parallel, Artificial Intelligence (AI) systems are being applied to sensitive fields such as healthcare, justice, or human resources, raising multiple concerns on the trustworthiness of such systems. Trust in AI systems is thus intrinsically linked to ethics, including the ethics of algorithms, the ethics of data, or the ethics of practice. In this paper, we formalise the requirements of trustworthy AI systems through an ethics perspective. We specifically focus on the aspects that can be integrated into the design and development of AI systems. After discussing the state of research and the remaining challenges, we show how a concrete use-case in smart cities can benefit from these methods.",2020,,10.1109/PerComWorkshops48775.2020.9156127
316,"Kraus, Naama and Carmel, David and Keidar, Idit",Fishing in the stream: Similarity search over endless data,Redundancy;Heuristic algorithms;Indexing;Approximation algorithms;Measurement;Runtime;Similarity search;Stream search;Retention policy;Locality sensitive hashing;Dynamic popularity,"Similarity search is the task of retrieving data items that are similar to a given query. In this paper, we introduce the time-sensitive notion of similarity search over endless data-streams (SSDS), which takes into account data quality and temporal characteristics in addition to similarity. SSDS is challenging as it needs to process unbounded data, while computation resources are bounded. We propose Stream-LSH, a randomized SSDS algorithm that bounds the index size by retaining items according to their freshness, quality, and dynamic popularity attributes. We show that Stream-LSH increases recall when searching for similar items compared to alternative approaches using the same space capacity.",2017,,10.1109/BigData.2017.8258016
317,"Suresh, T. and Murugan, A.",Strategy for Data Center Optimization : Improve Data Center capability to meet business opportunities,Data centers;Cloud computing;Optimization;Organizations;Servers;Maintenance engineering;Data Center;energy saving;green computing;server;network devices;cloud storage,"Considering current evolving technology and the way data are growing, IT consulting and outsourcing industry expected to be strategic partner for technology innovation in addition to support on-going business with reduced operational cost. Data Center is backbone for digital economy, big data, cloud, artificial intelligence, IoT or wearable technology. Data growth and on-demand data access changed the focus of data center as storage and disaster recovery to access data instantly from cloud without compromising security controls and data quality. These technology transformations create demand for latency. Every organization like Facebook, Equinix, Amazon, and Google are having their own data centers and expanding their business on cloud services. Data Center plays major critical on success of digital business. It is important to find possible options to optimize infrastructure and improve efficiency and productivity of Data Center. At the same time, we need to make sure that environment is up and running without compromising quality and security of data. This paper gives few solutions to get more from Data Center, reduce operational cost and optimize infrastructure utilization.",2018,,10.1109/I-SMAC.2018.8653702
318,"Haneem, Faizura and Ali, Rosmah and Kama, Nazri and Basri, Sufyan",Descriptive analysis and text analysis in Systematic Literature Review: A review of Master Data Management,Text analysis;Databases;Technological innovation;Frequency-domain analysis;Text mining;Quality assessment;Systematic Literature Review;Descriptive Analysis;Text Analysis;Master Data Management,"Systematic Literature Review (SLR) is a structured way of conducting a review of existing research works produced by the earlier researchers. The application of right data analysis technique during the SLR evaluation stage would give an insight to the researcher in achieving the SLR objective. This paper presents how descriptive analysis and text analysis can be applied to achieve one of the common SLR objectives which is to study the progress of specific research domain. These techniques have been demonstrated to synthesis the progress of Master Data Management research domain. Using descriptive analysis technique, this study has identified a trend of related literary works distribution by years, sources, and publication types. Meanwhile, text analysis shows the common terms and interest topics in the Master Data Management research which are 1) master data, 2) data quality, 3) business intelligence, 4) business process, 5) data integration, 6) big data, 7) data governance, 8) information governance, 9) data management and 10) product data. It is hoped that other researchers would be able to replicate these analysis techniques in performing SLR for other research domains.",2017,,10.1109/ICRIIS.2017.8002473
319,"Al-Sabbagh, Khaled Walid and Staron, Miroslaw and Hebig, Regina and Meding, Wilhelm",Improving Data Quality for Regression Test Selection by Reducing Annotation Noise,Training;Testing;Annotations;Predictive models;Noise reduction;Feature extraction;Dictionaries;Annotation Noise;Regression Testing;Machine Learning Models,"Big data and machine learning models have been increasingly used to support software engineering processes and practices. One example is the use of machine learning models to improve test case selection in continuous integration. However, one of the challenges in building such models is the identification and reduction of noise that often comes in large data. In this paper, we present a noise reduction approach that deals with the problem of contradictory training entries. We empirically evaluate the effectiveness of the approach in the context of selective regression testing. For this purpose, we use a curated training set as input to a tree-based machine learning ensemble and compare the classification precision, recall, and f-score against a non-curated set. Our study shows that using the noise reduction approach on the training instances gives better results in prediction with an improvement of 37% on precision, 70% on recall, and 59% on f-score.",2020,,10.1109/SEAA51224.2020.00042
320,"Yang, Dazhi and Zhang, Allan N. and Yan, Wenjing","Performing literature review using text mining, Part I: Retrieving technology infrastructure using Google Scholar and APIs",Google;Metadata;Logistics;Uniform resource locators;Transportation;Databases;Text mining;technology infrastructure;text mining;last mile logistics;Google Scholar,"Technology infrastructure (TechInfra) refers to metadata describing an academic field, such as journals & conferences, authors, publications and organizations. Understanding the TechInfra is often the first step in performing a literature review on a particular topic. In this paper, a study is conducted to retrieve TechInfra for a topic in supply chain management, namely, last mile logistics. Google Scholar is used as the primary tool for data collection. The first 1,000 results returned by Google Scholar are downloaded as HTML files. Subsequently, various application programming interfaces (APIs) - e.g., ScienceDirect, IEEE, CrossRef APIs - are used to enhance the data quality. Some plots are used to provide visualization of TechInfra of last mile logistics.",2017,,10.1109/BigData.2017.8258313
321,"Lee, Jay",Transformation: Case Studies and Lessons Learned : Keynote 2,,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. Industrial AI, Big Data Analytics, Machine Learning, and Cyber Physical Systems are changing the way we design product, manufacturing, and service systems. It is clear that as more sensors and smart analytics software are integrated in the networked industrial products and manufacturing systems, predictive technologies can further learn and autonomously optimize service productivity and performance. This presentation will address the trends of Industrial AI for smart service realization. First, Industrial AI systematic approach will be introduced. Case studies on advanced predictive analytics technologies for different maintenance and service operations will be demonstrated. In addition, issues on data quality for high performance and real-time data analytics in future digital service will be discussed.",2021,,10.1109/SERVICES51467.2021.00055
322,"Zhang, Yang and Guo, Hanqi and Shang, Lanyu and Wang, Dong and Peterka, Tom",A Multi-branch Decoder Network Approach toAdaptive Temporal Data Selection andReconstruction for Big Scientific Simulation Data,Data models;Adaptation models;Computational modeling;Analytical models;Image reconstruction;Data integrity;Adaptive systems;Big Scientific Simulation Data;Adaptive Temporal Data Selection and Reconstruction;Multi-branch Decoder Network,"A key challenge in scientific simulation is that the simulation outputs often require intensive I/O and storage space to store the results for effective post hoc analysis. This paper focuses on a quality-aware adaptive temporal data selection and reconstruction problem where the goal is to adaptively select simulation data samples at certain key timesteps in situ and reconstruct the discarded samples with quality assurance during post hoc analysis. This problem is motivated by the limitation of current solutions that a significant amount of simulation data samples are either discarded or aggregated during the sampling process, leading to inaccuratemodeling of the simulated phenomena. Two unique challenges exist: 1) the sampling decisions have to be made in situ and adapted tothe dynamics of the complex scientific simulation data; 2) the reconstruction error must be strictly bounded to meet the application requirement. To address the above challenges, we developDeepSample, an error-controlled convolutional neural network framework, that jointly integrates a set of coherent multi-branch deep decoders to effectively reconstruct the simulation data with rigorous quality assurance. The results on two real-world scientific simulation applications show that DeepSample significantly outperforms other state-of-the-art methods on both sampling efficiency and reconstructed simulation data quality.",2021,,10.1109/TBDATA.2021.3092174
323,"Zhi, Yanling and Liu, Gang and Wang, Huimin",Research on data gray correction model based on grey interval number — A case study of Chinese ecological civilization evaluation,Biological system modeling;Economic indicators;Data models;Educational institutions;Accuracy;Mathematical model;Analytical models;data quality;gray interval number;ecological civilization evaluation,"In the age of Big Data, we must do best to economically extract value from very large volumes of a wide variety of statistics. However, because of subjective and objective reasons, it is becoming increasing clear that much data is of poor quality, which has serious effects on the research results. With the analysis on the cause and process of the low quality data, this paper introduces the concept of gray system and proposes a data-gray-correction model, which could change the original data into gray interval number and reduce the influence from the former. Assessing the quality of data with classical econometric model and correcting the data by error correction model, then find the reasonable range of the real data and instead the crisp data by interval number, which contain much more information. An example is provided to illustrate the ecological civilization evaluation process under gray interval number.",2014,,10.1109/ICSSSM.2014.6943406
324,"Yan, Xiaowen and Zhou, Yu and Huang, Fuxing and Wang, Xiaofen and Yuan, Peisen",Privacy protection method of power metering data in clustering based on differential privacy,Differential privacy;Analytical models;Power measurement;Laplace equations;Data integrity;Conferences;Companies;power metering data;cluster analysis;differential privacy;Laplacian noise,"Power companies can use the power grid big data platform to cluster analysis of power metering data, which can improve the personalized service quality of power grid companies for different users and discover the power stealing behavior of users to protect the interests of power grid companies. However, in the cluster analysis of power measurement data, the privacy information of power users may also be disclosed. To defend the privacy information of power users, the article applies differential privacy technology to cluster analysis of power metering data to avoid power users’ privacy leakage. First, the article presents the attack model that exists in the cluster analysis of power metering data. Then, the article add Laplacian noise to the power metering data to defend against attacks in the cluster analysis of attackers. Next, to enhance the data availability of noise-added power measurement data in cluster analysis, the article limits noise distance based on the results of the cluster analysis. Experiments show that method proposed in article can guarantee the privacy information of power data during the cluster analysis of power metering data, and ensure the data quality of the power metering data after privacy protection.",2021,,10.1109/CIEEC50170.2021.9510797
325,"Yang, Longzhi and Neagu, Daniel",Integration strategies for toxicity data from an empirical perspective,Reliability;Mathematical model;Data integration;Bayes methods;Equations;Uncertainty,"The recent development of information techniques, especially the state-of-the-art “big data” solutions, enables the extracting, gathering, and processing large amount of toxicity information from multiple sources. Facilitated by this technology advance, a framework named integrated testing strategies (ITS) has been proposed in the predictive toxicology domain, in an effort to intelligently jointly use multiple heterogeneous toxicity data records (through data fusion, grouping, interpolation/extrapolation etc.) for toxicity assessment. This will ultimately contribute to accelerating the development cycle of chemical products, reducing animal use, and decreasing development costs. Most of the current study in ITS is based on a group of consensus processes, termed weight of evidence (WoE), which quantitatively integrate all the relevant data instances towards the same endpoint into an integrated decision supported by data quality. Several WoE implementations for the particular case of toxicity data fusion have been presented in the literature, which are collectively studied in this paper. Noting that these uncertainty handling methodologies are usually not simply developed from conventional probability theory due to the unavailability of big datasets, this paper first investigates the mathematical foundations of these approaches. Then, the investigated data integration models are applied to a representative case in the predictive toxicology domain, with the experimental results compared and analysed.",2014,,10.1109/UKCI.2014.6930153
326,"Li, Ling and Li, Weibang and Zhu, Lidong and Li, Chengjie and Zhang, Zhen",Automatic Data Repairs with Statistical Relational Learning,Data integrity;Transforms;Markov processes;Maintenance engineering;Probabilistic logic;Cleaning;Inference algorithms;Data repairing;data cleaning;data quality;statistical relational learning;DeepDive;factor graph,"Dirty data is ubiquitous in real-world, and data cleaning is a long-standing problem. The importance of data cleaning is growing in the era of big data. In this paper we propose a novel data repairing approach by leveraging statistical relational learning (SRL). We learn Bayesian networks of attributes from the dirty data, then transform the dependency relationships among attributes into first-order logic formulas. We calculate the weight of each formula based on the mutual information of the attributes involved in the formula and obtain Markov logic network (often abbreviated as MLN) by assigning weight to each first-order logic formula. Then we transform Markov logic networks into inference rules and conduct these inference rules on DeepDive. The inference results are utilized to repair dirty data at last. Experiments on real-world datasets demonstrate that our approach has higher accuracy in terms of different situations and is universal for different kinds of datasets.",2021,,10.1109/ISNCC52172.2021.9615868
327,"Konanahalli, Ashwini and Marinelli, Marina and Oyedele, Lukumon",Drivers and Challenges Associated With the Implementation of Big Data Within U.K. Facilities Management Sector: An Exploratory Factor Analysis Approach,Frequency modulation;Organizations;Big Data;Maintenance engineering;Data mining;Internet of Things;Analytics;Big Data (BD);facilities management (FM);technology implementation,"The recent advances in Internet of Things (IoT), computational analytics, processing power, and assimilation of Big Data (BD) are playing an important role in revolutionizing maintenance and operations regimes within the wider facilities management (FM) sector. The BD offers the potential for the FM to obtain valuable insights from a large amount of heterogeneous data collected through various sources and IoT allows for the integration of sensors. The aim of this article is to extend the exploratory studies conducted on Big Data analytics (BDA) implementation and empirically test and categorize the associated drivers and challenges. Using exploratory factor analysis (EFA), the researchers aim to bridge the current knowledge gap and highlight the principal factors affecting the BDA implementation. Questionnaires detailing 26 variables are sent to the FM organization in the U.K. who are in the process or have already implemented BDA initiatives within their FM operations. Fifty-two valid responses are analyzed by conducting EFA. The findings suggest that driven by market competition and ambitious sustainability goals, the industry is moving to holistically integrate analytics into its decision making. However, data quality, technological barriers, inadequate preparedness, data management, and governance issues and skill gaps are posing to be significant barriers to the fulfillment of expected opportunities. The findings of this study have important implications for FM businesses that are evaluating the potential of the BDA and IoT applications for their operations. Most importantly, it addresses the role of the BD maturity in FM organizations and its implications for perception of drivers.",2020,,10.1109/TEM.2019.2959914
328,"Fomferra, Norman and Böttcher, Martin and Zühlke, Marco and Brockmann, Carsten and Kwiatkowska, Ewa","Calvalus: Full-mission EO cal/val, processing and exploitation services",File systems;Algorithm design and analysis;Calibration;Reliability;Servers;Programming;Data processing;Algorithm prototyping;data processing;big data handling;map-reduce;calibration/validation,"ESA's Earth Observation (EO) missions provide a unique dataset of observational data of our environment. Calibration, algorithm development and validation of the derived products are indispensable tasks for an efficient exploitation of EO data and form the basis for reliable scientific conclusions. In spite of its importance, the cal/val and algorithm development work is often hindered by insufficient means to access data, time consuming work used to identify suitable in-situ data matching the EO data, incompatible software and limited possibilities for a rapid prototyping and testing of ideas. In view of the amount of data produced by the future ESAs series of Sentinel satellites, a very efficient technological backbone is required to maintain the ability of ensuring data quality and algorithm performance. Brockmann Consult has developed such a backbone based on leading edge technologies within an ESA R&D study. Calvalus is a new processing system that utilises the map-reduce programming model with a distributed file system.",2012,,10.1109/IGARSS.2012.6352418
329,"Ahmadov, Ahmad and Thiele, Maik and Eberius, Julian and Lehner, Wolfgang and Wrembel, Robert",Towards a Hybrid Imputation Approach Using Web Tables,Indexes;Lakes;Companies;Big data;Data mining;Industries;Data analysis;Web mining;Data preprocessing;Machine learning,"Data completeness is one of the most important data quality dimensions and an essential premise in data analytics. With new emerging Big Data trends such as the data lake concept, which provides a low cost data preparation repository instead of moving curated data into a data warehouse, the problem of data completeness is additionally reinforced. While traditionally the process of filling in missing values is addressed by the data imputation community using statistical techniques, we complement these approaches by using external data sources from the data lake or even the Web to lookup missing values. In this paper we propose a novel hybrid data imputation strategy that, takes into account the characteristics of an incomplete dataset and based on that chooses the best imputation approach, i.e. either a statistical approach such as regression analysis or a Web-based lookup or a combination of both. We formalize and implement both imputation approaches, including a Web table retrieval and matching system and evaluate them extensively using a corpus with 125M Web tables. We show that applying statistical techniques in conjunction with external data sources will lead to a imputation system which is robust, accurate, and has high coverage at the same time.",2015,,10.1109/BDC.2015.38
330,"Tavakoli, Mohammadreza and Elias, Mirette and Kismihók, Gábor and Auer, Sören",Quality Prediction of Open Educational Resources A Metadata-based Approach,Metadata;Quality control;Predictive models;Open Educational Resources;Measurement;Data analysis;OER;open educational resources;metadata quality;OER quality;Big data;data analysis;quality prediction,"In the recent decade, online learning environments have accumulated millions of Open Educational Resources (OERs). However, for learners, finding relevant and high quality OERs is a complicated and time-consuming activity. Furthermore, metadata play a key role in offering high quality services such as recommendation and search. Metadata can also be used for automatic OER quality control as, in the light of the continuously increasing number of OERs, manual quality control is getting more and more difficult. In this work, we collected the metadata of 8,887 OERs to perform an exploratory data analysis to observe the effect of quality control on metadata quality. Subsequently, we propose an OER metadata scoring model, and build a metadata-based prediction model to anticipate the quality of OERs. Based on our data and model, we were able to detect high-quality OERs with the F1 score of 94.6%.",2020,,10.1109/ICALT49669.2020.00007
331,"Abdellaoui, Sabrina and Bellatreche, Ladjel and Nader, Fahima",A Quality-Driven Approach for Building Heterogeneous Distributed Databases: The Case of Data Warehouses,Ontologies;Semantics;Measurement;Unified modeling language;Standards;Proposals;Data warehouse;ETL design;Ontologies;Data quality;Semantic Database Sources,"Data Warehouse (DW) is a collection of data, consolidated from several heterogeneous sources, used to perform data analysis and support decision making in an organization. Extract-Transform-Load (ETL) phase plays a crucial role in designing DW. To overcome the complexity of the ETL phase, different studies have recently proposed the use of ontologies. Ontology-based ETL approaches have been used to reduce heterogeneity between data sources and ensure automation of the ETL process. Existing studies in semantic ETL have largely focused on fulfilling functional requirements. However, the ETL process quality dimension has not been sufficiently considered by these studies. As the amount of data has exploded with the advent of big data era, dealing with quality challenges in the early stages of designing the process become more important than ever. To address this issue, we propose to keep data quality requirements at the center of the ETL phase design. We present in this paper an approach, defining the ETL process at the ontological level. We define a set of quality indicators and quantitative measures that can anticipate data quality problems and identify causes of deficiencies. Our approach checks the quality of data before loading them into the target data warehouse to avoid the propagation of corrupted data. Finally, our proposal is validated through a case study, using Oracle Semantic DataBase sources (SDBs), where each source references the Lehigh University BenchMark ontology (LUBM).",2016,,10.1109/CCGrid.2016.79
332,"Hou, Jiaxin and Chen, Jing and Liao, Shijie and Wen, Junhao and Xiong, Qingyu",Predicting Traffic Flow via Ensemble Deep Convolutional Neural Networks with Spatio-temporal Joint Relations,Predictive models;Data models;Optimization;Task analysis;Kernel;Convolutional neural networks,"Traffic flow prediction is a crucial task for the intelligent traffic management and control. Various machine learning based methods have been applied in this field. Most of these methods encounter three fundamental issues: feature representation of traffic patterns, learning from single location or network, and data quality. In order to address these issues, in this work we present a deep architecture for traffic flow prediction that learns deep hierarchical feature representation with spatio-temporal relations over the traffic network. Furthermore, we design an ensemble learning strategy via random subspace learning to make the model be able to tolerate incomplete data. The experimental results corroborate the effectiveness of the proposed approach compared with the state of the art methods.",2018,,10.1109/ICPR.2018.8545080
333,"Drakopoulos, Georgios and Megalooikonomou, Vasileios",Regularizing large biosignals with finite differences,Time series analysis;Signal processing algorithms;Laplace equations;Cost function;Electrocardiography;Finite difference methods;Big data;Finite difference matrix;Regularization;Biosignal processing;Big data analytics;Conjugate gradient;Discrete Laplace operator;Electrocardiogram;Heartbeat rate,"In the biomedical analytics pipeline data preprocessing is the first and crucial step as subsequent results and visualization depend heavily on original data quality. However, the latter often contain a large number of outliers or missing values. Moreover, they may be corrupted by noise of unknown characteristics. This is in many cases aggravated by lack of sufficient information to construct a data cleaning mechanism. Regularization techniques remove erroneous values and complete missing ones while requiring little or no information regarding either data or noise dynamics. This paper examines the theory and practice of a regularization class based on finite differences and implemented through the conjugate gradient method. Moreover, it explores the connection of finite differences to the discrete Laplace operator. The results obtained from applying the proposed regularization techniques to heart rate time series from the MIT-BIH dataset are discussed.",2016,,10.1109/IISA.2016.7785346
334,"Wang, Shiyang",The Prediction Method of KPIs by Using LS-TSVR,Support vector machines;Data integrity;Key performance indicator;Time series analysis;Production;Prediction methods;Big Data;time series prediction;support vector regression;least square approximation,"Closely monitoring service performance and making predictions of Key Performance Indicators (KPIs) are critical for Internet-based services. However, fast yet accurate prediction of these seasonal KPIs with various patterns and data quality has been a great challenge. This paper tackles this challenge through a novel approach based on auto-regressive Least Square Twin Support Vector Regression (LS-TSVR). As an improved version of SVR, LS-TSVR can handle big data without any external optimization, and meanwhile, the prediction accuracy is better than that of SVR. For seasonal KPI data in a production dataset, our methods satisfy or approximate a mean average error (MAE) of around 0.013, which is significantly lower than the baseline method.",2022,,10.1109/ICBDA55095.2022.9760331
335,"López-Acosta, Araceli and García-Hernández, Alejandra and Vázquez-Reyes, Sodel and Mauricio-González, Alejandro",A Metadata Application Profile to Structure a Scientific Database for Social Network Analysis (SNA),Metadata;Social networking (online);Databases;Search engines;Interoperability;Internet;Libraries;Scientific Data;Metadata;Elasticsearch;Social Network Analysis,"There are a number of challenges associated to metadata in its different applications including data quality, data acquisition, computing resources, interoperability, and discoverability. This work presents an approach to structure metadata of scientific information for social network analysis based on an academic case study from scientific articles published by universities, to evaluate the area of risk assessment. Studying metadata for scientists' social networks helps identify authors' relevance based on their position within the network. By using Elasticsearch (ES) and Python technologies, this work addresses big data analysis issues related to data structure and volume, given ES full-text search engine capabilities for indexing and searching data, and Python's processing support. The data is obtained from the ArnetMiner (Aminer) open scientific database providing a fresh overview of scientific records up to January 2019. From a sample of 64,070 publications, a total of 45, 000 relations are graphed in a co-authorship network. Through the computation of network centrality measures, this work identifies central-positioned authors, clusters of research, and their affiliations. The results show that degree centrality is an important measure to identify prominent scientists in this co-authorship network, and closeness and betweenness centralities together are dominant measures to pinpoint the key players in the flow of information within the network. We conclude that the application of this approach allows rapid full-text search, visualizing dense co-authorship networks, and identifying central authors through centrality metrics. The results presented in this work can help researchers or research groups identify key research collaborators, multi-disciplinary areas, and international stakeholders.",2020,,10.1109/CONISOFT50191.2020.00038
336,"Wu, Chieh-Han and Song, Yang",Robust and distributed web-scale near-dup document conflation in microsoft academic service,Algorithm design and analysis;Noise measurement;Resource management;Data models;Robustness;Computational modeling;Proteins;Near-duplicate detection;shingling algorithm;n-gram;entity conflation,"In modern web-scale applications that collect data from different sources, entity conflation is a challenging task due to various data quality issues. In this paper, we propose a robust and distributed framework to perform conflation on noisy data in the Microsoft Academic Service dataset. Our framework contains two major components. In the offline component, we train a GBDT model to determine whether two papers from different sources should be conflated to the same paper entity. In the online component, we propose a scalable shingling algorithm that can apply our offline model to over 100 million instances. The result shows that our algorithm can conflate noisy data robustly and efficiently.",2015,,10.1109/BigData.2015.7364059
337,"Müftüoğlu, Zümrüt and Kizrak, M. Ayyüce and Yildlnm, Tülay",Differential Privacy Practice on Diagnosis of COVID-19 Radiology Imaging Using EfficientNet,Machine learning;Privacy;Computed tomography;Medical diagnostic imaging;COVID-19;deep learning;EfficientNet;X-Ray;radiology imaging;PATE;differential privacy,"Medical sciences are an important application area of artificial intelligence. Healthcare requires meticulousness in the whole process from collecting data to processing. It should also be handled in terms of data quality, data size, and data privacy. Various data are used within the scope of the COVID-19 outbreak struggle. Medical and location data collected from mobile phones and wearable devices are used to prevent the spread of the epidemic. In addition to this, artificial intelligence approaches are presented by using medical images in order to identify COVID-19 infected people. However, studies should be carried out by taking care not to endanger the security of the data, people, and countries needed for these useful applications. Therefore, differential privacy (DP) application, which was an interesting research subject, has been included in this study. CXR images have been collected from COVID-19 infected 139 and a total of 373 public data sources were used for a diagnostic concept. It has been trained with EfficientNet- B0, a recent and robust deep learning model, and proposal the possibility of infected with an accuracy of 94.7%. Other evaluation parameters were also discussed in detail. Despite the data constraint, this performance showed that it can be improved by augmenting the dataset. The most important aspect of the study was the proposal of differential privacy practice for such applications to be reliable in real-life use cases. With this view, experiments were repeated with DP applied images and the results obtained were presented. Here, Private Aggregation of Teacher Ensembles (PATE) approach was used to ensure privacy assurance.",2020,,10.1109/INISTA49547.2020.9194651
338,"Wang, Liang and Yang, Congying and Yu, Zhiwen and Liu, Yimeng and Wang, Zhu and Guo, Bin",CrackSense: A CrowdSourcing Based Urban Road Crack Detection System,Roads;Crowdsourcing;Sensors;Estimation;Data models;Data integrity;Three-dimensional displays;Mobile crowdsourcing;road crack detection;image processing;sensors,"As a common road surface distress, cracks pose a serious threat to road infrastructure and traffic safety in cities today. Consequently, road crack detection is considered as an essential step for effective road maintenance and road structure sustainability. However, due to the high cost incurred by dedicated devices and professional operators, it is impossible for existing systems to achieve universal spatiotemporal coverage across citywide road networks. To fill this gap, in this paper, we present the CrackSense, a mobile crowdsourcing based system to detect urban road crack and estimate its damage degree. Specifically, for the heterogeneous crack data, we put forward a crowdsourcing data quality evaluation and selection mechanism. And then, by utilizing the multi-source sensing data aggregation, we propose tow algorithms, namely RCTR and RCDE, to recognize road crack types, i.e., horizontal crack, vertical crack, and net crack, and estimate the crack damage degree, respectively. We implement the system and develop a smartphone APP for mobile users. By conducting intensive experiments and field study, the results demonstrate the accuracy and effectiveness of our proposed approaches.",2019,,10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00188
339,"Gopal, R. Chandangole and Bharat, A. Tidke",A generic tool to process mongodb or Cassandra dataset using Hadoop streaming,Decision support systems;Handheld computers;Conferences;Hadoop Streaming;Cassandra;Mongodb;MapReduce,"Now a days Bulk of data generating on the system. This data is very important for user and today's user accessing, searching and sorting the data from database is very difficult. To overcome this problem, data is distributed in different node using Hadoop technology. A system is proposed in which the collected data is to be distributed using map reduce technique for sorting the data is very easily on Hadoop environment. In this case used Cassandra and mongodb tools to storing large amount of data on Hadoop Framework. NoSQL data is stores in unstructured data format which is a key focus area for “Big Data” research. The quantity and quality of unstructured data growing high. The Hadoop Framework used to large amount of data on a different nodes in a cluster data. NoSQL databases using different structure and unstructured data of high scalability for getting high performance of system. To present the approaches solving Problem of NoSQL data to stores with MapReduce process to under in non-Java application. A Cassandra is to provide the platform for the fast and efficient data queries. In this paper presents the tools of the Cassandra and the mongodb using NoSQL database for connecting different node with the Hadoop MapReduce engine.",2016,,
340,"Xie, Sihong and Kong, Xiangnan and Gao, Jing and Fan, Wei and Yu, Philip S.",Multilabel Consensus Classification,Predictive models;Correlation;Measurement;Prediction algorithms;Data models;Bipartite graph;Algorithm design and analysis;multilabel classification;ensemble,"In the era of big data, a large amount of noisy and incomplete data can be collected from multiple sources for prediction tasks. Combining multiple models or data sources helps to counteract the effects of low data quality and the bias of any single model or data source, and thus can improve the robustness and the performance of predictive models. Out of privacy, storage and bandwidth considerations, in certain circumstances one has to combine the predictions from multiple models or data sources without accessing the raw data. Consensus-based prediction combination algorithms are effective for such situations. However, current research on prediction combination focuses on the single label setting, where an instance can have one and only one label. Nonetheless, data nowadays are usually multilabeled, such that more than one label have to be predicted at the same time. Direct applications of existing prediction combination methods to multilabel settings can lead to degenerated performance. In this paper, we address the challenges of combining predictions from multiple multilabel classifiers and propose two novel algorithms, MLCM-r (MultiLabel Consensus Maximization for ranking) and MLCM-a (MLCM for microAUC). These algorithms can capture label correlations that are common in multilabel classifications, and optimize corresponding performance metrics. Experimental results on popular multilabel classification tasks verify the theoretical analysis and effectiveness of the proposed methods.",2013,,10.1109/ICDM.2013.97
341,"Fang, Dianjun and Zhang, Yin and Spicher, Klaus",Forecasting accuracy analysis based on two new heuristic methods and Holt-Winters-Method,Forecasting;Time series analysis;Market research;Mathematical model;Measurement;Predictive models;time series;forecasting method;data quality;error measurement;B2C/B2B forecasting,"Since 1970s, many academic researchers and business practitioners have started to develop different forecasting methods and models. Most of them are still used in the IT-Systems nowadays. However, they don't perform well enough in practice. People pay much attention to data collection but ignore the data quality, which could lead to low forecasting accuracy. In this paper, we will introduce two new heuristic business forecasting techniques (Revinda and Metrix). Both methods utilize inherent structures of time series. The error analysis is based on B2C and B2B aggregated commercial data. In addition, these two methods will be compared with HOLT-WiNTERS-Methods (HWM) by using error measures MAPE, percentage better and THEIL's U2.",2016,,10.1109/ICBDA.2016.7509814
342,"Kardash, Adam and Morin, Suzanne",The Practices and Challenges of Generating Nonidentifiable Data,Data privacy;Big data;Computer security;Data quality,"This article summarizes the key findings of a Canadian Anonymization Network study of several large data custodians who utilize deidentification and similar privacy-enhancing processes prior to engaging in analytics, secondary uses, and disclosure of personal information.",2022,,10.1109/MSEC.2021.3126185
343,"Martelli, Cristina and Bellini, Emanuele",Using Value Network Analysis to Support Data Driven Decision Making in Urban Planning,Cities and towns;Decision making;IEEE 802.11 Standards;Time series analysis;Context;Urban planning;Data Driven Decision Making;Value Network Analysis;urban planning,This article provides a methodology of assessing the (Big)/(Open) Data quality in Data Driven Decision Making with the Value Network Analysis approach discovering the value creation failure point(s) in the network and evaluating the impact of loss of vale of data in DDDM process.,2013,,10.1109/SITIS.2013.161
344,"Bhargava, Deepshikha and Poonia, Ramesh C. and Arora, Upma",Design and development of an intelligent agent based framework for predictive analytics,Intelligent agents;Predictive models;Analytical models;Market research;Conferences;Data mining;Software Agents;Intelligent Agents;Predictive Analytics,"The arrival of World Wide Web has led to the explosive growth of information on the web. There is a sudden boom in quality of raw data/information, which is commonly referred as Big Data. Very often, this raw information contains very useful insights which are ignored most of the time and are difficult to analyze due to the enormous size of these datasets. The feasibility for human to extract this information from the vast web and build useful application on the top of it, is very low. Hence to create predictive models, there is a huge need for intelligent and autonomous software agents which can procure useful information from the large datasets of raw information. Predictive analytics models can be created from these datasets which can be further used for various applications in security, future prediction etc. This research paper gives an overview of how these software agents will become the most important tools in coming days for building predictive models.",2016,,
345,"Makoondlall, Y.K. and Khaddaj, S. and Makoond, B. and Kethan, K.",ZDLC : Layered Lineage Report across Technologies,Databases;Tools;Software;Big Data;Data mining;Companies;Zero Deviation Life Cycle (ZDLC);ZDLC;layered lineage;lineage report;data lineage;data quality,"As technology moves from being an enabler to become the lifeblood of businesses in the digital era, so does the data used to support the implementation of these technological shifts. In order for the data to be an asset and not a liability, it is primordial to ensure that the data captured and maintained over time is accurate, traceable, reliable and current. However, very often, in order to complete a business function the data from one system may be exported to another system or to a third party tool, which also changes the data. In order to keep track of the data and maximize the use and analysis of data, the Zero Deviation Life Cycle (ZDLC) framework proposes a series of tools to which can trace the data lineage, across several database technologies.",2016,,10.1109/CSE-EUC-DCABES.2016.252
346,"Wang, Hongbing and Wang, Lei and Yu, Qi and Zheng, Zibin",Learning the Evolution Regularities for BigService-Oriented Online Reliability Prediction,Reliability;Time series analysis;Web services;Computer network reliability;Meteorology;Big data;Quality of service;Temporal evolution regularities;online reliability prediction;big service;convolutional neural networks,"Service computing is an emerging technology in System of Systems Engineering (SoS Engineering or SoSE), which regards a System as a Service, and aims at constructing a robust and value-added complex system by outsourcing external component systems through service composition. The burgeoning Big Service computing just covers the significant challenges in constructing and maintaining a stable service-oriented SoS. A service-oriented SoS runs under a volatile and uncertain environment. As a step toward big service, service fault tolerance (FT) can guarantee the run-time quality of a service-oriented SoS. To successfully deploy FT in an SoS, online reliability time series prediction, which aims at predicting the reliability in near future for a service-oriented SoS arises as a grand challenge in SoS research. In particular, we need to tackle a number of big data related issues given the large and fast increasing size of the historical data that will be used for prediction purpose. The decision-making of prediction solution space be more complex. To provide highly accurate prediction results, we tackle the prediction challenges by identifying the evolution regularities of component systems' running states via different machine learning models. We present in this paper the motifs-based Dynamic Bayesian Networks (or m_DBNs) to perform one-step-ahead online reliability time series prediction. We also propose a multi-steps trajectory DBNs (or multi_DBNs) to further improve the accuracy of future reliability prediction. Finally, a Convolutional Neural Networks (CNN)-based prediction approach is developed to deal with the big data challenges. Extensive experiments conducted on real-world Web services demonstrate that our models outperform other well-known approaches consistently.",2019,,10.1109/TSC.2016.2633264
347,"Matsubara, Masaki and Kobayashi, Masaki and Morishima, Atsuyuki",A Learning Effect by Presenting Machine Prediction as a Reference Answer in Self-correction,Task analysis;Painting;Training data;Quality assurance;Training;Machine learning;Crowdsourcing;Quality Assurance;Self Correction;Machine Teaching,"Can people learn from machines behavior in microtask based crowdsourcing? Can we train the machines as our mentor even without domain expertise? In this paper, we investigate how the task results improve concerning quality during and after presenting machine prediction as a reference answer in self-correction. Four reference types were examined in the experiment; Correct, Random, Machine prediction trained by correct answers, and that trained by human answers. Learning effects were observed only in presenting machine prediction, although those accuracy rates were far from correct (100%). Moreover, there were no learning effects in ""Correct"" and ""Random"". This suggests the following hypothesis: Since machine learners make some ""models"" for the problem, it is easier for humans to interpret the outputs of machine learners than the results without via them; it is more difficult to interpret not only random answers but also the correct answers in a case where the perfect interpretation of the problem is difficult. Furthermore, some workers answered with higher accuracy rate than machines in the post-test. Therefore, this strategy can be expected to be useful for bootstrapping solutions in the situation where unknown problems occur without expertise or at a low cost.",2018,,10.1109/BigData.2018.8622435
348,"Palacio, Ana León and López, Óscar Pastor",Infoxication in the Genomic Data Era and Implications in the Development of Information Systems,Bioinformatics;Genomics;Diseases;DNA;Task analysis;Databases;Infoxication;Genomics;Information Systems;SILE method,"We live in an age where data acquisition is no longer a problem and the real challenge is how to determine which information is the right one to take important and sometimes difficult decisions. Infoxication (also known as Infobesity or Information Overload) is a term used to describe the difficulty of adapting to new situations and effectively making decisions when there is too much information to manage. With the advent of the Big Data, infoxication is affecting critical domains such as Health Sciences, where tough decisions for patient's health is being taken every day based on heterogeneous, unconnected and sometimes conflicting information. In order to understand the magnitude of the challenge, based on the information publicly available about the genetic causes of the disease and using data quality assessment techniques, we performed an exhaustive analysis of the DNA variations that have been associated to the risk of suffering migraine headache. The same analysis has been repeated 8 months after, and the results have allowed us to exemplify i) how fragile is the information in this domain, ii) the difficulty of finding repositories of contrasted and reliable data, and iii) the need to have information systems that, far from integrating and storing huge volumes of data, are able to support the decision-making process by providing mechanisms agile and flexible enough to be able to adapt to the changing user needs.",2019,,10.1109/RCIS.2019.8877003
349,"Pełech-Pilichowski, T.",On adaptive prediction of nonstationary and inconsistent large time series data,Time series analysis;Interpolation;Predictive models;Forecasting;Prediction algorithms;Extrapolation;Adaptation models;time series analysis;prediction;forecasting;interpolation;adaptive prediction algorithms;Big Data;IoT,"The use of time series prediction results in benefits for an organization. Forecasting efficiency relies on applied prediction formula and quality of data received from technical devices and manually inputted. They are often of low quality, with inconsistencies. However, high data quality is crucial for efficient forecasting/prediction purposes (also event detection from time series and pattern recognition), in particular during large data sets processing (often heterogeneous, including data obtained from IoT devices). Such processing should cover inconsistency analysis, interpolation of missing/lacking data, as well as the use of data pre-transformations. The paper presents problems of inconsistent, nonstationary data prediction on the example of stock level daily forecasting. Selected methods of time series interpolation are outlined. Results of implementation of algorithms for short-term time series prediction are illustrated and discussed. Prediction quality measured based on errors values calculated both in total and in a moving window is discussed. A concept of an adaptive algorithm based on a change in the prognostic formula depending on short-term characteristics of time series is outlined.",2018,,10.23919/MIPRO.2018.8400228
350,"Feng, Zikun and Yang, Haojie and Li, Xinyi and Li, Yan and Liu, Zhao and Liu, Ryan Wen",Real-Time Vessel Trajectory Data-Based Collison Risk Assessment in Crowded Inland Waterways,Marine vehicles;Artificial intelligence;Interpolation;Trajectory;Navigation;Rivers;Accidents;Ship domain;trajectory data;ship collision risk;automatic identification system;Monte Carlo method,"With the rapid development of maritime industries, the vessel traffic density has been gradually increased leading to increasing the potential risk of ship collision accidents in crowded inland waterways. It will bring negative effects on human life safety and enterprise economy. Therefore, it is of vital significance to study the risk of ship collision in practical applications. This paper proposes to quantitatively estimate the ship collision risk based on ship domain modeling and real-time vessel trajectory data. In particular, the trajectory data quality is improved using the cubic spline interpolation method. We assume that the ship collision risk is highly related to the cross areas of ship domains between different ships, which are computed via the Monte Carlo probabilistic algorithm. For the sake of better understanding, the kernel density estimation method is adopted to visually generate the ship collision risk in maps. Experimental results have illustrated the effectiveness of the proposed method in crowded inland waterways.",2019,,10.1109/ICBDA.2019.8712843
351,"Duan, Gui-Jiang and Yan, Xin",A Real-Time Quality Control System Based on Manufacturing Process Data,Manufacturing processes;Production;Quality control;Product design;Real-time systems;Manufacturing;Quality assessment;Quality management;production control;prediction methods,"Quality prediction is one of the key links of quality control. Benefitting from the development of digital manufacturing, manufacturing process data have grown rapidly, which allows product quality predictions to be made based on a real-time manufacturing process. A real-time quality control system (RTQCS) based on manufacturing process data is presented in this paper. In this study, the relationship between the product real-time quality status and processing task process was established by analyzing the relationship between the product manufacturing resources and the quality status. The key quality characteristics of the product were identified by analyzing the similarity of the product quality characteristic variations in the manufacturing process based on the big data technology, and a quality-resource matrix was constructed. Based on the quality-resource matrix, the RTQCS was established by introducing an association-rule incremental-update algorithm. Finally, the RTQCS was applied in actual production, and the performance of RTQCS was verified by experiments. The experiments showed that the RTQCS can effectively guarantee the quality of product manufacturing and improve the manufacturing efficiency during production.",2020,,10.1109/ACCESS.2020.3038394
352,"Wang, Sheng and Fu, Lieyong and Yao, Jianmin and Li, Yun",The Application of Deep Learning in Biomedical Informatics,Conferences;Robots;Intelligent systems;Deep Learning;Healthcare;Biomedical informatics,"The expansion of big data in biomedical and health field has driven the need of new effective analysis technology. Deep learning is a powerful machine learning method. With the contribution of rapid computational power improvement, it is becoming a promising technique to generate new knowledge, interpretation and gain insights from high-throughout, heterogeneous and complex biomedical data from different sources, such as medical imaging, clinical genomics, and electronic health records. This paper presents an overview of the application of deep learning approach in the biomedical informatics. First we introduce the development of artificial neural network and deep learning, then mainly focus on the researches applying deep learning in biomedical informatics field. We also discuss the challenges for future improvement, such as data quality and interpretability.",2018,,10.1109/ICRIS.2018.00104
353,"Li, Peng and Luo, Hong and Wu, Tin-Yu and Obaidat, Mohammad S.",QoS prediction method for data supply chain based on context,Quality of service;Supply chains;Web services;Mathematical model;Prediction methods;Time factors;Corporate acquisitions;Data supply chain;QoS model;processing context;QoS prediction,"Due to the execution paradigm may be different at different invocation time, users obtain different QoS when interacting with the same Data Supply Chain (DSC). However, existing QoS prediction methods seldom took this observation into consideration, which shall decrease the prediction accuracy. In this paper, we propose a context-based QoS prediction method for data supply chain. First, a QoS mathematical model is developed for considering the mass data transmission across elementary sub-chains. Then, two execution paradigms of data supply chain are discussed. Besides, we explored several special context factors of data supply chain (such as invocation time, data source update period and execution paradigm) which influence QoS. By processing such context information, we can obtain the part of data supply chain which is need to execute when the user query occurs and leverage them to predict QoS. Experimental results indicate that our approach improves the prediction accuracy and efficiency of QoS when compared to previous methods.",2017,,10.1109/ICC.2017.7997260
354,"Zhichun, Yang and Yu, Shen and Fan, Yang and Yang, LEI and Lei, Su and Fangbin, Yan",Topology identification method of low voltage distribution network based on data association analysis,Meters;Low voltage;Substations;Network topology;Power supplies;Distribution networks;Transformers;Low voltage distribution network;Internet of things;topology identification;association analysis;topology verification,"This paper introduces a topology identification method of low-voltage distribution network based on data association analysis. The low-voltage distribution network to be identified is divided into the single distribution transformer power off station areas, multiple distribution transformer station areas caused by 10kV distribution line power outage and the distribution transformer areas without power interruption based on low-voltage distribution network blackout event, restoration power on event and geographic location information. In each type of station area, Tanimoto similarity coefficient is used to calculate the correlation and non-correlation between distribution transformer, branch box, meter box and smart meter in each group, so as to achieve the topology identification of the low-voltage distribution network. And then the identified topology can be verified by combining the topology verification rules of the same distribution transformer station area has the same of outage and live state, outage duration, geographical location, power supply radius and so on. Through the actual case, it is proved that the method proposed in this paper can solve the problems of large amount of calculation, inaccuracy of calculation results, and inability to verify based on the existing big data mining methods. It realizes the efficient and accurate identification of distribution transformer substation topology, and improves the information level and data quality of distribution network.",2020,,10.1109/ACPEE48638.2020.9136335
355,"Yang, Wanting and Chi, Xuefen and Zhao, Linlin",Proactive VoD delivery pattern reconfiguration based on temporal-spatial channel prediction,Delays;Quality of service;Prediction algorithms;Resource management;Estimation;Probability;Big Data;channel state prediction;reconfiguration;delivery pattern;martingales;VoD;spectrum efficiency;delay-QoS,"With the help of big data analytics, predictive resource allocation (PRA) techniques for video on demand (VoD) have been recognized as promising methods to save time-frequency resources, for a number of VoD packets can be transmitted in good channels in advance to avoid the predicted transmissions in bad channel conditions. With the increasing demands on a fantastic user quality of experience, a smooth playback and a low start-up delay are of equal importance to the emerging VoD with high fidelity, which inevitably leads to a critical delay requirement of VoD packets. However, the issue of resource estimation with quality of service (QoS) requirements is still an unsolved puzzle in PRA. In this paper, we propose a martingales-based physical resource block (PRB) abstraction method, where the random characteristics of the service process are embedded in the minimum PRB consumption. Based on the method, a proactive QoS-guaranteed reconfiguration algorithm is developed to optimize the multi-user delivery pattern applied in the prediction window, aiming to maximize spectrum efficiency. In this algorithm, since the delay sensitivity of VoD content transmitted in advance is dulled compared with the original VoD stream, we divide the original VoD slice into two sub-slices and derive a three-dimensional delivery pattern. The gain of resource saving and the capability of QoS guarantee brought by the reconfiguration have been demonstrated by the simulation results.",2020,,10.1109/ICC40277.2020.9149070
356,"Ouyang, Jianna and Liang, Shuo and Chen, Shaonan and Li, Shan and Zhou, Yangjun and Liwen, QIN",Design and Realization of Data Application Architecture Oriented to the Requirements of Distribution Network,Data integration;Systems architecture;Distribution networks;Production;Big Data applications;Reliability engineering;Business;distribution network;application requirements;data application architecture;design and realization,"In recent years, the rapid growth of all kinds of data and information in power grid has brought great challenges to the safe and stable operation and data analysis of the system. This paper constructs the data application architecture oriented to the requirements of distribution network based on the data requirements of reliability and economy evaluation, operation state evaluation and weak link identification, asset operation efficiency evaluation and lean management. It can realize the functions of data automatic classification storage, data processing, data quality monitoring and evaluation, multi-source heterogeneous data fusion and hierarchical classification database construction, etc. It supports the lean management of production business in distribution network comprehensively.",2020,,10.1109/iSPEC50848.2020.9351123
357,"Ahlawat, Deepak and Kaur, Amandeep and Gupta, Deepali",Enhancement of the Accuracy and QoS in Clustering of Data,Throughput;Clustering algorithms;Big Data;Quality of service;Data mining;Tools;Matlab;Clustering;Cosine Similarity;Gaussian Similarity;Hybrid Similarity;AODV,"Clustering is an important data mining and tool for examining data. The paper compares the two techniques of clustering, in the first technique only Cosine similarity is used and in the second technique Improved Rank Similarity (Cosine Similarity + Gaussian Similarity) is used. The results are compared with various parameters constituting the Accuracy in NetBeans and QoS parameters using AODV routing protocol. The simulation is done on MATLAB, a network is created and communication from source node to target node is noted.",2020,,10.1109/ICRITO48877.2020.9197774
358,Yan Zhou and Haitian Xie,The integration technology of sensor network based on web crawler,Service-oriented architecture;web crawler;sensor network;Sensor Observation Service (SOS);Tomact;Istsos,"Along with the development of the sensor system and sensor network, the wide applications of sensor networks have arisen at the historic moment. In reality, all kinds of sensors monitor every aspect of our life, which provides various services and brings the challenge: how to effectively integrate those distributed sensor resources and then can be used to find more advanced information or implement the sharing of resources are the big problems to be solved. Based on the framework of Sensor Web Enablement(SWE) which was proposed by Open GIS Consortium (OGC)and combined with the function of web crawler, we study and find Sensor Observation Service (SOS) service which is the core components of the SWE then we design a system based on the web crawler technology and the Istituto Scienze della Terra Sensor Observation Service (Istsos) architecture. The design of sensor network technology integration architecture includes three parts. The layer of data access which is the lowest layer encapsulates the access to the database or other source of resources. The layer of business logic it provides the core operation of component which was named Request Operator, this layer is used for processing various requests from the lowest layer in order to return the classes of listening. The layer of web and the client is connected, which can provide some thin client of SOS. The published server includes the ability of new services creation, addition of new sensors and relative metadata, visualization, and manipulation of stored observations, registration of new measures and setting of system properties like observable properties and data quality codes. In order to get sensor data, web crawler technology is used in our research, which can make us get sensor data from the target website, and the standardized sensor data is gotten by filtering the original data and then the data is uploaded to the database of Istsos with the standardized format. At last, the implementation of SOS architecture has been configured. The test's results show that the integrated architecture of services can effectively obtain the required sensor data and display them graphically.",2015,,10.1109/GEOINFORMATICS.2015.7378670
359,"Ding, Jian and Ma, Chunlei and Fu, Bin and Liu, Bing",Active distribution network state estimation algorithm based on decision tree of self-identification,DATA CHECK;DATA QUALITY IDENTIFICATION;DECISION TREE;STATE ESTIMATION;BIG DATA,"Under the background of active distribution network, this paper proposes a state estimation algorithm of managing analysis data to solve the problem of big data, data missing and complex analysis. This paper proposes an active distribution network state estimation algorithm based on decision tree self-identification. Setting appropriate quality weight of big data based on the check rules different from traditional single-phase currents. Data in the input state estimation model is better compatible by estimating the pre-processed data, classifying and correcting the data including voltage and current. Moreover, on the premise of lacking of distributed energy measurement devices, this paper establishes a state estimation model for distributed power, which is used to correct the default data of distributed energy and improve the quality of input data in wind power and photovoltaic. The method can be verified in the actual example. Compared with the traditional state estimation, the active distribution network state estimation algorithm based on decision tree self-identification has better estimation effect and faster iteration speed. Therefore, the proposed algorithm can be effectively applied to the current state estimation of large-scale distributed energy access.",2019,,10.1049/cp.2019.0490
360,"Curé, Olivier and Kerdjoudj, Fadhela and Faye, David and Le Duc, Chan and Lamolle, Myriam",On the Potential Integration of an Ontology-Based Data Access Approach in NoSQL Stores,Ontologies;Diseases;Indexes;Drugs;Semantics;Context;Ontology Based Data Access (OBDA);NoSQL;Document store;SPARQL;Social Application,"No SQL stores are emerging as an efficient alternative to relational database management systems in the context of big data. Many actors in this domain consider that to gain a wider adoption, several extensions have to be integrated. Some of them focus on the ways of proposing more schema, supporting adapted declarative query languages and providing integrity constraints in order to control data consistency and enhance data quality. We consider that these issues can be dealt with in the context of Ontology Based Data Access (OBDA). OBDA is a new data management paradigm that exploits the semantic knowledge represented in ontologies when querying data stored in a database. We provide a proof of concept of OBDA's ability to tackle these three issues in a social application related to the medical domain.",2012,,10.1109/EIDWT.2012.27
361,"Wang, Fei and Wang, Hongbo",Record Linkage Using the Combination of Twice Iterative SVM Training and Controllable Manual Review,Couplings;Support vector machines;Training;Indexing;Manuals;Motion pictures;record linkage;internet video;support vector machine;manual review,"Record linkage is widely used in many fields, which is a crucial step to increase data quality before data analyzing and data mining. The task of record linkage is to identify records that correspond to the same entities from multi-sources data. In this paper, we describe detailed process of record linkage through an application of internet video, with the purpose of guiding the practice. A method of combination of twice Iterative SVM (Support Vector Machine) training and controllable manual review has been presented. The experiment based on abundant actual data achieves over 98% in F-score.",2016,,10.1109/DASC-PICom-DataCom-CyberSciTec.2016.21
362,"Qiang, Li and Zhengwei, Jiang and Zeming, Yang and Baoxu, Liu and Xin, Wang and Yunan, Zhang",A Quality Evaluation Method of Cyber Threat Intelligence in User Perspective,"Indexes;Testing;Quantization (signal);Feeds;Blacklisting;Business;Threat Intelligence, Quality Evaluation, User Perspective, Vendor","With the widely use of cyber threat intelligence, the influence of security threats and cyber attacks have been relieved and controlled in a degree. More and more users have accepted the conception of threat intelligence and are trying to use threat intelligence in routine security protection. Then, how to choose appropriate threat intelligence vendors and services has become a crucial issue. The present research of threat intelligence evaluation mainly focused on one-sided threat intelligence contents and approaches, which was lack of comprehensiveness and effectiveness. Aiming at this situation, we propose the comprehensive evaluation architecture of threat intelligence in user perspective to evaluate threat intelligence services in several dimensions with quantitative index system. We also carried out typical experiments for threat intelligence data feeds and comprehensive situation to verify the feasibility of proposed method. The results show that the proposed evaluation method has a clear advantage in coverage and partition degree.",2018,,10.1109/TrustCom/BigDataSE.2018.00049
363,"Li, Xinyi and Feng, Zikun and Li, Yan and Liu, Zhao and Liu, Ryan Wen",Spatio-Temporal Vessel Trajectory Smoothing Using Empirical Mode Decomposition and Wavelet Transform,Trajectory;Wavelet transforms;Noise reduction;Artificial intelligence;Marine vehicles;Navigation;Empirical mode decomposition;wavelet transform;data denoising;automatic identification system;trajectory data,"The Automatic Identification System (AIS) has attracted increasing attention in recent years for its superior properties in ocean engineering and maritime management. The spatio-temporal vessel trajectory data is highly related to the received AIS data. However, the AIS raw data often suffer from undesirable noise during signal acquisition and analog-to-digital conversion. To improve AIS-based vessel trajectory data quality, we propose to develop a vessel trajectory smoothing method by combining empirical mode decomposition (EMD) with wavelet transform. In particular, EMD is introduced to decompose the original vessel trajectory data into several sub-trajectories. The EMD decomposition is able to assist in enhancing the robustness of trajectory smoothing. Wavelet transform is directly adopted to smooth the decomposed sub-trajectories. The final high-quality trajectory is obtained by combining the smoothed sub-trajectories in this work. The proposed method has the capacity of removing the unwanted noise while preserving the important trajectory features. Numerous experiments have illustrated the superior smoothing performance of the proposed combined method.",2019,,10.1109/ICBDA.2019.8713242
364,"Appiktala, Nirupama and Chen, Miao and Natkovich, Michael and Walters, Joshua",Demystifying dark matter for online experimentation,Sociology;Statistics;Servers;Testing;Finance;Google;Measurement;online experimentation;quality assurance;bucket size gap;dark matter;loss of traffic;data quality,"The rise of online controlled experimentation, a.k.a. A/B testing began around the turn of the millennium with the emergence of internet giants like Amazon, Bing, Facebook, Google, LinkedIn, and Yahoo. A step towards good experimental design includes the planning for sample size, confidence level, metrics to be measured and test duration. Generally, these factors impact the quality and validity of an experiment. In practice, additional factors may also impact the validity of an experiment. One such critical factor is the discrepancy between the planned bucket size and the actual bucket size. We call this hidden gap “Experimentation Dark Matter”. Experimentation dark matter is invisible to A/A or A/B validation of experimental analysis but can impact the validity of an experiment. In this paper, we have demonstrated in detail, this gap that may cause the loss of statistical power as well as the loss of representativeness and generalizability of an experiment. We have proposed a framework to monitor experimentation dark matter that may go unnoticed in a balanced AB test. We have further discussed the remediation of a recent dark matter issue using our framework. This scalable, low-latency framework is effective and applicable to similar online controlled experimentation systems.",2017,,10.1109/BigData.2017.8258096
365,"Azmy, Sherif B. and Zorba, Nizar and Hassanein, Hossam S.",Quality Estimation for Scarce Scenarios Within Mobile Crowdsensing Systems,Measurement;Internet of Things;Standards;Smart cities;Task analysis;Intelligent sensors;Data quality;Internet of Things (IoT);IoT architectures;IoT-based services;mobile crowdsensing (MCS);small data (SD),"Mobile crowdsensing (MCS) is a paradigm that exploits the presence of a crowd of moving human participants to acquire, or generate, data from their environment. As a part of the Internet-of-Things (IoT) paradigm, MCS serves the quest for a more efficient operation of a smart city. Big data techniques employed on this data produce inferences about the participants' environment, the smart city. However, sufficient amounts of data are not always available. Sometimes, the available data are scarce as it is obtained at different times, locations, and from different MCS participants who may not be present. As a consequence, the scale of data acquired may be small and susceptible to errors. In such scenarios, the MCS system requires techniques that acquire reliable inferences from such limited data sets. To that end, we resort to small data (SD) techniques that are relevant for scarce and erroneous scenarios. In this article, we discuss SD and propose schemes to tackle the problems associated with such limited data sets, in the context of the smart city. We propose two novel quality metrics: 1) MAD quality metric (MAD-Q) and 2) MAD bootstrap quality metric (MADBS-Q), to deal with SD, focusing on evaluating the quality of a data set within MCS. We also propose an MCS-specific coverage metric that combines the spatial dimension with MAD-Q and MADBS-Q. We show the performance of all the presented techniques through closed-form mathematical expressions, with which simulation results were found to be consistent.",2020,,10.1109/JIOT.2020.2994556
366,"Zhao, Liang and Chen, Zhikui and Yang, Zhennan and Hu, Yueming",A Hybrid Method for Incomplete Data Imputation,Clustering algorithms;Partitioning algorithms;Algorithm design and analysis;Accuracy;Feature extraction;Integrated circuits;Time complexity;missing values;data imputation;stacked auto-encoder;incremental clustering,"With the explosive increase of data volume, the research of data quality and data usability draws extensive attention. In this work, we focus on one aspect of data usability -- incomplete data imputation, and present a novel missing value imputation method using stacked auto-encoder and incremental clustering (SAICI). Specifically, SAICI's functionality rests on four pillars: (i) a distinctive value assigned to impute missing values initially, (ii) the stacked auto-encoder(SAE) applied to locate principal features, (iii) a new incremental clustering utilized to partition incomplete data set, and (iv) the top nearest neighbors' weighted values designed to refill the missing values. Most importantly, stages (ii)~(iv) iterate until convergence condition is satisfied. Experimental results demonstrate that the proposed scheme not only imputes the missing data values effectively, but also has better time performance. Moreover, this work is suitable for distributed data processing framework, which can be applied to the imputation of incomplete big data.",2015,,10.1109/HPCC-CSS-ICESS.2015.103
367,"Liu, Yangxiaoyue and Yang, Yaping and Jing, Wenlong",Potential Applicability of SMAP in ECV Soil Moisture Gap-Filling: A Case Study in Europe,Microwave radiometry;Meteorology;Satellite broadcasting;Sensors;Soil moisture;Synthetic aperture radar;Soil measurements;Gap-filling;satellite retrieved soil moisture;the essential climate variable soil moisture;the soil moisture active passive soil moisture,"The Essential Climate Variable (ECV) soil moisture (SM) datasets, originated from the European Space Agency, have revealed great potential for application in hydrology and agriculture. Hence, it is essential to continuously enhance the data quality and spatial completeness to satisfy the increasing scientific research requirements. In this study, we explore the potential possibility of Soil Moisture Active Passive (SMAP) datasets in filling the gaps of ECV SM. The comprehensive assessment results show that: (1) The data missing percent of gap-filled ECV decreases 20% on average, which can be one step closer to generate a seamlessly covered global land surface SM product with favorable quality. (2) Compared to the original ECV, the gap-filled ECV products express similar good response to the in-situ measurements, suggesting that the SMAP SM products could be taken to efficiently fill the gaps and consistently maintain favorable accuracy at the same time. (3) Compared to the in-situ measurements, the original ECV SM products demonstrate extremely high probability density peak percentages. Fortunately, this eminent high value could be effectively rectified through gap-filling progress using SMAP. Overall, this study conducts objective and detailed evaluation on the performance of applying SMAP to fill the gaps of ECV, and is expected to act as a valuable reference in ECV SM gap-filling method.",2020,,10.1109/ACCESS.2020.3009977
368,"Wallis, Kevin and Schillinger, Fabian and Reich, Christoph and Schindelhauer, Christian",Safeguarding Data Integrity by Cluster-Based Data Validation Network,Logic gates;Data integrity;Task analysis;Maintenance engineering;Blockchain;Industrial Internet of Things;Internet of Things;Data Validation;Cluster-Based Data Validation;Big Data,"Ensuring data quality is central to the digital transformation in industry. Business processes such as predictive maintenance or condition monitoring can be implemented or improved based on the available data. In order to guarantee high data quality, a single data validation system are usually used to validate the production data for further use. However, using a single system allows an attacker only to perform one successful attack to corrupt the whole system. We present a new approach in which a data validation system using multiple different validators minimizes the probability of success for the attacker. The validators are arranged in clusters based on their properties. For a validation process, a challenge is given that specifies which validators should perform the current validation. Validation results from other validators are dropped. This ensures that even for more than half of the validators being corrupted anomalies can be detected during the validation process.",2019,,10.1109/WorldS4.2019.8904039
369,"Chang, Huijuan and Yu, Zhiyong and Yu, Zhiwen and Guo, Bin",Selecting Sensing Location Leveraging Spatial and Cross-Domain Correlations,Sensors;Correlation;Air quality;Data models;Estimation;Data integrity;Task analysis;Active learning;location selection;kriging interpolation;regression tree,"In environmental monitoring applications, selecting appropriate locations to sense is important relating to data quality and Sensing cost. This paper addresses the challenge by collecting data from a subset of locations, then leveraging the spatial and cross-domain correlations to deduce data of other locations, thus can obtain acceptable data quality with lower sensing cost. Referring to active learning, the proposed framework is constructed by two types modules (i.e., estimators and selectors) and a cyclic process of estimating and selecting. Estimators based on kriging interpolation and regression tree are implemented, and their corresponding selectors are designed. We evaluate the effectiveness of the framework by taking air quality sensing as an example. Results show that to reach data quality of about 25% MAPE, the framework only needs 15% locations, while random selector needs 25% locations.",2019,,10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00149
370,"Ilyas, Ihab F. and Chu, Xu",,Data Cleaning and Information Extraction;Data Integration and Exchange,"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and wrong business decisions. According to a report by InsightSquared in 2012, poor data across businesses and the government cost the United States economy 3.1 trillion dollars a year. To detect data errors, data quality rules or integrity constraints (ICs) have been proposed as a declarative way to describe legal or correct data instances. Any subset of data that does not conform to the defined rules is considered erroneous, which is also referred to as a violation. Various kinds of data repairing techniques with different objectives have been introduced where algorithms are used to detect subsets of the data that violate the declared integrity constraints, and even to suggest updates to the database such that the new database instance conforms with these constraints. While some of these algorithms aim to minimally change the database, others involve human experts or knowledge bases to verify the repairs suggested by the automatic repeating algorithms. Trends in Cleaning Relational Data: Consistency and Deduplication discusses the main facets and directions in designing error detection and repairing techniques. It proposes a taxonomy of current anomaly detection techniques, including error types, the automation of the detection process, and error propagation. It also sets out a taxonomy of current data repairing techniques, including the repair target, the automation of the repair process, and the update model. It concludes by highlighting current trends in ""big data"" cleaning.",2015,,10.1561/1900000045
371,"Tas, Yucel and Baeth, Mohamed Jehad and Aktas, Mehmet S.",An Approach to Standalone Provenance Systems for Big Social Provenance Data,Social network services;Scalability;Data privacy;Distributed databases;Web services;provenance systems;social provenance data;big provenance data;provenance storage systems;decentralized provenance systems,"Provenance about data derivations in social networks is usually called social data provenance, which helps in the assessment of data quality, resource tracking, and understanding the dissemination of information in social networks. The collection and processing of social data provenance leads to some challenges such as scalability, data quality, and privacy awareness. This study introduces a test suite to evaluate the current state-of-the-art standalone and centralized provenance systems. We conduct performance (responsiveness) and scalability experiments and investigate whether the standalone provenance systems are capable of handling large-size social provenance data. We also propose a software architecture for a decentralized and scalable provenance management system for big social provenance data.",2016,,10.1109/SKG.2016.010
372,"Li, Zhi and Guo, Hanyang and Wang, Wai Ming and Guan, Yijiang and Barenji, Ali Vatankhah and Huang, George Q. and McFall, Kevin S. and Chen, Xin",A Blockchain and AutoML Approach for Open and Automated Customer Service,Customer services;Blockchain;Companies;Machine learning;Personnel;Data models;Smart contracts;Automated customer service;automated machine learning (AutoML);blockchain;open customer service,"Customer service is transforming from traditional manual service toward automated service, which utilizes different computational informatics to achieve a higher efficient and quality services. Automated customer service requires big data and expertise in data analysis as prerequisites. However, many companies, especially small and medium enterprises, do not have sufficient data and experience due to their limited scale and resources. They need to rely on third parties, and this reliance results in the lack of development of core customer service competency. In order to overcome these challenges, an open and automated customer service platform based on Internet of things (IoT), blockchain, and automated machine learning (AutoML) is proposed. The data are gathered with the use of IoT devices during the customer service. An open but secured environment to achieve data trading is ensured by using blockchain. AutoML is adopted to automate the data analysis processes for reducing the reliance of costly experts. The proposed platform is analyzed through use case evaluation. A prototype system has also been developed and evaluated. The simulation results show that our platform is scalable and efficient.",2019,,10.1109/TII.2019.2900987
373,"Chakravorty, Antorweep and Wlodarczyk, Tomasz Wiktor and Rong, Chunming",A Scalable K-Anonymization Solution for Preserving Privacy in an Aging-in-Place Welfare Intercloud,Data privacy;Partitioning algorithms;Cryptography;Smart homes;Scalability;Dictionaries;Privacy;privacy;k-anonymization;hadoop;intercloud;aging in place,"Aging-in-Place solutions are becoming increasingly prevalent in our society. New age big data technologies can harness upon enormous amount of data generated from sensors in smart homes to provide enabling services. Added care and preventive services can be furnished through interoperability and bidirectional dataflow across the value chain. However the nature of the problem domain which although allows establishing better care through sharing of information also risks disclosing complete living behavior of individuals. In this paper, we introduce and evaluate a novel scalable k-anonymization solution based upon the distributed map-reduce paradigm for preserving privacy of the shared data in a welfare intercloud. Our evaluation benchmarks both information loss and data quality metrics and demonstrates better scalability/performance than any other available solutions.",2014,,10.1109/IC2E.2014.43
374,"Islam, MD. Mofijul and Razzaque, MD. Abdur and Hassan, Mohammad Mehedi and Ismail, Walaa Nagy and Song, Biao",Mobile Cloud-Based Big Healthcare Data Processing in Smart Cities,Cloud computing;Medical services;Mobile communication;Smart cities;Servers;Real-time systems;Quality of service;Smart health care;smart city;big data;quality of service (QoS);virtual machine migration;ant colony optimization,"In recent years, the Smart City concept has become popular for its promise to improve the quality of life of urban citizens. The concept involves multiple disciplines, such as Smart health care, Smart transportation, and Smart community. Most services in Smart Cities, especially in the Smart healthcare domain, require the real-time sharing, processing, and analyzing of Big Healthcare Data for intelligent decision making. Therefore, a strong wireless and mobile communication infrastructure is necessary to connect and access Smart healthcare services, people, and sensors seamlessly, anywhere at any time. In this scenario, mobile cloud computing (MCC) can play a vital role by offloading Big Healthcare Data related tasks, such as sharing, processing, and analysis, from mobile applications to cloud resources, ensuring quality of service demands of end users. Such resource migration, which is also termed virtual machine (VM) migration, is effective in the Smart healthcare scenario in Smart Cities. In this paper, we propose an ant colony optimization-based joint VM migration model for a heterogeneous, MCC-based Smart Healthcare system in Smart City environment. In this model, the user’s mobility and provisioned VM resources in the cloud address the VM migration problem. We also present a thorough performance evaluation to investigate the effectiveness of our proposed model compared with the state-of-the-art approaches.",2017,,10.1109/ACCESS.2017.2707439
375,"Tshikomba, Salome C. and Estrice, Milton and Ojo, Evans and Davidson, Innocent E",Curbing Electricity Theft Using Wireless Technique with Communication Constraints,ZigBee;Meters;Wireless communication;Automation;Smart meters;Monitoring;Sensors;Losses;smart meters;AMI;wireless technique;ZigBee technology;NTL,"Utility services are experiencing a common problem of power losses, which impose a significant impact on their annual budget. Practically, power losses consist of technical losses and non-technical losses. Technical losses are due to operations and aging of infrastructure, while nontechnical losses (NTL) are due to non-metered energy. The focus is on managing non-technical losses using an automation wireless method. The wireless ZigBee technique is proposed and further investigated for communication failure over long distances while solving the problem of stealing electricity. Advance-metering infrastructure (AMI) technique and smart meters are feasible for system integration; that is why they are chosen to be part of this study. The success of the study depends on quality data of the Utility, meaning the more accurate the data, the easier the analysis of outliers. The operation and planning of revenue protection contain a large amount of data that needs to be worked on, so data mining assists in that regard. Then the load profiling method assists in illustrating the variation in demand/electricalload over a specific time. This is a preliminary investigation using a wireless communication technique as a viable solution in curbing electricity theft. The uniqueness of the proposed ZigBee system is that it recognizes the everyday act of stealing electricity through tempering with the meter box and tapping of the supply.",2020,,10.1109/icABCD49160.2020.9183812
376,"Darari, Fariz and Nutt, Werner and Razniewski, Simon",Comparing Index Structures for Completeness Reasoning,Cognition;Resource description framework;Metadata;Data integrity;Tools;Iris;Complexity theory,"Data quality is a major issue in the devel- opment of knowledge graphs. Data completeness is a key factor in data quality pertaining to how broad and deep is information contained in knowledge graphs. As for large- scale knowledge graphs (e.g., DBpedia, Wikidata), it is conceivable that given the vast amount of information contained in there, they may be complete for a wide range of topics, such as children of Joko Widodo, cantons of Switzerland, and presidents of Indonesia. Previous research has shown how one can augment knowledge graphs with statements about their completeness, stating which parts of data are complete. Such meta-information can be leveraged to check query completeness, that is, whether the answer returned by a query is complete. Yet, it is still unclear how such a check can be done in practice, especially when many completeness statements are involved. We devise implementation techniques to make completeness reasoning in the presence of large sets of completeness statements feasible, and experimentally evaluate their effectiveness in realistic settings based on the characteristics of real-world knowledge graphs.",2018,,10.1109/IWBIS.2018.8471712
377,"Shuai, Hong-Han and Yang, De-Nian and Shen, Chih-Ya and Yu, Philip S. and Chen, Ming-Syan",QMSampler: Joint Sampling of Multiple Networks with Quality Guarantee,Facebook;Roads;Big Data;Electronic mail;LinkedIn;Measurement;Social network;graph sampler;data quality analysis;optimization,"Because Online Social Networks (OSNs) have become increasingly important in the last decade, they have motivated a great deal of research on Social Network Analysis (SNA). Currently, SNA algorithms are evaluated on real datasets obtained from large-scale OSNs, which are usually sampled by Breadth-First-Search (BFS), Random Walk (RW), or some variations of the latter. However, none of the released datasets provides any statistical guarantees on the difference between the sampled datasets and the ground truth. Moreover, all existing sampling algorithms only focus on sampling a single OSN, but each OSN is actually a sampling of a complete social network. Hence, even if the whole dataset from a single OSN is sampled, the results may still be skewed and may not fully reflect the properties of the complete social network. To address the above issues, we have made the first attempt to explore the joint sampling of multiple OSNs and propose an approach called Quality-guaranteed Multi-network Sampler (QMSampler) that can jointly sample multiple OSNs. QMSampler provides a statistical guarantee on the difference between the sampled real dataset and the ground truth (the perfect integration of all OSNs). Our experimental results demonstrate that the proposed approach generates a much smaller bias than any existing method. QMSampler has also been released as a free download.",2018,,10.1109/TBDATA.2017.2715847
378,"Manjula, K. R. and Gangothri, R.",Hybrid model based uncertainty analysis for geospatial metadata supporting decision making for spatial exploration,Metadata;Uncertainty;Geospatial analysis;Standards;Spatial databases;Analytical models;Geospatial Metadata;GIS;ontology;standard deviation;probability density function,"The proliferation of Geospatial data analytics has greatly increased the usage of GIS in all smartphones and gadgets in today's Big Data environment. The footprints of GIS are found in all fields from government to business analytics. Therefore the error propagation in such a substantial need may lead to misunderstood decisions and confusions during emergency. In this paper we take up a chance to document the data quality assurance using the geospatial metadata based uncertainty analysis approach. The paper takes up an initial attempt to state that chances occur for existence of uncertainty in metadata. And it proposes a hybrid model combing ontology, standard deviation and probability density function for detecting the occurrence of uncertainties in geospatial metadata.",2016,,
379,"Qadri, Yazdan Ahmad and Nauman, Ali and Zikria, Yousaf Bin and Vasilakos, Athanasios V. and Kim, Sung Won",The Future of Healthcare Internet of Things: A Survey of Emerging Technologies,Internet of Things;Medical services;Edge computing;Sensors;Blockchain;Quality of service;Big Data;H-IoT;WBAN;machine learning;fog computing;edge computing;blockchain;software defined networks,"The impact of the Internet of Things (IoT) on the advancement of the healthcare industry is immense. The ushering of the Medicine 4.0 has resulted in an increased effort to develop platforms, both at the hardware level as well as the underlying software level. This vision has led to the development of Healthcare IoT (H-IoT) systems. The basic enabling technologies include the communication systems between the sensing nodes and the processors; and the processing algorithms for generating an output from the data collected by the sensors. However, at present, these enabling technologies are also supported by several new technologies. The use of Artificial Intelligence (AI) has transformed the H-IoT systems at almost every level. The fog/edge paradigm is bringing the computing power close to the deployed network and hence mitigating many challenges in the process. While the big data allows handling an enormous amount of data. Additionally, the Software Defined Networks (SDNs) bring flexibility to the system while the blockchains are finding the most novel use cases in H-IoT systems. The Internet of Nano Things (IoNT) and Tactile Internet (TI) are driving the innovation in the H-IoT applications. This paper delves into the ways these technologies are transforming the H-IoT systems and also identifies the future course for improving the Quality of Service (QoS) using these new technologies.",2020,,10.1109/COMST.2020.2973314
380,"Fei, Chen",Research on Lidar Data Error Correction Method Based on Bayesian Network,Laser radar;Orbits (stellar);Bayes methods;Error correction;Calibration;Reliability;Remote sensing;Bayesian network;lidar;data error;correction methoIntroduction,"Data quality analysis is the first and key step of remote sensing mechanism/application research (especially quantitative remote sensing), which directly affects the accuracy of remote sensing inversion and the effect of remote sensing applications. Lidar (light detection and ranging, referred to as lidar) is a new active remote sensing technology. The research on hardware system development and data post-processing algorithm needs to be further strengthened, especially for the quality analysis of lidar data. After initializing the Bayesian network and checking the error, a correction mathematical model is established. Experiments have proved that after correction, the angle error of the radar is significantly improved, which verifies the feasibility and reliability of the precision orbit star calibration method.",2020,,10.1109/IAAI51705.2020.9332846
381,"Baoyu, Li and Guoxing, Li and Guiyu, Wang and Guofeng, Zhang and Man, Yang",Research on CART Model of Mass Concrete Temperature Prediction Based on Big Data Processing Technology,Temperature sensors;Temperature distribution;Temperature measurement;Temperature control;Predictive models;Data models;Cooling;Mass concrete temperature;big data processing technology;CART prediction model,"Due to the influence of temperature changes or temperature gradients in the construction process of mass concrete, temperature cracks will occur in the concrete. In order to achieve a reasonable prediction of the temperature change of the mass concrete during the construction process and accurately obtain the temperature change trend, this paper attempts to construct a CART prediction model based on the big data processing technology based on the characteristics of the temperature change of the mass concrete. This paper introduces in detail how to use data processing methods such as outlier identification, missing value filling and random error elimination to improve data quality, as well as the method for constructing the CART prediction model, and combines engineering examples to demonstrate the feasibility of the model method. The results show that the model and method can better predict the temperature change of mass concrete. It has high prediction accuracy and can provide necessary guidance for practical engineering.",2022,,10.1109/ACCESS.2022.3161556
382,"Yustiawan, Yoga and Ramadhan, Hani and Kwon, Joonho",A Stacked Denoising Autoencoder and Long Short-Term Memory Approach With Rule-Based Refinement to Extract Valid Semantic Trajectories,Semantics;Trajectory;Location awareness;Hidden Markov models;Deep learning;Noise measurement;Indoor environment;Deep learning;indoor localization;the Internet of Things;rule-based refinement;semantic trajectories,"Indoor location-based services have been widely investigated to take advantage of semantic trajectories for providing user oriented services in indoor environments. Although indoor semantic trajectories can provide seamless understanding to users regarding the provided location-based services, studies on the application of deep learning approaches for robust and valid semantic indoor localization are lacking. In this study, we combined a stacked denoising autoencoder and long short term memory technique with a rule-based refinement method applying a rule-based hidden Markov model (HMM) to perform robust and valid semantic trajectory extraction. In particular, our rule-based HMM approach incorporates a direct set of rules into HMM to resolve invalid movements of the extracted semantic trajectories and is extensible to various deep learning techniques. We compared the performance of our proposed approach with that of other cutting-edge deep learning approaches on two different real-world data sets. The experimental results demonstrate the feasibility of our proposed approach to produce more robust and valid semantic trajectories.",2021,,10.1109/ACCESS.2021.3080288
383,"Shen, Jian and Zhou, Tianqi and Wang, Kun and Peng, Xin and Pan, Li",Artificial Intelligence Inspired Multi-Dimensional Traffic Control for Heterogeneous Networks,Heterogeneous networks;Backpropagation;Telecommunication traffic;Big Data;Neural networks;Traffic control;Intelligent networks;Big Data;Quality of service;Networked control systems;Recurrent neural networks,"The heterogeneous network is the foundation of next-generation networks. It aims to explore the existing network resources effectively, and providing better QoS for every kind of traffic flow as far as possible. However, the diversity and dynamic nature of heterogeneous networks will bring a huge burden and big data to the network traffic control. Therefore, how to achieve efficient and intelligent network traffic control becomes the key problem of heterogeneous networks. In this article, an AI-inspired traffic control scheme is proposed. In order to realize fine-grained traffic control in heterogeneous networks, multi-dimensional (i.e., inter-layer, intra-layer, and caching and pushing) network traffic control is introduced. It is worth noting that backpropagation in deep recurrent neural networks is applied in the intra-layer such that an intelligent traffic control scheme can be derived efficiently when facing the huge traffic load in heterogeneous networks. Moreover, DBSCAN is adopted in the inter-layer, which supports efficient classification in the inter-layer. In addition, caching and pushing is adopted to make full use of network resources and provide better QoS. Simulation results demonstrate the effectiveness and practicability of the proposed scheme.",2018,,10.1109/MNET.2018.1800120
384,"Jiang, Haoyu and Chen, Kai and Ge, Quanbo and Wang, Yun and Xu, Jinqiang and Li, Chunxi",Fault Diagnosis of Power IoT System Based on Improved Q-KPCA-RF Using Message Data,Monitoring;Internet of Things;Random forests;Logic gates;Feature extraction;Clustering algorithms;Neural networks;Communication message;power Internet of Things (IoT) system;Q learning;random forest (RF),"As the power system develops from informatization to intelligence. Research on data services based on the Internet of Things (IoT) focuses more on application functions, but the research on the data quality of the IoT itself is insufficient. Long-term continuous operation of the big data IoT system has the risk of performance degradation or even partial fault, which leads to a decrease in the availability of collected data for intelligent analysis. In this article, based on the power IoT message data, the characteristics are established through a variety of improved detection methods, and then the abnormal data type is obtained through Q learning and fusion of the random forest (RF) identification features. Finally, the topology of the specific power user IoT system is combined with kernel principal component analysis (KPCA) + improved RF algorithm getting the abnormal location of the IoT. The results show that the research method has a significantly higher positioning accuracy (from 61% to 97%) than the traditional RF method, and the combination method has more advantages in parameter adjustment and classification accuracy than directly using a multilayer perceptron (MLP).",2021,,10.1109/JIOT.2021.3058563
385,"Chen, Chien-Chih and Chang, Yu-Jung and Chung, Wei-Chun and Lee, Der-Tsai and Ho, Jan-Ming",CloudRS: An error correction algorithm of high-throughput sequencing data based on scalable framework,Error correction;Algorithm design and analysis;Sequential analysis;Assembly;Bioinformatics;Genomics;Benchmark testing;error correction;mapreduce;genome assembly;next-generation sequencing,"Next-generation sequencing (NGS) technologies produce huge amounts of data. These sequencing data unavoidably are accompanied by the occurrence of sequencing errors which constitutes one of the major problems of further analyses. Error correction is indeed one of the critical steps to the success of NGS applications such as de novo genome assembly and DNA resequencing as illustrated in literature. However, it requires computing time and memory space heavily. To design an algorithm to improve data quality by efficiently utilizing on-demand computing resources in the cloud is a challenge for biologists and computer scientists. In this study, we present an error-correction algorithm, called the CloudRS algorithm, for correcting errors in NGS data. The CloudRS algorithm aims at emulating the notion of error correction algorithm of ALLPATHS-LG on the Hadoop/ MapReduce framework. It is conservative in correcting sequencing errors to avoid introducing false decisions, e.g., when dealing with reads from repetitive regions. We also illustrate several probabilistic measures we introduce into CloudRS to make the algorithm more efficient without sacrificing its effectiveness. Running time of using up to 80 instances each with 8 computing units shows satisfactory speedup. Experiments of comparing with other error correction programs show that CloudRS algorithm performs lower false positive rate for most evaluation benchmarks and higher sensitivity on genome S. cerevisiae. We demonstrate that CloudRS algorithm provides significant improvements in the quality of the resulting contigs on benchmarks of NGS de novo assembly.",2013,,10.1109/BigData.2013.6691642
386,"Lin, Mengting and Zhao, Youping",Artificial intelligence-empowered resource management for future wireless communications: A survey,Resource management;Artificial intelligence;5G mobile communication;Wireless communication;Big Data;Quality of service;Network slicing;5G;beyond 5G (B5G);6G;artificial intelligence (AI);machine learning (ML);network slicing;resource management,"How to explore and exploit the full potential of artificial intelligence (AI) technologies in future wireless communications such as beyond 5G (B5G) and 6G is an extremely hot inter-disciplinary research topic around the world. On the one hand, AI empowers intelligent resource management for wireless communications through powerful learning and automatic adaptation capabilities. On the other hand, embracing AI in wireless communication resource management calls for new network architecture and system models as well as standardized interfaces/protocols/data formats to facilitate the large-scale deployment of AI in future B5G/6G networks. This paper reviews the state-of-art AI-empowered resource management from the framework perspective down to the methodology perspective, not only considering the radio resource (e.g., spectrum) management but also other types of resources such as computing and caching. We also discuss the challenges and opportunities for AI-based resource management to widely deploy AI in future wireless communication networks.",2020,,10.23919/JCC.2020.03.006
387,"Lattuada, Marco and Barbierato, Enrico and Gianniti, Eugenio and Ardagna, Danilo",Optimal Resource Allocation of Cloud-Based Spark Applications,Cloud computing;Sparks;Big Data;Task analysis;Resource management;Computational modeling;Optimization;Big Data;Quality of Service;Elastic resource provisioning;Cluster management,"Nowadays, the big data paradigm is consolidating its central position in the industry, as well as in society at large. As big data applications gain more and more importance over time and given the dynamic nature of cloud resources, it is fundamental to develop an intelligent resource management system to provide Quality of Service guarantees to end-users. This paper presents a set of run-time optimization-based resource management policies for advanced big data analytics. Users submit Spark applications characterized by a priority and by a hard or soft deadline. Optimization policies address two scenarios: i) identification of the minimum capacity to run a Spark application within the deadline; ii) re-balance of the cloud resources in case of heavy load, minimising the weighted soft deadline application tardiness. The results obtained in the first scenario demonstrate that the percentage error of the prediction of the optimal resource usage with respect to system measurement and exhaustive search is the range 4%-29% while literature-based techniques present an average error in the range 6%-63%. Moreover, in the second scenario, the proposed algorithms can address complex scenarios with an error of 8% on average while literature-based approaches obtain an average error of about 57%.",2020,,10.1109/TCC.2020.2985682
388,"Yaseen, Muhammad Usman and Anjum, Ashiq and Antonopoulos, Nick",Spatial Frequency Based Video Stream Analysis for Object Classification and Recognition in Clouds,Streaming media;Cameras;Cloud computing;Lighting;Image color analysis;Empirical mode decomposition;Feature extraction;Empirical Mode Decomposition;Local Ternary Patterns;Riesz Transform;Amplitude Spectrum;Cloud Computing;Big Data Analytics;Object Classification,"The recent rise in multimedia technology has made it easier to perform a number of tasks. One of these tasks is monitoring where cheap cameras are producing large amount of video data. This video data is then processed for object classification to extract useful information. However, the videodata obtained by these cheap cameras is often of low qualityand results in blur video content. Moreover, various illuminationeffects caused by lightning conditions also degradethe video quality. These effects present severe challenges forobject classification. We present a cloud-based blur and illumination invariant approach for object classification fromimages and video data. The bi-dimensional empirical modedecomposition (BEMD) has been adopted to decompose avideo frame into intrinsic mode functions (IMFs). TheseIMFs further undergo to first order Reisz transform to generatemonogenic video frames. The analysis of each IMF hasbeen carried out by observing its local properties (amplitude, phase and orientation) generated from each monogenic videoframe. We propose a stack based hierarchy of local patternfeatures generated from the amplitudes of each IMF whichresults in blur and illumination invariant object classification. The extensive experimentation on video streams aswell as publically available image datasets reveals that oursystem achieves high accuracy from 0.97 to 0.91 for increasingGaussian blur ranging from 0.5 to 5 and outperformsstate of the art techniques under uncontrolled conditions. The system also proved to be scalable with high throughputwhen tested on a number of video streams using cloud infrastructure.",2016,,
389,"McMorran, A. W. and Rudd, S. E. and Shand, C. M. and Simmins, J. J. and McCollough, N. and Stewart, E. M.",Data integration challenges for standards-compliant mobile applications,Computer integrated manufacturing;Mobile communication;IEC standards;Logic gates;Servers;Data models;Asset management;Application virtualization;Virtual reality;Visualization;Standards;Data handling;Data visualization;CIM;Data integration;Big Data,"Modern mobile devices are capable of running sophisticated, network-enabled applications exploiting a variety of sensors on a single low-cost piece of hardware. The electrical industry can benefit from these new platforms to automate existing processes and provide engineers and field crew with access to large amounts of complex data in real-time, anywhere in the world. The development of a standards-based application decouples the mobile client application from a single vendor or existing enterprise system, but requires a complex data integration architecture to support the use and exploitation of large amounts of data spread across multiple existing systems. The integration with a mobile application introduces new challenges when dealing with remote devices where data network communications cannot be relied on, especially under storm conditions, and the devices themselves are at risk of being lost or stolen. Addressing these challenges offers the potential to improve data quality, enable access to accurate, up-to-date information in the field and ultimately save a utility time and money.",2014,,10.1109/TDC.2014.6863306
390,"Pan, Yongsheng and Liu, Mingxia and Lian, Chunfeng and Xia, Yong and Shen, Dinggang",Spatially-Constrained Fisher Representation for Brain Disease Identification With Incomplete Multi-Modal Neuroimages,Magnetic resonance imaging;Feature extraction;Diseases;Positron emission tomography;Medical diagnosis;Brain modeling;Deep learning;Multi-modal neuroimage;incomplete data;generative adversarial network;fisher vector;brain disease diagnosis;MRI;PET,"Multi-modal neuroimages, such as magnetic resonance imaging (MRI) and positron emission tomography (PET), can provide complementary structural and functional information of the brain, thus facilitating automated brain disease identification. Incomplete data problem is unavoidable in multi-modal neuroimage studies due to patient dropouts and/or poor data quality. Conventional methods usually discard data-missing subjects, thus significantly reducing the number of training samples. Even though several deep learning methods have been proposed, they usually rely on pre-defined regions-of-interest in neuroimages, requiring disease-specific expert knowledge. To this end, we propose a spatially-constrained Fisher representation framework for brain disease diagnosis with incomplete multi-modal neuroimages. We first impute missing PET images based on their corresponding MRI scans using a hybrid generative adversarial network. With the complete (after imputation) MRI and PET data, we then develop a spatially-constrained Fisher representation network to extract statistical descriptors of neuroimages for disease diagnosis, assuming that these descriptors follow a Gaussian mixture model with a strong spatial constraint (i.e., images from different subjects have similar anatomical structures). Experimental results on three databases suggest that our method can synthesize reasonable neuroimages and achieve promising results in brain disease identification, compared with several state-of-the-art methods.",2020,,10.1109/TMI.2020.2983085
391,"Song, Guanli and Wang, Yinghui and Zhang, Runshun and Liu, Baoyan and Zhou, Xuezhong and Song, Guanbo and Xie, Liang and Huang, Xinghuan",Methods and technologies of traditional Chinese medicine clinical information datamation in real world,Medical diagnostic imaging;Clinical diagnosis;Electronic medical records;History;Maintenance engineering;Discharges (electric);Hospitals;traditional Chinese Medicine (TCM);clinical research paradigm;clinical research information sharing system;datamation;structured electronic medical record,"Under the guidance of clinical research paradigm of traditional Chinese medicine (TCM) in real world, the research group developed the clinical research information sharing system, in which structured electronic medical record system of traditional Chinese medicine is the technology platform of datamation of clinical diagnosis and treatment information. The clinical diagnosis and treatment information can be activated and used effectively only after datamation and truly become the treasures of knowledge of TCM. This paper discusses the implementation process and technologies and methods of TCM clinical information datamation, and take admission records as an example to demonstrate the contents and realization way of datamation, and a brief introduction of the effect of implementation and application of datamation. By making full use of technologies and methods of datamation, strengthening data quality control in the datamation process, greatly improving the quality of TCM clinical research data, to lay a good foundation for establishment of knowledge base through further statistical analysis or data mining of TCM clinical data.",2014,,10.1109/CIBD.2014.7011533
392,"Ren, Lei and Meng, Zihao and Wang, Xiaokang and Lu, Renquan and Yang, Laurence T.",A Wide-Deep-Sequence Model-Based Quality Prediction Method in Industrial Process Analysis,Feature extraction;Predictive models;Data models;Quality assessment;Product design;Data mining;Analytical models;Industrial artificial intelligence (AI);industrial big data;Industrial Internet of Things;product quality prediction;wide-deep-sequence (WDS) model,"Product quality prediction, as an important issue of industrial intelligence, is a typical task of industrial process analysis, in which product quality will be evaluated and improved as feedback for industrial process adjustment. Data-driven methods, with predictive model to analyze various industrial data, have been received considerable attention in recent years. However, to get an accurate prediction, it is an essential issue to extract quality features from industrial data, including several variables generated from supply chain and time-variant machining process. In this article, a data-driven method based on wide-deep-sequence (WDS) model is proposed to provide a reliable quality prediction for industrial process with different types of industrial data. To process industrial data of high redundancy, in this article, data reduction is first conducted on different variables by different techniques. Also, an improved wide-deep (WD) model is proposed to extract quality features from key time-invariant variables. Meanwhile, an long short-term memory (LSTM)-based sequence model is presented for exploring quality information from time-domain features. Under the joint training strategy, these models will be combined and optimized by a designed penalty mechanism for unreliable predictions, especially on reduction of defective products. Finally, experiments on a real-world manufacturing process data set are carried out to present the effectiveness of the proposed method in product quality prediction.",2020,,10.1109/TNNLS.2020.3001602
393,"Finkelstein, Joseph and Jeong, In Cheol",Using CART for advanced prediction of asthma attacks based on telemonitoring data,Medical treatment;Diseases;Prediction algorithms;Predictive models;Pediatrics;Monitoring;Big data analytics;artificial intelligence;asthma;telemonitoring;exacerbation prediction,"Advanced prediction of asthma exacerbations may significantly improve patient quality of life and reduce costs of urgent care delivery. Majority of current algorithms predict who is likely to experience asthma exacerbation rather than when it is about to occur. We used data from asthma home-based telemonitoring for advanced prediction of asthma exacerbation. The goal of this project was to develop an algorithm that predicts asthma exacerbation one day in advance based on previous 7-day window. CART was used for predictive modeling. Resulting algorithm had specificity 0.971, sensitivity of 0.647, and accuracy of 0.809. We concluded that machine learning has great potential for advanced prediction of chronic disease exacerbations based on home telemonitoring data.",2016,,10.1109/UEMCON.2016.7777890
394,"Lu, Tielin and Fan, Zitian and Lei, Yue and Shang, Yujia and Wang, Chunxi",The Edge Computing Cloud Architecture Based on 5G Network for Industrial Vision Detection,Industries;Cloud computing;Visualization;5G mobile communication;Wireless networks;Data integrity;Computer architecture;5G network;vision detection;communication structure;edge computing;information systems,"The emergence of a large number of real-time data putforward higher requirements on network transmission technology. The new edge computing cloud technology based on 5G network has become an important research direction of vision detection. However, for the industrial users, they still confuse the architecture of the non-public 5G network (NPN) and misunderstand the data quality of service (QOS). In order to overcome the unstable network structure of 5G for vision detection in industry in a limited bandwidth, achieve high-quality transmission of detection image, and obtain intelligent optimal results, has become an urgent problem to be solved. This paper proposes the network configuration and mode, also design a intelligent edge computing cloud based on 5G scheme. In the ends, an vision detection architecture case has been developed on the 5G communication structure and verified visual detection application scene design its feasibility purpose in the wireless network.",2021,,10.1109/ICBDA51983.2021.9402999
395,"Yu, Han and Hanafy, Sherif M. and Liu, Lulu",A Weighted Closure-Phase Statics Correction Method: Synthetic and Field Data Examples,Receivers;Mathematical models;Surface treatment;Sea surface;Indexes;Earth;Computational modeling;Statics;closure phase;first arrivals;interferometry,"Recorded seismograms are usually distorted by statics owing to complex geological conditions, such as lateral variations in sediment thickness or complex topographies. These distorted and discontinuous signals usually exist in either arrival times or amplitudes of waves, and they are mostly likely to be smeared as velocity perturbations along their associated raypaths. Therefore, statics may blur images of the target bodies, or even worse, introduce unexpected and false anomalies into subsurface structures. To partly resolve this problem, we develop a weighted statics correction method to estimate unwanted temporal shifts of traces using the closure-phase technique which is utilized in astronomical imaging. In the proposed method, the source and the receiver statics are regarded as independent quantities contributing to the waveform shifts based on their acquisition geometries. Numerical tests on both the synthetic and field cases show noticeable, although gradual, improvements of data quality compared to the conventional Plus-Minus method. In general, this method provides a straightforward strategy to reedit the traveltimes in seismic profiles without inverting for a near-surface velocity model. Moreover, it can be extended to any interferometrical methods in seismic data processing that satisfy the closure-phase conditions.",2022,,10.1109/TGRS.2022.3169519
396,"Zhang, Yang and Wang, Dong",Poster: On Cost-Sensitive Task Allocation in Social Sensing: An Online Learning Approach,Sensors;Task analysis;Resource management;Real-time systems;Big Data;Dynamic scheduling;Air quality;Social Sensing;Task Allocation;Online Learning,"Social sensing has emerged as a new sensing paradigm where human sensors collectively report measurements about the physical world. This paper focuses on the cost-sensitive task allocation problem in social sensing where the goal is to effectively allocate sensing tasks to the human sensors to meet the desirable data quality requirement of the applications while minimizing the sensing cost. While recent progress has been made to tackle the cost-sensitive task allocation problem, an important challenge has not been well addressed, namely ""real time task allocation"", the task allocation schemes need to respond quickly to the potential large dynamics of the measured variables in social sensing. To address this challenge, this paper presents a Cost-Sensitive Task Allocation (CSTA) scheme inspired by techniques from online learning. The preliminary results show that our new scheme significantly outperforms the-state-of-the-art baselines.",2018,,10.1109/DCOSS.2018.00024
397,"Zhang, Yuhui and Li, Ming and Yang, Dejun and Tang, Jian and Xue, Guoliang and Xu, Jia",Tradeoff Between Location Quality and Privacy in Crowdsensing: An Optimization Perspective,Privacy;Crowdsensing;Degradation;Optimization;Perturbation methods;Sensors;Approximation algorithms;Crowdsensing;location data quality;location privacy;k-anonymity,"Crowdsensing enables a wide range of data collection, where the data are usually tagged with private locations. Protecting users' location privacy has been a central issue. The study of various location perturbation techniques, e.g., k-anonymity, for location privacy has received widespread attention. Despite the huge promise and considerable attention, provable good algorithms considering the tradeoff between location privacy and location information quality from the optimization perspective in crowdsensing are lacking in the literature. In this article, we study two related optimization problems from two different perspectives. The first problem is to minimize the location quality degradation caused by the protection of users' location privacy. We present an efficient optimal algorithm OLoQ for this problem. The second problem is to maximize the number of protected users, subject to a location quality degradation constraint. To satisfy the different requirements of the platform, we consider two cases for this problem: 1) overlapping and 2) nonoverlapping perturbations. For the former case, we give an efficient optimal algorithm OPUMO. For the latter case, we first prove its NP-hardness. We then design a (1-E)-approximation algorithm NPUMN and a fast and effective heuristic algorithm HPUMN. Extensive simulations demonstrate that OLoQ, OPUMO, and HPUMN significantly outperform an existing algorithm.",2020,,10.1109/JIOT.2020.2972555
398,"Yan, Cairong and Song, Yalong and Wang, Jian and Guo, Wenjing",Eliminating the Redundancy in MapReduce-Based Entity Resolution,Redundancy;Computational modeling;Accuracy;Big data;Parallel processing;Time complexity;Conferences;entity resolution;MapReduce;blocking;redundancy elimination,"Entity resolution is the basic operation of data quality management, and the key step to find the value of data. The parallel data processing framework based on MapReduce can deal with the challenge brought by big data. However, there exist two important issues, avoiding redundant pairs led by the multi-pass blocking method and optimizing candidate pairs based on the transitive relations of similarity. In this paper, we propose a multi-signature based parallel entity resolution method, called multi-sig-er, which supports unstructured data and structured data. Two redundancy elimination strategies are adopted to prune the candidate pairs and reduce the number of similarity computation without affecting the resolution accuracy. Experimental results on real-world datasets show that our method tends to handle large datasets and it is more suitable for complex similarity computation than simple object matching.",2015,,10.1109/CCGrid.2015.24
399,"Guo, Wenzhong and Zhu, Weiping and Yu, Zhiyong and Wang, Jiangtao and Guo, Bin",A Survey of Task Allocation: Contrastive Perspectives From Wireless Sensor Networks and Mobile Crowdsensing,Sensors;Task analysis;Wireless sensor networks;Resource management;Data integrity;Wireless communication;Mobile handsets;Mobile crowdsensing (MCS);task allocation;wireless sensor networks (WSNs),"Wireless sensor networks (WSNs) and mobile crowdsensing (MCS) are two important paradigms in urban dynamic sensing. In both sensing paradigms, task allocation is a significant problem, which may affect the completion quality of sensing tasks. In this paper, we give a survey of task allocation in WSNs and MCS from the contrastive perspectives in terms of data quality and sensing cost, which help to better understand related objectives and strategies. We first analyze the different characteristics of two sensing paradigms, which may lead to difference in task allocation issues or strategies. Then, we present some common issues in task allocation with objectives in data quality and sensing cost. Furthermore, we provide reviews of unique task allocation issues in MCS according to its new characteristics. Finally, we identify some potential opportunities for the future research.",2019,,10.1109/ACCESS.2019.2896226
400,"Francis, Akindipe Olusegun and Emmanuel, Bugingo and Zhang, Defu and Zheng, Wei and Qin, Yingsheng and Zhang, Dongzhan",Exploration of Secured Workflow Scheduling Models in Cloud Environment: A Survey,Cloud computing;Security;Task analysis;Processor scheduling;Quality of service;Scheduling;Computational modeling;cloud computing;workflow scheduling;security;survey,"Cloud computing (CC) is a useful tool for executing complex applications. As a result of this, it has become so popular and used in diverse domains such as science, engineering, medicine. etc. CC structure is composed of a number of virtual machines(VMs) provisioned on demand and charged on a ""Pay-as-you-go"" basis, it is deployed in different form of access levels. Complex applications needed to be executed on clouds are represented as workflows. Workflow scheduling (WS) is one of the most important concepts in cloud computing. WS model contributes to minimizing cost, makespan and energy as well as maximize the quality of service(QoS) of applications in clouds. Despite the security constraints set by each provider, CC has become so critical due to the considerations of applications with sensitive intermediate data, this thereby requires a security level known as Secured workflow Scheduling(SWS). This security is on the level of executing workflows. It indicates that applications with sensitive interdependent data have to be protected during their execution across different cloud VMs. The addition of security in workflow execution generates time overhead, making it complex to meet up with the QoS required by the users. Some research works have proposed algorithms for providing the QoS requirements and security at the same time. In this work, we survey some existing works, by defining the factors needed in securing workflows during execution, clarifying the domains for security, sources of security threats and their solutions as well as cloud computing services that needs security and lastly classify the proposed algorithm depending cloud computing components.",2018,,10.1109/CBD.2018.00022
401,"Wu, Junhang and Hu, Ruimin and Li, Dengshi and Xiao, Yilin and Ren, Lingfei and Hu, Wenyi",Multi-network Embedding for Missing Point-of-Interest Identification,Social networking (online);Data integrity;Predictive models;Task analysis;Tuning;Context modeling;location-based social networks (LBSNs);missing POI identification;multi-network embedding,"The large volume of data flowing throughout location-based social networks (LBSNs) provides an opportunity for human mobility behavior understanding and prediction. However, data quality issues (e.g., historical check-in POI missing, data sparsity) limit the effectiveness of existing LBSN-oriented studies, e.g., Point-of-Interest (POI) recommendation or prediction. Contrary to previous efforts in next POI recommendation or prediction, we focus on identifying the missing POI which the user has visited at a past specific time and proposed a multi-network Embedding (MNE) method. Specifically, the model jointly captures temporal cyclic effect, user preference and sequence transition influence in a unified way by embedding five relational information graphs into a shared dimensional space from both POI- and category-instance levels. The proposed model also incorporates region-level spatial proximity to explore geographical influence, and derives the ranking score list of candidates for missing POI identification. We conduct extensive experiments to evaluate the performance of our model on two real large-scale datasets, and the experimental results show its superiority over other competitors. Significantly, it also proves that the proposed model can be naturally transferred to general next POI recommendation and prediction tasks with competitive performances.",2021,,10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00189
402,"Liang, Zilu and Chapa Martell, Mario Alberto",Achieving Accurate Ubiquitous Sleep Sensing with Consumer Wearable Activity Wristbands Using Multi-class Imbalanced Classification,Sleep;Training;Machine learning algorithms;Classification algorithms;Standards;Vegetation;Heart rate;wearable;data quality;sleep;machine learning;Fitbit,"Consumer activity wristbands such as Fitbit provide an affordable method for ubiquitous sleep sensing in daily settings. These devices are also increasingly used in scientific studies as measurement tools of sleep outcomes. Nevertheless, the accuracy of Fitbit has raised wide concern. In this paper, we explore the feasibility of applying machine learning to improve the quality of Fitbit sleep data. The problem of interest was formulated into a multiclass imbalanced classification problem. We examined the performance of different combinations of seven machine learning algorithms and three resampling techniques. The preliminary results showed that the accuracy in detecting wakefulness and light sleep was improved by up to 43% and 44% respectively compared to the proprietary algorithm of Fitbit. Our future work will focus on improving the overall accuracy of the classification models in detecting all sleep stages.",2019,,10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00143
403,"Gowsalya, M and Krushitha, K and Valliyammai, C",Predicting the risk of readmission of diabetic patients using MapReduce,Atmospheric measurements;Particle measurements;Diabetes;Sociology;Statistics;Artificial neural networks;Big data;Healthcare;Predictive analytics;Diabetes,"From the banking to retail, many sectors have already embraced big data regardless of whether the information comes from public or private sources. In the clinical sphere, the amount of patient data has grown exponentially because of computer based information systems. E-Health monitoring applications have some particularities concerning the importance on data quality. This paper presents a novel solution using Hadoop Mapreduce to analyze large datasets and extract useful insights from the dataset which helps doctors to effectively allocate resources. The successful healthcare delivery and planning strongly rely on data (e.g. sensed data, diagnosis, administration information); the higher quality of the data, the better will be the patient assistance. The applications are also particularly exposed to a contextual environment (i.e., patient's mobility, communication technologies, performance, information heterogeneity, etc.) that has an important impact on information management and application achievement. The main objective of our system is to predict the risk of diabetic patients for readmission in the next 30 days by measuring the probability using MapReduce. This risk score helps the physicians in recommending appropriate care for the patients.",2014,,10.1109/ICoAC.2014.7229729
404,"Hu, Pan and Gu, Hailin and Qi, Jun and Gao, Qiang and Xia, Yu and Qu, Ruiting",Design of two-stage federal learning incentive mechanism under specific indicators,Analytical models;Biological system modeling;Games;Machine learning;Nash equilibrium;Linear programming;Collaborative work;Federal learning;Incentive mechanism;Stackelberg game;Nash equilibrium;Specific indicators,"As a privacy-focused distributed machine learning, federated learning can not only train models effectively but also prevent data sets from being leaked easily. However, like crowdsensing perception and other technologies, participants often lack the motivation to learn and the quality of participation is not high. Therefore, this paper mainly designs a two-stage federal learning incentive mechanism based on the Stackelberg game model under a specific model accuracy index. Firstly, we combine data quality and data quantity to construct a federal learning incentive mechanism model under specific indicators. Then, we conduct a two-stage Stackelberg game analysis on the incentive mechanism model based on the utility function construction of the server platform and data island. In the first stage, the platform server is the leader and the data island is the follower. The second stage is a Nash equilibrium game between data islands. Finally, we construct the objective function of the server platform and data island, namely utility maximization, deduce the optimal equilibrium solution of the two-stage game, and determine the optimal strategy of the platform server and data island.",2021,,10.1109/BDEIM55082.2021.00103
405,"Huang, Chao and Lin, Mingwei and Chen, Riqing",Probabilistic Linguistic VIKOR Method Based on TODIM for Reliable Participant Selection Problem in Mobile Crowdsensing,"Linguistics;Sensors;Crowdsensing;Task analysis;Decision making;Probabilistic logic;Cloud computing;Mobile crowdsensing, Probabilistic linguistic term set, Participants ranking, TODIM, VIKOR","In the mobile crowdsensing systems, the participants of great variety and diversity voluntarily submit their sensing data. Evaluating the participants and ranking them is a critical problem that should be solved to ensure the data quality. In this paper, we introduce the concept of probabilistic linguistic term sets (PLTSs) to model the group preference information during the process of ranking candidate participants and then propose novel VIKOR methods based on TODIM for solving the process of ranking reliable participants and selecting the best one in the mobile crowdsensing system. This proposed methods combine the advantages from the VIKOR method and TODIM method. To show the implementation process of evaluating participants and selecting the best one under the PLTS information context, a practical case is given to verify the feasibility of the proposed methods. Compared with the existing decision making methods, the proposed methods show their effectiveness.",2019,,10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00108
406,"Caihong, Zhou and Zengyuan, Wu and Chang, Liu",A Study on Quality Prediction for Smart Manufacturing Based on the Optimized BP-AdaBoost Model,smart manufacturing;big data;quality prediction;BP neural network;AdaBoost algorithm;BP-AdsysBoost model,"To accurately predict the product quality in smart manufacturing, this paper designs the BP-AdsysBoost model on the basis of BP neural network and AdaBoost algorithm. The BP-AdsysBoost model considers both the data characteristics and the technology advantages, which pays more attentions to the unqualified products wrongly predicted. To further examine the model, the 110560 data of smart manufacturing from German BOSCH company is used for this research. The proposed BP-AdsysBoost model is compared with the BP neural network and the unmodified BP-AdaBoost model according to prediction performance. The results show that the BP-AdsysBoost model has significant advantages in prediction accuracy and FDR, which proves its satisfied prediction ability for product quality in smart manufacturing.",2019,,10.1109/SMILE45626.2019.8965303
407,"Zhang, Jinkui and Zhu, Yongxin and Shi, Weiwei and Sheng, Gehao and Chen, Yufeng",An Improved Machine Learning Scheme for Data-Driven Fault Diagnosis of Power Grid Equipment,Fault diagnosis;Principal component analysis;Accuracy;Power grids;Correlation;Power transformers;Correlation coefficient;transformer fault diagnosis;PCC;PCA;BPNN,"In recent power grid systems, data-driven approach has been taken to grid condition evaluation and classification after successful adoption of big data techniques in internet applications. However, the raw training data from single monitoring system, e.g. dissolved gas analysis (DGA), are rarely sufficient for training in the form of valid instances and the data quality can rarely meet the requirement of precise data analytics since raw data set usually contains samples with noisy data. This paper proposes a machine learning scheme (PCA_IR) to improve the accuracy of fault diagnose, which combines dimension-increment procedure based on association analysis, dimension-reduction procedure based on principal component analysis and back propagation neural network (BPNN). First, the dimension of training data is increased by adding selected data which originates from different source such as production management system (PMS) to the original data obtained by DGA. The added data would also inevitably result in more noise. Thus, we then take advantage of the PCA method to reduce the noise in the training data as well as retaining significant information for classification. Finally, the new training data yielded after PCA procedure is inputted into BPNN for classification. We test the PCA_IR scheme on fault diagnosis of power transformers in power grid system. The experimental results show that the classifiers based on our scheme achieve higher accuracy than traditional ones. Therefore, the scheme PCA_IR would be successfully deployed for fault diagnosis in power grid system.",2015,,10.1109/HPCC-CSS-ICESS.2015.236
408,"Dai, Minghui and Su, Zhou and Wang, Yuntao and Xu, Qichao",Contract Theory Based Incentive Scheme for Mobile Crowd Sensing Networks,Sensors;Contracts;Task analysis;Conferences;Big Data;Edge computing;Smart cities;Mobile crowd sensing;trust scheme;optimal contract,"Mobile crowd sensing networks (MCSNs) have emerged as a promising paradigm to provide various sensing services. With the increasing number of mobile users, how to develop an effective scheme to provide the high-quality and secure sensing data becomes a new challenge. In this paper, we propose a contract theory based scheme to provide sensing service in MCSNs. At first, with the analysis of the interaction experience between the crowd sensing platform and mobile user, a trust scheme is introduced to guarantee the quality of sensing data by considering the direct trust and indirect trust. Next, according to the transaction between crowd sensing platform and mobile user, an optimal contract based on incentive scheme is designed to stimulate mobile users to participate in crowd sensing network, where the contract item can not only maximize the platform utility, but also satisfy individual rationality and incentive compatibility. Finally, the numerical results show that the proposal outperforms the conventional schemes.",2018,,10.1109/MoWNet.2018.8428903
409,"Shi, Jingyi and Zheng, Mingna and Yao, Lixia and Ge, Yaorong",A Publication-Based Popularity Index (PPI) for Healthcare Dataset Ranking,Medical services;Market research;Indexes;Data integrity;Informatics;Histograms;Software;popularity index;healthcare dataset;data quality;quantified measurement;regression,"Data are critical in this age of big data and machine learning. Due to their inherent complexity, health-related data are unique in that the datasets are usually acquired for specific purposes and with special designs. As more and more healthcare datasets become available, of which many are public, choosing a quality dataset that is suitable for specific research inquiries is becoming a challenging question for health informatics researchers, especially the learners of this field. On the other hand, from the data provider's perspective, it is important to identify features of datasets that make some datasets more valuable than others so as to improve the design and acquisition of future datasets. To address these questions, we need to develop formal mechanisms to measure the goodness of datasets according to certain criteria. In this study, we propose one way of measuring the value of healthcare datasets that is based on how often the datasets are used and reported by researchers, which we call the Publication-based Popularity Index (PPI). In this article, we describe the design of the PPI and discuss its properties. We demonstrate the utility of the PPI by ranking 14 representative healthcare datasets. We believe that the PPI can enable an overall ranking of all healthcare datasets and thus provide an important dimension to sort search results for dataset integration systems as well as a starting point for identifying and examining the design of the most valuable healthcare datasets so that features of these datasets can inform future designs.",2018,,10.1109/ICHI.2018.00035
410,"Zheng, Liwei",AMD Based Service Agent Collaboration and Specification,Collaboration;Ontologies;Dynamic scheduling;Vectors;Web services;Multi-Agent;Collaboration;Mechanism design,"With the emergence of Big Data in Internet, composing existing web services for satisfying new requirements, such as data quality enhancing, effective data choosing, knowledge discovering etc, has gained daily expanding attentions and interests. Many efforts have been pursued for supporting the essential activities in service composition. However, the existing techniques only focus on passive services which are waiting there for being discovered and invoked. We argue that it might be more attractive when Web services become active entities (Service Agent) distributed in Internet which can recognize the newly emergent requirements and compete with others for realize (part of) the requirements. Retreating or refinement of Big data will hardly be accomplished by one or two data handling center, Service Agent collaboration would be a competitive method for the big data handling problem. Mostly more than one service agents have to collaborate to satisfy requirements in current internet environment especially with social networks. That could be called as the requirement driven agent collaboration. Research on such collaboration might be useful for the previous problem. We have given a preliminary model for the requirement driven agent collaboration based on a function ontology and the automated mechanism design in the earlier work. This paper extended the Function Ontology, and enhanced the AMD model. That makes the interactions in MAS generated by agent collaboration can be described. A negotiation frame for the evaluation and choice of collaboration solutions is also given in this paper. It helps the requester evaluate the possible MAS systems, and helps the service agents make decisions to choose a good enough solution by negotiation. According to the dependencies provided in Function Ontology, a specification is given to describe the execution process of the chosen MAS. And also a method is given to translate the specification to BPEL which is more standard, acceptable, and easier to understood.",2013,,10.1109/HPCC.and.EUC.2013.327
411,"Xiong, Jinbo and Chen, Xiuhua and Tian, Youliang and Ma, Rong and Chen, Lei and Yao, Zhiqiang",MAIM: A Novel Incentive Mechanism Based on Multi-Attribute User Selection in Mobile Crowdsensing,Sensors;Task analysis;Analytic hierarchy process;Training;Technological innovation;Data integrity;Simulation;Mobile crowdsensing;incentive mechanism;analytic hierarchy process;multi-attribute user selection;participation intention analysis,"In the user selection phase of mobile crowdsensing, most existing incentive mechanisms focus on either single-attribute selection or random selection, which possibly lead to serious consequences such as low user enthusiasm, decreased task completion rate, and increased cost of platform consumption. To tackle these issues, in this paper, we propose a novel incentive mechanism MAIM, which is based on multi-attribute user selection and participation intention analysis function in mobile crowdsensing. In this mechanism, the sensing platform employs the analytic hierarchy process to determine the weights of three attributes: participation threshold, cost, and reputation. The weight calculation results of each sensing user with respect to each attribute are then integrated to obtain the sorted weight of each user, with which the sensing platform will then obtain the optimal user set. From the users' perspective, they can autonomously decide whether to accept task processing requests, as enabled by the participation intention analysis function, thereby voiding the absolute authority and control of the sensing platform over users and achieving a two-way selection between the sensing platform and the sensing users. Furthermore, the sensing platform establishes a score-based reputation reward to inspire active performers and utilizes a punishment mechanism to overawe malicious vandals, which substantially helps activize enthusiasm of user participation and improve sensing data quality. Simulation results indicate that the proposed MAIM has significantly improved the sensing task completion ratio and the budget surplus ratio compared with the existing incentive mechanisms in mobile crowdsensing.",2018,,10.1109/ACCESS.2018.2878761
412,"Zhao, Lixia and Jin, Wei",Analysis on the Design and Implementation of the Metadata Management Model in the Cloud Computing Business Intelligence Platform,Analytical models;Renewable energy sources;Codes;Databases;Computational modeling;Data integrity;Metadata;Metadata Management Model;Cloud Computing;Physical Business Intelligence Platform;Big Data,"Through the method of metadata management development, it can give full play to its advantages and make up for its disadvantages. In order to fully grasp the composition, conversion, analysis and processing process of data in the platform, from metadata sources, metadata scope, metadata classification, metadata users, metadata integration project development methods, metadata models and metadata standards, metadata management the implementation of the system and other aspects expounded the metadata management strategy in the business intelligence system. Effective metadata management has increased the usability of the platform by 5.6% and the data quality of the platform by 7.8%.",2022,,10.1109/ICEARS53579.2022.9752191
413,"Wiktorski, Tomasz and Hacker, Thomas and Hansen, Raymond A. and Rodgers, Gregory",Experience with Problem-Based Learning in a Hybrid Classroom,Cloud computing;Computer science;Big data;Computers;Education;Virtual machining;problem-based learning;constructive alignment;data intensive systems;data science,"Constructive alignment has been shown to elicit higher levels of learning among students. Problem-based Learning is one of the forms of constructive alignment often used in medicine and engineering education. We have applied a PBL-based approach to a graduate (master) course in Data Intensive Systems taught simultaneously through a video link at two universities in Europe and USA. Application of standardized measuring methodology shows inconclusive impact of PBL approach on students' learning. We present survey results and analyze major factors that could have lead to inconclusive result, including: low data quality, general applicability of constructivism in computer science, and issues with hybrid classroom and alternative laboratory environments. Finally, we discuss reversed classroom method as a potential solution to the issues encountered.",2015,,10.1109/CloudCom.2015.70
414,"Zhang, Xuejun and Chen, Qian and Peng, Xiaohui and Jiang, Xinlong",Differential Privacy-Based Indoor Localization Privacy Protection in Edge Computing,"Privacy;Training;Fingerprint recognition;Edge computing;Cloud computing;Indoor localization, Differential privacy, Privacy preserving, Edge computing.","With the popularity of smart devices and the widespread use of the Wi-Fi-based indoor localization, edge computing is becoming the mainstream paradigm of processing massive sensing data to acquire indoor localization service. However, these data which were conveyed to train the localization model unintentionally contain some sensitive information of users/devices, and were released without any protection may cause serious privacy leakage. To solve this issue, we propose a lightweight differential privacy-preserving mechanism for the edge computing environment. We extend ε-differential privacy theory to a mature machine learning localization technology to achieve privacy protection while training the localization model. Experimental results on multiple real-world datasets show that, compared with the original localization technology without privacy-preserving, our proposed scheme can achieve high accuracy of indoor localization while providing differential privacy guarantee. Through regulating the value of ε, the data quality loss of our method can be controlled up to 8.9% and the time consumption can be almost negligible. Therefore, our scheme can be efficiently applied in the edge networks and provides some guidance on indoor localization privacy protection in the edge computing.",2019,,10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00125
415,"Han, Shengqiang and Qu, Jianhua and Song, Jinyi and Liu, Zijing",Second-hand Car Price Prediction Based on a Mixed-Weighted Regression Model,Training;Analytical models;Data preprocessing;Linear regression;Predictive models;Feature extraction;Cleaning;second-hand car price prediction;weighted regression model;LightGBM;XGBoost;random forest,"With the development of motor vehicles, the circulation demand of motor vehicles in the form of ""second-hand cars"" in circulation links is increasing. As a special ""e-commerce commodity"", second-hand cars are more complicated than ordinary e-commerce commodities. As a result, it is difficult to estimate the price of second-hand cars, which is not only influenced by the basic configuration of the car, but also by the car conditions. At present, the state has not issued a standard to judge the value of second-hand car. To solve this problem, in this paper, first making feature engineering, which includes data preprocessing and feature screening. Data preprocessing includes data cleaning and data transformation, data cleaning includes removing outliers and filling missing values, and data transformation is used to unify data format to improve data quality. The feature screening includes correlation analysis and feature extraction based on LightMBG, and the screened features provide the basis for model building, training and prediction. Then, five regression models are constructed by using the feature attributes obtained by the feature engineering for training, and evaluated. Then, Random Forest and XGBoost are weighted and mixed to got a novel regression model, and the effect of the model is better than that of the five regression models. Finally, the novel regression model is used to predict the price of second-hand cars.",2022,,10.1109/ICBDA55095.2022.9760371
416,"Zhao, Peng and Quan, Dekui and Yu, Wei and Yang, Xinyu and Fu, Xinwen",Towards Deep Learning-Based Detection Scheme with Raw ECG Signal for Wearable Telehealth Systems,Electrocardiography;Feature extraction;Biomedical monitoring;Sensors;Cloud computing;Computational modeling;Data analysis,"The electrocardiogram (ECG) signal, as one of the most important vital signs, can provide indications of many heart-related diseases. Nonetheless, in the case of telehealth context, the automated analysis and accurate detection of ECG signals remain unsolved issues, because the poor data quality collected by the wearable devices and unprofessional users further increases the complexity of hand-crafted feature extraction, ultimately affecting the efficiency of feature extraction and the detection accuracy. To address this issue and improve the detection accuracy, in this paper we present a novel detection scheme with the raw ECG signal in wearable telehealth system. Our system benefits from the concept of big data, sensing and pervasive computing and the emerging deep learning technology. In particular, a Deep Heartbeat Classification (DHC) scheme is proposed to analyze the ECG signal for arrhythmia detection. Distinct from existing solutions, the detection model in DHC can be trained directly on the raw ECG signal without hand-crafted feature extraction. A cloud-based prototypical system is also designed and implemented with the functions of data acquisition, wireless transmission, back-end data management, and ECG detection. The experimental results demonstrate that our prototypical system is feasible and effective in real-world practice, and extensive experimentation based on the MIT-BIH database demonstrates that the proposed DHC scheme outperforms baseline schemes.",2019,,10.1109/ICCCN.2019.8847069
417,Hailong Liu and Zhanhuai Li and Cheqing Jin and Qun Chen,Web-based techniques for automatically detecting and correcting information errors in a database,World Wide Web;Databases;Knowledge based systems;Computational fluid dynamics;Data mining;Web sites;Strain,"It is critical to detect and correct information errors effectively to achieve higher data quality in many applications. Most existing techniques only use the intrinsic information to detect and correct a database, provided that data is adequate and well-structured. These techniques will not work properly if there is no sufficient data available. Integrating the information from external sources, like the World Wide Web (WWW), can help us overcome the shortcomings of existing techniques to a great extent. In this paper, we introduce our on-going work that is capable of detecting and correcting data errors in a database by integrating external information from the WWW. The goal of our research is to pursuit another effective way to enhance information quality.",2016,,10.1109/BIGCOMP.2016.7425923
418,"Liang, Tingting and Chen, Yishan and Gao, Wei and Chen, Ming and Zheng, Meilian and Wu, Jian",Exploiting User Tagging for Web Service Co-Clustering,Web services;Tagging;Search engines;Clustering algorithms;Task analysis;Feature extraction;Web service;WSDL documents clustering;co-clustering;tag recommendation,"We propose a novel Web services clustering framework by considering the word distribution of WSDL documents and tags. Typically, tags are annotated to Web services by users for organization. In this paper, four strategies are proposed to integrate the tagging data and WSDL documents in the process of service clustering. Tagging data is inherently uncontrolled, ambiguous, and overly personalized. Two tag recommendation approaches are proposed to improve the tagging data quality and service clustering performance. Comprehensive experiments demonstrate the effectiveness of the proposed framework using a real-world dataset.",2019,,10.1109/ACCESS.2019.2950355
419,"Moysen, Jessica and Giupponi, Lorenza and Mangues-Bafalluy, Josep",On the potential of ensemble regression techniques for future mobile network planning,Quality of service;Planning;Training;Computers;Measurement;Principal component analysis;3GPP;Machine Learning;Big Data;Quality of Service;Prediction;Network planning;Minimization of Drive Tests,"Planning of current and future mobile networks is becoming increasingly complex due to the heterogeneity of deployments, which feature not only macrocells, but also an underlying layer of small cells whose deployment is not fully under the control of the operator. In this paper, we focus on selecting the most appropriate Quality of Service (QoS) prediction techniques for assisting network operators in planning future dense deployments. We propose to use machine learning as a tool to extract the relevant information from the huge amount of data generated in current 4G and future 5G networks during normal operation, which is then used to appropriately plan networks. In particular, we focus on radio measurements to develop correlative statistical models with the purpose of improving QoS-based network planning. In this direction, we combine multiple learners by building ensemble methods and use them to do regression in a reduced space rather than in the original one. We then compare the QoS prediction accuracy of various approaches that take as input the 3GPP Minimization of Drive Tests (MDT) measurements collected throughout a heterogeneous network and analyse their trade-offs. We also explain how the collected data is processed and used to predict QoS expressed in terms of Physical Resource Block (PRB)/ Megabit (MB) transmitted. This metric was selected because of the interest it may have for operators in planning, since it relates lower layer resources with their impact in terms of QoS up in the protocol stack, hence closer to the end-user.",2016,,10.1109/ISCC.2016.7543784
420,"Wu, Hao and Liu, Qi and Liu, Xiaodong and Zhang, Yonghong and Xu, Xiaolong and Bilal, Muhammad",A FCN Approach to Blockage Correction in Radars,Deep learning;Reflectivity;Meteorological radar;Semantics;Neural networks;Radar detection;Weather forecasting;Deep learning;convolutional neural networks;encoder-decoder network;weather radar;image inpainting;blockage correction,"Doppler weather radar is the most widely used convection detector with the highest resolution in the ground. Echo reflectance data from the weather radar is the key reference for the meteorological department to carry out severe convective weather forecast and early warning, quantitative precipitation estimation(QPE) and quantitative precipitation forecast(QPF). However, in the process of radar detection, it is inevitable to be affected by obstacles, ground object echo interference, radar echo attenuation and other phenomena, resulting in poor data quality of detection results. Therefore, it is very important to correct the missing or disturbed data. On the other hand, with the rapid development of artificial intelligence technology in recent years, more and more meteorological researchers begin to introduce deep learning and other machine learning methods into the research of meteorological field such as weather radar data processing. In this paper, a deep convolutional encoder-decoder network is proposed to correct the beam blocking of weather radar. In this study, the correction of radar beam blockage is regarded as an image inpainting problem. It's the first trying to use deep learning to realize the correction of radar beam blockage. Experiment shows that the method proposed in this paper is significantly better than the traditional method in accuracy, error rate, false alarm rate and other aspects. The method can directly identify and correct the blocking area, and the operation procedure is simple compared traditional methods.",2021,,10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00086
421,"Fresco Zamora, Jane Louie and Sawada, Naoya and Sahara, Takemi and Kashihara, Shigeru and Taenaka, Yuzo and Yamaguchi, Suguru",Surface weather observation via distributed devices,Temperature measurement;Temperature sensors;Rain;Smart phones;Humidity;Distributed devices;Measurement;Localized Rain,"With rain-related hazards, it is difficult to forecast and prepare for it due to the decline in the availability and reliability of global daily weather reports. Therefore, we need to make use of available unconventional commercial weather instruments to provide supplementary information to existing systems. A more accurate and reliable weather forecast can then be made due to the prompt availability of information from ubiquitous devices. As smartphones become a widely used device, we propose a conceptual design of a multi-device ground weather observation network using smartphones and other sensors. In this paper, we first investigate the differences between smartphone-based sensors and other sensors to determine issues to address for big data on global weather information. In our experiments, we found that the data quality differs among devices in a very small area of 100 m grid.",2014,,10.1109/I2MTC.2014.6860977
422,"Yang, Bikai and Bing, Han and Zhao, Haiyang and Gu, Tianshu and Cai, Tianxiao",Multi dimensional disease intelligent detection device for underwater pier column structure through machine learning,Underwater structures;Bridges;Road transportation;Employee welfare;Structural panels;Water quality;Rail transportation;Railway bridges;Pier underwater structure;Non destructive testing;Disease location;Machine learning,"The total number of railway bridges in China has exceeded twenty thousand. As an important part of the railway line, the existing railway bridge bears great live load, excessive concentrated force, obvious dynamic effect and high requirements for foundation stability. However, because the underwater structure of Railway Pier is constantly affected by many factors such as water scouring and ship collision during its service, it leads to a variety of diseases such as concrete peeling and exposed reinforcement. These diseases are often hidden in the water, which poses a serious threat to the structural safety of the bridge and the safety of people&#x0027;s lives and property. However, the traditional detection method of underwater structure of Railway Pier is affected by water flow, water quality, water depth and other environment, which has the pain points of high detection risk, low efficiency, poor detection data quality and missing disease location, so it is difficult to effectively detect underwater structure diseases; In this context, a multi-dimensional disease detection device is designed and developed for Wuhan Yangtze River Bridge (both highway and railway). The device includes a fixed module on water and an underwater detection module, which can realize safe and efficient detection under the condition of rapids and deep water, solve the problems that it is difficult for personnel and existing equipment to reach the structure to be tested, and the detection effect obtained by existing detection means is not ideal The detection information is not comprehensive, which is difficult to be used for key problems such as follow-up structural technical state evaluation.",2022,,10.1109/EEBDA53927.2022.9744889
423,"Kazemi, Arefeh and Mozafari, Jamshid and Nematbakhsh, Mohammad Ali",PersianQuAD: The Native Question Answering Dataset for the Persian Language,Internet;Online services;Encyclopedias;Training;Task analysis;Machine translation;Buildings;Dataset;deep learning;natural language processing;Persian;question answering;machine reading comprehension,"Developing Question Answering systems (QA) is one of the main goals in Artificial Intelligence. With the advent of Deep Learning (DL) techniques, QA systems have witnessed significant advances. Although DL performs very well on QA, it requires a considerable amount of annotated data for training. Many annotated datasets have been built for the QA task; most of them are exclusively in English. In order to address the need for a high-quality QA dataset in the Persian language, we present PersianQuAD, the native QA dataset for the Persian language. We create PersianQuAD in four steps: 1) Wikipedia article selection, 2) question-answer collection, 3) three-candidates test set preparation, and 4) Data Quality Monitoring. PersianQuAD consists of approximately 20,000 questions and answers made by native annotators on a set of Persian Wikipedia articles. The answer to each question is a segment of the corresponding article text. To better understand PersianQuAD and ensure its representativeness, we analyze PersianQuAD and show it contains questions of varying types and difficulties. We also present three versions of a deep learning-based QA system trained with PersianQuAD. Our best system achieves an F1 score of 82.97% which is comparable to that of QA systems on English SQuAD, made by the Stanford University. This shows that PersianQuAD performs well for training deep-learning-based QA systems. Human performance on PersianQuAD is significantly better (96.49%), demonstrating that PersianQuAD is challenging enough and there is still plenty of room for future improvement. PersianQuAD and all QA models implemented in this paper are freely available.",2022,,10.1109/ACCESS.2022.3157289
424,,IEEE Guide for Architectural Framework and Application of Federated Machine Learning,IEEE Standards;Machine learning;Privacy;Modeling;Economics;Collaborative work;Metasearch;computation efficiency;economic viability;federated machine learning (FML);IEEE 3652.1™;incentive mechanism;machine learning;model performance;privacy;privacy regulations;security,"Federated machine learning defines a machine learning framework that allows a collective model to be constructed from data that is distributed across repositories owned by different organizations or devices. A blueprint for data usage and model building across organizations and devices while meeting applicable privacy, security and regulatory requirements is provided in this guide. It defines the architectural framework and application guidelines for federated machine learning, including description and definition of federated machine learning; the categories federated machine learning and the application scenarios to which each category applies; performance evaluation of federated machine learning; and associated regulatory requirements.",2021,,10.1109/IEEESTD.2021.9382202
425,"Ahmad, Arshad and Feng, Chong and Li, Kan and Asim, Syed Mohammad and Sun, Tingting",Toward Empirically Investigating Non-Functional Requirements of iOS Developers on Stack Overflow,Software;Market research;Data mining;Application programming interfaces;Mobile communication;Mobile applications;Technological innovation;Non-functional requirements (NFRs);quality requirements;iOS;Latent Dirichlet allocation (LDA);Stack Overflow,"Context: Mobile application developers are getting more concerned due to the importance of quality requirements or non-functional requirements (NFR) in software quality. Developers around the globe are actively asking a question(s) and sharing solutions to the problems related to software development on Stack Overflow (SO). The knowledge shared by developers on SO contains useful information related to software development such as feature requests (functional/non-functional), code snippets, reporting bugs or sentiments. Extracting the NFRs shared by iOS developers on programming Q&A website SO has become a challenge and a less researched area. Objective: To identify and understand the real problems, needs, trends, and the critical NFRs or quality requirements discussed on Stack Overflow related to iOS mobile application development. Method: We extracted and used only the iOS posts data of SO. We applied the well-known statistical topical model Latent Dirichlet Allocation (LDA) to identify the main topics in iOS posts on SO. Then, we labeled the extracted topics with quality requirements or NFRs by using the wordlists to assess the trend, evolution, hot and unresolved NFRs in all iOS discussions. Results: Our findings revealed that the highly frequent topics the iOS developers discussed are related to usability, reliability, and functionality followed by efficiency. Interestingly, the most problematic areas unresolved are also usability, reliability, and functionality though followed by portability. Besides, the evolution trend of each of the six different quality requirements or NFRs over time is depicted through comprehensive visualization. Conclusion: Our first empirical investigation on approximately 1.5 million iOS posts and comments of SO gives insight on comprehending the NFRs in iOS application development through the lens of real-world practitioners.",2019,,10.1109/ACCESS.2019.2914429
426,"Qasim, Amer and El Refae, Ghaleb A. and Issa, Hussein and Eletter, Shorouq",The Impact of Drone Technology on The Accounting Profession: The Case of Revenue Recognition in Long-Term Construction Contracts,Costs;Data integrity;Inspection;Safety;Blockchains;Task analysis;Information technology;Drones;Artificial Intelligence;Real-Estate Accounting;ERP systems;Revenue Recognition,"The accounting profession has gone through radical changes due to recent technological advancements in AI, blockchain technologies, big data, etc. More recently, the accounting literature discussed the possibility of using the drone innovative technology in conducting inventory observation as well as internal and external auditing. This study is a visionary paper which investigates the applicability of a remotely auditing process using drone technology in real-estate accounting. The drone will be used to conduct site inspection to assess and monitor the construction progress through applying the percentage of completion method to recognize revenues from long-term contracts. This innovative technology has the ability to collect better data quality, saving cost and saving time with improved site safety which will help improve the auditor tasks.",2021,,10.1109/ACIT53391.2021.9677226
427,"Rudnitckaia, Julia and Venkatachalam, Hari Santhosh and Essmann, Roland and Hruška, Tomáš and Colombo, Armando Walter",Screening Process Mining and Value Stream Techniques on Industrial Manufacturing Processes: Process Modelling and Bottleneck Analysis,Data mining;Manufacturing;Information management;Production;Manufacturing processes;Companies;Analytical models;Bottleneck analysis;manufacturing process;process mining;process modelling;information management system;value stream,"One major result of the Industrial Digitalization is the access to a large set of digitalized data and information, i.e. Big Data. The market of analytic tools offers a huge variety of algorithms and software to exploit big datasets. Implementing their advantages into one approach brings better results and empower possibilities for process analysis. Its application in the manufacturing industry requires a high level of effort and remains to be challenging due to product complexity, human-centric processes, and data quality. In this manuscript, the authors combine process mining and value streams methods for analyzing the data from the information management system, applying the approach to the data delivered by one specific manufacturing system. The manufacturing process to be examined is the process of assembling gas meters in the manufacture. This specific and important part of the whole supply-chain process was taken as suitable for the study due to almost full-automated line with data about each process activity of the value-stream in the information system. The paper applies process mining algorithms in discovering a descriptive process model that plays the main role as a basis for further analysis. At the same time, modern techniques of the bottleneck analysis are described, and two new comprehensible methods of bottlenecks detection (TimeLag and Confidence intervals methods), as well as their advantages, will be discussed. Achieved results can be subsequently used for other sources of big data and industrial-compliant Information Management Systems.",2022,,10.1109/ACCESS.2022.3152211
428,"Ruester, Christian and Haussel, Fabian and Huehn, Thomas and El Sayed, Nadim",VEREDELE-FACDS Field Trial: Wide Area Power Quality Assessment With IOT Sensors and Cloud-Based Analytics,,"With the increasing share of renewable generation in low voltage distribution, the edge of the power grid is slowly replacing classical, large-scale power stations, taking up roles and responsibilities that were unimaginable only a few years ago. However, while feed-in on the 400V level is now commonplace, grid state and power quality monitoring still lag far behind because of obvious cost and complexity reasons. In fact, only very few parts of Germanys approx. 1,1 million km long 400V distribution grid are actively monitored today and even basic grid quality parameters such as the voltage level at the end of the line are typically unknown. Reducing the cost per measurement point and the complexity of data analysis is thus of paramount importance for enabling wide-area monitoring of the LV power grid. The authors explore the feasibility of this goal by examining the performance of a nonconventional measurement system. It consists of a network of Internet-of-Things (IOT)-based power quality sensors, connected to a cloud-based big data analysis platform. Specifically, measurement nodes comprise of voltage sensors attached to consumer-grade smartphones and WIFI access points. Sensor data is automatically uploaded to the cloud system with the MQTT protocol. First results from a field trial in a rural area in Germany indicate good data quality and show excellent promise for detailed assessments of the edge of the power grid.",2017,,
429,"Yuan, Yanhua O and Bozdağ, Ebru and Ciardelli, Caio and Gao, Fuchun and Simons, F J","The exponentiated phase measurement, and objective-function hybridization for adjoint waveform tomography",Inverse theory;Time-series analysis;Seismic tomography,"Seismic tomography has arrived at the threshold of the era of big data. However, how to extract information optimally from every available time-series remains a challenge; one that is directly related to the objective function chosen as a distance metric between observed and synthetic data. Time-domain cross-correlation and frequency-dependent multitaper traveltime measurements are generally tied to window selection algorithms in order to balance the amplitude differences between seismic phases. Even then, such measurements naturally favour the dominant signals within the chosen windows. Hence, it is difficult to select all usable portions of seismograms with any sort of optimality. As a consequence, information ends up being lost, in particular from scattered waves. In contrast, measurements based on instantaneous phase allow extracting information uniformly over the seismic records without requiring their segmentation. And yet, measuring instantaneous phase, like any other phase measurement, is impeded by phase wrapping. In this paper, we address this limitation by using a complex-valued phase representation that we call ‘exponentiated phase’. We demonstrate that the exponentiated phase is a good substitute for instantaneous-phase measurements. To assimilate as much information as possible from every seismogram while tackling the non-linearity of inversion problems, we discuss a flexible hybrid approach to combine various objective functions in adjoint seismic tomography. We focus on those based on the exponentiated phase, to take into account relatively small-magnitude scattered waves; on multitaper measurements of selected surface waves; and on cross-correlation measurements on specific windows to select distinct body-wave arrivals. Guided by synthetic experiments, we discuss how exponentiated-phase, multitaper and cross-correlation measurements, and their hybridization, affect tomographic results. Despite their use of multiple measurements, the computational cost to evaluate gradient kernels for the objective functions is scarcely affected, allowing for issues with data quality and measurement challenges to be simultaneously addressed efficiently.",2019,,10.1093/gji/ggaa063
430,"Liu, Jiaxin and Wang, Shuai and Lu, Xuchen and Li, Tong",Research on Online Status Evaluation Technology for Main Equipment of Power Transmission and Transformation Based on Digital Twin,Digital twin;Power transmission;System integration;Maintenance engineering;Reliability engineering;Transformers;Real-time systems;digital twin;status evaluation;online evaluation;fault diagnosis;status prediction;transformer,"Traditional status evaluation for main equipment of power transmission and transformation has some shortages, such as low timeliness, low data quality and difficulty for evaluation model construction. Based on digital twin technology system, this paper presents technology of equipment status, and constructs digital twin for power transmission and transformation equipment. According to operation characteristics of power transmission and transformation equipment, fusion and cleansing of perception data is realized. Relying on big data analysis and data mining, status evaluation differentiation, accurate fault diagnosis and status prediction for power transmission and transformation equipment is realized. Further more, this paper analyzes the application of digital twin technology in on-line status evaluation for transformer equipment, expounds specific application of digital twin technology including data governance and model building, and summarizes application prospect of digital twin technology in on-line status evaluation for main equipment of power transmission and transformation.",2021,,10.1109/EI252483.2021.9713501
431,"Xue, Chunlu and Guo, Lin and Hu, Hualang and Pei, Zhiyuan",Management and spatial evolution of rural land circulation: Taking Zhengjia Town as an example,Correlation;Agriculture;Vegetation;Geographic information systems;Economics;Information management;GIS;land circulation;right registration;spatial evolution,"The scale operation in various forms of rural land circulation in China is the way for the development of modern agriculture, and is also the basic direction of agricultural reform. The right to rural land contractual management registration makes the position of cadastral land and its interrelated information, as contractor and area, clear by Geographic Information Systems (GIS). That supplied a data base for land circulation spatial management. In this paper, we discussed the correlation between the management and the spatial position of transferred rural land, in order to support the policy and decision for agriculture. Taking Zhengjia Town as an example, which located in the west of Dongchangfu District, Liaocheng, Shandong, we made a survey and got the information about the land circulation, the operators who engaged in the transferred land, and the farmers who had transferred their farmlands from 2011 to 2015. Based on the outcome data of right to rural land contractual management registration, the circulation parcels and the cadastral parcels were linked by their only parcel code, and then formed the land circulation spatial information. Using ArcGIS 10.1 for spatial analysis and correlation analysis, we analyzed the correlation between the management and the spatial evolution of the transferred land on some indicators including acquirement intention, operation style, land use, circulation variability, distribution of circulation management right, and compared the income of the operators and the farmers. According to the survey, the scope of the transferred land was gradually increased. The area was 16.09ha in 2011 and 147.27ha in 2015, 19.22% of the contracted land. The results show that the main form of land circulation was to rent, and scale operation in northern and fragmented operation in southern. The transferred lands which were acquired by family contract have the largest area. However, in the south, the farmers seemed like to transfer their lands which were acquired by bidding. The operators aged in 40-49 managed the most transferred land, accounting for 77.00% of the total circulation area. Followed by 39 years old under, and the over 50 years old was the least. Before transferred, all farmland was used to grow grain. But 93% farmlands were changed the crops after land circulation. As the investment of time, capital, farmland and implied labor increased, there was a certain increase in income of the operators and the farmers, however, and a few operators had no profit because of the business model. There was no spatial correlation between the farmlands position and the farmers' income at the fragmented operation region. The results can provide an idea for spatial information management of land circulation based on GIS, and defend the farmers' contractual rights when the boundaries are destroyed. With the spatial informatization of farmland and the improvement of data quality and quantity, big data will further predict land circulation spatial arrangement and business model.",2016,,10.1109/Agro-Geoinformatics.2016.7577612
432,,Big Data Quality Assurance Workshop Chairs and Committee,,Provides a listing of current committee members and society officers.,2017,,10.1109/BigDataService.2017.57
433,"Garg, S. and Guizani, M. and Guo, S. and Verikoukis, C.",Guest Editorial Special Section on AI-Driven Developments in 5G-Envisioned Industrial Automation: Big Data Perspective,Special issues and sections;Artificial intelligence;Big Data;Quality of service;Automation;5G mobile communication,"The papers in this special section examine artificial intelligence (AI)-driven developments in 5G mobile communications for industrial automation applications from a Big Data perspective. With the recent advances in information and communication technologies, industrial automation is expanding at a rapid pace. This transition is characterized by “Industry 4.0”, the fourth revolution in the field of manufacturing. Industry 4.0, also called as “Industrial Internet of Things (IIoT)” or “Smart Factories”, is a reflection of new industrial revolution that is not only interconnected, but also communicate, analyze, and use the information to create a more holistic and better connected ecosystem for the industries.",2020,,10.1109/TII.2019.2955963
434,,[Title page i],,The following topics are dealt with: data mining; social network; data privacy; learning; query processing; big data processing; big data quality; big data platform; big data semantics; health care; network management; distributed processing; social media; and image processing.,2015,,10.1109/BigDataCongress.2015.1
435,"Nicklas, Daniela","Keynote: Context, big data, and digital prejudices",context;big data,"In pervasive computing research and literature, context has mostly been seen as an information source for applications that adapt their behavior according to the current situation of their user or their (often physical) environment. This adaptation could be the change of the user interface, the performance of actions (like sending messages or triggering actuators), or the change of used resources (like network bandwidth or processing power). To determine relevant situations, many heterogeneous data sources could be used, ranging from sensor data over mined patterns in files to explicit user input. Since most sensors are not perfect, context quality has to be considered. And since many context-aware applications are mobile, the set of data sources may change during runtime. According to the widely used definition by Anind Dey, context can be “any information that can be used to characterize the situation of an entity”. In the past years, we have seen a significant increase in the so-called “big data” domain, in research, technology, and industrial usage. The desire to analyze, gain knowledge and use more and more data it in new ways is rising in a way that resemble a gold rush. Data is the new oil. Beside applications like predictive maintenance of machines or optimization of industrial processes, a main target for big data analyses are humans - in their roles as travelers, current or potential clients, or application users. We could say that big data is “any information that can be used to characterize the situation of a user”, and relate these approaches to what have been done in context modelling and reasoning. This gets even clearer when these analyses leave the virtual world (e.g., client behavior in web shops) and enter the real world (e.g., client behavior in retail). In addition to the ambiguities of the analysis itself that only leads to predictions with a limited probability, sensor data quality becomes an issue: the sensor data might be inaccurate, outdated or conflicting with other observations or physical laws; in addition, sensor data processing algorithms like object classification or tracking might lead to ambiguous results. In this talk, we will shortly review these two domains and derive what could be learned for context-aware applications. A special focus will be given on quality of context on all semantic levels, and how the improper consideration of quality issues can lead to dangerous digital prejudices.",2015,,10.1109/PERCOMW.2015.7133983
436,,[Front cover],,"The following topics are dealt with: intelligent data mining; frameworks for Big Data processing; Big Data processing and mining; Big Data analysis; smart city Big Data; data analytics and visualization; Big Data applications; Big Data framework, technology and solutions; security services for smart cities; Big Data for security; Big Data quality assurance and validation; and quality assurance and validation for Big Data-based applications.",2017,,10.1109/BigDataService.2017.59
437,,[Title page],,"The following topics are dealt with: SDLC SPASI v. 4.0. business process; information extraction; statistics indicator tables; rule generalizations; ontology; conventional learning system; ICT-based learning; job training system; time-series data; RAID; software-based accelerator; virtualization environment; enterprise architecture government organization; TOGAF ADM; SONA; e- library; modified quantitative models for performance measurement system method; business process improvement; district government innovation service case study; government organization; m-government implementation evaluation; trusted Big Data; official statistics study case; data profiling; data quality improvement; secure internet access; copyright protection; color images; transform domain; luminance component; information network architecture; local government; software as a service; expert system; meningitis disease; certainty factor method; digital asset management system; broadcasting organizations; e-portofolio definition; system security requirement identification; electronic payment system; Internet-based long distance education; operational model data governance; requirement engineering; open government information network development; process capability assessment; information security management; information security governance; national cyber physical systems; e-learning readiness; remote control system; serial communications mobile; microcontroller; knowledge sharing; indonesia higher educational institutions; cultural heritage metadata; geo linked open data; NUSANTARA: knowledge management system; adaptive personalized learning system; interactive learning media; RPP ICT; government human capital management; knowledge management tools utilization; knowledge management readiness; analytic hierarchy process; government institutions; usability testing; scrum methodology; assistant information system; automatic arowana raiser; pSPEA2; strength Pareto evolutionary algorithm 2; early diagnosis expert system deficiency; digital forensic; user acceptance; human resource information system; automated plasmodium detection; malaria diagnosis; thin blood smear image; 3D virtual game; MOODLE; SLOODLE; open simulator case study; color-based segmentation; feature detection; ball post; goal post; mobile soccer robot game field; smart farming; real time q-log-based feature normalization; distant speech recognition; Monte Carlo localization; robot operating system; finite element method; 3D DC resistivity modeling; multi GPU; breast cancer lesions; adaptive thresholding; morphological operation; gamification framework; online training; collaborative working system; classification breast cancer ultrasound images; posterior features; three-wheeled omnidirectional robot controller; public services satisfaction; sentiment analysis; color blind test quantification; RGB primary color cluster; ERP modules requirement; micro, small and medium enterprise fashion industry; small culinary enterprises; business system requirement; small craft companies ; power analysis attack; DES and IT value model.",2016,,10.1109/ICITSI.2016.7858181
438,Xiaofang Zhou,Keynote speech V deriving values from spatial trajectories,,"Spatial trajectory data is widely available today. Over a sustained period of time, trajectory data has been collected from numerous GPS devices, smartphones, sensors and social media applications. How do we manage them? What values can a business derive from them, and how? Due to their very large volumes, the nature of streaming itself, highly variable levels of data quality, as well as many possible links with other types of data, making sense of spatial trajectory data is one of the crucial areas for big data analytics. In this talk, we will introduce this increasingly important research area in the context of new applications, new problems and new opportunities. We will discuss recent advances in trajectory data management and trajectory mining, from their foundations to high performance processing with modern computing infrastructures.",2015,,10.1109/TAAI.2015.7407050
439,Tomas Skripcak and Claus Belka and Walter Bosch and Carsten Brink and Thomas Brunner and Volker Budach and Daniel Büttner and Jürgen Debus and Andre Dekker and Cai Grau and Sarah Gulliford and Coen Hurkmans and Uwe Just and Mechthild Krause and Philippe Lambin and Johannes A. Langendijk and Rolf Lewensohn and Armin Lühr and Philippe Maingon and Michele Masucci and Maximilian Niyazi and Philip Poortmans and Monique Simon and Heinz Schmidberger and Emiliano Spezi and Martin Stuschke and Vincenzo Valentini and Marcel Verheij and Gillian Whitfield and Björn Zackrisson and Daniel Zips and Michael Baumann,Creating a data exchange strategy for radiotherapy research: Towards federated databases and anonymised public datasets,"Data pooling, Interoperability, Data exchange, Large scale studies, Public data, Radiotherapy","Disconnected cancer research data management and lack of information exchange about planned and ongoing research are complicating the utilisation of internationally collected medical information for improving cancer patient care. Rapidly collecting/pooling data can accelerate translational research in radiation therapy and oncology. The exchange of study data is one of the fundamental principles behind data aggregation and data mining. The possibilities of reproducing the original study results, performing further analyses on existing research data to generate new hypotheses or developing computational models to support medical decisions (e.g. risk/benefit analysis of treatment options) represent just a fraction of the potential benefits of medical data-pooling. Distributed machine learning and knowledge exchange from federated databases can be considered as one beyond other attractive approaches for knowledge generation within “Big Data”. Data interoperability between research institutions should be the major concern behind a wider collaboration. Information captured in electronic patient records (EPRs) and study case report forms (eCRFs), linked together with medical imaging and treatment planning data, are deemed to be fundamental elements for large multi-centre studies in the field of radiation therapy and oncology. To fully utilise the captured medical information, the study data have to be more than just an electronic version of a traditional (un-modifiable) paper CRF. Challenges that have to be addressed are data interoperability, utilisation of standards, data quality and privacy concerns, data ownership, rights to publish, data pooling architecture and storage. This paper discusses a framework for conceptual packages of ideas focused on a strategic development for international research data exchange in the field of radiation therapy and oncology.",2014,,https://doi.org/10.1016/j.radonc.2014.10.001
440,Badr-Eddine Boudriki Semlali and Chaker El Amrani and Guadalupe Ortiz and Juan Boubeta-Puig and Alfonso Garcia-de-Prado,SAT-CEP-monitor: An air quality monitoring software architecture combining complex event processing with satellite remote sensing,"Remote sensing, Satellite sensors, Air quality, Complex event processing, Big data, Decision-making","Air pollution is a major problem today that causes serious damage to human health. Urban areas are the most affected by the degradation of air quality caused by anthropogenic gas emissions. Although there are multiple proposals for air quality monitoring, in most cases, two limitations are imposed: the impossibility of processing data in Near Real-Time (NRT) for remote sensing approaches and the impossibility of reaching areas of limited accessibility or low network coverage for ground data approaches. We propose a software architecture that efficiently combines complex event processing with remote sensing data from various satellite sensors to monitor air quality in NRT, giving support to decision-makers. We illustrate the proposed solution by calculating the air quality levels for several areas of Morocco and Spain, extracting and processing satellite information in NRT. This study also validates the air quality measured by ground stations and satellite sensor data.",2021,,https://doi.org/10.1016/j.compeleceng.2021.107257
441,Horacio Paggi and Javier Soriano and Juan A. Lara and Ernesto Damiani,Towards the definition of an information quality metric for information fusion models,"Adaptive Peer-to-Peer systems, Information fusion, Uncertain information handling, Information quality metric","Managing information quality has become important in cyber-physical systems dealing with big data. In this regard, different models have been proposed, mainly in flat peer-to-peer networks, in which exchanging information efficiently is a key aspect due to scarce resources. However, little research has been conducted on information quality metrics for cyber-physical scenarios. In this paper, we propose an information quality metric and show its application to an information fusion model. It is a “model-oriented quality metric” since it allows non-predefined variants on its configuration depending on the application domain. The model was tested on several simulations using open datasets. The results obtained in the performance of the model confirm the validity of the information quality metric, proposed in this paper, on which the model is based. The model may have a wide variety of applications such as mobile recommendation or decision making in critical environments (emergencies, war, and so on).",2021,,https://doi.org/10.1016/j.compeleceng.2020.106907
442,Steve Watt and Chris Milne and David Bradley and David Russell and Peter Hehenberger and Jorge Azorin-Lopez,Privacy Matters – Issues within Mechatronics,"Privacy, Users, Big Data, Security, Mechatronics, Cyber-Physical Systems, Internet of Things","Abstract:
As mechatronic devices and components become increasingly integrated with and within wider systems concepts such as Cyber-Physical Systems and the Internet of Things, designer engineers are faced with new sets of challenges in areas such as privacy. The paper looks at the current, and potential future, of privacy legislation, regulations and standards and considers how these are likely to impact on the way in which mechatronics is perceived and viewed. The emphasis is not therefore on technical issues, though these are brought into consideration where relevant, but on the soft, or human centred, issues associated with achieving user privacy.",2016,,https://doi.org/10.1016/j.ifacol.2016.10.641
443,Evelyn J.S. Hovenga and Cherrie Lowe,Chapter 11 - Measuring health service quality,"Performance measurement, Nursing ecosystem, Productivity, Nursing practice environments, Collegial culture, Accountability, Data quality, Data governance, Accreditation, Standards","Quality can be defined in multiple ways and is impacted by multiple factors. It applies to any operational process within health care and has a strong relationship with the performance of individual staff members as well as overall organizational performance outcomes. The characteristics of any nursing practice environment influence the quality of service provided. The ability to measure the quality of services provided is largely dependent upon the availability and type of data that can be accessed and processed. Meaningful measurement, trend analysis and monitoring to enable continuous improvements to be made, are only possible when governed data standards are used. This chapter has a strong focus on health and nursing, including acuity and clinical data use and provides global recommendations on health data, data standards and governance. Reference is made to other types of related standards, including accreditation standards and standards governance. The chapter concludes with an examination of various international and national outcomes research organizations, their comparative studies, and use of performance indicator data sets, clinical standards and guidelines, big data and secondary data use. Caring has been well defined yet doesn't appear to be routinely measured even though this is a major component directly impacting patient satisfaction.",2020,,https://doi.org/10.1016/B978-0-12-816977-3.00011-3
444,Ali Kalantari and Amirrudin Kamsin and Shahaboddin Shamshirband and Abdullah Gani and Hamid Alinejad-Rokny and Anthony T. Chronopoulos,"Computational intelligence approaches for classification of medical data: State-of-the-art, future challenges and research directions","Computational intelligence, Medical application, Big data, Detection, Ensemble algorithm","The explosive growth of data in volume, velocity and diversity that are produced by medical applications has contributed to abundance of big data. Current solutions for efficient data storage and management cannot fulfill the needs of heterogeneous data. Therefore, by applying computational intelligence (CI) approaches in medical data helps get better management, faster performance and higher level of accuracy in detection. This paper aims to investigate the state-of-the-art of computational intelligence approaches in medical data and to categorize the existing CI techniques, used in medical fields, as single and hybrid. In addition, the techniques and methodologies, their limitations and performances are presented in this study. The limitations are addressed as challenges to obtain a set of requirements for Computational Intelligence Medical Data (CIMD) in establishing an efficient CIMD architectural design. The results show that on the one hand Support Vector Machine (SVM) and Artificial Immune Recognition System (AIRS) as a single based computational intelligence approach were the best methods in medical applications. On the other hand, the hybridization of SVM with other methods such as SVM-Genetic Algorithm (SVM-GA), SVM-Artificial Immune System (SVM-AIS), SVM-AIRS and fuzzy support vector machine (FSVM) had great performances achieving better results in terms of accuracy, sensitivity and specificity.",2018,,https://doi.org/10.1016/j.neucom.2017.01.126
445,Jérôme Béranger,3 - Management and Governance of Personal Health Data,"Controlled regulation, Data lifecycle, Environmental digital ecosystem, Governance, Management, Quality control, Regulatory and organizational aspects, Relational and cultural aspects, Strategic and methodological aspects, Structural and technological aspects","Abstract:
Every company has its own culture, its organization, its governance mode and its project management models. Nevertheless, a number of significant and universal principles concerning governance can be identified, both at the approach level as well as that of actors and of responsibilities. Data governance is one of the key factors to success in the protection of information. It is one of the components that defines the rules, guides and charters of good practice, establishes references and policies (management, classification, storage, and conservation of personal data), and further describes the responsibilities and controls their application. Therefore, it becomes paramount to understand: how can the complexity around personal data management be apprehended and in particular in health fields? In addition, what are the possible mechanisms to process these data pools to turn them into consistent and relevant information? To this end, it is essential to have a detailed and an accurate understanding of algorithmic governance, of the environmental numerical ecosystem, of safety and protection, and of the lifecycle of Big Data.",2016,,https://doi.org/10.1016/B978-1-78548-025-6.50003-8
446,Xiaolin Yang and Zhe Wang and Hongjie Pan and Yan Zhu,Ontology: Footstone for Strong Artificial Intelligence,"ontology, artificial intelligence, biomedicine, big data","Abstract
In the past ten years, the application of artificial intelligence (AI) in biomedicine has increased rapidly, which roots in the rapid growth of biomedicine data, the improvement of computing performance, and the development of deep learning methods. At present, there are great difficulties in front of AI for solving complex and comprehensive medical problems. Ontology can play an important role in how to make machines have stronger intelligence and has wider applications in the medical field. By using ontologies, (meta) data can be standardized so that data quality is improved and more data analysis methods can be introduced, data integration can be supported by the semantics relationships which are specified in ontologies, and effective logic expression in nature language can be better understood by machine. This can be a pathway to stronger AI. Under this circumstance, the Chinese Conference on Biomedical Ontology and Terminology was held in Beijing in autumn 2019, with the theme “Making Machine Understand Data”. The success of this conference further improves the development of ontology in the field of biomedical information in China, and will promote the integration of Chinese ontology research and application with the international standards and the findability, accessibility, interoperability, and reusability(FAIR) Data Principle.",2019,,https://doi.org/10.24920/003701
447,Philip Woodall and Vaggelis Giannikas and Wenrong Lu and Duncan McFarlane,Potential Problem Data Tagging: Augmenting information systems with the capability to deal with inaccuracies,"Data quality, Information quality, Accuracy, Metadata, Data analytics, Data tags","Data quality tags are a means of informing decision makers about the quality of the data they use from information systems. Unfortunately, data quality tags have not been successfully adopted despite their potential to assist decision makers. One reason for the non-adoption is that maintaining the tags is expensive and time-consuming: having a tag that represents accuracy, for example, would be massively time-consuming to measure because it requires some physical observation of reality to check the true value. We argue that a useful surrogate tag for accuracy can be created—without having to physically measure it—by counting the number of times the data has been exposed to an event that could cause it to become inaccurate. Experimental results show that the tags can help to avoid problems caused by inaccuracies, and also to help find the inaccuracies themselves.",2019,,https://doi.org/10.1016/j.dss.2019.04.007
448,Paneez Khoury and Renganathan Srinivasan and Sujani Kakumanu and Sebastian Ochoa and Anjeni Keswani and Rachel Sparks and Nicholas L. Rider,"A Framework for Augmented Intelligence in Allergy and Immunology Practice and Research—A Work Group Report of the AAAAI Health Informatics, Technology and Education Committee","Artificial intelligence, Asthma, Primary immunodeficiency, Atopic dermatitis, Augmented intelligence, Clinical decision support, Electronic health records, Equity, Machine learning, Natural language processing, Medical education","Artificial and augmented intelligence (AI) and machine learning (ML) methods are expanding into the health care space. Big data are increasingly used in patient care applications, diagnostics, and treatment decisions in allergy and immunology. How these technologies will be evaluated, approved, and assessed for their impact is an important consideration for researchers and practitioners alike. With the potential of ML, deep learning, natural language processing, and other assistive methods to redefine health care usage, a scaffold for the impact of AI technology on research and patient care in allergy and immunology is needed. An American Academy of Asthma Allergy and Immunology Health Information Technology and Education subcommittee workgroup was convened to perform a scoping review of AI within health care as well as the specialty of allergy and immunology to address impacts on allergy and immunology practice and research as well as potential challenges including education, AI governance, ethical and equity considerations, and potential opportunities for the specialty. There are numerous potential clinical applications of AI in allergy and immunology that range from disease diagnosis to multidimensional data reduction in electronic health records or immunologic datasets. For appropriate application and interpretation of AI, specialists should be involved in the design, validation, and implementation of AI in allergy and immunology. Challenges include incorporation of data science and bioinformatics into training of future allergists-immunologists.",2022,,https://doi.org/10.1016/j.jaip.2022.01.047
449,Keke Gai and Meikang Qiu and Xiaotong Sun,A survey on FinTech,"FinTech, Cloud computing, Cyber security, Big data, Financial computing, Data-driven framework","As a new term in the financial industry, FinTech has become a popular term that describes novel technologies adopted by the financial service institutions. This term covers a large scope of techniques, from data security to financial service deliveries. An accurate and up-to-date awareness of FinTech has an urgent demand for both academics and professionals. This work aims to produce a survey of FinTech by collecting and reviewing contemporary achievements, by which a theoretical data-driven FinTech framework is proposed. Five technical aspects are summarized and involved, which include security and privacy, data techniques, hardware and infrastructure, applications and management, and service models. The main findings of this work are fundamentals of forming active FinTech solutions.",2018,,https://doi.org/10.1016/j.jnca.2017.10.011
450,Dominik C. Hezel and Markus Harak and Guy Libourel,What we know about elemental bulk chondrule and matrix compositions: Presenting the ChondriteDB Database,"Chondrules, Matrix, Elemental composition, ChondritedDB, Database","Chondrules and matrix are the major components of chondritic meteorites and represent a significant evolutionary step in planet formation. The formation and evolution of chondrules and matrix and, in particular, the mechanics of chondrule formation remain the biggest unsolved challenge in meteoritics. A large number of studies of these major components not only helped to understand these in ever greater detail, but also produced a remarkably large body of data. Studying all available data has become known as ‹big data› analyses and promises deep insights – in this case – to chondrule and matrix formation and relationships. Looking at all data may also allow one to better understand the mechanism of chondrule formation or, equally important, what information we might be missing to identify this process. A database of all available chondrule and matrix data further provides an overview and quick visualisation, which will not only help to solve actual problems, but also enable students and future researchers to quickly access and understand all we know about these components. We collected all available data on elemental bulk chondrule and matrix compositions in a database that we call ChondriteDB. The database also contains petrographic and petrologic information on chondrules. Currently, ChondriteDB contains about 2388 chondrule and 1064 matrix data from 70 different publications and 161 different chondrites. Future iterations of ChondriteDB will include isotope data and information on other chondrite components. Data quality is of critical importance. However, as we discuss, quality is not an objective category, but a subjective judgement. Quantifiable data acquisition categories are required that allow selecting the appropriate data from a database in the context of a given research problem. We provide a comprehensive overview on the contents of ChondriteDB. The database is available as an Excel file upon request from the senior author of this paper, or can be accessed through MetBase.",2018,,https://doi.org/10.1016/j.chemer.2017.05.003
451,Kaile Zhou and Shanlin Yang,5.11 Smart Energy Management,"Demand side management, Energy big data, Energy consumption behavior, Energy informatics, Energy social informatics, Smart energy management, Smart energy system","Smart energy management is the path to achieve the management and operational objectives of smart energy systems (SESs). First, some related concepts of smart energy management are introduced. The evolution of energy systems in four stages and the three dimensions of smart energy management are also proposed. Then the overall structure and key technologies of SESs are provided, followed by the introduction of the composition of energy big data and its application in demand side management (DSM). Furthermore, the Ubiquitous Energy Internet in China, the smart energy management in smart buildings, smart manufacturing, and smart transportation are discussed as case studies of smart energy management. Finally, the research paradigms of smart energy management are presented and future directions are pointed out.",2018,,https://doi.org/10.1016/B978-0-12-809597-3.00525-3
452,Yang Ye and Linyan Huang and Qiming Zheng and Chenxin Liang and Baiyu Dong and Jinsong Deng and Xiuzhen Han,A feasible framework to downscale NPP-VIIRS nighttime light imagery using multi-source spatial variables and geographically weighted regression,"Nighttime light (NTL), Downscaling, Geographically weighted regression (GWR), Impervious surface detection","The cloud-free monthly composite of global nighttime light (NTL) data of the Suomi National Polar-orbiting Partnership with the Visible Infrared Imaging Radiometer Suite (NPP-VIIRS) day/night band (DNB) provides indispensable indications of human activities and settlements. However, the coarse spatial resolution (15 arc sec) of NTL imagery greatly restricts its application potential. This study proposes a feasible framework to downscale NPP-VIIRS NTL using muti-source spatial variables and geographically weighted regression (GWR) method. High-resolution auxiliary variables were acquired from the Landsat 8 OLI/ TIRS and social media platforms. GWR-based downscaling procedures were consequently implemented to obtain NTL at a 100-m resolution. The downscaled NTL data were validated against Loujia1-01 imagery based on the coefficient of determination (R2) and root-mean-square error (RMSE). The results suggest that the data quality was suitably improved after downscaling, yielding higher R2 (0.604 vs. 0.568) and lower RMSE (8.828 vs. 9.870 nW/cm2/sr) values than those of the original NTL data. Finally, the NTL was extendedly applied to detect impervious surfaces, and the downscaled NTL had higher accuracy than the original NTL. Therefore, this study facilitates data quality improvement of NPP-VIIRS NTL imagery by downscaling, thus enabling more accurate applications.",2021,,https://doi.org/10.1016/j.jag.2021.102513
453,P.C. Taylor and M. Abeysekera and Y. Bian and D. Ćetenović and M. Deakin and A. Ehsan and V. Levi and F. Li and R. Oduro and R. Preece and P.G. Taylor and V. Terzija and S.L. Walker and J. Wu,An interdisciplinary research perspective on the future of multi-vector energy networks,"Energy markets, Information and communication technologies, Modelling, Multi-vector energy networks, Policy, Risk","Understanding the future of multi-vector energy networks in the context of the transition to net zero and the energy trilemma (energy security, environmental impact and social cost) requires novel interdisciplinary approaches. A variety of challenges regarding systems, plant, physical infrastructure, sources and nature of uncertainties, technological in general and more specifically Information and Communication Technologies requirements, cyber security, big data analytics, innovative business models and markets, policy and societal changes, are critically important to ensure enhanced flexibility and higher resilience, as well as reduced costs of an integrated energy system. Integration of individual energy networks into multi-vector entities opens a number of opportunities, but also presents a number of challenges requiring interdisciplinary perspectives and solutions. Considering drivers like societal evolution, climate change and technology advances, this paper describes the most important aspects which have to be taken into account when designing, planning and operating future multi-vector energy networks. For this purpose, the issues addressing future architecture, infrastructure, interdependencies and interactions of energy network infrastructures are elaborated through a novel interdisciplinary perspective. Aspects related to optimal operation of multi-vector energy networks, implementation of novel technologies, jointly with new concepts and algorithms, are extensively discussed. The role of policy, markets and regulation in facilitating multi-vector energy networks is also reported. Last but not least, the aspects of risks and uncertainties, relevant for secure and optimal operation of future multi-vector energy networks are discussed.",2022,,https://doi.org/10.1016/j.ijepes.2021.107492
454,Jinha Jung and Murilo Maeda and Anjin Chang and Mahendra Bhandari and Akash Ashapure and Juan Landivar-Bowles,The potential of remote sensing and artificial intelligence as tools to improve the resilience of agriculture production systems,,"Modern agriculture and food production systems are facing increasing pressures from climate change, land and water availability, and, more recently, a pandemic. These factors are threatening the environmental and economic sustainability of current and future food supply systems. Scientific and technological innovations are needed more than ever to secure enough food for a fast-growing global population. Scientific advances have led to a better understanding of how various components of the agricultural system interact, from the cell to the field level. Despite incredible advances in genetic tools over the past few decades, our ability to accurately assess crop status in the field, at scale, has been severely lacking until recently. Thanks to recent advances in remote sensing and Artificial Intelligence (AI), we can now quantify field scale phenotypic information accurately and integrate the big data into predictive and prescriptive management tools. This review focuses on the use of recent technological advances in remote sensing and AI to improve the resilience of agricultural systems, and we will present a unique opportunity for the development of prescriptive tools needed to address the next decade’s agricultural and human nutrition challenges.",2021,,https://doi.org/10.1016/j.copbio.2020.09.003
455,Gema {Del Río Castro} and María Camino {González Fernández} and Ángel {Uruburu Colsa},Unleashing the convergence amid digitalization and sustainability towards pursuing the Sustainable Development Goals (SDGs): A holistic review,"Sustainability, Sustainable development goals (SDGs), Digitalization, ICT, Big data, Artificial intelligence","The Sustainable Development Goals (SDGs) within the United Nations 2030 Agenda emerged in 2015, becoming an unprecedented global compass for navigating extant sustainability challenges. Nevertheless, it still represents a nascent field enduring uncertainties and complexities. In this regard, the interplay between digitalization and sustainability unfolds bright opportunities for shaping a greener economy and society, paving the way towards the SDGs. However, little evidence exists so far, about a genuine contribution of digital paradigms to sustainability. Besides, their role to tackle the SDGs research gaps remains unexplored. Thus, a holistic characterization of the aforementioned topics has not been fully explored in the emerging literature, deserving further research. The article endeavors a twofold purpose: (1) categorizing the main SDGs research gaps; (2) coupled with a critical exploration of the potential contribution of digital paradigms, particularly Big Data and Artificial Intelligence, towards overcoming the aforesaid caveats and pursuing the 2030 Agenda. Ultimately, the study seeks to bridge literature gaps by providing a first-of-its-kind overview on the SDGs and their nexus with digitalization, while unraveling policy implications and future research directions. The methodology has consisted of a systematic holistic review and in-depth qualitative analysis of the literature on the realms of the SDGs and digitalization. Our findings evidence that the SDGs present several research gaps, namely: flawed understanding of complexities and interlinkages; design shortcomings and imbalances; implementation and governance hurdles; unsuitable indicators and assessment methodologies; truncated adoption and off-target progress; unclear responsibilities and lacking coordination; untapped role of technological innovation and knowledge management. Moreover, our results show growing expectations about the added value brought by digitalization for pursuing the SDGs, through novel data sources, enhanced analytical capacities and collaborative digital ecosystems. However, current research and practice remains in early-stage, pointing to ethical, social and environmental controversies, along with policy caveats, which merit additional research. In light of the findings, the authors suggest a first-approach exploration of research and policy implications. Results suggest that further multidisciplinary research, dialogue and concerted efforts for transformation are required. Reframing the Agenda, while aligning the sustainable development and digitalization policies, seems advisable to ensure a holistic sustainability. The findings aim at guiding and stimulating further research and science-policy dialogue on the promising nexus amid the SDGs and digitalization.",2021,,https://doi.org/10.1016/j.jclepro.2020.122204
456,Oleksii Pasichnyi and Jörgen Wallin and Fabian Levihn and Hossein Shahrokni and Olga Kordas,Energy performance certificates — New opportunities for data-enabled urban energy policy instruments?,"Energy performance certificate (EPC), Building energy efficiency, Data applications, Data quality, Sweden","Energy performance certificates (EPC) were introduced in European Union to support reaching energy efficiency targets by informing actors in the building sector about energy efficiency in buildings. While EPC have become a core source of information about building energy, the domains of its applications have not been studied systematically. This partly explains the limitation of conventional EPC data quality studies that fail to expose the essential problems and secure effective use of the data. This study reviews existing applications of EPC data and proposes a new method for assessing the quality of EPCs using data analytics. Thirteen application domains were identified from systematic mapping of 79 papers, revealing increases in the number and complexity of studies and advances in applied data analysis techniques. The proposed data quality assurance method based on six validation levels was tested using four samples of EPC dataset for the case of Sweden. The analysis showed that EPC data can be improved through adding or revising the EPC features and assuring interoperability of EPC datasets. In conclusion, EPC data have wider applications than initially intended by the EPC policy instrument, placing stronger requirements on the quality and content of the data.",2019,,https://doi.org/10.1016/j.enpol.2018.11.051
457,Christoph Schock and Jonas Dumler and Frank Doepper,Data Acquisition and Preparation – Enabling Data Analytics Projects within Production,"Data Analytics, CRISP-DM, Data Acquisition, Data Preparation, Feature Engineering, Process Monitoring, Condition Monitoring","The increasing amount of available data in production systems is associated with great potential for process optimization. Due to lack of a data analytics methodology and low data quality within production these potentials often remain unused. Therefore, in this paper we present a model for data acquisition and data preparation including feature engineering for characteristic sensor signals of production machines. The model allows the extraction of relevant process information from the signal, which can be used for monitoring, KPI tracking, trend analysis and anomaly detection. The approach is evaluated on an industrial turning process.",2021,,https://doi.org/10.1016/j.procir.2021.11.107
458,Guolong Liu and Jinjin Gu and Junhua Zhao and Fushuan Wen and Gaoqi Liang,Super Resolution Perception for Smart Meter Data,"Super resolution perception, Smart meter data, High-frequency data, Big data analysis","In this paper, we present the problem formulation and methodology framework of Super Resolution Perception (SRP) on smart meter data. With the widespread use of smart meters, a massive amount of electricity consumption data can be obtained. Smart meter data is the basis of automated billing and pricing, appliance identification, demand response, etc. However, the provision of high-quality data may be expensive in many cases. In this paper, we propose a novel problem - the SRP problem as reconstructing high-quality data from unsatisfactory data in smart grids. Advanced generative models are then proposed to solve the problem. This technology makes it possible for empowering existing facilities without upgrading existing meters or deploying additional meters. We first mathematically formulate the SRP problem under the Maximum a Posteriori (MAP) estimation framework. The dataset namely Super Resolution Perception Dataset (SRPD) is designed for this problem and released. A case study is then presented, which performs SRP on smart meter data. A network namely Super Resolution Perception Convolutional Neural Network (SRPCNN) is proposed to generate high-frequency load data from low-frequency data. Experiments demonstrate that our SRP models can reconstruct high-frequency data effectively. Moreover, the reconstructed high-frequency data can lead to better appliance identification results.",2020,,https://doi.org/10.1016/j.ins.2020.03.088
459,Ratchayuda Kongboon and Shabbir H. Gheewala and Sate Sampattagul,Greenhouse gas emissions inventory data acquisition and analytics for low carbon cities,"Sustainable city, Low carbon city, Greenhouse gas inventory, Greenhouse gas emissions, Municipalities","This paper studied greenhouse gas inventory data acquisition and analytics for municipalities in Thailand. A complete and transparent GHG inventory of eight municipalities was developed to document the current situation, and to help decision-makers to clarify their priorities for reducing greenhouse gas emissions. The Global Protocol for Community-Scale Greenhouse Gas Emissions Inventories guidelines was used to investigate and calculate the greenhouse gas emissions and assess data accuracy. The results indicated that the data source, data format, and data collection of each municipality are relatively similar. Moreover, the activity data needed to be obtained from several authorities. The results showed that Nonthaburi Municipality had the highest greenhouse gas emissions at 2,286,838 tCO2e/yr and Buriram Municipality, the lowest at 239,795 tCO2e/yr. On a per-capita basis, Lamphun Municipality was the highest with 10.1 tCO2e/capita and Buriram Municipality the lowest with 3.8 tCO2e/capita. The results suggest that the municipalities should continually develop a GHG database by creating a routine procedure. An information management system should be produced in the shape of big data which can lead to state policies, plans, and actions for city development to ensure the reduction of greenhouse gas emissions. This in turn will lead to a low carbon city.",2022,,https://doi.org/10.1016/j.jclepro.2022.130711
460,Martin Lnenicka and Jitka Komarkova,Big and open linked data analytics ecosystem: Theoretical background and essential elements,"Big and open linked data, Ecosystem approach, Dimensions, Data analytics lifecycle, Stakeholders, Conceptual framework","Big and open linked data are often mentioned together because storing, processing, and publishing large amounts of these data play an increasingly important role in today's society. However, although this topic is described from the political, economic, and social points of view, a technical dimension, which is represented by big data analytics, is insufficient. The aim of this review article was to provide a theoretical background of big and open linked data analytics ecosystem and its essential elements. First, the key terms were introduced including related dimensions. Then, the key lifecycle phases were defined and involved stakeholders were identified. Finally, a conceptual framework was proposed. In contrast to previous research, the new ecosystem is formed by interactions of stakeholders in the following dimensions and their sub-dimensions: transparency, engagement, legal, technical, social, and economic. These relationships are characterized by the most important requisites and public policy choices affecting the data analytics ecosystem together with the key phases and activities of the data analytics lifecycle. The findings should contribute to relevant initiatives, strategies, and policies and their effective implementation.",2019,,https://doi.org/10.1016/j.giq.2018.11.004
461,Arthur Vinicius Rodrigues and Gabriel Nakamura and Vanessa Graziele Staggemeier and Leandro Duarte,Species misidentification affects biodiversity metrics: Dealing with this issue using the new R package naturaList,"Occurrence records, Biodiversity, Species misidentification, Taxonomy, Ecological patterns","Biodiversity databases are increasingly available and have fostered accelerated advances in many disciplines within ecology and evolution. However, the quality of the evidence generated depends critically on the quality of the input data, and species misidentifications are present in virtually any occurrence dataset. Yet, the lack of automatized tools makes the assessment of the quality of species identification in big datasets time-consuming, which often induces researchers to assume that all species are reliably identified. In this study, we address this issue by evaluating how species misidentification can impact our ability to capture ecological patterns, and by presenting an R package, called naturaList, designed to classify species occurrence data according to identification reliability. naturaList allows the classification of species occurrences up to six confidence levels, in which the highest level is assigned to records identified by specialists. We obtained a list of specialists by using the species occurrence dataset itself, based on the identifier names within it, and by entering an independent list, obtained by contacting experts. Further, we evaluate the effects of filtering out occurrence records not identified by specialists on the estimations of species niche and diversity patterns. We used the tribe Myrteae (Myrtaceae) as a study model, which is a species-rich group in Central and South America and with challenging taxonomy. We found a significant change in species niche in 13% of species when using only occurrences identified by specialists. We found changes in patterns of alpha diversity in four genera and changes in beta diversity in all genera analyzed. We show how the uncertainty in species identification in occurrence datasets affects conclusions on macroecological patterns by generating bias or noise in different aspects of macroecological patterns (niche, alpha, and beta diversity). Therefore, to guarantee reliability in species identification in big data sets we recommend the use of automated tools such as the naturaList package, especially when analyzing variation in species composition. This study also represents a step forward to increasing the quality of large-scale studies that rely on species occurrence data.",2022,,https://doi.org/10.1016/j.ecoinf.2022.101625
462,Xiaofeng Jin and Conghui Liu and Tailin Xu and Lei Su and Xueji Zhang,Artificial intelligence biosensors: Challenges and prospects,"Wearable biosensor, Artificial intelligence, Biomarker, Wireless communication, Machine learning, Healthcare","Artificial intelligence (AI) and wearable sensors are two essential fields to realize the goal of tailoring the best precision medicine treatment for individual patients. Integration of these two fields enables better acquisition of patient data and improved design of wearable sensors for monitoring the wearers' health, fitness and their surroundings. Currently, as the Internet of Things (IoT), big data and big health move from concept to implementation, AI-biosensors with appropriate technical characteristics are facing new opportunities and challenges. In this paper, the most advanced progress made in the key phases for future wearable and implantable technology from biosensing, wearable biosensing to AI-biosensing is summarized. Without a doubt, material innovation, biorecognition element, signal acquisition and transportation, data processing and intelligence decision system are the most important parts, which are the main focus of the discussion. The challenges and opportunities of AI-biosensors moving forward toward future medicine devices are also discussed.",2020,,https://doi.org/10.1016/j.bios.2020.112412
463,Md. Mehedi Hassan Onik and Satyabrata Aich and Jinhong Yang and Chul-Soo Kim and Hee-Cheol Kim,Chapter 8 - Blockchain in Healthcare: Challenges and Solutions,"Blockchain, Big data, Healthcare, Data privacy, EHR security","The main challenge in distributing electronic health records (EHRs) for patient-centered research, market analysis, medicine investigation, healthcare data mining etc., is data privacy. Handling the large-scale data and preserving the privacy of patients has been a challenge to researchers for a long period of time. On the contrary, blockchain technology has alleviated some of the problems by providing a protected and distributed platform. Sadly, existing electronic health record (EHR) management systems suffer from data manipulation, delayed communication, and trustless cooperation in data collection, storage, and distribution. This chapter discusses the current issues of healthcare data privacy and existing and upcoming regulations on this sector. This chapter also includes an overview of the architecture, existing issues, and future scope of blockchain technology for successfully handling privacy and management of current and future medical records. This chapter also presents few blockchain solutions that advocate the future research scopes in healthcare, big data, and blockchain.",2019,,https://doi.org/10.1016/B978-0-12-818146-1.00008-8
464,Cheng Fan and Fu Xiao and Zhengdao Li and Jiayuan Wang,Unsupervised data analytics in mining big building operational data for energy efficiency enhancement: A review,"Unsupervised data mining, Big data, Building operational performance, Building energy management, Building energy efficiency","Building operations account for the largest proportion of energy use throughout the building life cycle. The energy saving potential is considerable taking into account the existence of a wide variety of building operation deficiencies. The advancement in information technologies has made modern buildings to be not only energy-intensive, but also information-intensive. Massive amounts of building operational data, which are in essence the reflection of actual building operating conditions, are available for knowledge discovery. It is very promising to extract potentially useful insights from big building operational data, based on which actionable measures for energy efficiency enhancement are devised. Data mining is an advanced technology for analyzing big data. It consists of two main types of data analytics, i.e., supervised and unsupervised analytics. Despite of the power of supervised analytics in predictive modeling, unsupervised analytics are more practical and promising in discovering novel knowledge given limited prior knowledge. This paper provides a comprehensive review on the current utilization of unsupervised data analytics in mining massive building operational data. The commonly used unsupervised analytics are summarized according to their knowledge representations and applications. The challenges and opportunities are elaborated as guidance for future research in this multi-disciplinary field.",2018,,https://doi.org/10.1016/j.enbuild.2017.11.008
465,Julien Leprince and Clayton Miller and Wim Zeiler,"Data mining cubes for buildings, a generic framework for multidimensional analytics of building performance data","Data mining, Data cube, Generic method, Multidimensional analytics, Machine learning, Building data","Over the last decade, collecting massive volumes of data has been made all the more accessible, pushing the building sector to embrace data mining as a powerful tool for harvesting the potential of big data analytics. However repetitive challenges still persist emerging from the need for a common analytical frame, effective application- and insight-driven targeted data selection, as well as benchmarked-supported claims. This study addresses these concerns by putting forward a generic stepwise multidimensional data mining framework tailored to building data, leveraging the dimensional-structures of data cubes. Using the open Building Data Genome Project 2 set, composed of 3053 energy meters from 1636 buildings, we provide an online, open access, implementation illustration of our method applied to automated pattern identification. We define a 3-dimensional building cube echoing typical analytical frames of interest, namely, bottom-up, top-down and temporal drill-in approaches. Our results highlight the importance of application and insight driven mining for effective dimensional-frame targeting. Impactful visualizations were developed allowing practical human inspection, paving the path towards more interpretable analytics.",2021,,https://doi.org/10.1016/j.enbuild.2021.111195
466,Mona Nashaat and Aindrila Ghosh and James Miller and Shaikh Quader and Chad Marston,M-Lean: An end-to-end development framework for predictive models in B2B scenarios,"Big data, Machine learning, Business-to-business, User trust, Case study","Context
The need for business intelligence has led to advances in machine learning in the business domain, especially with the rise of big data analytics. However, the resulting predictive systems often fail to maintain a satisfactory level of performance in production. Besides, for predictive systems used in business-to-business scenarios, user trust is subject to the model performance. Therefore, the processes of creating, evaluating, and deploying machine learning systems in the business domain need innovative solutions to solve the critical challenges of assuring the quality of the resulting systems.
Objective
Applying machine learning in business-to-business situations imposes specific requirements. This paper aims at providing an integrated solution to businesses to help them transform their data into actions.
Method
The paper presents MLean, an end-to-end framework, that aims at guiding businesses in designing, developing, evaluating, and deploying business-to-business predictive systems. The framework employs the Lean Startup methodology and aims at maximizing the business value while eliminating wasteful development practices.
Results
To evaluate the proposed framework, with the help of our industrial partner, we applied the framework to a case study to build a predictive product. The case study resulted in a predictive system to predict the risks of software license cancellations. The system was iteratively developed and evaluated while adopting the management and end-user perspectives.
Conclusion
It is concluded that, in industry, it is important to be aware of the businesses requirements before considering the application of machine learning. The framework accommodates business perspective from the beginning to produce a holistic product. From the results of the case study, we think that this framework can help businesses define the right opportunities for applying machine learning, developing solutions, evaluating the effectiveness of these solutions, and maintaining their performance in production.",2019,,https://doi.org/10.1016/j.infsof.2019.05.009
467,Marta Amengual-Gual and Adriana Ulate-Campos and Tobias Loddenkemper,"Status epilepticus prevention, ambulatory monitoring, early seizure detection and prediction in at-risk patients","Epilepsy, Status epilepticus, Closed-loop systems, Machine learning, Seizure detection sensors, Automated seizure detection","Purpose
Status epilepticus is an often apparently randomly occurring, life-threatening medical emergency which affects the quality of life in patients with epilepsy and their families. The purpose of this review is to summarize information on ambulatory seizure detection, seizure prediction, and status epilepticus prevention.
Method
Narrative review.
Results
Seizure detection devices are currently under investigation with regards to utility and feasibility in the detection of isolated seizures, mainly in adult patients with generalized tonic-clonic seizures, in long-term epilepsy monitoring units, and occasionally in the outpatient setting. Detection modalities include accelerometry, electrocardiogram, electrodermal activity, electroencephalogram, mattress sensors, surface electromyography, video detection systems, gyroscope, peripheral temperature, photoplethysmography, and respiratory sensors, among others. Initial detection results are promising, and improve even further, when several modalities are combined. Some portable devices have already been U.S. FDA approved to detect specific seizures. Improved seizure prediction may be attainable in the future given that epileptic seizure occurrence follows complex patient-specific non-random patterns. The combination of multimodal monitoring devices, big data sets, and machine learning may enhance patient-specific detection and predictive algorithms. The integration of these technological advances and novel approaches into closed-loop warning and treatment systems in the ambulatory setting may help detect seizures sooner, and tentatively prevent status epilepticus in the future.
Conclusions
Ambulatory monitoring systems are being developed to improve seizure detection and the quality of life in patients with epilepsy and their families.",2019,,https://doi.org/10.1016/j.seizure.2018.09.013
468,Guoyan Li and Chenxi Yuan and Sagar Kamarthi and Mohsen Moghaddam and Xiaoning Jin,Data science skills and domain knowledge requirements in the manufacturing industry: A gap analysis,"Industry 4.0, Labor market analysis, Skills gap, Data science","Manufacturing has adopted technologies such as automation, robotics, industrial Internet of Things (IoT), and big data analytics to improve productivity, efficiency, and capabilities in the production environment. Modern manufacturing workers not only need to be adept at the traditional manufacturing technologies but also ought to be trained in the advanced data-rich computer-automated technologies. This study analyzes the data science and analytics (DSA) skills gap in today's manufacturing workforce to identify the critical technical skills and domain knowledge required for data science and intelligent manufacturing-related jobs that are highly in-demand in today's manufacturing industry. The gap analysis conducted in this paper on Emsi job posting and profile data provides insights into the trends in manufacturing jobs that leverage data science, automation, cyber, and sensor technologies. These insights will be helpful for educators and industry to train the next generation manufacturing workforce. The main contribution of this paper includes (1) presenting the overall trend in manufacturing job postings in the U.S., (2) summarizing the critical skills and domain knowledge in demand in the manufacturing sector, (3) summarizing skills and domain knowledge reported by manufacturing job seekers, (4) identifying the gaps between demand and supply of skills and domain knowledge, and (5) recognize opportunities for training and upskilling workforce to address the widening skills and knowledge gap.",2021,,https://doi.org/10.1016/j.jmsy.2021.07.007
469,Thanasis Kotsiopoulos and Panagiotis Sarigiannidis and Dimosthenis Ioannidis and Dimitrios Tzovaras,Machine Learning and Deep Learning in smart manufacturing: The Smart Grid paradigm,"Industry 4.0, Machine Learning, Deep Learning, Industrial AI, Smart Grid","Industry 4.0 is the new industrial revolution. By connecting every machine and activity through network sensors to the Internet, a huge amount of data is generated. Machine Learning (ML) and Deep Learning (DL) are two subsets of Artificial Intelligence (AI), which are used to evaluate the generated data and produce valuable information about the manufacturing enterprise, while introducing in parallel the Industrial AI (IAI). In this paper, the principles of the Industry 4.0 are highlighted, by giving emphasis to the features, requirements, and challenges behind Industry 4.0. In addition, a new architecture for AIA is presented. Furthermore, the most important ML and DL algorithms used in Industry 4.0 are presented and compiled in detail. Each algorithm is discussed and evaluated in terms of its features, its applications, and its efficiency. Then, we focus on one of the most important Industry 4.0 fields, namely the smart grid, where ML and DL models are presented and analyzed in terms of efficiency and effectiveness in smart grid applications. Lastly, trends and challenges in the field of data analysis in the context of the new Industrial era are highlighted and discussed such as scalability, cybersecurity, and big data.",2021,,https://doi.org/10.1016/j.cosrev.2020.100341
470,Karen Sugden and Eilis J. Hannon and Louise Arseneault and Daniel W. Belsky and David L. Corcoran and Helen L. Fisher and Renate M. Houts and Radhika Kandaswamy and Terrie E. Moffitt and Richie Poulton and Joseph A. Prinz and Line J.H. Rasmussen and Benjamin S. Williams and Chloe C.Y. Wong and Jonathan Mill and Avshalom Caspi,Patterns of Reliability: Assessing the Reproducibility and Integrity of DNA Methylation Measurement,DSML 3:  Data science output has been rolled out/validated across multiple domains/problems,"Summary
DNA methylation plays an important role in both normal human development and risk of disease. The most utilized method of assessing DNA methylation uses BeadChips, generating an epigenome-wide “snapshot” of >450,000 observations (probe measurements) per assay. However, the reliability of each of these measurements is not equal, and little consideration is paid to consequences for research. We correlated repeat measurements of the same DNA samples using the Illumina HumanMethylation450K and the Infinium MethylationEPIC BeadChips in 350 blood DNA samples. Probes that were reliably measured were more heritable and showed consistent associations with environmental exposures, gene expression, and greater cross-tissue concordance. Unreliable probes were less replicable and generated an unknown volume of false negatives. This serves as a lesson for working with DNA methylation data, but the lessons are equally applicable to working with other data: as we advance toward generating increasingly greater volumes of data, failure to document reliability risks harming reproducibility.",2020,,https://doi.org/10.1016/j.patter.2020.100014
471,Alejandro Maté and Jesús Peral and Antonio Ferrández and David Gil and Juan Trujillo,A hybrid integrated architecture for energy consumption prediction,"Data mining, Energy consumption, Information Extraction, Big data, Decision trees, Social networks","Irresponsible and negligent use of natural resources in the last five decades has made it an important priority to adopt more intelligent ways of managing existing resources, especially the ones related to energy. The main objective of this paper is to explore the opportunities of integrating internal data already stored in Data Warehouses together with external Big Data to improve energy consumption predictions. This paper presents a study in which we propose an architecture that makes use of already stored energy data and external unstructured information to improve knowledge acquisition and allow managers to make better decisions. This external knowledge is represented by a torrent of information that, in many cases, is hidden across heterogeneous and unstructured data sources, which are recuperated by an Information Extraction system. Alternatively, it is present in social networks expressed as user opinions. Furthermore, our approach applies data mining techniques to exploit the already integrated data. Our approach has been applied to a real case study and shows promising results. The experiments carried out in this work are twofold: (i) using and comparing diverse Artificial Intelligence methods, and (ii) validating our approach with data sources integration.",2016,,https://doi.org/10.1016/j.future.2016.03.020
472,Tracie Risling,Educating the nurses of 2025: Technology trends of the next decade,"Informatics, Curriculum development, Technology","The pace of technological evolution in healthcare is advancing. In this article key technology trends are identified that are likely to influence nursing practice and education over the next decade. The complexity of curricular revision can create challenges in the face of rapid practice change. Nurse educators are encouraged to consider the role of electronic health records (EHRs), wearable technologies, big data and data analytics, and increased patient engagement as key areas for curriculum development. Student nurses, and those already in practice, should be offered ongoing educational opportunities to enhance a wide spectrum of professional informatics skills. The nurses of 2025 will most certainly inhabit a very different practice environment than what exists today and technology will be key in this transformation. Nurse educators must prepare now to lead these practitioners into the future.",2017,,https://doi.org/10.1016/j.nepr.2016.12.007
473,Dalwoo Nam and Junyeong Lee and Heeseok Lee,Business analytics adoption process: An innovation diffusion perspective,"Business analytics, Innovation diffusion, Adoption process, Data infrastructure, Data quality management, Analytics centralization","Although business analytics (BA) have been increasingly adopted into businesses, there is limited empirical research examining the drivers of each stage of BA adoption in organizations. Drawing upon technological-organizational-environmental framework and innovation diffusion process, we developed an integrative model to examine BA adoption processes and tested with 170 Korean firms. The analysis shows data-related technological characteristics derive all stages of BA adoption: initiation, adoption and assimilation. While organizational characteristics are associated with adoption and assimilation stage, only competition intensity in environmental characteristics is associated with initiation stage. Our findings help practitioners and researchers to understand what factors can enable companies to adopt BA in each stage.",2019,,https://doi.org/10.1016/j.ijinfomgt.2019.07.017
474,Matthias Dziubany and Marcel Garling and Anke Schmeink and Guido Burger and Guido Dartmann and Stefan Naumann and Klaus-Uwe Gollmer,Chapter 11 - Machine learning-based artificial nose on a low-cost IoT-hardware,"Artificial Nose, PCA, SVM, Feature selection, low cost","In order to make Internet of things applications easily available and cost-effective, we aim at using low-cost hardware for typical measurement tasks, and in return putting more effort into the signal processing and data analysis. By the example of beverage recognition with a low-cost temperature-modulated gas sensor, we demonstrate the benefits of processing techniques in big data such as feature selection and dimensionality reduction. Specifically, we determine a subset of temperatures that yields good support vector machine classification results and thereby shortens the measurement process.",2019,,https://doi.org/10.1016/B978-0-12-816637-6.00011-7
475,Sai-Ho Chung and Hoi-Lam Ma and Mark Hansen and Tsan-Ming Choi,Data science and analytics in aviation,"Data science, Aviation, Analytics, Flight, Air logistics","Data science and analytics are attracting more and more attention from researchers and practitioners in recent years. Due to the rapid development of advanced technologies nowadays, a massive amount of real time data regarding flight information, flight performance, airport conditions, air traffic conditions, weather, ticket prices, passengers comments, crew comments, etc., are all available from a diverse set of sources, including flight performance monitoring systems, operational systems of airlines and airports, and social media platforms. Development of data analytics in aviation and related applications is also growing rapidly. This paper concisely examines data science and analytics in aviation studies in several critical areas, namely big data analysis, air transport network management, forecasting, and machine learning. The papers featured in this special issue are also introduced and reviewed, and future directions for data science and analytics in aviation are discussed.",2020,,https://doi.org/10.1016/j.tre.2020.101837
476,Zhiming Ding and Shan Jiang and Xinrun Xu and Yanbo Han,An Internet of Things Based Scalable Framework for Disaster Data Management,"Disaster Data Management, IoT, Disaster Detection, Big data, Artificial Intelligence","In recent years, undesirable disasters attacked the cities frequently, leaving heavy casualties and serious economic losses. Meanwhile, disaster detection based on the Internet of Things(IoT) has become a hot spot benefited by the established development of smart city construction. And the IoT is visibly sensitive to the management and monitor of disasters, but massive amounts of monitoring data have brought huge challenges to data storage and data analysis. This article develops a new and much more general framework for disaster emergency management under the IoT environment. The framework is a bottom-up integration of highly scalable Raw Data Storages(RD-Stores) technology, hybrid indexing and queries technology, and machine learning technology for emergency disasters. Experimental results show that hybrid index and query technology have better performance under the condition of supporting multi-modal retrieval, and providing a better solution to offer real-time retrieval for the massive sensor sampling data in the IoT. In addition, further works to evaluate the top-level sub-application system in this framework were performed based on the GPS trajectory data of 35,000 Beijing taxis and the volumetric ground truth data of 7,500 images. The results show that the framework has desirable scalability and higher utility.",2021,,https://doi.org/10.1016/j.jnlssr.2021.10.005
477,Mehrbakhsh Nilashi and Hossein Ahmadi and Goli Arji and Khalaf Okab Alsalem and Sarminah Samad and Fahad Ghabban and Ahmed Omar Alzahrani and Ali Ahani and Ala Abdulsalam Alarood,Big social data and customer decision making in vegetarian restaurants: A combined machine learning method,"Online reviews, Food quality, Vegetarian friendly restaurants, Text mining, Segmentation","Customers increasingly use various social media to share their opinion about restaurants service quality. Big data collected from social media provides a data platform to improve the service quality of restaurants through customers' online reviews, where online reviews are a trustworthy and reliable source that helps consumers to evaluate food quality. Developing methods for effective evaluation of customer-generated reviews of restaurant services is important. This study develops a new method through effective learning techniques for customer segmentation and their preferences prediction in vegetarian friendly restaurants. The method is developed through text mining (Latent Dirichlet Allocation), cluster analysis (Self Organizing Map) and predictive learning technique (Classification and Regression Trees) to reveal the customer’ satisfaction levels from the service quality in vegetarian friendly restaurants. Based on the obtained results of our experiments on the data vegetarian friendly restaurants in Bangkok, the models constructed by Classification and Regression Trees were able to give an accurate prediction of customers' preferences on the basis of restaurants' quality factors. The results showed that customers’ online reviews analysis can be an effective way for customers segmentation to predict their preferences and help the restaurant managers to set priority instructions for service quality improvements.",2021,,https://doi.org/10.1016/j.jretconser.2021.102630
478,Laura Sebastian-Coleman,Chapter 7 - The People Challenge: Building Data Literacy,"Data literacy, data visualization, analytics, metadata management, critical thinking, scientific thinking, data management","This chapter addresses the skills, knowledge, and experience people require to create, use, and interpret data. Data literacy is the ability to read, understand, interpret, and learn from data in different contexts and to communicate about data with other people. The people challenge is both a skills challenge and a knowledge challenge. No single individual can know everything about an organization’s data. But, together, people can solve more problems in better ways if they understand data as a construct, recognize the risks associated with data production and use, cultivate a level of skepticism about data, and develop skill in visualizing and interpreting data. They will solve even more problems if the organization supports these efforts through disciplined metadata management and data quality management.",2022,,https://doi.org/10.1016/B978-0-12-821737-5.00007-9
479,Yihong Yuan and Yu Liu and Guixing Wei,Exploring inter-country connection in mass media: A case study of China,"Time series, Inter-country relations, Spatio-temporal data mining, Mass media events, GDELT","The development of theories and techniques for big data analytics offers tremendous possibility for investigating large-scale events and patterns that emerge over space and time. In this research, we utilize a unique open dataset “The Global Data on Events, Location and Tone” (GDELT) to model the image of China in mass media, specifically, how China has related to the rest of the world and how this connection has evolved upon time. The results of this research contribute to both the methodological and the empirical perspectives: We examined the effectiveness of the dynamic time warping (DTW) distances in measuring the differences between long-term mass media data. We identified four types of connection strength patterns between China and its top 15 related countries. With that, the distance decay effect in mass media is also examined and compared with social media and public transportation data. While using multiple datasets and focusing on mass media, this study generates valuable input regarding the interpretation of the diplomatic and regional correlation for the nation of China. It also provides methodological references for investigating international relations in other countries and regions in the big data era.",2017,,https://doi.org/10.1016/j.compenvurbsys.2016.10.012
480,Kate E. Larkin and Andrée-Anne Marsan and Nathalie Tonné and Nathalie {Van Isacker} and Tim Collart and Conor Delaney and Mickaël Vasquez and Eleonora Manca and Helen Lillis and Jan-Bart Calewaert,Chapter Five - Connecting marine data to society,"Big data, Climate change, Data visualization, Digital ocean, Ecosystems, FAIR, Hackathon, Knowledge broker, Marine data, Marine map, Ocean, Ocean literacy, Open data, Science communication, Seabed habitats","This chapter looks at connecting marine data to society, with a focus on key developments in Europe, set in a global context. It presents the European Marine Observation and Data Network-EMODnet as an exemplar in marine domain. EMODnet has significantly advanced European capability for Findable, Accessible, Interoperable, and Reusable marine knowledge, offering access to standardized and harmonized in-situ marine data and added value data products across seven marine environmental themes. Open and free data, products and associated metadata, are available for discovery and access through a wide range of web/data services. These ensure that the wealth of existing ocean observations and marine data collected in Europe and beyond can be easily discovered and used by a growing, and diversifying, user community. Interoperability with key services is crucial toward a pan-European and global approach. Key partnerships include the Copernicus Marine Environment Monitoring Service and international initiatives, e.g., the International Oceanographic Data and Information Exchange. Looking at societal tools and applications, the chapter provides a case study of the European Atlas of the Seas, a web-mapping tool that communicates marine and other open-source data and information in an attractive and interactive way. The EU Atlas is a key tool for the European ocean literacy initiative EU4Ocean, contributing to engage citizens and drive the societal change that is required for Europe to meet the ambitious targets to be climate neutral by 2050. The paper introduces examples of emerging tools for data visualization and presents hackathons, a powerful method to cocreate and innovate applications for society. Finally, the chapter looks toward the digital era and addresses the emerging challenges and opportunities of marine data, e.g., big data and plans for a digital twin of the Ocean, as tools to enable a step-change in societal connection, understanding, and action regarding the ocean.",2022,,https://doi.org/10.1016/B978-0-12-823427-3.00003-7
481,Saeed Piri,Missing care: A framework to address the issue of frequent missing values;The case of a clinical decision support system for Parkinson's disease,"Electronic health records, Data missing values, Clinical decision support systems, Predictive healthcare analytics, Imbalanced data learning, Parkinson's disease","In recent decades, the implementation of electronic health record (EHR) systems has been evolving worldwide, leading to the creation of immense data volume in healthcare. Moreover, there has been a call for research studies to enhance personalized medicine and develop clinical decision support systems (CDSS) by analyzing the available EHR data. In EHR data, usually, there are millions of patients records with hundreds of features collected over a long period of time. This enormity of EHR data poses significant challenges, one of which is dealing with many variables with very high degrees of missing values. In this study, the data quality issue of incompleteness in EHR data is discussed, and a framework called ‘Missing Care’ is introduced to address this issue. Using Missing Care, researchers will be able to select the most important variables at an acceptable missing values degree to develop predictive models with high predictive power. Moreover, Missing Care is applied to analyze a unique, large EHR data to develop a CDSS for detecting Parkinson's disease. Parkinson is a complex disease, and even a specialist's diagnosis is not without error. Besides, there is a lack of access to specialists in more remote areas, and as a result, about half of the patients with Parkinson's disease in the US remain undiagnosed. The developed CDSS can be integrated into EHR systems or utilized as an independent tool by healthcare practitioners who are not necessarily specialists; therefore, making up for the limited access to specialized care in remote areas.",2020,,https://doi.org/10.1016/j.dss.2020.113339
482,Jun Ma and Jack C.P. Cheng and Feifeng Jiang and Weiwei Chen and Mingzhu Wang and Chong Zhai,A bi-directional missing data imputation scheme based on LSTM and transfer learning for building energy data,"Bi-directional estimation, Building energy, Deep learning, Electric power, Missing data, Transfer learning","Improving the energy efficiency of the buildings is a worldwide hot topic nowadays. To assist comprehensive analysis and smart management, high-quality historical data records of the energy consumption is one of the key bases. However, the energy data records in the real world always contain different kinds of problems. The most common problem is missing data. It is also one of the most frequently reported data quality problems in big data/machine learning/deep learning related literature in energy management. However, limited studied have been conducted to comprehensively discuss different kinds of missing data situations, including random missing, continuous missing, and large proportionally missing. Also, the methods used in previous literature often rely on linear statistical methods or traditional machine learning methods. Limited study has explored the feasibility of advanced deep learning and transfer learning techniques in this problem. To this end, this study proposed a methodology, namely the hybrid Long Short Term Memory model with Bi-directional Imputation and Transfer Learning (LSTM-BIT). It integrates the powerful modeling ability of deep learning networks and flexible transferability of transfer learning. A case study on the electric consumption data of a campus lab building was utilized to test the method. Results show that LSTM-BIT outperforms other methods with 4.24% to 47.15% lower RMSE under different missing rates.",2020,,https://doi.org/10.1016/j.enbuild.2020.109941
483,Raymond G. Cavalcante and Tingting Qin and Maureen A. Sartor,Chapter 4-2 - Novel Bioinformatics Methods for Toxicoepigenetics,"Epigenomic analysis, Toxicoepigenomics, Bisulfite sequencing, Chromatin accessibility, ChIP-seq, Integrative analysis, Chromosomal interactions","The use of high-throughput, genome-wide assays in toxicoepigenetics is rapidly developing and expanding. With recent advances in experimental technologies, a great amount of multiomics epigenomic data has been generated requiring the development of correspondingly advanced bioinformatics approaches to analyze and interpret such big data. This chapter discusses analysis methods for current epigenomic assays available for use in toxicoepigenetic and novel bioinformatics approaches to interpret, visualize, and integrate a variety of epigenomic data and data resources. The epigenomic features covered include DNA methylation, DNA hydroxymethylation, histone modification, chromatin accessibility, and chromatin interaction. For each type of assay used to interrogate those features, bioinformatics tools for data quality control, epigenetic mark detection, comparative analysis, data visualization, functional analysis, and integrative analysis are suggested. Looking forward, it is anticipated that researchers in toxicoepigenomics will adopt newer techniques such as single-cell assays and the bioinformatics methods will continue to evolve.",2019,,https://doi.org/10.1016/B978-0-12-812433-8.00012-5
484,S. Chakraborty and S. Adhikari and R. Ganguli,The role of surrogate models in the development of digital twins of dynamic systems,"Digital twin, Vibration, Response, Frequency, Surrogate","Digital twin technology has significant promise, relevance and potential of widespread applicability in various industrial sectors such as aerospace, infrastructure and automotive. However, the adoption of this technology has been slower due to the lack of clarity for specific applications. A discrete damped dynamic system is used in this paper to explore the concept of a digital twin. As digital twins are also expected to exploit data and computational methods, there is a compelling case for the use of surrogate models in this context. Motivated by this synergy, we have explored the possibility of using surrogate models within the digital twin technology. In particular, the use of Gaussian process (GP) emulator within the digital twin technology is explored. GP has the inherent capability of addressing noisy and sparse data and hence, makes a compelling case to be used within the digital twin framework. Cases involving stiffness variation and mass variation are considered, individually and jointly, along with different levels of noise and sparsity in data. Our numerical simulation results clearly demonstrate that surrogate models, such as GP emulators, have the potential to be an effective tool for the development of digital twins. Aspects related to data quality and sampling rate are analysed. Key concepts introduced in this paper are summarised and ideas for urgent future research needs are proposed.",2021,,https://doi.org/10.1016/j.apm.2020.09.037
485,Jane Zhao and Raquel Forsythe and Alexander Langerman and Genevieve B. Melton and David F. Schneider and Gretchen Purcell Jackson,The Value of the Surgeon Informatician,"Clinical informatics, Surgery, Health information technology, Interoperability, Telemedicine, Clinical decision support","Clinical informatics is an interdisciplinary specialty that leverages big data, health information technologies, and the science of biomedical informatics within clinical environments to improve quality and outcomes in the increasingly complex and often siloed health care systems. Core competencies of clinical informatics primarily focus on clinical decision making and care process improvement, health information systems, and leadership and change management. Although the broad relevance of clinical informatics is apparent, this review focuses on its application and pertinence to the discipline of surgery, which is less well defined. In doing so, we hope to highlight the importance of the surgeon informatician. Topics covered include electronic health records, clinical decision support systems, computerized order entry, data analytics, clinical documentation, information architectures, implementation science, quality improvement, simulation, education, and telemedicine. The formal pathway for surgeons to become clinical informaticians is also discussed.",2020,,https://doi.org/10.1016/j.jss.2020.04.003
486,Vijay Khatri,Managerial work in the realm of the digital universe: The role of the data triad,"Analytics, Big data, Managerial decision making, Managerial work, Digital universe","With the explosion of the digital universe, it is becoming increasingly important to understand how organizational decision making (i.e., the business-oriented perspective) is intertwined with an understanding of enterprise data assets (i.e., the data-oriented perspective). This article first compares the business- and data-oriented perspectives to describe how the two views mesh with each other. It then presents three elements in the data-oriented perspective that are collectively referred to as the data triad: (1) use, (2) design and storage, and (3) processes and people. In describing the data triad, this article highlights practices, architectural techniques, and example tools that are used to manage, access, analyze, and deliver data. By presenting different elements of the data-oriented perspective, this article broadly and concretely describes the data triad and how it can play a role in the redefined scope of work for data-driven business managers.",2016,,https://doi.org/10.1016/j.bushor.2016.06.001
487,Jonathan Cinnamon,Geographic Information Systems; Ethics,"Access, Big data, Cartography, Digital divide, Geodemographics, Mapping, Morals, Privacy, Profiling, Representation, Responsibilities, Spatial data, Surveillance, Values","The development and use of geographic information system (GIS) within particular sociopolitical contexts means that ethical issues can arise both from how GIS is used and also due to the affordances and constraints of the software, hardware, and data. Ethics is a longstanding concern in the field of geographic information science (GIScience), arising amid the critical cartography and GIS and Society debates beginning in the late 1980s, in which human geographers and GIS scholars began to call for more attention to the implications of maps, GIS, and spatial data. Ethics in GIS draws on normative frameworks including deontological (duties and obligations) and teleological (consequences and outcomes) ethical perspectives, as well as nonnormative critical ethics to understand concerns that arise with GIS and map-based representation, uneven access to spatial data and technologies, and the use of GIS in geodemographic profiling, location analytics, and war. Attention to ethics in GIS has led to the development of ethics education, guidance, and codes of conduct for GIS users. Recent advancements in the availability of geolocated personal data, wider societal use of geospatial technologies, and data analytics have pulled GIS ethics to the forefront of the larger domain of information ethics, as location becomes increasingly central to wider ethical debates in the era of big data, automation, and artificial intelligence.",2020,,https://doi.org/10.1016/B978-0-08-102295-5.10554-2
488,Daniel Linstedt and Michael Olschimke,Chapter 1 - Introduction to Data Warehousing,"data, data warehouse, big data, decision support systems, scalability, business intelligence","This chapter introduces basic terminology of data warehousing, its applications, and the business context. It provides a brief description of its history and where it is heading. Basic data warehouse architectures that have been established in the industry are presented. Issues faced by data warehouse practitioners are explained, including topics such as big data, changing business requirements, performance issues, complexity, auditability, restart checkpoints, and fluctuation of team members.",2016,,https://doi.org/10.1016/B978-0-12-802510-9.00001-5
489,Chiehyeon Lim and Ki-Hun Kim and Min-Jun Kim and Jun-Yeon Heo and Kwang-Jae Kim and Paul P. Maglio,From data to value: A nine-factor framework for data-based value creation in information-intensive services,"Big data, Data-based value creation, Information-intensive service, Factor, Data–Value Chain","Service is a key context for the application of IT, as IT digitizes information interactions in service and facilitates value creation, thereby contributing to service innovation. The recent proliferation of big data provides numerous opportunities for information-intensive services (IISs), in which information interactions exert the greatest effect on value creation. In the modern data-rich economy, understanding mechanisms and related factors of data-based value creation in IISs is essential for using IT to improve such services. This study identified nine key factors that characterize this data-based value creation: (1) data source, (2) data collection, (3) data, (4) data analysis, (5) information on the data source, (6) information delivery, (7) customer (information user), (8) value in information use, and (9) provider network. These factors were identified and defined through six action research projects with industry and government that used specific datasets to design new IISs and by analyzing data usage in 149 IIS cases. This paper demonstrates the usefulness of these factors for describing, analyzing, and designing the entire value creation chain, from data collection to value creation, in IISs. The main contribution of this study is to provide a simple yet comprehensive and empirically tested basis for the use and management of data to facilitate service value creation.",2018,,https://doi.org/10.1016/j.ijinfomgt.2017.12.007
490,Shiwen He and Jian Song and Yeyu Ou and Yuanhong Yuan and Xiaojie Zhang and Xiaohua Xu,GARD: Gender difference analysis and recognition based on machine learning,"Gender difference analysis, Gender recognition, Medical examination data, Machine learning","In recent years, intelligent diagnosis and intelligent medical treatment based on big data of medical examinations have become the main trend of medical development in the future. In this paper, we propose a method for analyzing the difference between males and females in medical examination items (medical attributes) and find that males and females of different ages have differences in medical attributes. Then, the cluster analysis method is used to further analyze the differences between male and female in medical examination items, such that some common important attributes (CIAs) that can be used for gender recognition are found within a specific age range. Following, we propose two gender recognition models (GRMs) by using the found CIAs to identify the gender. A large number of experimental results are provided to validate the effectiveness of the proposed GRMs. Experimental results show that the medical attributes with a large value of difference really contribute to gender recognition. Within a certain age range, such as 17 to 51 years old, the proposed GRM can reach 92.8% accuracy using only six medical attributes.",2022,,https://doi.org/10.1016/j.array.2022.100140
491,Lee Squitieri and Kevin C. Chung,"Deriving Evidence from Secondary Data in Hand Surgery: Strengths, Limitations, and Future Directions","Registry, Hand surgery, Administrative, Claims, Electronic health records, Big data, Secondary data analysis",,2020,,https://doi.org/10.1016/j.hcl.2020.01.011
492,Xiangjie Liu and Hao Zhang and Xiaobing Kong and Kwang Y. Lee,Wind speed forecasting using deep neural network with feature selection,"Wind speed forecasting, Deep neural network, Mutual information, Stacked auto-encoder, Denoising, Long short-term memory network","With the rapid growth of wind power penetration into modern power grids, wind speed forecasting (WSF) becomes an increasing important task in the planning and operation of electric power and energy systems. However, WSF is quite challengeable due to its highly varying and complex features. In this paper, a novel hybrid deep neural network forecasting method is constituted. A feature selection method based on mutual information is developed in the WSF problem. With the real-time big data from the wind farm running log, the deep neural network model for WSF is established using a stacked denoising auto-encoder and long short-term memory network. The effectiveness of the deep neural network is evaluated by 10-minutes-ahead WSF. Comparing with the traditional multi-layer perceptron network, conventional long short-term memory network and stacked auto-encoder, the resulting deep neural network significantly improves the forecasting accuracy.",2020,,https://doi.org/10.1016/j.neucom.2019.08.108
493,Yadi Zhou and Fei Wang and Jian Tang and Ruth Nussinov and Feixiong Cheng,Artificial intelligence in COVID-19 drug repurposing,,"Summary
Drug repurposing or repositioning is a technique whereby existing drugs are used to treat emerging and challenging diseases, including COVID-19. Drug repurposing has become a promising approach because of the opportunity for reduced development timelines and overall costs. In the big data era, artificial intelligence (AI) and network medicine offer cutting-edge application of information science to defining disease, medicine, therapeutics, and identifying targets with the least error. In this Review, we introduce guidelines on how to use AI for accelerating drug repurposing or repositioning, for which AI approaches are not just formidable but are also necessary. We discuss how to use AI models in precision medicine, and as an example, how AI models can accelerate COVID-19 drug repurposing. Rapidly developing, powerful, and innovative AI and network medicine technologies can expedite therapeutic development. This Review provides a strong rationale for using AI-based assistive tools for drug repurposing medications for human disease, including during the COVID-19 pandemic.",2020,,https://doi.org/10.1016/S2589-7500(20)30192-8
494,Danette McGilvray,Chapter 4 - The Ten Steps Process,"business needs, information environment, information life cycle, data quality dimensions, business impact techniques, root causes, improvement, correction, prevention, controls, monitor, communicate, ethics, change management","This chapter contains the step-by-step guide for creating, assessing, improving, sustaining, and managing information and data quality. Concrete instructions, sample output and templates, and practical advice for executing every step of the Ten Steps Process are provided. A step summary table gives an at-a-glance overview of objectives, purpose, inputs and outputs, techniques and tools, communication, and checkpoints for each step. The Ten Steps Process was designed to be flexible. Suggestions are given to help the reader select and adjust the Ten Steps to various situations, business needs, and data quality issues. The layout allows for quick reference with an easy-to-use format highlighting key concepts and definitions, important checkpoints, communication activities, best practices, and warnings. The experience of actual clients and users of the Ten Steps are highlighted in callout boxes called Ten Steps in Action.",2021,,https://doi.org/10.1016/B978-0-12-818015-0.00006-2
495,Long Liu,Objects detection toward complicated high remote basketball sports by leveraging deep CNN architecture,"Object detection, Sport action recognition, Image recognition, Basketball recognition","The analysis of high-difficulty action recognition technology in basketball is mainly to identify and analyze the physical behavior of basketball players in the video to complete the technical action. The purpose of video recognition is to provide an important guarantee for improving the level of basketball training. The current target recognition technology has achieved some results. It shows that the application of target detection technology in basketball sports scene is of great significance and can improve the effect of sports training. However, traditional sports target recognition is limited by technology and injury, and the analysis of difficult sports skills is limited by the scene, dynamic background and technology, and cannot achieve the desired effect. This is not conducive to the improvement of athletes’ skills. Therefore, this article aims to develop a big data motion target detection system based on deep convolutional neural network for sports difficult motion image recognition. More specifically, we use the high discriminative power of the convolutional neural network to extract images to perform computational preprocessing for the recognition of each human motion image in the video stream. Then, the skeleton recognition algorithm based on LSTM is used to detect the key points of the human body, which is of great significance for modeling different movements. Finally, we developed an object detection system to reconstruct each movement. By selecting five groups of highly difficult actions that are likely to cause sports injuries to conduct experimental research, the results prove the effectiveness of the target detection system we proposed.",2021,,https://doi.org/10.1016/j.future.2021.01.020
496,K.U. Jaseena and Binsu C. Kovoor,Deterministic weather forecasting models based on intelligent predictors: A survey,"Weather forecasting, Artificial neural networks, Deep learning, Autoencoders, Recurrent neural networks","Weather forecasting is the practice of predicting the state of the atmosphere for a given location based on different weather parameters. Weather forecasts are made by gathering data about the current state of the atmosphere. Accurate weather forecasting has proven to be a challenging task for meteorologists and researchers. Weather information is essential in every facet of life like agriculture, tourism, airport system, mining industry, and power generation. Weather forecasting has now entered the era of Big Data due to the advancement of climate observing systems like satellite meteorological observation and also because of the fast boom in the volume of weather data. So, the traditional computational intelligence models are not adequate to predict the weather accurately. Hence, deep learning-based techniques are employed to process massive datasets that can learn and make predictions more effectively based on past data. The effective implementation of deep learning in various domains has motivated its use in weather forecasting and is a significant development for the weather industry. This paper provides a thorough review of different weather forecasting approaches, along with some publicly available datasets. This paper delivers a precise classification of weather forecasting models and discusses potential future research directions in this area.",2020,,https://doi.org/10.1016/j.jksuci.2020.09.009
497,Aizatul Shafiqah {Mohd Faizal} and T. Malathi Thevarajah and Sook Mei Khor and Siow-Wee Chang,A review of risk prediction models in cardiovascular disease: conventional approach vs. artificial intelligent approach,"Cardiovascular diseases, Risk prediction, Artificial intelligence, Machine learning, Deep learning","Cardiovascular disease (CVD) is the leading cause of death worldwide and is a global health issue. Traditionally, statistical models are used commonly in the risk prediction and assessment of CVD. However, the adoption of artificial intelligent (AI) approach is rapidly taking hold in the current era of technology to evaluate patient risks and predict the outcome of CVD. In this review, we outline various conventional risk scores and prediction models and do a comparison with the AI approach. The strengths and limitations of both conventional and AI approaches are discussed. Besides that, biomarker discovery related to CVD are also elucidated as the biomarkers can be used in the risk stratification as well as early detection of the disease. Moreover, problems and challenges involved in current CVD studies are explored. Lastly, future prospects of CVD risk prediction and assessment in the multi-modality of big data integrative approaches are proposed.",2021,,https://doi.org/10.1016/j.cmpb.2021.106190
498,Alessandro Mantelero,Personal data for decisional purposes in the age of analytics: From an individual to a collective dimension of data protection,"Big data, Right to privacy, Data protection, Group privacy, Collective interests, Data protection authorities, Risk assessment","In the big data era, new technologies and powerful analytics make it possible to collect and analyse large amounts of data in order to identify patterns in the behaviour of groups, communities and even entire countries. Existing case law and regulations are inadequate to address the potential risks and issues related to this change of paradigm in social investigation. This is due to the fact that both the right to privacy and the more recent right to data protection are protected as individual rights. The social dimension of these rights has been taken into account by courts and policymakers in various countries. Nevertheless, the rights holder has always been the data subject and the rights related to informational privacy have mainly been exercised by individuals. This atomistic approach shows its limits in the existing context of mass predictive analysis, where the larger scale of data processing and the deeper analysis of information make it necessary to consider another layer, which is different from individual rights. This new layer is represented by the collective dimension of data protection, which protects groups of persons from the potential harms of discriminatory and invasive forms of data processing. On the basis of the distinction between individual, group and collective dimensions of privacy and data protection, the author outlines the main elements that characterise the collective dimension of these rights and the representation of the underlying interests.",2016,,https://doi.org/10.1016/j.clsr.2016.01.014
499,Ann Hill Duin and Jason Tham,The Current State of Analytics: Implications for Learning Management System (LMS) Use in Writing Pedagogy,"Learning management systems, Academic and learning analytics, Writing pedagogy, Student privacy, Access","Amid the burgeoning interest in and use of academic and learning analytics through learning management systems (LMS), the implications of big data and their uses should be central to computers and writing scholarship. In this case study we describe the UMN Canvas LMS experience in such as way so that writing instructors might become more familiar with levels of access to academic and learning analytics, more acquainted with the analytical capabilities in LMSs, and more mindful of implications of learning analytics stemming from LMS use in writing pedagogy. We provide a historical account on the development and infusion of LMS in writing pedagogy and demonstrate how these systems are affecting the way computers and composition scholars consider writing instruction and assessment. We then respond critically to the collection of data drawn from the authors’ use of these systems in on-campus and online teaching. We conclude with implications for writing pedagogy along with a matrix for addressing ethical concerns.",2020,,https://doi.org/10.1016/j.compcom.2020.102544
500,Maria E. Mondejar and Ram Avtar and Heyker Lellani Baños Diaz and Rama Kant Dubey and Jesús Esteban and Abigail Gómez-Morales and Brett Hallam and Nsilulu Tresor Mbungu and Chukwuebuka Christopher Okolo and Kumar Arun Prasad and Qianhong She and Sergi Garcia-Segura,Digitalization to achieve sustainable development goals: Steps towards a Smart Green Planet,"Digitalization, Food-water-energy nexus, Internet of things, Geographic information system (GIS), Sustainable development","Digitalization provides access to an integrated network of unexploited big data with potential benefits for society and the environment. The development of smart systems connected to the internet of things can generate unique opportunities to strategically address challenges associated with the United Nations Sustainable Development Goals (SDGs) to ensure an equitable, environmentally sustainable, and healthy society. This perspective describes the opportunities that digitalization can provide towards building the sustainable society of the future. Smart technologies are envisioned as game-changing tools, whereby their integration will benefit the three essential elements of the food-water-energy nexus: (i) sustainable food production; (ii) access to clean and safe potable water; and (iii) green energy generation and usage. It then discusses the benefits of digitalization to catalyze the transition towards sustainable manufacturing practices and enhance citizens' health wellbeing by providing digital access to care, particularly for the underserved communities. Finally, the perspective englobes digitalization benefits by providing a holistic view on how it can contribute to address the serious challenges of endangered planet biodiversity and climate change.",2021,,https://doi.org/10.1016/j.scitotenv.2021.148539
501,Hanane Lamaazi and Rabeb Mizouni and Hadi Otrok and Shakti Singh and Ernesto Damiani,Smart-3DM: Data-driven decision making using smart edge computing in hetero-crowdsensing environment,"Smart edge computing, Crowdsensing, Distributed architecture, Data assessment, Data quality","Mobile Edge Computing (MEC) has recently emerged as a promising paradigm for Mobile Crowdsensing (MCS) environments. In a given Area of Interest (AoI), the sensing process is performed based on task requirements, which usually ask for a specific quality of the sensing outcome. In this work, a two-stage Data-Driven Decision-making Mechanism using smart edge computing (Smart-3DM) is proposed. It advocates the use of smart edge to better fulfill the data-related task requirements. Depending on the type of data to be collected, the minimum quality of the data required, and the heuristics to apply for each type of crowdsensing service, the smart edge orchestrates the selection of workers in MEC. Our approach relies on (a) smart-edge deployment: where a cluster-based distributed architecture using smart edge nodes is considered. Here, two entities are defined: the main edge node (MEN) and the local edge nodes (LENs); and (b) data management offloading where a two-layer re-selection strategy that considers data type and context-awareness is adopted, to reduce data computation complexity and to increase data quality while meeting the task target. The proposed Smart-3DM is evaluated using a real-life dataset and is compared to one-stage local and global approaches. The overall results show that by using two-stage re-selection strategies, better performance with lower processing power (CPU), less Storage(RAM), and improved execution time is achieved, when compared to the benchmarks.",2022,,https://doi.org/10.1016/j.future.2022.01.014
502,Ziheng Sun and Laura Sandoval and Robert Crystal-Ornelas and S. Mostafa Mousavi and Jinbo Wang and Cindy Lin and Nicoleta Cristea and Daniel Tong and Wendy Hawley Carande and Xiaogang Ma and Yuhan Rao and James A. Bednar and Amanda Tan and Jianwu Wang and Sanjay Purushotham and Thomas E. Gill and Julien Chastang and Daniel Howard and Benjamin Holt and Chandana Gangodagamage and Peisheng Zhao and Pablo Rivas and Zachary Chester and Javier Orduz and Aji John,A review of Earth Artificial Intelligence,"Geosphere, Hydrology, Atmosphere, Artificial intelligence/machine learning, Big data, Cyberinfrastructure","In recent years, Earth system sciences are urgently calling for innovation on improving accuracy, enhancing model intelligence level, scaling up operation, and reducing costs in many subdomains amid the exponentially accumulated datasets and the promising artificial intelligence (AI) revolution in computer science. This paper presents work led by the NASA Earth Science Data Systems Working Groups and ESIP machine learning cluster to give a comprehensive overview of AI in Earth sciences. It holistically introduces the current status, technology, use cases, challenges, and opportunities, and provides all the levels of AI practitioners in geosciences with an overall big picture and to “blow away the fog to get a clearer vision” about the future development of Earth AI. The paper covers all the majorspheres in the Earth system and investigates representative AI research in each domain. Widely used AI algorithms and computing cyberinfrastructure are briefly introduced. The mandatory steps in a typical workflow of specializing AI to solve Earth scientific problems are decomposed and analyzed. Eventually, it concludes with the grand challenges and reveals the opportunities to give some guidance and pre-warnings on allocating resources wisely to achieve the ambitious Earth AI goals in the future.",2022,,https://doi.org/10.1016/j.cageo.2022.105034
503,Qinqin Zhu,Supervised Block-Aware Factorization Machine for Multi-Block Quality-Relevant Monitoring,"Quality-relevant monitoring, block-aware factorization machine, supervised learning, multi-block processes","Multi-block multivariate statistical methods have been developed to extract useful information from process and quality data in the era of big data, where process variables are partitioned into several meaningful blocks. However, most of these methods did not consider cross-correlations among divided blocks, which leads to inferior monitoring performance. In this article, a block-aware factorization machine (BAFM) algorithm is proposed to exploit information from process and quality data. In BAFM, quality data are first classified into normal and abnormal labels with principal component analysis based quality monitoring framework. Afterwards, a block number is attached to each process variable, and the interactions among different variables (both within and cross blocks) are learned through latent variables, which is supervised by the classified quality labels. Apart from the variable relation within the same block, BAFM also incorporates the block information; thus, both inner and cross correlations are constructed. The monitoring framework based on BAFM is developed, and its effectiveness and superiority are demonstrated through the Tennessee Eastman process.",2020,,https://doi.org/10.1016/j.ifacol.2020.12.370
504,Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis,Chapter 3 - Medical data sharing,"Data curation, Data quality management, Data sharing, Data sharing frameworks, Standardization","This chapter introduces the primary step toward the realization of a strategically federated platform, namely data sharing. The rationale behind medical data sharing is related to the interlinking of medical cohorts with respect to data protection regulations and dealing with the unmet needs in various clinical domains. Emphasis is given on data quality management and especially on the existence of a data curation mechanism toward an effective data quality assessment procedure. Data curation and standardization methods are presented along with the latest advances in data sharing assessment and case studies on real data. Existing data sharing frameworks and global initiatives are extensively discussed. Crucial barriers toward data sharing are finally stated along with solutions and guidelines against the misuse of shared data.",2020,,https://doi.org/10.1016/B978-0-12-816507-2.00003-7
505,K.C. Chan and Victor T.T. Wong and Anthony K.F. Yow and P.L. Yuen and Christopher Y.H. Chao,Development and performance evaluation of a chiller plant predictive operational control strategy by artificial intelligence,"Chiller plant optimization, Artificial intelligence, Artificial neural network, Particle swarm optimization, VSD chiller, Building energy saving","Traditionally, chiller plants are controlled and monitored by a predetermined control strategy to ensure appropriate operation based on the designed system configuration. With the use of new technology of variable speed drive (VSD) for compressors, smart control strategies could be leveraged to enhance the system efficiency in lieu of traditional control strategies. For example, using orderly and straightforward switching procedures without considering various factors in switching the units, including the high-efficiency partial load range benefitted from the VSD, the actual performance of the units as a whole and the variable chilled water flow rate, result in the chiller plant not operating at maximum performance and efficiency. To address these issues, a hybrid predictive operational chiller plant control strategy is proposed to optimize the performance of the chiller plant. Artificial intelligence is employed as the data mining algorithm, with big data analysis based on the actual acquired voluminous operation data by fully considering the characteristics of chiller plants without additional installation of large-sized and high-priced equipment. Artificial neural network (ANN) was employed in the control strategy to predict the future outdoor temperature, building cooling load demand and the corresponding power consumption of the chiller plants. At the same time, particle swarm optimization (PSO) was applied to search for the optimized setpoints, e.g., chilled water supply temperature, operating sequence, chilled water flow rate, for the chiller plants. The developed control strategy has been launched in a chiller plant with a cooling capacity of 7,700 kW installed in a hospital in Hong Kong. The system coefficient of performance (COP) and overall energy consumption of the chiller plants were enhanced by about 8.6% and reduced by about 7.9%, respectively, compared with the traditional control strategy. This real-time, continuous, automatic optimization control strategy can determine the most efficient combination of operating parameters of a chiller plant with different control settings. This ensures that the chiller plant operates in its most efficient mode year-round under various operational conditions.",2022,,https://doi.org/10.1016/j.enbuild.2022.112017
506,Fatima-Zahra Benjelloun and Ahmed Oussous and Amine Bennani and Samir Belfkih and Ayoub {Ait Lahcen},Improving outliers detection in data streams using LiCS and voting,"Data streams, Outlier detection, High-dimensional data, Big data mining, Intrusion detection","Detecting outliers in real-time is increasingly important for many real-world applications such as detecting abnormal heart activity, intrusions to systems, spams or abnormal credit card transactions. However, detecting outliers in data streams rises many challenges such as high-dimensionality, dynamic data distribution and unpredictable relationships. Our simulations demonstrate that some advanced solutions still show drawbacks. In this paper, first, we improve the capacity to detect outliers of both micro-clusters based algorithms (MCOD) and distance-based algorithms (Abstract-C and Exact-Storm) known for their performance. This is by adding a layer called LiCS that classifies online the K-nearest-neighbors (Knn) of each node based on their evolutionary status. This layer aggregates the results and uses a count threshold to better classify nodes. Experiments on SpamBase datasets confirmed that our technique enhances the accuracy and the precision of such algorithm and helps to reduce the unclassified nodes.Second, we propose a hybrid solution based on iterative majority voting and our LiCS. Experiments on real data proves that it outperforms discussed algorithms in terms of accuracy, precision and sensitivity in detecting outliers. It also minimizes the issue of unclassified instances and consolidate the different outputs of algorithms.",2021,,https://doi.org/10.1016/j.jksuci.2019.08.003
507,Christiane Bahlo and Peter Dahlhaus,Livestock data – Is it there and is it FAIR? A systematic review of livestock farming datasets in Australia,"Livestock data quality, Systematic data review, FAIR data, FAIR assessment, Precision livestock farming, Extensive livestock farming","The global adoption of the FAIR principles for scientific data: findable, accessible, interoperable and reusable, has been relatively slow in agriculture, compared to other disciplines. A recent review of the literature showed that the use of precision farming technologies and the development and adoption of open data standards was particularly low in extensive livestock farming. However, a plethora of public datasets exist that have the potential to be used to inform precision farming decision tools. Using extensive livestock farming in Australia as example, we investigate the quantity and quality of datasets available via a systematic dataset review. This systematic review of datasets begins with a search of open data catalogues and querying these to find datasets. Software scripts are developed and used to query the Application Programming Interfaces (APIs) of many of the large data catalogues in Australia, while catalogues without public APIs are queried manually via available web portals. Following the systematic search, a combined list of all datasets is collated and tested for FAIRness and other quality metrics. The contribution of this work is the resulting overview of the state of open datasets within the livestock farming domain on the one hand, but also the development of a systematic dataset search strategy, reusable methods and software scripts.",2021,,https://doi.org/10.1016/j.compag.2021.106365
508,Shichao Jin and Xiliang Sun and Fangfang Wu and Yanjun Su and Yumei Li and Shiling Song and Kexin Xu and Qin Ma and Frédéric Baret and Dong Jiang and Yanfeng Ding and Qinghua Guo,Lidar sheds new light on plant phenomics for plant breeding and management: Recent advances and future prospects,"Lidar, Traits, Phenomics, Breeding, Management, Multi-omics","Plant phenomics is a new avenue for linking plant genomics and environmental studies, thereby improving plant breeding and management. Remote sensing techniques have improved high-throughput plant phenotyping. However, the accuracy, efficiency, and applicability of three-dimensional (3D) phenotyping are still challenging, especially in field environments. Light detection and ranging (lidar) provides a powerful new tool for 3D phenotyping with the rapid development of facilities and algorithms. Numerous efforts have been devoted to studying static and dynamic changes of structural and functional phenotypes using lidar in agriculture. These progresses also improve 3D plant modeling across different spatial–temporal scales and disciplines, providing easier and less expensive association with genes and analysis of environmental practices and affords new insights into breeding and management. Beyond agriculture phenotyping, lidar shows great potential in forestry, horticultural, and grass phenotyping. Although lidar has resulted in remarkable improvements in plant phenotyping and modeling, the synthetization of lidar-based phenotyping for breeding and management has not been fully explored. We identify three main challenges in lidar-based phenotyping development: 1) developing low cost, high spatial–temporal, and hyperspectral lidar facilities, 2) moving into multi-dimensional phenotyping with an endeavor to generate new algorithms and models, and 3) embracing open source and big data.",2021,,https://doi.org/10.1016/j.isprsjprs.2020.11.006
509,Xueqin Pang and Christopher B. Forrest and Félice Lê-Scherban and Aaron J. Masino,Prediction of early childhood obesity with machine learning and electronic health record data,"Data quality control, Early childhood obesity, Electronic health record, Machine learning, Prediction","Objective
This study compares seven machine learning models developed to predict childhood obesity from age > 2 to ≤ 7 years using Electronic Healthcare Record (EHR) data up to age 2 years.
Materials and methods
EHR data from of 860,510 patients with 11,194,579 healthcare encounters were obtained from the Children’s Hospital of Philadelphia. After applying stringent quality control to remove implausible growth values and including only individuals with all recommended wellness visits by age 7 years, 27,203 (50.78 % male) patients remained for model development. Seven machine learning models were developed to predict obesity incidence as defined by the Centers for Disease Control and Prevention (age/sex adjusted BMI>95th percentile). Model performance was evaluated by multiple standard classifier metrics and the differences among seven models were compared using the Cochran's Q test and post-hoc pairwise testing.
Results
XGBoost yielded 0.81 (0.001) AUC, which outperformed all other models. It also achieved statistically significant better performance than all other models on standard classifier metrics (sensitivity fixed at 80 %): precision 30.90 % (0.22 %), F1-socre 44.60 % (0.26 %), accuracy 66.14 % (0.41 %), and specificity 63.27 % (0.41 %).
Discussion and conclusion
Early childhood obesity prediction models were developed from the largest cohort reported to date. Relative to prior research, our models generalize to include males and females in a single model and extend the time frame for obesity incidence prediction to 7 years of age. The presented machine learning model development workflow can be adapted to various EHR-based studies and may be valuable for developing other clinical prediction models.",2021,,https://doi.org/10.1016/j.ijmedinf.2021.104454
510,Kim S. Betts and Steve Kisely and Rosa Alati,Predicting postpartum psychiatric admission using a machine learning approach,"Administrative data linkage, Postpartum psychiatric admissions, Predictive models, Machine learning","Aims
The accurate identification of mothers at risk of postpartum psychiatric admission would allow for preventive intervention or more timely admission. We developed a prediction model to identify women at risk of postpartum psychiatric admission.
Methods
Data included administrative health data of all inpatient live births in the Australian state of Queensland between January 2009 and October 2014. Analyses were restricted to mothers with one or more indicator of mental health problems during pregnancy (n = 75,054 births). The predictors included all maternal data up to and including the delivery, and neonatal data recorded at delivery. We used multiple machine learning methods to predict hospital admission in the 12 months following delivery in which the primary diagnosis was recorded as an ICD-10 psychotic, bipolar or depressive disorders.
Results
The boosted trees algorithm produced the best performing model, predicting postpartum psychiatric admission in the validation data with good discrimination [AUC = 0.80; 95% CI = (0.76, 0.83)] and achieving good calibration. This model outperformed benchmark logistic regression model and an elastic net model. In addition to indicators of maternal metal health history, maternal and neonatal anthropometric measures and social/lifestyle factors were strong predictors.
Conclusion
Our results indicate the potential of a big data approach when aiming to identify mothers at risk of postpartum psychiatric admission. Mothers at risk could be followed-up and supported after neonatal discharge to either remove the need for admission or facilitate more timely admission.",2020,,https://doi.org/10.1016/j.jpsychires.2020.07.002
511,Felix Krieger and Paul Drews and Patrick Velte,Explaining the (non-) adoption of advanced data analytics in auditing: A process theory,"Audit digitization, Audit data analytics, Big data, Machine learning, Advanced data analytics in auditing, Audit innovation","Audit firms are increasingly engaging with advanced data analytics to improve the efficiency and effectiveness of external audits through the automation of audit work and obtaining a better understanding of the client’s business risk and thus their own audit risk. This paper examines the process by which audit firms adopt advanced data analytics, which has been left unaddressed by previous research. We derive a process theory from expert interviews which describes the activities within the process and the organizational units involved. It further describes how the adoption process is affected by technological, organizational and environmental contextual factors. Our work contributes to the extent body of research on technology adoption in auditing by using a previously unused theoretical perspective, and contextualizing known factors of technology adoption. The findings presented in this paper emphasize the importance of technological capabilities of audit firms for the adoption of advanced data analytics; technological capabilities within audit teams can be leveraged to support both the ideation of possible use cases for advanced data analytics, as well as the diffusion of solutions into practice.",2021,,https://doi.org/10.1016/j.accinf.2021.100511
512,Chetna Chauhan and Vinit Parida and Amandeep Dhir,Linking circular economy and digitalisation technologies: A systematic literature review of past achievements and future promises,"Circular economy, Sustainability, Product-service system (PSS), Circular business model, Artificial intelligence, Internet of things","The circular economy (CE) has the potential to capitalise upon emerging digital technologies, such as big data, artificial intelligence (AI), blockchain and the Internet of things (IoT), amongst others. These digital technologies combined with business model innovation are deemed to provide solutions to myriad problems in the world, including those related to circular economy transformation. Given the societal and practical importance of CE and digitalisation, last decade has witnessed a significant increase in academic publication on these topics. Therefore, this study aims to capture the essence of the scholarly work at the intersection of the CE and digital technologies. A detailed analysis of the literature based on emerging themes was conducted with a focus on illuminating the path of CE implementation. The results reveal that IoT and AI play a key role in the transition towards the CE. A multitude of studies focus on barriers to digitalisation-led CE transition and highlight policy-related issues, the lack of predictability, psychological issues and information vulnerability as some important barriers. In addition, product-service system (PSS) has been acknowledged as an important business model innovation for achieving the digitalisation enabled CE. Through a detailed assessment of the existing literature, a viable systems-based framework for digitalisation enabled CE has been developed which show the literature linkages amongst the emerging research streams and provide novel insights regarding the realisation of CE benefits.",2022,,https://doi.org/10.1016/j.techfore.2022.121508
513,Maryam Ghasemaghaei and Sepideh Ebrahimi and Khaled Hassanein,Data analytics competency for improving firm decision making performance,"Data analytics competency, Data quality, Bigness of data, Analytical skills, Domain knowledge, Tools sophistication, Decision making performance","This study develops and validates the concept of Data Analytics Competency as a five multidimensional formative index (i.e., data quality, bigness of data, analytical skills, domain knowledge, and tools sophistication) and empirically examines its impact on firm decision making performance (i.e., decision quality and decision efficiency). The findings based on an empirical analysis of survey data from 151 Information Technology managers and data analysts demonstrate a large, significant, positive relationship between data analytics competency and firm decision making performance. The results reveal that all dimensions of data analytics competency significantly improve decision quality. Furthermore, interestingly, all dimensions, except bigness of data, significantly increase decision efficiency. This is the first known empirical study to conceptualize, operationalize and validate the concept of data analytics competency and to study its impact on decision making performance. The validity of the data analytics competency construct as conceived and operationalized, suggests the potential for future research evaluating its relationships with possible antecedents and consequences. For practitioners, the results provide important guidelines for increasing firm decision making performance through the use of data analytics.",2018,,https://doi.org/10.1016/j.jsis.2017.10.001
514,Alireza Rahimi and Siaw-Teng Liaw and Jane Taggart and Pradeep Ray and Hairong Yu,Validating an ontology-based algorithm to identify patients with Type 2 Diabetes Mellitus in Electronic Health Records,"Ontology, SPARQL, Electronic Health Records, Diabetes Mellitus, Type 2, Validation studies","Background
Improving healthcare for people with chronic conditions requires clinical information systems that support integrated care and information exchange, emphasizing a semantic approach to support multiple and disparate Electronic Health Records (EHRs). Using a literature review, the Australian National Guidelines for Type 2 Diabetes Mellitus (T2DM), SNOMED-CT-AU and input from health professionals, we developed a Diabetes Mellitus Ontology (DMO) to diagnose and manage patients with diabetes. This paper describes the manual validation of the DMO-based approach using real world EHR data from a general practice (n=908 active patients) participating in the electronic Practice Based Research Network (ePBRN).
Method
The DMO-based algorithm to query, using Semantic Protocol and RDF Query Language (SPARQL), the structured fields in the ePBRN data repository were iteratively tested and refined. The accuracy of the final DMO-based algorithm was validated with a manual audit of the general practice EHR. Contingency tables were prepared and Sensitivity and Specificity (accuracy) of the algorithm to diagnose T2DM measured, using the T2DM cases found by manual EHR audit as the gold standard. Accuracy was determined with three attributes – reason for visit (RFV), medication (Rx) and pathology (path) – singly and in combination.
Results
The Sensitivity and Specificity of the algorithm were 100% and 99.88% with RFV; 96.55% and 98.97% with Rx; and 15.6% and 98.92% with Path. This suggests that Rx and Path data were not as complete or correct as the RFV for this general practice, which kept its RFV information complete and current for diabetes. However, the completeness is good enough for this purpose as confirmed by the very small relative deterioration of the accuracy (Sensitivity and Specificity of 97.67% and 99.18%) when calculated for the combination of RFV, Rx and Path. The manual EHR audit suggested that the accuracy of the algorithm was influenced by data quality such as incorrect data due to mistaken units of measurement and unavailable data due to non-documentation or documented in the wrong place or progress notes, problems with data extraction, encryption and data management errors.
Conclusion
This DMO-based algorithm is sufficiently accurate to support a semantic approach, using the RFV, Rx and Path to define patients with T2DM from EHR data. However, the accuracy can be compromised by incomplete or incorrect data. The extent of compromise requires further study, using ontology-based and other approaches.",2014,,https://doi.org/10.1016/j.ijmedinf.2014.06.002
515,Pierfrancesco Bellini and Monica Benigni and Riccardo Billero and Paolo Nesi and Nadia Rauch,Km4City ontology building vs data harvesting and cleaning for smart-city services,"Smart city, Knowledge base construction, Reconciliation, Validation and verification of knowledge base, Smart city ontology, Linked open graph, Km4city","Presently, a very large number of public and private data sets are available from local governments. In most cases, they are not semantically interoperable and a huge human effort would be needed to create integrated ontologies and knowledge base for smart city. Smart City ontology is not yet standardized, and a lot of research work is needed to identify models that can easily support the data reconciliation, the management of the complexity, to allow the data reasoning. In this paper, a system for data ingestion and reconciliation of smart cities related aspects as road graph, services available on the roads, traffic sensors etc., is proposed. The system allows managing a big data volume of data coming from a variety of sources considering both static and dynamic data. These data are mapped to a smart-city ontology, called KM4City (Knowledge Model for City), and stored into an RDF-Store where they are available for applications via SPARQL queries to provide new services to the users via specific applications of public administration and enterprises. The paper presents the process adopted to produce the ontology and the big data architecture for the knowledge base feeding on the basis of open and private data, and the mechanisms adopted for the data verification, reconciliation and validation. Some examples about the possible usage of the coherent big data knowledge base produced are also offered and are accessible from the RDF-store and related services. The article also presented the work performed about reconciliation algorithms and their comparative assessment and selection.",2014,,https://doi.org/10.1016/j.jvlc.2014.10.023
516,Joseph P. Boomgard-Zagrodnik and David J. Brown,Machine learning imputation of missing Mesonet temperature observations,"Machine learning, Big data, Surface weather observations, Degree day models, Missing data imputation","Uninterrupted and reliable weather data is a necessary foundation for agricultural decision making, required for models based on accumulated growing degree days (GDD), chill units, and evapotranspiration. When a weather station experiences a mechanical or communications failure, a replacement (imputed) value should be substituted for any missing data. This study introduces a machine learning, network-based approach to imputing missing 15-minute and daily maximum/minimum air temperature observations from 8.5 years of air temperature, relative humidity, wind, and solar radiation observations at 134 AgWeatherNet (AWN) stations in Washington State. A random forest imputation model trained on temperature and humidity observations from the full network predicted 15-minute, daily maximum, and daily minimum temperature values with mean absolute errors of 0.43 °C, 0.53 °C, and 0.70 °C, respectively. Sensitivity experiments determined that imputation skill was related a number of external factors including volume and type of training data, proximity of surrounding stations, and regional topography. In particular, nocturnal cold air flows in the upper Yakima Valley of south-central Washington caused temperature to be less correlated with surrounding stations in the overnight hours. In a separate experiment, the imputation model was used to predict base- 10 °C GDD on 2020 July 1 trained entirely on 15-minute station data from previous years. Even with the entire season of observations missing, the model predicted the GDD value within an average error 1.4% with 125 of 134 stations within 5% of observations. Since missing data can typically be resolved within a timeframe of a few days, the network-based imputation model is a sufficient substitute for short periods of missing observational weather data. Other potential applications of an imputation model are briefly discussed.",2022,,https://doi.org/10.1016/j.compag.2021.106580
517,Doina Olaru and Alejandro Tudela,Workshop Synthesis: Passive and sensor data - potential and application,"passive data, big data, sensor, GPS, Wi-Fi, smartcard, transport, travel surveys","The workshop on technology, tools and applications around passive and sensor travel data is summarized in this paper. Such data requires protocols for collection, storing/retrieving, sharing and processing; as well as the need for validation methods and multi-disciplinary work. Traditional surveys can complement passive and sensor data to aid a deeper understanding of travel behaviour. Passive data is particularly beneficial for planning long-distance travel and freight transport. While access to massive data collected by licensed operators should be guaranteed, maintaining data privacy and cultural sensitivity is a priority.",2018,,https://doi.org/10.1016/j.trpro.2018.10.010
518,Huaxiong Jiang and Stan Geertman and Patrick Witte,Ignorance is bliss? An empirical analysis of the determinants of PSS usefulness in practice,"Smart city, Implementation gap, Success and failure factors, Utility, Usability, Context","Planning support systems (PSS) enabled by smart city technologies (big data and information and communication technologies (ICTs)) are becoming more widespread in their availability, but have not yet been fully recognized as being useful in planning practice. Thus, a better understanding of the determinants of PSS usefulness in practice helps to improve the functional support of PSS for smart cities. This study is based on a recent international questionnaire (268 respondents) designed to evaluate the perceptions of scholars and practitioners in the smart city planning field. Based on the empirical evidence, this paper recommends that it is imperative for PSS developers and users to be more responsive to the fit for task-technology and user-technology (i.e., utility and usability, respectively) since they positively contribute to PSS usefulness in practice and to be more sensitive to the potential negative effects of contextual factors on PSS usefulness in smart cities. The empirical analyses further suggest that rather than merely striving for integrating smart city technologies into advancing PSS, the way that innovative PSS are integrated into the planning framework (i.e., how well PSS can satisfy the needs of planning tasks and users by considering context-specificities) is of great significance in promoting PSS's actual usefulness.",2020,,https://doi.org/10.1016/j.compenvurbsys.2020.101505
519,Jacob W. Malcom and John H. Malone,Chapter 14 - Analysis of Deep Sequencing Data: Insights and Challenges,"Deep sequencing, Seq, Big data, Statistical analysis","Modern biomedical research demands that investigators become familiar with deep sequencing data analysis, yet the vast nature of deep sequencing data creates a variety of roadblocks for biologists not familiar with the analysis of such large datasets. In this chapter, we provide an introduction to data analysis for biologists, review first principles, point out areas of concern, and suggest software tools that are becoming standards for analysis of deep sequencing data. Perhaps the biggest challenge in the analysis of deep sequencing data will be data management and storage and repeating complex, multitier computational analyses. The future of deep sequencing data analysis will be likely data-driven and rely on principles gleaned from “big data” analysis.",2014,,https://doi.org/10.1016/B978-0-444-62651-6.00015-5
520,Olivia J Kirtley and Kasper {van Mens} and Mark Hoogendoorn and Navneet Kapur and Derek {de Beurs},Translating promise into practice: a review of machine learning in suicide research and prevention,,"Summary
In ever more pressured health-care systems, technological solutions offering scalability of care and better resource targeting are appealing. Research on machine learning as a technique for identifying individuals at risk of suicidal ideation, suicide attempts, and death has grown rapidly. This research often places great emphasis on the promise of machine learning for preventing suicide, but overlooks the practical, clinical implementation issues that might preclude delivering on such a promise. In this Review, we synthesise the broad empirical and review literature on electronic health record-based machine learning in suicide research, and focus on matters of crucial importance for implementation of machine learning in clinical practice. The challenge of preventing statistically rare outcomes is well known; progress requires tackling data quality, transparency, and ethical issues. In the future, machine learning models might be explored as methods to enable targeting of interventions to specific individuals depending upon their level of need—ie, for precision medicine. Primarily, however, the promise of machine learning for suicide prevention is limited by the scarcity of high-quality scalable interventions available to individuals identified by machine learning as being at risk of suicide.",2022,,https://doi.org/10.1016/S2215-0366(21)00254-6
521,Lachezar Filchev and Lyubka Pashova and Vasil Kolev and Stuart Frye,"Chapter 6 - Surveys, Catalogues, Databases/Archives, and State-of-the-Art Methods for Geoscience Data Processing","Geosensor networks, Remote sensing, Earth observations, Geoinformation, Big Data, Databases/archives, Satellite image processing, Geographic information systems, Web geoportals, ICT, Decision analysis and technologies, Spectral imaging, Fourier analysis, Principal component analysis, Karhunen–Loève transform, Continuous and discrete wavelet, Multiwavelet transforms, Hyperspectral images, Classification methods, Image denoising","Recent years are marked with rapid growth in sources and availability of geospatial data and information providing new opportunities and challenges for scientific knowledge and technology solutions on time. This chapter represents a general overview of modern ICT tools and methods for acquiring Earth observation (EO) data storage, processing, analysis, and interpretation for many research and applied purposes. The main contribution to Big Data developments in EO is the space activities of the space and governmental agencies, such as CNES, CSA, CSIRO, DLR, ESA, INPE, ISRO, JAXA, NASA, RADI, and Roscosmos. Special attention is devoted to the international archives, catalogues, and databases of satellite EO, which already become an indispensable and crucial source of information in support of many sectors of social-economic activities and resolving environmental issues. Main technological and information products, geoportals, and services to deal with Big EO datasets are shortly discussed. Some advanced contemporary approaches for processing big EO data, compressing, clustering, and denoising, and hyperspectral images in the geoinformation science are outlined.",2020,,https://doi.org/10.1016/B978-0-12-819154-5.00016-3
522,Xiaoning Zhang and Xiqiang Wu and Younggi Park and Tianhang Zhang and Xinyan Huang and Fu Xiao and Asif Usmani,Perspectives of big experimental database and artificial intelligence in tunnel fire research,"Big data, Empirical model, Deep learning, Critical event, Smart firefighting","Tunnel fire is one of the most severe global fire hazards and causes a significant amount of economic losses and casualties every year. Over the last 50 years, numerous full-scale and reduced-scale tunnel fire tests, as well as numerical simulations have been conducted to quantify the critical fire events and key parameters to guide the fire safety design of the tunnel. In light of the recent advances in big data and artificial intelligence, this paper aims to establish a database that contains all existing experimental data of tunnel fire, based on an extensive literature review on tunnel fire tests. This tunnel-fire database summarizes seven key parameters of flame, ventilation, and smoke in that is open access at a GitHub site: https://github.com/PolyUFire/Tunnel_Fire_Database. The test conditions, experimental phenomena, and data of each literature work were organized and categorized in a standard format that could be conveniently accessed and continuously updated. Based on this database, machine learning is applied to predict the critical ventilation velocity of a tunnel fire as a demonstration. The review of the current database not only reveals more valuable information and hidden problems in the conventional collection of test data, but also provides new directions in future tunnel fire research. The established database and methodology help promote the application of artificial intelligence and smart firefighting in tunnel fire safety.",2021,,https://doi.org/10.1016/j.tust.2020.103691
523,Arzoo Miglani and Neeraj Kumar,Blockchain management and machine learning adaptation for IoT environment in 5G and beyond networks: A systematic review,"Blockchain, Machine learning, Federated learning, Internet of Things, Deep learning, 5G, 6G","Keeping in view of the constraints and challenges with respect to big data analytics along with security and privacy preservation for 5G and B5G applications, the integration of machine learning and blockchain, two of the most promising technologies of the modern era is inevitable. In comparison to the traditional centralized techniques for security and privacy preservation, blockchain uses decentralized consensus algorithms for verification and validation of different transactions which are supposed to become an integral part of blockchain network. Starting with the existing literature survey, we introduce the basic concepts of blockchain and machine learning in this article. Then, we presented a comprehensive taxonomy for integration of blockchain and machine learning in an IoT environment. We also explored federated learning, reinforcement learning, deep learning algorithms usage in blockchain based applications. Finally, we provide recommendations for future use cases of these emerging technologies in 5G and B5G technologies.",2021,,https://doi.org/10.1016/j.comcom.2021.07.009
524,Hyerim Ji and Seok Kim and Soyoung Yi and Hee Hwang and Jeong-Whun Kim and Sooyoung Yoo,Converting clinical document architecture documents to the common data model for incorporating health information exchange data in observational health studies: CDA to CDM,"Clinical document architecture, Common data model, Observational Medical Outcomes Partnership, Referral documents","Background
Utilization of standard health information exchange (HIE) data is growing due to the high adoption rate and interoperability of electronic health record (EHR) systems. However, integration of HIE data into an EHR system is not yet fully adopted in clinical research. In addition, data quality should be verified for the secondary use of these data. Thus, the aims of this study were to convert referral documents in a Health Level 7 (HL7) clinical document architecture (CDA) to the common data model (CDM) to facilitate HIE data availability for longitudinal data analysis, and to identify data quality levels for application in future clinical studies.
Methods
A total of 21,492 referral CDA documents accumulated for over 10 years in a tertiary general hospital in South Korea were analyzed. To convert CDA documents to the Observational Medical Outcomes Partnership (OMOP) CDM, processes such as CDA parsing, data cleaning, standard vocabulary mapping, CDA-to-CDM mapping, and CDM conversion were performed. The quality of CDM data was then evaluated using the Achilles Heel and visualized with the Achilles tool.
Results
Mapping five CDA elements (document header, problem, medication, laboratory, and procedure) into an OMOP CDM table resulted in population of 9 CDM tables (person, visit_occurrence, condition_occurrence, drug_exposure, measurement, observation, procedure_occurrence, care_site, and provider). Three CDM tables (drug_era, condition_era, and observation_period) were derived from the converted table. From vocabulary mapping codes in CDA documents according to domain, 98.6% of conditions, 68.8% of drugs, 35.7% of measurements, 100% of observation, and 56.4% of procedures were mapped as standard concepts. The conversion rates of the CDA to the OMOP CDM were 96.3% for conditions, 97.2% for drug exposure, 98.1% for procedure occurrence, 55.1% for measurements, and 100% for observation.
Conclusions
We examined the possibility of CDM conversion by defining mapping rules for CDA-to-CDM conversion using the referral CDA documents collected from clinics in actual medical practice. Although mapping standard vocabulary for CDM conversion requires further improvement, the conversion could facilitate further research on the usage patterns of medical resources and referral patterns.",2020,,https://doi.org/10.1016/j.jbi.2020.103459
525,Julian Eduardo Plazas and Sandro Bimonte and Michel Schneider and Christophe {de Vaulx} and Pietro Battistoni and Monica Sebillo and Juan Carlos Corrales,"Sense, Transform & Send for the Internet of Things (STS4IoT): UML profile for data-centric IoT applications","Data-centric conceptual modelling, Model-driven architecture, Automatic code generation, Internet of Things","The Internet of Things is currently one of the most representative sources of Big Data. It can acquire real-time data from multiple spatially distributed points, allowing for the extraction of valuable insights. However, an appropriate integration, processing, and analysis of these data depends on several factors starting from the correct definition of the information systems. This paper introduces STS4IoT, a UML profile and automatic code-generation tool for model-driven IoT, to address this issue. STS4IoT allows designing and implementing an IoT application from the required data only, bridging the gaps between the IoT and database design worlds. The IoT data design includes both different in-network transformations and the join of streams from multiple sources. Besides, it follows the Model-Driven Architecture (MDA) guidelines to provide abstraction levels oriented to the different roles participating in the application design. The STS4IoT validation shows it has an excellent structure and is highly understandable. Its instance models are well-formed, highly abstract and readable. And the automatic implementation tool can generate complete code for complex real-world applications involving multiple IoT devices. Then, STS4IoT simplifies the definition and development of IoT applications and their integration into other information systems, such as stream data warehouses.",2022,,https://doi.org/10.1016/j.datak.2021.101971
526,David Nettleton,Chapter 7 - Data Sampling and Partitioning,"sampling, data reduction, partitioning, business criteria, train, test, Big Data","This chapter discusses various types of sampling such as random sampling and sampling based on business criteria (age of customer, time as client, etc.). It also discusses extracting train and test datasets for specific business objectives and considers the issue of Big Data, given that it is currently a hot topic.",2014,,https://doi.org/10.1016/B978-0-12-416602-8.00007-8
527,Mohammed AlShaer and Yehia Taher and Rafiqul Haque and Mohand-Saïd Hacid and Mohamed Dbouk,IBRIDIA: A hybrid solution for processing big logistics data,"Realtime processing, Clustering, Big data, Internet of Things, Logistics, Hierarchical clustering algorithm","Internet of Things (IoT) is leading to a paradigm shift within the logistics industry. Logistics services providers use sensor technologies such as GPS or telemetry to track and manage their shipment processes. Additionally, they use external data that contain critical information about events such as traffic, accidents, and natural disasters. Correlating data from different sensors and social media and performing analysis in real-time provide opportunities to predict events and prevent unexpected delivery delay at run-time. However, collecting and processing data from heterogeneous sources foster problems due to the variety and velocity of data. In addition, processing data in real-time is heavily challenging that it cannot be dealt with using conventional logistics information systems. In this paper, we present a hybrid framework for processing massive volume of data in batch style and real-time. Our framework is built upon Johnson’s hierarchical clustering (HCL) algorithm which produces a dendrogram that represents different clusters of data objects.",2019,,https://doi.org/10.1016/j.future.2019.02.044
528,Maik Frye and Dávid Gyulai and Júlia Bergmann and Robert H. Schmitt,Production rescheduling through product quality prediction,"Machine Learning, Production Scheduling, Product Quality Prediction, Data Quality","In production management, efficient scheduling is key towards smooth and balanced production. Scheduling can be well-supported by real-time data acquisition systems, resulting in decisions that rely on actual or predicted status of production environment and jobs in progress. Utilizing advanced monitoring systems, prediction-based rescheduling method is proposed that can react on in-process scrap predictions, performed by machine learning algorithms. Based on predictions, overall production can be rescheduled with higher efficiency, compared to rescheduling after completion of the whole machining process with realization of scrap. Series of numerical experiments are presented to demonstrate potentials in prediction-based rescheduling, with early-stage scrap detection.",2021,,https://doi.org/10.1016/j.promfg.2021.07.022
529,Komal Peer and William G. Adams and Aaron Legler and Megan Sandel and Jonathan I. Levy and Renée Boynton-Jarrett and Chanmin Kim and Jessica H. Leibler and M. Patricia Fabian,Developing and evaluating a pediatric asthma severity computable phenotype derived from electronic health records,"Asthma, electronic health records, big data, respiratory function tests, selection bias, health care disparities, delivery of health care, observer variation, National Heart, Lung, and Blood Institute (US), pediatrics","Background
Extensive data available in electronic health records (EHRs) have the potential to improve asthma care and understanding of factors influencing asthma outcomes. However, this work can be accomplished only when the EHR data allow for accurate measures of severity, which at present are complex and inconsistent.
Objective
Our aims were to create and evaluate a standardized pediatric asthma severity phenotype based in clinical asthma guidelines for use in EHR-based health initiatives and studies and also to examine the presence and absence of these data in relation to patient characteristics.
Methods
We developed an asthma severity computable phenotype and compared the concordance of different severity components contributing to the phenotype to trends in the literature. We used multivariable logistic regression to assess the presence of EHR data relevant to asthma severity.
Results
The asthma severity computable phenotype performs as expected in comparison with national statistics and the literature. Severity classification for a child is maximized when based on the long-term medication regimen component and minimized when based only on the symptom data component. Use of the severity phenotype results in better, clinically grounded classification. Children for whom severity could be ascertained from these EHR data were more likely to be seen for asthma in the outpatient setting and less likely to be older or Hispanic. Black children were less likely to have lung function testing data present.
Conclusion
We developed a pragmatic computable phenotype for pediatric asthma severity that is transportable to other EHRs.",2021,,https://doi.org/10.1016/j.jaci.2020.11.045
530,Carina Mieth and Anne Meyer and Michael Henke,Framework for the usage of data from real-time indoor localization systems to derive inputs for manufacturing simulation,"real-time indoor localization system, input data management, cyber-physical system, manufacturing simulation, digital twin","Discrete event simulation is becoming increasingly important in the planning and operation of complex manufacturing systems. A major problem with today’s approach to manufacturing simulation studies is the collection and processing of data from heterogeneous sources, because the data is often of poor quality and does not contain all the necessary information for a simulation. This work introduces a framework that uses a real-time indoor localization systems (RTILS) as a central main data harmonizer, that is designed to feed production data into a manufacturing simulation from a single source of truth. It is shown, based on different data quality dimensions, how this contributes to a better overall data quality in manufacturing simulation. Furthermore, a detailed overview on which simulation inputs can be derived from the RTILS data is given.",2019,,https://doi.org/10.1016/j.procir.2019.03.216
531,D. Douglas Miller and Elena A. Wood,"Chapter 10 - AI, autonomous machines and human awareness: Towards shared machine-human contexts in medicine","Medicine, Health care, Artificial intelligence, Medical education, Applications, Challenges","Medical curricula trend to integrate clinical skills training and to create efficiencies in preclinical medical sciences, but the rapid emergence big data-intensive health care has led to initiating collaborations among data scientists, computer engineers, and medical educators that might generate novel educational high-technology platforms and innovative AI practice applications. The preprocessing of big data improves neural network feature recognition, improving the speed and accuracy of AI diagnostics and permitting chronic disease predictions. Applications of generative adversarial networks to create virtual patient phenotypes and image sets exposes medical learners to endless illness presentations, improving system-1 critical thinking for differential diagnosis development. AI offers great potential for education data managers working in support of medical educators and learners. These opportunities to build a shared context, in keeping with these themes of this book, include emerging data-driven AI applications for medical education and provider training include individual aptitude-based career advising, early identification of learners with academic difficulties, highly focused e-tutoring interventions, and natural language processing of standardized exam questions.",2020,,https://doi.org/10.1016/B978-0-12-820543-3.00010-9
532,Emma E. Glennon and Marjolein Bruijning and Justin Lessler and Ian F. Miller and Benjamin L. Rice and Robin N. Thompson and Konstans Wells and C. Jessica E. Metcalf,Challenges in modeling the emergence of novel pathogens,"Immune landscape, Genotype to phenotype map, Big data, Data integration, Fundamental theory, Health system functioning","The emergence of infectious agents with pandemic potential present scientific challenges from detection to data interpretation to understanding determinants of risk and forecasts. Mathematical models could play an essential role in how we prepare for future emergent pathogens. Here, we describe core directions for expansion of the existing tools and knowledge base, including: using mathematical models to identify critical directions and paths for strengthening data collection to detect and respond to outbreaks of novel pathogens; expanding basic theory to identify infectious agents and contexts that present the greatest risks, over both the short and longer term; by strengthening estimation tools that make the most use of the likely range and uncertainties in existing data; and by ensuring modelling applications are carefully communicated and developed within diverse and equitable collaborations for increased public health benefit.",2021,,https://doi.org/10.1016/j.epidem.2021.100516
533,Jack C. Yue and Hsin-Chung Wang and Yin-Yee Leong and Wei-Ping Su,Using Taiwan National Health Insurance Database to model cancer incidence and mortality rates,"Cancer insurance, Longevity risk, Big data, Stochastic models, National Health Insurance","The increasing cancer incidence and decreasing mortality rates in Taiwan worsened the loss ratio of cancer insurance products and created a financial crisis for insurers. In general, the loss ratio of long-term health products seems to increase with the policy year. In the present study, we used the data from Taiwan National Health Insurance Research Database to evaluate the challenge of designing cancer products. We found that the Lee–Carter and APC models have the smallest estimation errors, and the CBD and Gompertz models are good alternatives to explore the trend of cancer incidence and mortality rates, especially for the elderly people. The loss ratio of Taiwan’s cancer products is to grow and this can be deemed as a form of longevity risk. The longevity risk of health products is necessary to face in the future, similar to the annuity products.",2018,,https://doi.org/10.1016/j.insmatheco.2017.09.016
534,Ana Isabel Canhoto,Leveraging machine learning in the global fight against money laundering and terrorism financing: An affordances perspective,"Big data, Artificial intelligence, Machine learning, Algorithm, Customer profiling, Financial services, Anti-money laundering, United Nations, Sustainable development goals","Financial services organisations facilitate the movement of money worldwide, and keep records of their clients’ identity and financial behaviour. As such, they have been enlisted by governments worldwide to assist with the detection and prevention of money laundering, which is a key tool in the fight to reduce crime and create sustainable economic development, corresponding to Goal 16 of the United Nations Sustainable Development Goals. In this paper, we investigate how the technical and contextual affordances of machine learning algorithms may enable these organisations to accomplish that task. We find that, due to the unavailability of high-quality, large training datasets regarding money laundering methods, there is limited scope for using supervised machine learning. Conversely, it is possible to use reinforced machine learning and, to an extent, unsupervised learning, although only to model unusual financial behaviour, not actual money laundering.",2021,,https://doi.org/10.1016/j.jbusres.2020.10.012
535,Daqing Wang and Haoli Xu and Yue Shi and Zhibin Ding and Zhengdong Deng and Zhixin Liu and Xingang Xu and Zhao Lu and Guangyuan Wang and Zijian Cheng and Xiaoning Zhao,The groundwater potential assessment system based on cloud computing: A case study in islands region,"Big data, Cloud computing, Remote sensing, Groundwater potential, Bedrock islands","Today’s intelligent system based on cloud computing platform can realize “unattended”, real-time monitoring observation and forecast by remote sensing. In order to import the development and efficiency of groundwater potential assessment(GPA) by remote sensing, the cloud computing platform was tried to use in the computing GPA. In this study, the Pearl River Estuary islands region(China) was selected as the study area. The slope, aspect, water-density(WD), land surface temperature(LST), NDVI and NDWI were used as the GPA indexes, which have been used before. Considering the similar geological and geomorphological conditions of the islands area, the analytic hierarchy process (AHP) method and these indexes can be used to assess GPA in the remote sensing cloud computing platform efficiently and conveniently. The results of the assessment were in good agreement with the actual hydrogeological map. Besides, the other intelligent algorithms can also be applied in this platform. Finally, this study realized the rapid “unattended” and “real-time monitoring” groundwater potential assessment, and carried out a multi-level GPA. It will be of certain reference significance to the exploitation of groundwater in the island area, which has realized convenient and efficient processing and analysis of data anytime and anywhere. At the same time, attention must be paid to the security of data and the maintenance of the system.",2021,,https://doi.org/10.1016/j.comcom.2021.06.028
536,R. Priyadarshini and Latha Tamilselvan and T. Khuthbudin and S. Saravanan and S. Satish,Semantic Retrieval of Relevant Sources for Large Scale Virtual Documents,"Virtual documents (VD), Source document, Hadoop file System (HDFS), DW Ranking algorithm, Top  algorithm.","The term big data has come into use in recent years. It is used to refer to the ever-increasing amount of data that organizations are storing, processing and analyzing. An Interesting fact with bigdata is that it differ in Volume, Variety, Velocity characteristics which makes it difficult to process using the conventional Database Management System. Hence there is a need of schema less Management Systems even this will never be complete solution to bigdata analysis since the processing has no focus on the semantic information as they consider only the structural information. Content Management System like Wikipedia stores and links huge amount of documents and files. There is lack of semantic linking and analysis in such systems even though this kind of CMS uses clusters and distributed framework for storing big data. The retrieved references for a particular article are random and enormous. In order to reduce the number of references for a selected content there is a need for semantic matching. In this paper we propose framework which make use of the distributed parallel processing capability of Hadoop Distributed File System (HDFS) to perform semantic analysis over the volume of documents (bigdata) to find the best matched source document from the collection source documents for the same virtual document.",2015,,https://doi.org/10.1016/j.procs.2015.06.043
537,Chenhao Tian and Chenghong Feng and Lei Chen and Qixuan Wang,Impact of water source mixture and population changes on the Al residue in megalopolitan drinking water,"Al residue, Mixed water sources, Big data analysis, Megalopolitan, Drinking water","This study establishes a new understanding of the contributions of Al residue in a megalopolitan drinking water supply system with mixed water sources. The different influences and contributions of foreign water source, resident migration and season changing to Al residue in drinking water were investigated. Especially, the role of Southern water transferred over 1200 km via the South-to-North Water Diversion Project in the Al residue of drinking water supply system of a northern megalopolitan were revealed for the first time. Comparisons of big data on Al residue in the water supply system with sole and mixed water sources showed that the introduction of Southern water enhanced the Al residue in drinking water by over 35%. The world's largest annual residents’ migration during Chinese Lunar New Year and the changes of season affect the water pipework hydrodynamics, which were embodied as the periodic changes of particulate aluminium and the relations with resident's temporal-spatial distribution in the megalopolitan. Because of the differences in water quality, Southern water promotes the release of historically deposited Al and facilitates the cleaning of old pipes.",2020,,https://doi.org/10.1016/j.watres.2020.116335
538,Guoyin Jiang and Xiaodong Feng and Wenping Liu and Xingjun Liu,Clicking position and user posting behavior in online review systems: A data-driven agent-based modeling approach,"Agent-based modeling, Big data, Online review systems, Clicking position, Posting behavior","In online review systems, a participant's level of knowledge impacts his/her posting behaviors, and an increase in knowledge occurs when the participant reads the reviews posted on the systems. To capture the collective dynamics of posting reviews, we used real-world big data collected over 153 months to drive an agent-based model for replicating the operation process of online review systems. The model explains the effects of clicking position (e.g., on a review webpage's serial list) and the number of items per webpage on posting contributions. Reading reviews from the last webpage only, or from the first webpage and last webpage simultaneously, can promote a greater review volume than reading reviews in other positions. This illustrates that representing primacy (first items) and recency (recent items) within one page simultaneously, or displaying recent items in reverse chronological order, are relatively better strategies for the webpage display of online reviews. The number of items plays a nonlinear moderating role in bridging the clicking position and posting behavior, and we determine the optimal number of items. To effectively establish strategies for webpage design in online review systems, business managers must switch from reliance on experience to reliance on an agent-based model as a decision support system for the formalized webpage design of online review systems.",2020,,https://doi.org/10.1016/j.ins.2019.09.053
539,Luri {Shirosaki Marçal de Souza} and Andréa Oliveira Nunes and Gabriela Giusti and Yovana M.B. Saavedra and Thiago Oliveira Rodrigues and Tiago E. {Nunes Braga} and Diogo A. {Lopes Silva},Evaluating and ranking secondary data sources to be used in the Brazilian LCA database – “SICV Brasil”,"Qualidata guide, Data quality, Data format, Life Cycle Inventory, Weighting factors","The generation of reliable life cycle inventories is essential towards Life Cycle Assessment (LCA) development, and the use of literature inventories as data sources can serve as a driving force for emerging LCA databases. The aim of this paper was to propose a method to select and rank scientific publications to be used as possible data sources for supplying LCA databases with new datasets. A case study was designed to identify eligible datasets to compose the emergent Brazilian Life Cycle Inventory Database System – the “SICV Brasil” launched in 2016. The methodology used was based on an exploratory research composed of three steps: i) a bibliographic survey on the scientific productions of Life Cycle Inventories (LCI) in Brazil from 2000 to 2017; ii) a cross-check of LCI data and information based on the 40 selected requirements used in order to analyze the quality of LCI datasets in terms of mandatory, recommended and optional requirements; and iii) an analysis of the data quality requirements for those datasets with support of principles of Analytical Hierarchy Process (AHP) to elect possible datasets to be included in the SICV Brasil database. In total, 57 publications were analyzed and the results indicated that mandatory requirements had under 50% acceptance and only 10 requirements (less than 25%) were fully met. The best LCI dataset received 73 points (90%) with the scoring method, while 16 datasets were given less than 40 points (50%). Therefore, it is necessary to improve data quality of LCI datasets found in literature before using them to integrate LCA databases. In this regard, this study proposed a guide with short, medium, and long-term measures to mitigate this problem. The idea is to put an action plan into practice to gather more LCI datasets from literature which may be eligible for publication to SICV Brasil to improve this national database with more and relevant high-quality datasets.",2021,,https://doi.org/10.1016/j.spc.2020.09.021
540,Claudia Caudai and Antonella Galizia and Filippo Geraci and Loredana {Le Pera} and Veronica Morea and Emanuele Salerno and Allegra Via and Teresa Colombo,AI applications in functional genomics,"Artificial intelligence, Functional genomics, Genomics, Proteomics, Epigenomics, Transcriptomics, Epitranscriptomics, Metabolomics, Machine learning, Deep learning","We review the current applications of artificial intelligence (AI) in functional genomics. The recent explosion of AI follows the remarkable achievements made possible by “deep learning”, along with a burst of “big data” that can meet its hunger. Biology is about to overthrow astronomy as the paradigmatic representative of big data producer. This has been made possible by huge advancements in the field of high throughput technologies, applied to determine how the individual components of a biological system work together to accomplish different processes. The disciplines contributing to this bulk of data are collectively known as functional genomics. They consist in studies of: i) the information contained in the DNA (genomics); ii) the modifications that DNA can reversibly undergo (epigenomics); iii) the RNA transcripts originated by a genome (transcriptomics); iv) the ensemble of chemical modifications decorating different types of RNA transcripts (epitranscriptomics); v) the products of protein-coding transcripts (proteomics); and vi) the small molecules produced from cell metabolism (metabolomics) present in an organism or system at a given time, in physiological or pathological conditions. After reviewing main applications of AI in functional genomics, we discuss important accompanying issues, including ethical, legal and economic issues and the importance of explainability.",2021,,https://doi.org/10.1016/j.csbj.2021.10.009
541,Krish Krishnan,5 - Pharmacy industry applications and usage,"Clinical Trials, Research, Multi-Teams, Non-Intrusive, Repeatable tests","One of the applications of big data applications and infrastructure is in the pharmaceutical industry. The complexity of the queries that are executed in these applications and the results they generate, make us feel the statement of torture the data and it will confess to anything. The relationships between the data in the different subject areas, the clinical trials and results, the communities in social media, the research labs and their outcomes, the clinical labs and patient results, and the financial outcomes of the pharmaceutical enterprise. Wow, think of all kinds of insights, add to this the markets, the competition, and the global industry, and we have phenomenal data to work with.",2020,,https://doi.org/10.1016/B978-0-12-815746-6.00005-3
542,Aekyeung Moon and Jaeyoung Kim and Jialing Zhang and Seung Woo Son,Evaluating fidelity of lossy compression on spatiotemporal data from an IoT enabled smart farm,"Smart farm, Lossy compression, IoT, Signal processing, Data fidelity","As the volume of data collected by various IoT sensors used in smart farm applications increases, the storing and processing of big data for agricultural applications become a huge challenge. The insight of this paper is that lossy compression can unleash the power of compression to IoT because, as compared with its counterpart (a lossless one), it can significantly reduce the data volume when the spatiotemporal characteristics of IoT sensor data are properly exploited. However, lossy compression faces the challenge of compressing too much data thus losing data fidelity, which might affect the quality of the data and potential analytics outcomes. To understand the impact of lossy compression on IoT data management and analytics, we evaluated four classification algorithms with reconstructed agricultural sensor data based on various energy concentration. Specifically, we applied three transformation-based lossy compression mechanisms to five real-world weather datasets collected at different sampling granularities from IoT weather stations. Our experimental results indicate that there is a strong positive correlation between the concentrated energy of the transformed coefficients and the compression ratio as well as the data quality. While we observed a general trend where much higher compression ratios can be achieved at the cost of a decrease in quality, we also observed that the impact on the classification accuracy varies among the data sets and algorithms we evaluated. Lastly, we show that the sampling granularity also influences the data fidelity in terms of the prediction performance and compression ratio.",2018,,https://doi.org/10.1016/j.compag.2018.08.045
543,Haowen Xu and Andy Berres and Yan Liu and Melissa R. Allen-Dumas and Jibonananda Sanyal,An overview of visualization and visual analytics applications in water resources management,,"Recent advances in information, communication, and environmental monitoring technologies have increased the availability, spatiotemporal resolution, and quality of water-related data, thereby leading to the emergence of many innovative big data applications. Among these applications, visualization and visual analytics, also known as the visual computing techniques, empower the synergy of computational methods (e.g., machine learning and statistical models) with human reasoning to improve the understanding and solution toward complex science and engineering problems. These approaches are frequently integrated with geographic information systems and cyberinfrastructure to provide new opportunities and methods for enhancing water resources management. In this paper, we present a comprehensive review of recent hydroinformatics applications that employ visual computing techniques to (1) support complex data-driven research problems, and (2) support the communication and decision-makings in the water resources management sector. Then, we conduct a technical review of the state-of-the-art web-based visualization technologies and libraries to share our experiences on developing shareable, adaptive, and interactive visualizations and visual interfaces for resources management applications. We close with a vision that applies the emerging visual computing technologies and paradigms to develop the next generation of water resources management applications.",2022,,https://doi.org/10.1016/j.envsoft.2022.105396
544,Jingjing Li and Ling Tang and Shouyang Wang,Forecasting crude oil price with multilingual search engine data,"Big data, Multilingual search engine index, Crude oil price forecasting, Google Trends, Artificial intelligence","In the big data era, search engine data (SED) have presented new opportunities for improving crude oil price prediction; however, the existing research were confined to single-language (mostly English) search keywords in SED collection. To address such a language bias and grasp worldwide investor attention, this study proposes a novel multilingual SED-driven forecasting methodology from a global perspective. The proposed methodology includes three main steps: (1) multilingual index construction, based on multilingual SED; (2) relationship investigation, between the multilingual index and crude oil price; and (3) oil price prediction, with the multilingual index as an informative predictor. With WTI spot price as studying samples, the empirical results indicate that SED have a powerful predictive power for crude oil price; nevertheless, multilingual SED statistically demonstrate better performance than single-language SED, in terms of enhancing prediction accuracy and model robustness.",2020,,https://doi.org/10.1016/j.physa.2020.124178
545,Achim Kampker and Heiner Heimes and Ulrich Bührer and Christoph Lienemann and Stefan Krotil,Enabling Data Analytics in Large Scale Manufacturing,"Automotive, Manufacturing, Data Analytics, Big Data, Optimization","Companies of the manufacturing industry face increasing process complexity. To remain competitive, increasing the knowledge concerning innovative manufacturing processes is necessary. In other areas, data analytics methods have been successfully applied for this purpose. Currently, their application in large scale manufacturing is hampered by insufficient data availability. Therefore, this study presents a solution approach that enables adaptive data availability by establishing a data-use-case-matrix (DUCM), which allows use case prioritization to support dimensioning of control systems and IT infrastructures. In order to support technology development, further proposed is a scalable implementation of the prioritized use cases starting in early prototyping phases.",2018,,https://doi.org/10.1016/j.promfg.2018.06.017
546,Zhengbing He and Geqi Qi and Lili Lu and Yanyan Chen,Network-wide identification of turn-level intersection congestion using only low-frequency probe vehicle data,"Big data, Floating car data, Urban road network, Traffic congestion, Road intersection","Locating the bottlenecks in cities where traffic congestion usually occurs is essential prior to solving congestion problems. Therefore, this paper proposes a low-frequency probe vehicle data (PVD)-based method to identify turn-level intersection traffic congestion in an urban road network. This method initially divides an urban area into meter-scale square cells and maps PVD into those cells and then identifies the cells that correspond to road intersections by taking advantage of the fixed-location stop-and-go characteristics of traffic passing through intersections. With those rasterized road intersections, the proposed method recognizes probe vehicles’ turning directions and provides preliminary analysis of traffic conditions at all turning directions. The proposed method is map-independent (i.e., no digital map is needed) and computationally efficient and is able to rapidly screen most of the intersections for turn-level congestion in a road network. Thereby, this method is expected to greatly decrease traffic engineers’ workloads by providing information regarding where and when to investigate and solve traffic congestion problems.",2019,,https://doi.org/10.1016/j.trc.2019.10.001
547,Marijn Janssen and Paul Brous and Elsa Estevez and Luis S. Barbosa and Tomasz Janowski,Data governance: Organizing data for trustworthy Artificial Intelligence,"Big data, Data governance, AI, Algorithmic governance, Information sharing, Artificial Intelligence, Trusted frameworks","The rise of Big, Open and Linked Data (BOLD) enables Big Data Algorithmic Systems (BDAS) which are often based on machine learning, neural networks and other forms of Artificial Intelligence (AI). As such systems are increasingly requested to make decisions that are consequential to individuals, communities and society at large, their failures cannot be tolerated, and they are subject to stringent regulatory and ethical requirements. However, they all rely on data which is not only big, open and linked but varied, dynamic and streamed at high speeds in real-time. Managing such data is challenging. To overcome such challenges and utilize opportunities for BDAS, organizations are increasingly developing advanced data governance capabilities. This paper reviews challenges and approaches to data governance for such systems, and proposes a framework for data governance for trustworthy BDAS. The framework promotes the stewardship of data, processes and algorithms, the controlled opening of data and algorithms to enable external scrutiny, trusted information sharing within and between organizations, risk-based governance, system-level controls, and data control through shared ownership and self-sovereign identities. The framework is based on 13 design principles and is proposed incrementally, for a single organization and multiple networked organizations.",2020,,https://doi.org/10.1016/j.giq.2020.101493
548,Jing Zhang and Rong Tan and Chunhua Su and Wen Si,Design and application of a personal credit information sharing platform based on consortium blockchain,"Consortium blockchain, Personal credit reporting, Credit information sharing, Big data crediting","The technical features of blockchain, including decentralization, data transparency, tamper-proofing, traceability, privacy protection and open-sourcing, make it a suitable technology for solving the information asymmetry problem in personal credit reporting transactions. Applying blockchain technology to credit reporting meets the needs of social credit system construction and may become an important technical direction in the future. This paper analyzed the problems faced by China’s personal credit reporting market, designed the framework of personal credit information sharing platform based on blockchain 3.0 architecture, studied the technical details of the platform and the technical advantages, and finally, applied the platform to the credit blacklist sharing transaction and explored the possible implementation approach. The in-depth integration of blockchain technology and personal credit reporting helps to realize the safe sharing of credit data and reduce the cost of credit data collection, thereby helping the technological and efficiency transformation of the personal credit reporting industry and promoting the overall development of the social credit system.",2020,,https://doi.org/10.1016/j.jisa.2020.102659
549,Sara Mozzoni and Massimo Di Francesco and Giulio Maternini and Benedetto Barabino,Transfer’s monitoring in bus transit services by Automatic Vehicle Location data,"Big Data, Transfer diagnosis, Automatic Vehicle Location Data","Since transfers increase the connectivity of routes, they improve the characteristics of transit networks. Designing and managing transfers are well-investigated issues arising at the tactical and operational level. Conversely, the monitoring phase was rarely faced to verify the consistency between well planned and/or delivered transfers. In this paper, we tailor an innovative methodology for measuring the rate of transfers between two routes by using archived Automatic Vehicle Location (AVL) data. This measurement is performed spatially, at shared and unshared (but reasonably quite close) bus stops, and temporally at each time period. The results are represented by easy-to-read control dashboards. This methodology is tested by about 240,000 AVL real records provided by the local bus operator of Cagliari (Italy) and provides valuable insights into the characterization of transfers.",2022,,https://doi.org/10.1016/j.trpro.2021.12.052
550,Z. Milosevic and W. Chen and A. Berry and F.A. Rabhi,Chapter 2 - Real-Time Analytics,"Real-time analytics, Complex event processing, Streaming, Event processing, Advanced analytics, Data analysis, Machine learning, Finance, EventSwarm","Real-time analytics is a special kind of Big Data analytics in which data elements are required to be processed and analyzed as they arrive in real time. It is important in situations where real-time processing and analysis can deliver important insights and yield business value. This chapter provides an overview of current processing and analytics platforms needed to support such analysis, as well as analytics techniques that can be applied in such environments. The chapter looks beyond traditional event processing system technology to consider a broader big data context that involves “data at rest” platforms and solutions. The chapter includes a case study showing the use of EventSwarm complex event processing engine for a class of analytics problems in finance. The chapter concludes with several research challenges, such as the need for new approaches and algorithms required to support real-time data filtering, data exploration, statistical data analysis, and machine learning.",2016,,https://doi.org/10.1016/B978-0-12-805394-2.00002-7
551,Luca Tagliaferri and Carlo Gobitti and Giuseppe Ferdinando Colloca and Luca Boldrini and Eleonora Farina and Carlo Furlan and Fabiola Paiar and Federica Vianello and Michela Basso and Lorenzo Cerizza and Fabio Monari and Gabriele Simontacchi and Maria Antonietta Gambacorta and Jacopo Lenkowicz and Nicola Dinapoli and Vito Lanzotti and Renzo Mazzarotto and Elvio Russi and Monica Mangoni,A new standardized data collection system for interdisciplinary thyroid cancer management: Thyroid COBRA,"Big data, Data pooling, Personalized medicine, Radiotherapy, Thyroid, Cancer management","The big data approach offers a powerful alternative to Evidence-based medicine. This approach could guide cancer management thanks to machine learning application to large-scale data. Aim of the Thyroid CoBRA (Consortium for Brachytherapy Data Analysis) project is to develop a standardized web data collection system, focused on thyroid cancer. The Metabolic Radiotherapy Working Group of Italian Association of Radiation Oncology (AIRO) endorsed the implementation of a consortium directed to thyroid cancer management and data collection. The agreement conditions, the ontology of the collected data and the related software services were defined by a multicentre ad hoc working-group (WG). Six Italian cancer centres were firstly started the project, defined and signed the Thyroid COBRA consortium agreement. Three data set tiers were identified: Registry, Procedures and Research. The COBRA-Storage System (C-SS) appeared to be not time-consuming and to be privacy respecting, as data can be extracted directly from the single centre's storage platforms through a secured connection that ensures reliable encryption of sensible data. Automatic data archiving could be directly performed from Image Hospital Storage System or the Radiotherapy Treatment Planning Systems. The C-SS architecture will allow “Cloud storage way” or “distributed learning” approaches for predictive model definition and further clinical decision support tools development. The development of the Thyroid COBRA data Storage System C-SS through a multicentre consortium approach appeared to be a feasible tool in the setup of complex and privacy saving data sharing system oriented to the management of thyroid cancer and in the near future every cancer type.",2018,,https://doi.org/10.1016/j.ejim.2018.02.012
552,Šoltýs Katarína and Kľoc Marek and Rabajdová Miroslava and Mareková Mária,Chapter 6 - Innovative technologies in precision healthcare,"Precision healthcare, biotechnology, emerging technologies, big data","Precision medicine is the intersection of data science, analytics, and biomedicine in creating a healthy learning system that conducts research in the context of clinical care while optimizing the tools and information used to provide better outcomes for patients. Emerging technologies represent a novel, innovative, and fast-evolving trend within a particular field. Among the latest trends as virtual reality, robotics, wearable, and implantable sensors, and removable tattoos, together with nanotechnologies, 3D printing, and others are considered. In addition, new advanced computing technologies including artificial intelligence, machine learning (ML), big data mining, and cloud computing form an integral part of personalized healthcare. Personalized medicine is not necessarily the same as precision medicine. From the point of view of technology development, precision medicine is an intermediate step to personalized medicine, which will be much more complex and will require even more data. There is a big challenge to combine multiomics approaches in analysis, as we can see in bioinformatics that there are a lot of techniques in one area. Analysis of more than one area such as the genome, transcriptome even microbiome, starts exponentially grown on science field. The integration of multiomics data analysis and machine learning can have led to the discovery of new biomarkers, and improve of differential diagnostics of latent diseases. In this chapter, we describe the use of emerging technologies as well as bioengineering and ML for precision healthcare.",2022,,https://doi.org/10.1016/B978-0-323-89837-9.00016-4
553,Haoyu Jiang and Kai Chen and Quanbo Ge and Jinqiang Xu and Yingying Fu and Chunxi Li,Data consistency method of heterogeneous power IOT based on hybrid model,"Power IOT system, Hybrid model, Heterogeneous data consistency, Machine learning combination method","The data of the power Internet of Things (IOT) system is transferred from the IaaS layer to the SaaS layer. The general data preprocessing method mainly solves the problem of big data anomalies and missing at the PaaS layer, but it still lacks the ability to judge the high error data that meets the timing characteristics, making it difficult to deal with heterogeneous power inconsistent issues. This paper shows this phenomenon and its physical mechanism, showing the difficulty of building a quantitative model forward. A data-driven method is needed to form a hybrid model to correct the data. The research object is the electricity meter data on both sides of a commercial building transformer, which comes from different power IOT systems. The low-voltage side was revised based on the high-voltage side. Compared with the correction method based on purely using neural networks, the combined method, Linear Regression (LS) + Differential Evolution (DE) + Extreme Learning Machine (ELM), further reduces the deviation from approximately 4% to 1%.",2021,,https://doi.org/10.1016/j.isatra.2021.01.056
554,Katalin Patonai and Ferenc Jordán,Integrating trophic data from the literature: The Danube River food web,"Aggregation, Danube River, Food web, Incomplete data, Taxonomy","In the era of bioinformatics and big data, ecological research depends on large and easily accessible databases that make it possible to construct complex system models. Open-access data repositories for food webs via publications and ecological databases (e.g. EcoBase) are becoming increasingly common, yet certain ecosystem types are underrepresented (e.g. rivers). In this paper, we compile the trophic connections (predator-prey relationships) for the Danube River ecosystem as gathered from globally available literature data. Data are analyzed by Danube regions separately (Upper, Middle, Lower Danube) as well as an integrated master network version. The master version has been aggregated into larger taxonomic categories. Local and global metrics were used to analyze and compare each network. We find disparity between regions (the Middle Danube having most nodes, but still quite heterogenous), we identify the most important trophic groups, and explain ways on evaluating missing data using each aggregation stage. This data-driven approach, summarizing our presently documented knowledge, can be used for preparing preliminary models and to further refine the Danube River food web in the future.",2021,,https://doi.org/10.1016/j.fooweb.2021.e00203
555,Juliette Mattioli and Pierre-Olivier Robic and Emeric Jesson,Information Quality: the cornerstone for AI-based Industry 4.0,"Industry 4.0, Data-driven AI, Knowledge-based AI, Data Quality, Information Quality","AI becomes a key enabler for Industry 4.0. Data / information quality become a real cornerstone on the overall process from user expectation to products / systems / solutions in a consistent perspective in order to ensure quality of the manufacturing production. This paper highlights some key characteristics in terms of information quality required to implement an effective AI based monitoring framework, in order to achieve operational excellence in Industry.",2022,,https://doi.org/10.1016/j.procs.2022.03.059
556,Diing D.M. Agany and Jose E. Pietri and Etienne Z. Gnimpieba,Assessment of vector-host-pathogen relationships using data mining and machine learning,"Systems Bioscience, OMICs, Pathogenicity, Transmission, Adaptation, Data Mining, Big Data, Machine Learning, Association Mining, Host-Pathogen, Interaction, Infectious Disease, Vector-Borne Disease","Infectious diseases, including vector-borne diseases transmitted by arthropods, are a leading cause of morbidity and mortality worldwide. In the era of big data, addressing broad-scale, fundamental questions regarding the complex dynamics of these diseases will increasingly require the integration of diverse datasets to produce new biological knowledge. This review provides a current snapshot of the systematic assessment of the relationships between microbial pathogens, arthropod vectors and mammalian hosts using data mining and machine learning. We employ PRISMA to identify 32 key papers relevant to this topic. Our analysis shows an increasing use of data mining and machine learning tasks and techniques, including prediction, classification, clustering, association rules mining, and deep learning, over the last decade. However, it also reveals a number of critical challenges in applying these to the study of vector-host-pathogen interactions at various systems biology levels. Here, relevant studies, current limitations and future directions are discussed. Furthermore, the quality of data in relevant papers was assessed using the FAIR (Findable, Accessible, Interoperable, Reusable) compliance criteria to evaluate and encourage reproducibility and shareability of research outcomes. Although shortcomings in their application remain, data mining and machine learning have significant potential to break new ground in understanding fundamental aspects of vector-host-pathogen relationships and their application in this field should be encouraged. In particular, while predictive modeling, feature engineering and supervised machine learning are already being used in the field, other data mining and machine learning methods such as deep learning and association rules analysis lag behind and should be implemented in combination with established methods to accelerate hypothesis and knowledge generation in the domain.",2020,,https://doi.org/10.1016/j.csbj.2020.06.031
557,Jay Lee and Hossein Davari and Jaskaran Singh and Vibhor Pandhare,Industrial Artificial Intelligence for industry 4.0-based manufacturing systems,"Industrial AI, Industry 4.0, Big data, Smart manufacturing, Cyber physical systems","The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.",2018,,https://doi.org/10.1016/j.mfglet.2018.09.002
558,Krish Krishnan,4 - Scientific research applications and usage,"CERN, God particle, Large electron–positron collider, Large hadron collider, Quarks, Scientific research, Standard model",Scientific research is one area of applications and usage of big data where we can generate lots of data in a single experiment and perform complex analytics on the same in the outcome of that experiment. The most famous example that we can talk about is the usage of all infrastructure technologies in the discovery of the “God particle” or “Higgs boson particle” which is leading us to uncover more exploration around the universe.,2020,,https://doi.org/10.1016/B978-0-12-815746-6.00004-1
559,Fátima Leal and Adriana E. Chis and Simon Caton and Horacio González–Vélez and Juan M. García–Gómez and Marta Durá and Angel Sánchez–García and Carlos Sáez and Anthony Karageorgos and Vassilis C. Gerogiannis and Apostolos Xenakis and Efthymios Lallas and Theodoros Ntounas and Eleni Vasileiou and Georgios Mountzouris and Barbara Otti and Penelope Pucci and Rossano Papini and David Cerrai and Mariola Mier,Smart Pharmaceutical Manufacturing: Ensuring End-to-End Traceability and Data Integrity in Medicine Production,"ALCOA, Blockchain, Data anaytics, Data quality, Intelligent agents, Smart contracts","Production lines in pharmaceutical manufacturing generate numerous heterogeneous data sets from various embedded systems which control the multiple processes of medicine production. Such data sets should arguably ensure end-to-end traceability and data integrity in order to release a medicine batch, which is uniquely identified and tracked by its batch number/code. Consequently, auditable computerised systems are crucial on pharmaceutical production lines, since the industry is becoming increasingly regulated for product quality and patient health purposes. This paper describes the EU-funded SPuMoNI project, which aims to ensure the quality of large amounts of data produced by computerised production systems in representative pharmaceutical environments. Our initial results include significant progress in: (i) end-to-end verification taking advantage of blockchain properties and smart contracts to ensure data authenticity, transparency, and immutability; (ii) data quality assessment models to identify data behavioural patterns that can violate industry practices and/or international regulations; and (iii) intelligent agents to collect and manipulate data as well as perform smart decisions. By analysing multiple sensors in medicine production lines, manufacturing work centres, and quality control laboratories, our approach has been initially evaluated using representative industry-grade pharmaceutical manufacturing data sets generated at an IT environment with regulated processes inspected by regulatory and government agencies.",2021,,https://doi.org/10.1016/j.bdr.2020.100172
560,Aydin Nassehi and Ray Y. Zhong and Xingyu Li and Bogdan I. Epureanu,Chapter 11 - Review of machine learning technologies and artificial intelligence in modern manufacturing systems,"Predictive maintenance, Artificial intelligence, Machine learning, Smart manufacturing, Industry 4.0","With the advent of new methods usually identified under the banners of artificial intelligence (AI) and machine learning (ML), statistical analysis methods of complex and uncertain manufacturing systems have been undergoing significant changes. Therefore, various definitions of AI, a brief history, and its differences with traditional statistics are presented. Moreover, ML is introduced to identify its place in data science and differences to topics such as big data analytics and manufacturing problems that use AI and ML are then characterized. Next, a lifecycle-based approach is adopted and the use of various methods in each phase is analyzed, identifying the most useful techniques and the unifying attributes of AI in manufacturing. Finally, the chapter maps out future developments of AI and the emerging trends and identifies a vision based on combining machine and human intelligence in a productive and empowering manner as well. This vision presents humans and increasingly more intelligent machines, not as competitors, but as partners allowing creative and innovative paradigms to emerge.",2022,,https://doi.org/10.1016/B978-0-12-823657-4.00002-6
561,Ramin Karim and Jesper Westerberg and Diego Galar and Uday Kumar,Maintenance Analytics – The New Know in Maintenance,"big data, maintenance analytics, eMaintenance, Knowledge discovery, maintenance decision support","Abstract:
Decision-making in maintenance has to be augmented to instantly understand and efficiently act, i.e. the new know. The new know in maintenance needs to focus on two aspects of knowing: 1) what can be known and 2) what must be known, in order to enable the maintenance decision-makers to take appropriate actions. Hence, the purpose of this paper is to propose a concept for knowledge discovery in maintenance with focus on Big Data and analytics. The concept is called Maintenance Analytics (MA). MA focuses in the new knowledge discovery in maintenance. MA addresses the process of discovery, understanding, and communication of maintenance data from four time-related perspectives, i.e. 1) “Maintenance Descriptive Analytics (monitoring)”; 2) “Maintenance Diagnostic Analytics”; 3) “Maintenance Predictive Analytics”; and 4) “Maintenance Prescriptive analytics”.",2016,,https://doi.org/10.1016/j.ifacol.2016.11.037
562,Hanne Seter and Petter Arnesen and Odd André Hjelkrem,The data driven transport research train is leaving the station. Consultants all aboard?,,"This study sets out to assess whether there is a knowledge gap between the research frontier and the consultation business in how transport data are collected, managed and analysed. The consulting business plays an important role in applying data and methods as they typically carry out public tasks in various parts of the transport system, which are becoming more and more specialised. At the same time, big data has emerged with the promise to provide new, more and better information to help understand society and execute policies more efficiently – what we refer to as the data driven transition. We conduct a literature review to identify the state of the art within international research and compare this with results from interviews and with a survey sent to representatives from the Norwegian consultation business. We find that there is a considerable gap between international researchers and the consulting business within the entire process of collection, management and analysis of traffic data, and that this gap is increasing with the emergence of the data driven transition. Finally, we argue that the results are applicable to other countries as well. Action should be taken to keep the consultants up to speed, which will require efforts from several actors, including governmental agencies, the education institutions, the consulting business and researchers.",2019,,https://doi.org/10.1016/j.tranpol.2019.05.016
563,Q. Peter He and Jin Wang and Devarshi Shah and Nader Vahdat,Statistical Process Monitoring for IoT-Enabled Cybermanufacturing: Opportunities and Challenges,"Cybermanufacturing, Internet of Things, sensors, statistical process monitoring, fault detection, fault diagnosis, statistics pattern analysis","Initiated from services and consumer products industries, there is a growing interest in using Internet of Things (IoT) technologies in various industries. In particular, IoT-enabled cybermanufacturing starts to draw increasing attention. Because IoT devices such as IoT sensors are usually much cheaper and smaller than the traditional sensors, there is a potential for instrumenting manufacturing systems with massive number of sensors. The premise is that the big data subsequently collected from IoT sensors can be utilized to advance manufacturing. Therefore, data-driven statistical process monitoring (SPM) is expected to contribute significantly to the advancement of cybermanufacturing. In this work, the state-of-the-art in cybermanufacturing is reviewed; an IoT-enabled manufacturing technology testbed (MTT) was built to explore the potential of IoT sensors for manufacturing, as well as to understand the characteristics of data produced by the IoT sensors; finally, the potentials and challenges associated with big data analytics presented by cybermanufacturing systems is discussed; and we propose statistics pattern analysis (SPA) as a promising SPM tool for cybermanufacturing.",2017,,https://doi.org/10.1016/j.ifacol.2017.08.2546
564,In Lee and Yong Jae Shin,"Machine learning for enterprises: Applications, algorithm selection, and challenges","Machine learning, Artificial intelligence, Deep learning, Big data, Neural networks, Chatbot, Innovation capability, Resources and capabilities","Machine learning holds great promise for lowering product and service costs, speeding up business processes, and serving customers better. It is recognized as one of the most important application areas in this era of unprecedented technological development, and its adoption is gaining momentum across almost all industries. In view of this, we offer a brief discussion of categories of machine learning and then present three types of machine-learning usage at enterprises. We then discuss the trade-off between the accuracy and interpretability of machine-learning algorithms, a crucial consideration in selecting the right algorithm for the task at hand. We next outline three cases of machine-learning development in financial services. Finally, we discuss challenges all managers must confront in deploying machine-learning applications.",2020,,https://doi.org/10.1016/j.bushor.2019.10.005
565,René Bohnsack and Meike Malena Liesner,What the hack? A growth hacking taxonomy and practical applications for firms,"Growth hacking, Digital transformation, Lean startup, Digital marketing, Big data","As companies become increasingly digital, growth hacking emerged as a new way of scaling businesses. While the term is fashionable in business, many executives remain confused about the concept. Even if firms have an idea of what growth hacking is, they may still be puzzled as to how to do it, creating a strategy-execution gap. Our article assists firms by bridging the growth hacking strategy-execution gap. First, we provide a growth hacking framework and deconstruct its building blocks: marketing, data analysis, coding, and the lean startup philosophy. We then present a taxonomy of 34 growth hacking patterns along the customer lifecycle of acquisition, activation, revenue, retention, and referral; categorize them on the two dimensions of resource intensity and time lag; and provide an example of how to apply the taxonomy in the case of a fitness application. Finally, we discuss seven opportunities and challenges of growth hacking that firms should keep in mind.",2019,,https://doi.org/10.1016/j.bushor.2019.09.001
566,Dominik Ehring and Janosch Luttmer and Robin Pluhnau and Arun Nagarajah,SMART standards - concept for the automated transfer of standard contents into a machine-actionable form,"SMART Standards, knowledge representation, automatic extraction, transfer, 3M model of Duisburg","Standards are - not directly visible to everyone – omnipresent in nearly every development process. In times of digitalization, where buzzwords such as ""connectivity of machines"", ""artificial intelligence"", “big data”, “cloud computing” or “smart factories” are often used, companies are still confronted with problems in handling standards throughout the entire product lifecycle. Today’s way of working with standards is characterized by manual viewing of documents, whereby a user searches for relevant information, such as formulas, and has to transfer this information to his process, method or tool. This manual process results in an increased time, loss of quality due to faulty manual transmission of information, a high adjustment effort for updates of standards and no guarantee for traceability. In order to reduce and minimize errors and needed time for work with information stored within standards, there is a need for a new form of knowledge representation for standards with sufficient data quality to ensure standard-compliant development activities. Consequently, there is a need for machine-actionable standards to ensure autonomous and efficient processes, whereby the effort for preparation is less than the benefit. The question arises how classified standards content can be represented in a machine-actionable way without loss of information. This paper shows a concept for the automatic extraction of standards content and their transfer into a machine-actionable knowledge representation. The concept, which is based on the “3M Framework of Duisburg” and thus answers questions of modularization, modeling and management, consists of six steps ""extraction"", ""modeling"", “modification”, ""fusion and storage"", ""provision"" and ""application"", to digitalize existing content, is presented and discussed.",2021,,https://doi.org/10.1016/j.procir.2021.05.025
567,Meghna Raj and Shashank Gupta and Vinay Chamola and Anubhav Elhence and Tanya Garg and Mohammed Atiquzzaman and Dusit Niyato,A survey on the role of Internet of Things for adopting and promoting Agriculture 4.0,,"There is a rapid increase in the adoption of emerging technologies like the Internet of Things (IoT), Unmanned Aerial Vehicles (UAV), Internet of Underground Things (IoUT), Data analytics in the agriculture domain to meet the increased food demand to cater to the increasing population. Agriculture 4.0 is set to revolutionize agriculture productivity by using Precision Agriculture (PA), IoT, UAVs, IoUT, and other technologies to increase agriculture produce for growing demographics while addressing various farm-related issues. This survey provides a comprehensive overview of how multiple technologies such as IoT, UAVs, IoUT, Big Data Analytics, Deep Learning Techniques, and Machine Learning methods can be used to manage various farm-related operations. For each of these technologies, a detailed review is done on how the technology is being used in Agriculture 4.0. These discussions include an overview of relevant technologies, their use cases, existing case studies, and research works that demonstrate the use of these technologies in Agriculture 4.0. This paper also highlights the various future research gaps in the adoption of these technologies in Agriculture 4.0.",2021,,https://doi.org/10.1016/j.jnca.2021.103107
568,Rene Abraham and Johannes Schneider and Jan {vom Brocke},"Data governance: A conceptual framework, structured review, and research agenda","Data governance, Information governance, Conceptual framework, Literature review, Research agenda","Data governance refers to the exercise of authority and control over the management of data. The purpose of data governance is to increase the value of data and minimize data-related cost and risk. Despite data governance gaining in importance in recent years, a holistic view on data governance, which could guide both practitioners and researchers, is missing. In this review paper, we aim to close this gap and develop a conceptual framework for data governance, synthesize the literature, and provide a research agenda. We base our work on a structured literature review including 145 research papers and practitioner publications published during 2001-2019. We identify the major building blocks of data governance and decompose them along six dimensions. The paper supports future research on data governance by identifying five research areas and displaying a total of 15 research questions. Furthermore, the conceptual framework provides an overview of antecedents, scoping parameters, and governance mechanisms to assist practitioners in approaching data governance in a structured manner.",2019,,https://doi.org/10.1016/j.ijinfomgt.2019.07.008
569,Leah M. Hamilton and Jacob Lahne,Fast and automated sensory analysis: Using natural language processing for descriptive lexicon development,"Natural language processing, Rapid descriptive methods, Big data, Whisky, Research methodology, Machine learning","As sensory evaluation relies upon humans accurately communicating their sensory experience, the diverse and overlapping vocabulary of flavor descriptors remains a major challenge. The lexicon generation protocols used in methods like Descriptive Analysis are expensive and time-consuming, while the post-facto analyses of natural vocabulary in “quick and dirty” methods like Free Choice or Flash Profiling require considerable subjective decision-making on the part of the analyst. A potential alternative for producing lexicons and analyzing the sensory attributes of products in nonstandardized text can be found in Natural Language Processing (NLP). NLP tools allow for the analysis of larger volumes of free text with fewer subjective decisions. This paper describes the steps necessary to automatically collect, clean, and analyze existing product descriptions from the web. As a case study, online reviews of international whiskies from two prominent websites (2309 reviews from WhiskyCast and 4289 reviews from WhiskyAdvocate) were collected, preprocessed to only retain potentially-descriptive nouns, adjectives, and verbs, and then the final term list was grouped into a flavor wheel using Correspondence Analysis and Agglomerative Hierarchical Clustering. The wheel is compared to an existing Scotch flavor wheel. The ease of collecting nonstandardized descriptions of products and the improved speed of automated methods can facilitate collection of descriptive sensory data for products where no lexicon exists. This has the potential to speed up and standardize many of the bottlenecks in rapid descriptive methods and facilitate the collection and use of very large datasets of product descriptions.",2020,,https://doi.org/10.1016/j.foodqual.2020.103926
570,Hongfang Lu and Lijun Guo and Mohammadamin Azimi and Kun Huang,Oil and Gas 4.0 era: A systematic review and outlook,"Oil and Gas 4.0, Big data, Digitization, IIoT, Intelligentization","Recently, with the development of “Industry 4.0”, “Oil and Gas 4.0” has also been put on the agenda in the past two years. Some companies and experts believe that “Oil and Gas 4.0” can completely change the status quo of the oil and gas industry, which can bring huge benefits because it accelerates the digitization and intelligentization of the oil and gas industry. However, the “Oil and Gas 4.0” is still in its infancy. Therefore, this paper systematically introduces the concept and core technologies of “Oil and Gas 4.0”, such as big data and the industrial Internet of Things (IIoT). Moreover, this paper analyzes typical application scenarios of the oil and gas industry chain (upstream, midstream and downstream) through examples, such as intelligent oilfield, intelligent pipeline, and intelligent refinery. It is concluded that the essence of “Oil and Gas 4.0” is a data-driven intelligence system based on the highly digitization. To the best of our knowledge, this is the first academic peer-reviewed paper on the “Oil and Gas 4.0” era, aiming to let more oil and gas industry personnel understand its benefits and application scenarios, so as to better apply it to practical engineering in the future. In the discussion section, this paper also analyzes the opportunities and difficulties that may be brought about by the “Oil and Gas 4.0” era. Finally, relevant policy recommendations are proposed.",2019,,https://doi.org/10.1016/j.compind.2019.06.007
571,Pall Rikhardsson and Ogan Yigitbasioglu,Business intelligence & analytics in management accounting research: Status and future focus,"Business intelligence, Management accounting, Big data, Analytics","Executives see technology, data and analytics as a transforming force in business. Many organizations are therefore implementing business intelligence & analytics (BI&A) technologies to support reporting and decision-making. Traditionally, management accounting is the primary support for decision-making and control in an organization. As such, it has clear links to and can benefit from applying BI&A technologies. This indicates an interesting research area for accounting and AIS researchers. However, a review of the literature in top accounting and information systems journals indicates that to date, little research has focused on this link. This article reviews the literature, points to several research gaps and proposes a framework for studying the relationship between BI&A and management accounting.",2018,,https://doi.org/10.1016/j.accinf.2018.03.001
572,Yu Lian Qiu and Guo Fang Xiao,Research on Cost Management Optimization of Financial Sharing Center Based on RPA,"RPA, Financial shared service center, Cost management, Process optimization, Big data","With the development of artificial intelligence technology, the widespread application of robot process automation (RPA) in the future financial field has become an inevitable trend. Through the review of the current situation of cost management of A Group’s financial shared service center, the article deeply expounds the problems that the current cross-system data cannot be automatically collected, the cost accounting is not timely, and the cost analysis report mode is too fixed. Based on Robot Process Automation (RPA), cost management process optimization and improvement were made on the cross-system data acquisition, ""Cloud Purchasing Platform"" construction, and comprehensive multi-dimensional cost analysis. It is expected to provide reference for the robot process automation application of the financial shared service center.",2020,,https://doi.org/10.1016/j.procs.2020.02.031
573,Ioanna G. Chouvarda and Dimitrios G. Goulis and Irene Lambrinoudaki and Nicos Maglaveras,Connected health and integrated care: Toward new models for chronic disease management,"Connected health, Integrated care, Personal health system, Electronic health","The increasingly aging population in Europe and worldwide brings up the need for the restructuring of healthcare. Technological advancements in electronic health can be a driving force for new health management models, especially in chronic care. In a patient-centered e-health management model, communication and coordination between patient, healthcare professionals in primary care and hospitals can be facilitated, and medical decisions can be made timely and easily communicated. Bringing the right information to the right person at the right time is what connected health aims at, and this may set the basis for the investigation and deployment of the integrated care models. In this framework, an overview of the main technological axes and challenges around connected health technologies in chronic disease management are presented and discussed. A central concept is personal health system for the patient/citizen and three main application areas are identified. The connected health ecosystem is making progress, already shows benefits in (a) new biosensors, (b) data management, (c) data analytics, integration and feedback. Examples are illustrated in each case, while open issues and challenges for further research and development are pinpointed.",2015,,https://doi.org/10.1016/j.maturitas.2015.03.015
574,Kim B. Stevens and Dirk U. Pfeiffer,Sources of spatial animal and human health data: Casting the net wide to deal more effectively with increasingly complex disease problems,"Big data, Data warehouse, Google Earth, mHealth, Spatial data, Volunteered geographic information","During the last 30years it has become commonplace for epidemiological studies to collect locational attributes of disease data. Although this advancement was driven largely by the introduction of handheld global positioning systems (GPS), and more recently, smartphones and tablets with built-in GPS, the collection of georeferenced disease data has moved beyond the use of handheld GPS devices and there now exist numerous sources of crowdsourced georeferenced disease data such as that available from georeferencing of Google search queries or Twitter messages. In addition, cartography has moved beyond the realm of professionals to crowdsourced mapping projects that play a crucial role in disease control and surveillance of outbreaks such as the 2014 West Africa Ebola epidemic. This paper provides a comprehensive review of a range of innovative sources of spatial animal and human health data including data warehouses, mHealth, Google Earth, volunteered geographic information and mining of internet-based big data sources such as Google and Twitter. We discuss the advantages, limitations and applications of each, and highlight studies where they have been used effectively.",2015,,https://doi.org/10.1016/j.sste.2015.04.003
575,Warwick Powell and Marcus Foth and Shoufeng Cao and Valéri Natanelov,Garbage in garbage out: The precarious link between IoT and blockchain in food supply chains,"Blockchain, Data quality, Distributed ledger technology, Meat industry, Internet of things, Food supply chains","The application of blockchain in food supply chains does not resolve conventional IoT data quality issues. Data on a blockchain may simply be immutable garbage. In response, this paper reports our observations and learnings from an ongoing beef supply chain project that integrates Blockchain and IoT for supply chain event tracking and beef provenance assurance and proposes two solutions for data integrity and trust in the Blockchain and IoT-enabled food supply chain. Rather than aiming for absolute truth, we explain how applying the notion of ‘common knowledge’ fundamentally changes oracle identity and data validity practices. Based on the learnings derived from leading an IoT supply chain project with a focus on beef exports from Australia to China, our findings unshackle IoT and Blockchain from being used merely to collect lag indicators of past states and liberate their potential as lead indicators of desired future states. This contributes: (a) to limit the possibility of capricious claims on IoT data performance, and; (b) to utilise mechanism design as an approach by which supply chain behaviours that increase the probability of desired future states being realised can be encouraged.",2022,,https://doi.org/10.1016/j.jii.2021.100261
576,Xuming Liang,Application and research of global grid database design based on geographic information,"Big data collection, Geographic information, Grid database, Data mining","Energy crisis and climate change have become two seriously concerned issues universally. As a feasible solution, Global Energy Interconnection (GEI) has been highly praised and positively responded by the international community once proposed by China. From strategic conception to implementation, GEI development has entered a new phase of joint action now. Gathering and building a global grid database is a prerequisite for conducting research on GEI. Based on the requirement of global grid data management and application, combining with big data and geographic information technology, this paper studies the global grid data acquisition and analysis process, sorts out and designs the global grid database structure supporting GEI research, and builds a global grid database system.",2018,,https://doi.org/10.14171/j.2096-5117.gei.2018.01.011
577,Damini Dey and Piotr J. Slomka and Paul Leeson and Dorin Comaniciu and Sirish Shrestha and Partho P. Sengupta and Thomas H. Marwick,Artificial Intelligence in Cardiovascular Imaging: JACC State-of-the-Art Review,"artificial intelligence, cardiovascular imaging, deep learning, machine learning","Data science is likely to lead to major changes in cardiovascular imaging. Problems with timing, efficiency, and missed diagnoses occur at all stages of the imaging chain. The application of artificial intelligence (AI) is dependent on robust data; the application of appropriate computational approaches and tools; and validation of its clinical application to image segmentation, automated measurements, and eventually, automated diagnosis. AI may reduce cost and improve value at the stages of image acquisition, interpretation, and decision-making. Moreover, the precision now possible with cardiovascular imaging, combined with “big data” from the electronic health record and pathology, is likely to better characterize disease and personalize therapy. This review summarizes recent promising applications of AI in cardiology and cardiac imaging, which potentially add value to patient care.",2019,,https://doi.org/10.1016/j.jacc.2018.12.054
578,Junfeng Wang,Chapter 8 - Real-Time Monitoring and Early Warning of a Train’s Running State and Operation Behavior,"High-speed railway, big data, train running status, monitoring, early warning","In the view of the whole system this chapter introduces the train state monitoring and early warning, which is based on big data and combines big data theory with human and signaling systems, comprehensively considering the coordination among the TCC, CBI, CTC, and other subsystems. We analyze the factors that can affect the train state of operation systematically, including the operation action of the signaling system and the operator, realizing the real-time and online monitoring and early warning of train running state then to ensure the safety of train operation.",2018,,https://doi.org/10.1016/B978-0-12-813304-0.00008-6
579,Hassiba Laifa and Raoudha khcherif and Henda Hajjami {Ben Ghezalaa},Train delay prediction in Tunisian railway through LightGBM model,"Delay prediction, Data Analysis, Machine learning, LightGBM","Train delays are one of the most important problems in the railway systems across the world, which urges the development of predictive analysis-based approaches to estimate it. In fact, with the advanced big data analysis and machine learning tools and technologies, the train delay-prediction systems can process and extract useful information from the large historical train movement data collected by the railway information system. Besides, accurate prediction of train delays can help train dispatchers make decisions through timetable rescheduling and service reliability improving. We propose, in this manuscript, a machine-learning model that captures the relationship between the arrival delay of passenger trains and the various characteristics of the railway system. We also apply, for the first time, lightGBM regressor based on optimal hyper-parameters to predict train delays. To evaluate the introduced model performance, the latter is compared with that of some other widely used existing models. Its R-squared, RMSE and RME were also compared with those of Support Vector Machine, Random Forest, XGBboost and Artificial Neural Network models. Statistical comparison indicates that the LightGBM outperforms the other models and is the fastest.",2021,,https://doi.org/10.1016/j.procs.2021.08.101
580,P Savolainen and J Magnusson and M. Gopalakrishnan and E. {Turanoglu Bekar} and A. Skoogh,Organisational Constraints in Data-driven Maintenance: a case study in the automotive industry,"Maintenance Management, Decision Support, Data Quality, Data-driven Decisions, Organisational Factors, Smart Maintenance","Technological development and innovations has been the focus of research in the field of smart maintenance, whereas there is less research regarding how maintenance organisations adapt the development. This case study focuses to understand what constraints maintenance organisations in the transition into applying more data-driven decisions in maintenance. This paper aims to emphasize the organisational challenges in data-driven maintenance, such as trustworthiness of data-driven decisions, data quality, management and competences. Through a case study at a global company in the automotive industry these challenges are highlighted and discussed through a questionnaire survey participated by 72 people and interviews with 7 people from the maintenance organisation.",2020,,https://doi.org/10.1016/j.ifacol.2020.11.015
581,C. Hurter and S. Conversy and D. Gianazza and A.C. Telea,Interactive image-based information visualization for aircraft trajectory analysis,"Trajectory manipulation, Air traffic control, Image based techniques, Information visualization","Objectives: The objective of the presented work is to present novel methods for big data exploration in the Air Traffic Control (ATC) domain. Data is formed by sets of airplane trajectories, or trails, which in turn records the positions of an aircraft in a given airspace at several time instants, and additional information such as flight height, speed, fuel consumption, and metadata (e.g. flight ID). Analyzing and understanding this time-dependent data poses several non-trivial challenges to information visualization. Materials and methods: To address this Big Data challenge, we present a set of novel methods to analyze aircraft trajectories with interactive image-based information visualization techniques.As a result, we address the scalability challenges in terms of data manipulation and open questions by presenting a set of related visual analysis methods that focus on decision-support in the ATC domain. All methods use image-based techniques, in order to outline the advantages of such techniques in our application context, and illustrated by means of use-cases from the ATC domain. Results: For each considered use-case, we outline the type of questions posed by domain experts, data involved in addressing these questions, and describe the specific image-based techniques we used to address these questions. Further, for each of the proposed techniques, we describe the visual representation and interaction mechanisms that have been used to address the above-mentioned goals. We illustrate these use-cases with real-life datasets from the ATC domain, and show how our techniques can help end-users in the ATC domain discover new insights, and solve problems, involving the presented datasets.",2014,,https://doi.org/10.1016/j.trc.2014.03.005
582,,Chapter One - Introduction,"RDF, Web of Data, Semantic Web, Big Data, Data management",This chapter motivates the importance of RDF data management through the Big Data and Web of Data/Semantic Web phenomena. It also provides some insights of existing RDF stores and presents the dimensions used in this book to compare these systems.,2015,,https://doi.org/10.1016/B978-0-12-799957-9.00001-8
583,Billy Pik Lik Lau and Sumudu Hasala Marakkalage and Yuren Zhou and Naveed Ul Hassan and Chau Yuen and Meng Zhang and U-Xuan Tan,A survey of data fusion in smart city applications,"Data fusion, Sensor fusion, Smart city, Big data, Internet of things, Multi-perspectives classification","The advancement of various research sectors such as Internet of Things (IoT), Machine Learning, Data Mining, Big Data, and Communication Technology has shed some light in transforming an urban city integrating the aforementioned techniques to a commonly known term - Smart City. With the emergence of smart city, plethora of data sources have been made available for wide variety of applications. The common technique for handling multiple data sources is data fusion, where it improves data output quality or extracts knowledge from the raw data. In order to cater evergrowing highly complicated applications, studies in smart city have to utilize data from various sources and evaluate their performance based on multiple aspects. To this end, we introduce a multi-perspectives classification of the data fusion to evaluate the smart city applications. Moreover, we applied the proposed multi-perspectives classification to evaluate selected applications in each domain of the smart city. We conclude the paper by discussing potential future direction and challenges of data fusion integration.",2019,,https://doi.org/10.1016/j.inffus.2019.05.004
584,Dimitrios Kourtesis and Jose María Alvarez-Rodríguez and Iraklis Paraskakis,Semantic-based QoS management in cloud systems: Current status and future challenges,"Cloud systems, Quality of service, Service oriented architectures, Semantics, Ontologies, Linked data, Sensor data, Big data","Cloud Computing and Service Oriented Architectures have seen a dramatic increase of the amount of applications, services, management platforms, data, etc. gaining momentum for the necessity of new complex methods and techniques to deal with the vast heterogeneity of data sources or services. In this sense Quality of Service (QoS) seeks for providing an intelligent environment of self-management components based on domain knowledge in which cloud components can be optimized easing the transition to an advanced governance environment. On the other hand, semantics and ontologies have emerged to afford a common and standard data model that eases the interoperability, integration and monitoring of knowledge-based systems. Taking into account the necessity of an interoperable and intelligent system to manage QoS in cloud-based systems and the emerging application of semantics in different domains, this paper reviews the main approaches for semantic-based QoS management as well as the principal methods, techniques and standards for processing and exploiting diverse data providing advanced real-time monitoring services. A semantic-based framework for QoS management is also outlined taking advantage of semantic technologies and distributed datastream processing techniques. Finally a discussion of existing efforts and challenges is also provided to suggest future directions.",2014,,https://doi.org/10.1016/j.future.2013.10.015
585,Benjamin Schleich and Kristina Wärmefjord and Rikard Söderberg and Sandro Wartzack,Geometrical Variations Management 4.0: towards next Generation Geometry Assurance,"Industry 4.0, Digital Twin, Geometry Assurance","Product realization processes are undergoing radical change considering the increasing digitalization of manufacturing fostered by cyber-physical production systems, the internet of things, big data, cloud computing, and the advancing use of digital twins. These trends are subsumed under the term “industry 4.0” describing the vision of a digitally connected manufacturing environment. The contribution gives an overview of future challenges and potentials for next generation geometry assurance and geometrical variations management in the context of industry 4.0. Particularly, the focus is set on potentials and risks of increasingly available manufacturing data and the use of digital twins in geometrical variations management.",2018,,https://doi.org/10.1016/j.procir.2018.04.078
586,J.R. López-Robles and J.R. Otegi-Olaso and I. {Porto Gómez} and M.J. Cobo,30 years of intelligence models in management and business: A bibliometric review,"Business Intelligence, Competitive Intelligence, Strategic Intelligence, Science information management, Mapping analysis","The critical factors in the big data era are collection, analysis, and dissemination of information to improve an organization’s competitive position and enhance its products and services. In this scenario, it is imperative that organizations use Intelligence, which is understood as a process of gathering, analyzing, interpreting, and disseminating high-value data and information at the right time for use in the decision-making process. Earlier, the concept of Intelligence was associated with the military and national security sector; however, in present times, and as organizations evolve, Intelligence has been defined in several ways for the purposes of different applications. Given that the purpose of Intelligence is to obtain real value from data, information, and the dynamism of the organizations, the study of this discipline provides an opportunity to analyze the core trends related to data collection and processing, information management, decision-making process, and organizational capabilities. Therefore, the present study makes a conceptual analysis of the existing definitions of intelligence in the literature by quantifying the main bibliometric performance indicators, identifying the main authors and research areas, and evaluating the development of the field using SciMAT as a bibliometric analysis software.",2019,,https://doi.org/10.1016/j.ijinfomgt.2019.01.013
587,Jesús Rojo and Jose Oteros and Rosa Pérez-Badia and Patricia Cervigón and Zuzana Ferencova and A. Monserrat Gutiérrez-Bustillo and Karl-Christian Bergmann and Gilles Oliver and Michel Thibaudon and Roberto Albertini and David {Rodríguez-De la Cruz} and Estefanía Sánchez-Reyes and José Sánchez-Sánchez and Anna-Mari Pessi and Jukka Reiniharju and Annika Saarto and M. Carmen Calderón and César Guerrero and Daniele Berra and Maira Bonini and Elena Chiodini and Delia Fernández-González and José García and M. Mar Trigo and Dorota Myszkowska and Santiago Fernández-Rodríguez and Rafael Tormo-Molina and Athanasios Damialis and Franziska Kolek and Claudia Traidl-Hoffmann and Elena Severova and Elsa Caeiro and Helena Ribeiro and Donát Magyar and László Makra and Orsolya Udvardy and Purificación Alcázar and Carmen Galán and Katarzyna Borycka and Idalia Kasprzyk and Ed Newbigin and Beverley Adams-Groom and Godfrey P. Apangu and Carl A. Frisk and Carsten A. Skjøth and Predrag Radišić and Branko Šikoparija and Sevcan Celenk and Carsten B. Schmidt-Weber and Jeroen Buters,Near-ground effect of height on pollen exposure,"Height, Pollen, Aerobiology, Monitoring network, Big data","The effect of height on pollen concentration is not well documented and little is known about the near-ground vertical profile of airborne pollen. This is important as most measuring stations are on roofs, but patient exposure is at ground level. Our study used a big data approach to estimate the near-ground vertical profile of pollen concentrations based on a global study of paired stations located at different heights. We analyzed paired sampling stations located at different heights between 1.5 and 50 m above ground level (AGL). This provided pollen data from 59 Hirst-type volumetric traps from 25 different areas, mainly in Europe, but also covering North America and Australia, resulting in about 2,000,000 daily pollen concentrations analyzed. The daily ratio of the amounts of pollen from different heights per location was used, and the values of the lower station were divided by the higher station. The lower station of paired traps recorded more pollen than the higher trap. However, while the effect of height on pollen concentration was clear, it was also limited (average ratio 1.3, range 0.7–2.2). The standard deviation of the pollen ratio was highly variable when the lower station was located close to the ground level (below 10 m AGL). We show that pollen concentrations measured at >10 m are representative for background near-ground levels.",2019,,https://doi.org/10.1016/j.envres.2019.04.027
588,Md Raihan Mia and Abu Sayed Md Latiful Hoque and Shahidul Islam Khan and Sheikh Iqbal Ahamed,A privacy-preserving National Clinical Data Warehouse: Architecture and analysis,"Privacy, Standardization, Clinical data warehousing, Data modeling, Big data analytics, Decision supports","A centralized clinical data repository is essential for inspecting patients’ medical history, disease analysis, population-wide disease research, treatment decision support, and improving existing healthcare policies and services. Bangladesh, a rapidly developing country, poses several unusual challenges for developing such a centralized clinical data repository as the existing Electronic Health Records (EHR) are stored in unconnected, heterogeneous sources with no unique patient identifier and consistency. Data integration with secure record linkage, privacy preservation, quality control, and data standardization are the main challenges for developing a consistent and interoperable centralized clinical data repository. Based on the findings from our previous researches, we have designed an anonymous National Clinical Data Warehouse (NCDW) framework to reinforce research and analysis. The architecture of NCDW is divided into five stages to overcome the challenges: (1) Wrapper-based anonymous data acquisition; (2) Data loading and staging; (3) Transformation, standardization, and uploading to the data warehouse; (4) Management and monitoring; (5) Data Mart design, OLAP server, data mining, and applications. A prototype of NCDW has been developed with a complete pipeline from data collection to analytics by integrating three data sources. The proposed NCDW model facilitates regional and national decision support, intelligent disease analysis, knowledge discovery, and data-driven research. We have inspected the analytical efficacy of the framework by qualitative evaluation of the national decision support from two derived disease data marts. The experimental result based on the analysis is satisfactory to extend the NCDW on a large scale.",2022,,https://doi.org/10.1016/j.smhl.2021.100238
589,Ngakan {Nyoman Kutha Krisnawijaya} and Bedir Tekinerdogan and Cagatay Catal and Rik van der Tol,Data analytics platforms for agricultural systems: A systematic literature review,"Data analytics platforms, Agriculture, Systematic literature review, Big Data","With the rapid developments in ICT, the current agriculture businesses have become increasingly data-driven and are supported by advanced data analytics techniques. In this context, several studies have investigated the adopted data analytics platforms in the agricultural sector. However, the main characteristics and overall findings on these platforms are scattered over the various studies, and to the best of our knowledge, there has been no attempt yet to systematically synthesize the features and obstacles of the adopted data analytics platforms. This article presents the results of an in-depth systematic literature review (SLR) that has explicitly focused on the domains of the platforms, the stakeholders, the objectives, the adopted technologies, the data properties and the obstacles. According to the year-wise analysis, it is found that no relevant primary study between 2010 and 2013 was found. This implies that the research of data analytics in agricultural sectors is a popular topic from recent years, so the results from before 2010 are likely less relevant. In total, 535 papers published from 2010 to 2020 were retrieved using both automatic and manual search strategies, among which 45 journal articles were selected for further analysis. From these primary studies, 33 features and 34 different obstacles were identified. The identified features and obstacles help characterize the different data analytics platforms and pave the way for further research.",2022,,https://doi.org/10.1016/j.compag.2022.106813
590,Soufiane Faieq and Rajaa Saidi and Hamid Elghazi and Moulay Driss Rahmani,C2IoT: A framework for Cloud-based Context-aware Internet of Things services for smart cities,"Internet of Things, Cloud Computing, Context-Awareness, Big Data, Service Composition, Smart City","The smart city vision was born by the integration of ICT in the day to day city management operations and citizens lives, owing to the need for novel and smart ways to manage the cities resources; making them more efficient, sustainable and transparent. However, the understanding of the crucial elements to this integration and how they can benefit from each other proves difficult and unclear. In this article, we investigate the intricate synergies between different technologies and paradigms involved in the smart city vision, to help design a robust framework, capable of handling the challenges impeding its successful implementation. To this end, we propose a context-aware centered approach to present a holistic view of a smart city as viewed from the different angles (Cloud, IoT, Big Data). We also propose a framework encompassing elements from the different enablers, leveraging their strengths to build and develop smart-x applications and services.",2017,,https://doi.org/10.1016/j.procs.2017.06.072
591,Xin Li and Ning Zhao and Rui Jin and Shaomin Liu and Xiaomin Sun and Xuefa Wen and Dongxiu Wu and Yan Zhou and Jianwen Guo and Shiping Chen and Ziwei Xu and Mingguo Ma and Tianming Wang and Yonghua Qu and Xinwei Wang and Fangming Wu and Yuke Zhou,Internet of Things to network smart devices for ecosystem monitoring,"Ecosystem monitoring, Fragile ecosystem, Internet of Things, Wireless sensor network, Smart device","Smart, real-time, low-cost, and distributed ecosystem monitoring is essential for understanding and managing rapidly changing ecosystems. However, new techniques in the big data era have rarely been introduced into operational ecosystem monitoring, particularly for fragile ecosystems in remote areas. We introduce the Internet of Things (IoT) techniques to establish a prototype ecosystem monitoring system by developing innovative smart devices and using IoT technologies for ecosystem monitoring in isolated environments. The developed smart devices include four categories: large-scale and nonintrusive instruments to measure evapotranspiration and soil moisture, in situ observing systems for CO2 and δ13C associated with soil respiration, portable and distributed devices for monitoring vegetation variables, and Bi-CMOS cameras and pressure trigger sensors for terrestrial vertebrate monitoring. These new devices outperform conventional devices and are connected to each other via wireless communication networks. The breakthroughs in the ecosystem monitoring IoT include new data loggers and long-distance wireless sensor network technology that supports the rapid transmission of data from devices to wireless networks. The applicability of this ecosystem monitoring IoT is verified in three fragile ecosystems, including a karst rocky desertification area, the National Park for Amur Tigers, and the oasis-desert ecotone in China. By integrating these devices and technologies with an ecosystem monitoring information system, a seamless data acquisition, transmission, processing, and application IoT is created. The establishment of this ecosystem monitoring IoT will serve as a new paradigm for ecosystem monitoring and therefore provide a platform for ecosystem management and decision making in the era of big data.",2019,,https://doi.org/10.1016/j.scib.2019.07.004
592,Guangjie Chen and Zhiqiang Ge,Robust Bayesian networks for low-quality data modeling and process monitoring applications,"Robust Bayesian network, Data quality feature, Process monitoring, Fault diagnosis","In this paper, a novel robust Bayesian network is proposed for process modeling with low-quality data. Since unreliable data can cause model parameters to deviate from the real distributions and make network structures unable to characterize the true causalities, data quality feature is utilized to improve the process modeling and monitoring performance. With a predetermined trustworthy center, the data quality measurement results can be evaluated through an exponential function with Mahalanobis distances. The conventional Bayesian network learning algorithms including structure learning and parameter learning are modified by the quality feature in a weighting form, intending to extract useful information and make a reasonable model. The effectiveness of the proposed method is demonstrated through TE benchmark process and a real industrial process.",2020,,https://doi.org/10.1016/j.conengprac.2020.104344
593,Gregory Vial,Reflections on quality requirements for digital trace data in IS research,"Digital trace data, Data quality, GitHub","In recent years an increasing number of academic disciplines, including IS, have sourced digital trace data for their research. Notwithstanding the potential of such data in (re)investigations of various phenomena of interest that would otherwise be difficult or impossible to study using other sources of data, we view the quality of digital trace data as an underappreciated issue in IS research. To initiate a discussion of how to evaluate and report on the quality of digital trace data in IS research, we couch our arguments within the broader tradition of research on data quality. We explain how the uncontrolled nature of digital trace data creates unique challenges for IS researchers, who need to collect, store, retrieve, and transform those data for the purpose of numerical analysis. We then draw parallels with concepts and patterns commonly used in data analysis projects and argue that, although IS researchers probably apply such concepts and patterns, this is not reported in publications, undermining the reader's ability to assess the reliability, statistical power and replicability of the findings. Using the case of GitHub to illustrate such challenges, we develop a preliminary set of guidelines to help researchers consider and report on the quality of the digital trace data they use in their research. Our work contributes to the debate on data quality and provides relevant recommendations for scholars and IS journals at a time when a growing number of publications are relying on digital trace data.",2019,,https://doi.org/10.1016/j.dss.2019.113133
594,Elaheh Gholamzadeh Nabati and Klaus-Dieter Thoben,Data Driven Decision Making in Planning the Maintenance Activities of Off-shore Wind Energy,"Maintenance, (Big) data analysis, Off-shore wind turbines, Decision making","Planning and scheduling for wind farms play a critical role in the costs of maintenance. The use and analysis of field data or so-called Product Use Information (PUI) to improve maintenance activities and to reduce the costs has gained attention in the recent years. The product use data consist of sources such as measure of sensors on the turbines, the alarms information or signals from the condition monitoring, Supervisory Control and Data Acquisition (SCADA) systems, which are currently used in maintenance activities. However, those data have the potential to offer alternative solutions to improve processes and provide better decisions, by transforming them into actionable knowledge. In order to make the right decision it is important to understand, which PUI data source and which data analysis methods, are suitable for what kind of decision making task. The aim of this study is to discover, how analysis of PUI can help in the maintenance processes of off-shore wind power. The techniques from the field of big data analytics for analyzing the PUI are here addressed. The results of this study contain suggestions on the basis of algorithms of data analytics, suitable for each decision type.",2017,,https://doi.org/10.1016/j.procir.2016.09.026
595,Devin S. Zarkowsky and David P. Stonko,Artificial intelligence's role in vascular surgery decision-making,,"ABSTRACT
Artificial intelligence (AI) is the next great advance informing medical science. Several disciplines, including vascular surgery, use AI-based decision-making tools to improve clinical performance. Although applied widely, AI functions best when confronted with voluminous, accurate data. Consistent, predictable analytic technique selection also challenges researchers. This article contextualizes AI analyses within evidence-based medicine, focusing on “big data” and health services research, as well as discussing opportunities to improve data collection and realize AI's promise.",2021,,https://doi.org/10.1053/j.semvascsurg.2021.10.005
596,P. Amuthabala and R. Santhosh,Robust analysis and optimization of a novel efficient quality assurance model in data warehousing,"Data warehouse, Distributed, Data complexity, Data quality, Quality assurance, Optimization, Machine learning","The significance of distributed data warehouses is to initiate the proliferation of various analytical applications. However, with the increase of ubiquitous devices, it is likely that massive volumes of data will be generated, which poses further problems based on the degradation of data quality. The practical reasons for the degradation of data quality in distributed warehouses are identified as heterogeneous data, uncertain inferior data which further affect predictions. The proposed system presents an integrated optimization model to address all the quality degradation problems and to provide a better computational model which effectively incorporates a higher degree of quality assurance. An analytical methodology is adopted in order to develop the proposed quality assurance model for distributed data warehouses.",2019,,https://doi.org/10.1016/j.compeleceng.2019.02.003
597,Louis Ehwerhemuepha and Kimberly Carlson and Ryan Moog and Ben Bondurant and Cheryl Akridge and Tatiana Moreno and Gary Gasperino and William Feaster,Cerner real-world data (CRWD) - A de-identified multicenter electronic health records database,"Cerner Real-World Data(CRWD), COVID-19, SARS-CoV-2, Electronic Health Records (EHR), HealtheIntent, HealtheDataLab™, Cerner learning Health Network (LHN)","Cerner Real-World DataTM (CRWD) is a de-identified big data source of multicenter electronic health records. Cerner Corporation secured appropriate data use agreements and permissions from more than 100 health systems in the United States contributing to the database as of March 2022. A subset of the database was extracted to include data from only patients with SARS-CoV-2 infections and is referred to as the Cerner COVID-19 Dataset. The December 2021 version of CRWD consists of 100 million patients and 1.5 billion encounters across all care settings. There are 2.3 billion, 2.9 billion, 486 million, and 11.5 billion records in the condition, medication, procedure, and lab (laboratory test) tables respectively. The 2021 Q3 COVID-19 Dataset consists of 130.1 million encounters from 3.8 million patients. The size and longitudinal nature of CRWD can be leveraged for advanced analytics and artificial intelligence in medical research across all specialties and is a rich source of novel discoveries on a wide range of conditions including but not limited to COVID-19.",2022,,https://doi.org/10.1016/j.dib.2022.108120
598,Gillian Harrison and Susan M. Grant-Muller and Frances C. Hodgson,New and emerging data forms in transportation planning and policy: Opportunities and challenges for “Track and Trace” data,"Transport policy, Track and Trace, Mobile phone data, Mobility profile, Big Data","High quality, reliable data and robust models are central to the development and appraisal of transportation planning and policy. Although conventional data may offer good ‘content’, it is widely observed that it lacks context i.e. who and why people are travelling. Transportation modelling has developed within these boundaries, with implications for the planning, design and management of transportation systems and policy-making. This paper establishes the potential of passively collected GPS-based “Track & Trace” (T&T) datasets of individual mobility profiles towards enhancing transportation modelling and policy-making. T&T is a type of New and Emerging Data Form (NEDF), lying within the broader ‘Big Data’ paradigm, and is typically collected using mobile phone sensors and related technologies. These capture highly grained mobility content and can be linked to the phone owner/user behavioural choices and other individual context. Our meta-analysis of existing literature related to spatio-temporal mobile phone data demonstrates that NEDF’s, and in particular T&T data, have had little mention to date within an applied transportation planning and policy context. We thus establish there is an opportunity for policy-makers, transportation modellers, researchers and a wide range of stakeholders to collaborate in developing new analytic approaches, revise existing models and build the skills and related capacity needed to lever greatest value from the data, as well as to adopt new business models that could revolutionise citizen participation in policy-making. This is of particular importance due to the growing awareness in many countries for a need to develop and monitor efficient cross-sectoral policies to deliver sustainable communities.",2020,,https://doi.org/10.1016/j.trc.2020.102672
599,Anna-Valentini Michailidou and Anastasios Gounaris and Moysis Symeonides and Demetris Trihinas,EQUALITY: Quality-aware intensive analytics on the edge,"Fog computing, Optimization, Sensors, Data quality","Our work is motivated by the fact that there is an increasing need to perform complex analytics jobs over streaming data as close to the edge devices as possible and, in parallel, it is important that data quality is considered as an optimization objective along with performance metrics. In this work, we develop a solution that trades latency for an increased fraction of incoming data, for which data quality-related measurements and operations are performed, in jobs running over geo-distributed heterogeneous and constrained resources. Our solution is hybrid: on the one hand, we perform search heuristics over locally optimal partial solutions to yield an enhanced global solution regarding task allocations; on the other hand, we employ a spring relaxation algorithm to avoid unnecessarily increased degree of partitioned parallelism. Through thorough experiments, we show that we can improve upon state-of-the-art solutions in terms of our objective function that combines latency and extent of quality checks by up to 2.56X. Moreover, we implement our solution within Apache Storm, and we perform experiments in an emulated setting. The results show that we can reduce the latency in 86.9% of the cases examined, while latency is up to 8 times lower compared to the built-in Storm scheduler, with the average latency reduction being 52.5%.",2022,,https://doi.org/10.1016/j.is.2021.101953
600,Alexander Borek and Ajith K. Parlikad and Jela Webb and Philip Woodall,Chapter 2 - Enterprise Information Management,"EIM Strategy, EIM Governance, EIM Components, Big Data and EIM, Challenges for EIM","This chapter gives an introduction to concept of enterprise information management, investigates the influence of Big Data on EIM, and discusses today's key challenges and pressures for EIM.",2014,,https://doi.org/10.1016/B978-0-12-405547-6.00002-X
601,Andrés Villa-Henriksen and Gareth T.C. Edwards and Liisa A. Pesonen and Ole Green and Claus Aage Grøn Sørensen,"Internet of Things in arable farming: Implementation, applications, challenges and potential","Smart farming, Internet of things, Wireless sensor network, Farm management information system, Big data, Machine learning","The Internet of Things is allowing agriculture, here specifically arable farming, to become data-driven, leading to more timely and cost-effective production and management of farms, and at the same time reducing their environmental impact. This review is addressing an analytical survey of the current and potential application of Internet of Things in arable farming, where spatial data, highly varying environments, task diversity and mobile devices pose unique challenges to be overcome compared to other agricultural systems. The review contributes an overview of the state of the art of technologies deployed. It provides an outline of the current and potential applications, and discusses the challenges and possible solutions and implementations. Lastly, it presents some future directions for the Internet of Things in arable farming. Current issues such as smart phones, intelligent management of Wireless Sensor Networks, middleware platforms, integrated Farm Management Information Systems across the supply chain, or autonomous vehicles and robotics stand out because of their potential to lead arable farming to smart arable farming. During the implementation, different challenges are encountered, and here interoperability is a key major hurdle throughout all the layers in the architecture of an Internet of Things system, which can be addressed by shared standards and protocols. Challenges such as affordability, device power consumption, network latency, Big Data analysis, data privacy and security, among others, have been identified by the articles reviewed and are discussed in detail. Different solutions to all identified challenges are presented addressing technologies such as machine learning, middleware platforms, or intelligent data management.",2020,,https://doi.org/10.1016/j.biosystemseng.2019.12.013
602,Evagelos D. Lioutas and Chrysanthi Charatsari,Enhancing the ability of agriculture to cope with major crises or disasters: What the experience of COVID-19 teaches us,"Agriculture, COVID-19, Major crises, Smart technology, Community marketing, Resilience","The COVID-19 outbreak was an unprecedented situation that uncovered forgotten interconnections and interdependencies between agriculture, society, and economy, whereas it also brought to the fore the vulnerability of agrifood production to external disturbances. Building upon the ongoing experience of the COVID-19 pandemic, in this short communication, we discuss three potential mechanisms that, in our opinion, can mitigate the impacts of major crises or disasters in agriculture: resilience-promoting policies, community marketing schemes, and smart farming technology. We argue that resilience-promoting policies should focus on the development of crisis management plans and enhance farmers' capacity to cope with external disturbances. We also stress the need to promote community marketing conduits that ensure an income floor for farmers while in parallel facilitating consumer access to agrifood products when mainstream distribution channels under-serve them. Finally, we discuss some issues that need to be solved to ensure that smart technology and big data can help farmers overcome external shocks.",2021,,https://doi.org/10.1016/j.agsy.2020.103023
603,Roberto O Andrade and Sang Guun Yoo,Cognitive security: A comprehensive study of cognitive science in cybersecurity,"Cognitive security, Cognitive science, Situation awareness, Cyber operations","Nowadays, IoT, cloud computing, mobile and social networks are generating a transformation in social processes. Nevertheless, this technological change rise to new threats and security attacks that produce new and complex cybersecurity scenarios with large volumes of data and different attack vectors that can exceeded the cognitive skills of security analysts. In this context, cognitive sciences can enhance the cognitive processes, which can help to security analysts to establish actions in less time and more efficiently within cybersecurity operations. This works presents a cognitive security model that integrates technological solutions such as Big Data, Machine Learning, and Support Decision Systems with the cognitive processes of security analysts used to generate knowledge, understanding and execution of security response actions. The model considers alternatives to establish the automation process in the execution of cognitive tasks defined in the cyber operations processes and includes the analyst as the central axis in the processes of validation and decision making through the use of MAPE-K, OODA and Human in the Loop.",2019,,https://doi.org/10.1016/j.jisa.2019.06.008
604,N. Omri and Z. {Al Masry} and N. Mairot and S. Giampiccolo and N. Zerhouni,Industrial data management strategy towards an SME-oriented PHM,"Small and medium-sized enterprises, Data-driven PHM, Industrial data management, Data quality metrics, PHM implementation strategy","The fourth industrial revolution is derived from advances in digitization and prognostic and health management (PHM) disciplines to make plants smarter and more efficient. However, an adapted approach for data-driven PHM process implementation in small and medium-sized enterprises (SMEs) has not been yet discussed. This research gap is due to the specificities of SMEs and the lack of documentation. In this paper, we examine existing standards for implementing PHM in the industrial field and discuss the limitations within SMEs. Based on that, a novel strategy to implement a data-driven PHM approach in SMEs is proposed. Accordingly, the data management process and the impact of data quality are reviewed to address some critical data problems in SMEs (e.g., data volume and data accuracy). A first set of simulations was carried out to study the impact of the data volume and percentage of missing data on classification problems in PHM. A general model of the evolution of the results accuracy in function of data volume and missing data is then generated, and an economic data volume notion is proposed for data infrastructure resizing. The proposed strategy and the developed models are then applied to the Scoder enterprise, which is a French SME. The feedback on the first results of this application is reported and discussed.",2020,,https://doi.org/10.1016/j.jmsy.2020.04.002
605,Ming-Lang Tseng and Tat-Dat Bui and Shulin Lan and Ming K. Lim and Abu Hashan Md Mashud,Smart product service system hierarchical model in banking industry under uncertainties,"Smart product-service systems, Digital technology, Sustainable innovation, Fuzzy delphi method, Decision-making trial and evaluation laboratory (DEMATEL), Diffusion of innovation theory","This study adopts the diffusion of innovation theory as to develop the smart product service system model in banking industry due to prior studies are lacking in identifying the attributes. The smart product service system functions are bearing high uncertainty and system complexity; hence, the hybrid method of fuzzy Delphi method and fuzzy decision-making trial and evaluation laboratory to construct a valid hierarchical model and identified the causal interrelationships among the attributes. The smart product service system hierarchical model with eight aspects and 41 criteria are proposed enriching the existing literature and that identify appropriate strategies to achieve operational performance. The results show that seven aspects and 22 criteria are determined as the valid hierarchical model. The institutional compression, digital platform operation, and e-knowledge management are the causing aspects helps to form smart product service system operational performance in high uncertainty. For practices, the banking decision-makers should develop innovative actions relied on the forcible compression, cyber-physical systems, industrial big data, cloud service allocation and sharing, and transparency improvement as they are most importance criteria playing a decisive role in a successful SPSS. This provides guidelines for banking industry practice in Taiwan encouraging the miscellany of digital technology accomplishment for sustainable target.",2021,,https://doi.org/10.1016/j.ijpe.2021.108244
606,Rohit Sharma and Sachin S. Kamble and Angappa Gunasekaran,Big GIS analytics framework for agriculture supply chains: A literature review identifying the current trends and future perspectives,"Agriculture supply chain, GIS analytics, Big data analytics, Internet of things, Drones, Smart farming","The world population is estimated to reach nine billion by 2050. Many challenges are adding pressure on the current agriculture supply chains that include shrinking land sizes, ever increasing demand for natural resources and environmental issues. The agriculture systems need a major transformation from the traditional practices to precision agriculture or smart farming practices to overcome these challenges. Geographic information system (GIS) is one such technology that pushes the current methods to precision agriculture. In this paper, we present a systematic literature review (SLR) of 120 research papers on various applications of big GIS analytics (BGA) in agriculture. The selected papers are classified into two broad categories; the level of analytics and GIS applications in agriculture. The GIS applications viz., land suitability, site search and selection, resource allocation, impact assessment, land allocation, and knowledge-based systems are considered in this study. The outcome of this study is a proposed BGA framework for agriculture supply chain. This framework identifies big data analytics to play a significant role in improving the quality of GIS application in agriculture and provides the researchers, practitioners, and policymakers with guidelines on the successful management of big GIS data for improved agricultural productivity.",2018,,https://doi.org/10.1016/j.compag.2018.10.001
607,Laura Bravo-Merodio and Animesh Acharjee and Dominic Russ and Vartika Bisht and John A. Williams and Loukia G. Tsaprouni and Georgios V. Gkoutos,Chapter Four - Translational biomarkers in the era of precision medicine,"Translational biomarkers, Omics, Big data, Artificial intelligence, Clinical trials","In this chapter we discuss the past, present and future of clinical biomarker development. We explore the advent of new technologies, paving the way in which health, medicine and disease is understood. This review includes the identification of physicochemical assays, current regulations, the development and reproducibility of clinical trials, as well as, the revolution of omics technologies and state-of-the-art integration and analysis approaches.",2021,,https://doi.org/10.1016/bs.acc.2020.08.002
608,Eivind Kristoffersen and Patrick Mikalef and Fenna Blomsma and Jingyue Li,Towards a business analytics capability for the circular economy,"Digital circular economy, Sustainability, Big data analytics, Competitive advantage, Resource-based view, Expert interviews","Digital technologies are growing in importance for accelerating firms’ circular economy transition. However, so far, the focus has primarily been on the technical aspects of implementing these technologies with limited research on the organizational resources and capabilities required for successfully leveraging digital technologies for circular economy. To address this gap, this paper explores the business analytics resources firms should develop and how these should be orchestrated towards a firm-wide capability. The paper proposes a conceptual model highlighting eight business analytics resources that, in combination, build a business analytics capability for the circular economy and how this relates to firms’ circular economy implementation, resource orchestration capability, and competitive performance. The model is based on the results of a thematic analysis of 15 semi-structured expert interviews with key positions in industry. Our approach is informed by and further develops, the theory of the resource-based view and the resource orchestration view. Based on the results, we develop a deeper understanding of the importance of taking a holistic approach to business analytics when leveraging data and analytics towards a more efficient and effective digital-enabled circular economy, the smart circular economy.",2021,,https://doi.org/10.1016/j.techfore.2021.120957
609,Francisco J. Montáns and Francisco Chinesta and Rafael Gómez-Bombarelli and J. Nathan Kutz,Data-driven modeling and learning in science and engineering,"Data-driven science, Data-driven modeling, Artificial intelligence, Machine learning, Data-science, Big data","In the past, data in which science and engineering is based, was scarce and frequently obtained by experiments proposed to verify a given hypothesis. Each experiment was able to yield only very limited data. Today, data is abundant and abundantly collected in each single experiment at a very small cost. Data-driven modeling and scientific discovery is a change of paradigm on how many problems, both in science and engineering, are addressed. Some scientific fields have been using artificial intelligence for some time due to the inherent difficulty in obtaining laws and equations to describe some phenomena. However, today data-driven approaches are also flooding fields like mechanics and materials science, where the traditional approach seemed to be highly satisfactory. In this paper we review the application of data-driven modeling and model learning procedures to different fields in science and engineering.",2019,,https://doi.org/10.1016/j.crme.2019.11.009
610,Alexander G. Rumson and Stephen H. Hallett,Innovations in the use of data facilitating insurance as a resilience mechanism for coastal flood risk,"Risk analytics, Adaptation, Remote sensing, Big Data","Insurance plays a crucial role in human efforts to adapt to environmental hazards. Effective insurance can serve as both a measure to distribute, and a method to communicate risk. In order for insurance to fulfil these roles successfully, policy pricing and cover choices must be risk-based and founded on accurate information. This is reliant on a robust evidence base forming the foundation of policy choices. This paper focuses on the evidence available to insurers and emergent innovation in the use of data. The main risk considered is coastal flooding, for which the insurance sector offers an option for potential adaptation, capable of increasing resilience. However, inadequate supply and analysis of data have been highlighted as factors preventing insurance from fulfilling this role. Research was undertaken to evaluate how data are currently, and could potentially, be used within risk evaluations for the insurance industry. This comprised of 50 interviews with those working and associated with the London insurance market. The research reveals new opportunities, which could facilitate improvements in risk-reflective pricing of policies. These relate to a new generation of data collection techniques and analytics, such as those associated with satellite-derived data, IoT (Internet of Things) sensors, cloud computing, and Big Data solutions. Such technologies present opportunities to reduce moral hazard through basing predictions and pricing of risk on large empirical datasets. The value of insurers' claims data is also revealed, and is shown to have the potential to refine, calibrate, and validate models and methods. The adoption of such data-driven techniques could enable insurers to re-evaluate risk ratings, and in some instances, extend coverage to locations and developments, previously rated as too high a risk to insure. Conversely, other areas may be revealed more vulnerable, which could generate negative impacts for residents in these regions, such as increased premiums. However, the enhanced risk awareness generated, by new technology, data and data analytics, could positively alter future planning, development and investment decisions.",2019,,https://doi.org/10.1016/j.scitotenv.2019.01.114
611,Manmeet Mahinderjit Singh and Anizah Abu Bakar,A Systemic Cybercrime Stakeholders Architectural Model,"Big data, Internet of Things (IoT), Cyberspace, Routine Activity Theory, Systemic","The increased of cybercrime incidents taking place in the world is at its perilous magnitude causing losses in term of money and trust. Even though there are various cybersecurity solutions in place; the threat of cybercrime is still a hard problem. Exploration of cybercrime challenges, especially the preventions and detections of the cybercrime should be investigated by composing all the stakeholders and players of a cybercrime issue. In this paper; an exploration of several cybercrime stakeholders is done. It is argued that cybercrime is a systemic threat and cannot be tackled with cybersecurity and legal systems. The architectural model proposed is significant and should become one of the considered milestones in designing security control in tackling cybercrime globally.",2019,,https://doi.org/10.1016/j.procs.2019.11.227
612,Ziyad R. Alashhab and Mohammed Anbar and Manmeet Mahinderjit Singh and Yu-Beng Leau and Zaher Ali Al-Sai and Sami {Abu Alhayja’a},Impact of coronavirus pandemic crisis on technologies and cloud computing applications,"Big data privacy, Cloud computing (CC) applications, COVID-19, Digital transformation, Security challenge, Work from home","In light of the COVID-19 outbreak caused by the novel coronavirus, companies and institutions have instructed their employees to work from home as a precautionary measure to reduce the risk of contagion. Employees, however, have been exposed to different security risks because of working from home. Moreover, the rapid global spread of COVID-19 has increased the volume of data generated from various sources. Working from home depends mainly on cloud computing (CC) applications that help employees to efficiently accomplish their tasks. The cloud computing environment (CCE) is an unsung hero in the COVID-19 pandemic crisis. It consists of the fast-paced practices for services that reflect the trend of rapidly deployable applications for maintaining data. Despite the increase in the use of CC applications, there is an ongoing research challenge in the domains of CCE concerning data, guaranteeing security, and the availability of CC applications. This paper, to the best of our knowledge, is the first paper that thoroughly explains the impact of the COVID-19 pandemic on CCE. Additionally, this paper also highlights the security risks of working from home during the COVID-19 pandemic.",2021,,https://doi.org/10.1016/j.jnlest.2020.100059
613,Allan Lee and Ilke Inceoglu and Oliver Hauser and Michael Greene,Determining causal relationships in leadership research using Machine Learning: The powerful synergy of experiments and data science,"Leadership effectiveness, Leadership processes, Machine Learning, Artificial intelligence, Causality, Experiments, Big Data, Heterogeneous treatment effects","Machine Learning (ML) techniques offer exciting new avenues for leadership research. In this paper we discuss how ML techniques can be used to inform predictive and causal models of leadership effects and clarify why both types of model are important for leadership research. We propose combining ML and experimental designs to draw causal inferences by introducing a recently developed technique to isolate “heterogeneous treatment effects.” We provide a step-by-step guide on how to design studies that combine field experiments with the application of ML to establish causal relationships with maximal predictive power. Drawing on examples in the leadership literature, we illustrate how the suggested approach can be applied to examine the impact of, for example, leadership behavior on follower outcomes. We also discuss how ML can be used to advance leadership research from theoretical, methodological and practical perspectives and consider limitations.",2020,,https://doi.org/10.1016/j.leaqua.2020.101426
614,Bing Wang and Chao Wu,"Safety informatics as a new, promising and sustainable area of safety science in the information age","Safety science, Information science, Safety information, Safety informatics, Safety 4.0","Safety is a central dimension in contemporary debates on human health, loss prevention, environmental protection, sustainability, and cleaner production. In the information age, especially in the era of big data, safety information is an essential strategy for safety, and safety informatics has become a major research interest and a popular issue in the field of safety science. In recent years, safety informatics—a new area of safety science—has received increasing attention, developing greatly with successful research on the subject. The three key purposes of this paper are: (i) to analyze the historical development of safety informatics, (ii) to review the research progress of safety informatics, and (iii) to review limitations and propose future directions in the field of safety informatics. First, the development process of safety informatics is divided into four typical stages: (i) the embryonic stage (1940–1980), (ii) the initial stage (1980–1990), (iii) the formation stage (1990–2010), and (iv) the deepening stage (2010–present). Then, a review of safety informatics research is provided from seven aspects, including: (i) the discipline construction of safety informatics, (ii) theoretical safety information model, (iii) accident causation model from a safety information perspective, (iv) safety management based on safety information, (v) safety big data, (vi) safety intelligence, and (vii) safety information technology. Finally, limitations and future research directions in the safety informatics area are briefly discussed.",2020,,https://doi.org/10.1016/j.jclepro.2019.119852
615,Cheng Fan and Fu Xiao and Henrik Madsen and Dan Wang,Temporal knowledge discovery in big BAS data for building energy management,"Temporal knowledge discovery, Time series data mining, Big data, Building automation system, Building energy management","With the advances of information technologies, today's building automation systems (BASs) are capable of managing building operational performance in an efficient and convenient way. Meanwhile, the amount of real-time monitoring and control data in BASs grows continually in the building lifecycle, which stimulates an intense demand for powerful big data analysis tools in BASs. Existing big data analytics adopted in the building automation industry focus on mining cross-sectional relationships, whereas the temporal relationships, i.e., the relationships over time, are usually overlooked. However, building operations are typically dynamic and BAS data are essentially multivariate time series data. This paper presents a time series data mining methodology for temporal knowledge discovery in big BAS data. A number of time series data mining techniques are explored and carefully assembled, including the Symbolic Aggregate approXimation (SAX), motif discovery, and temporal association rule mining. This study also develops two methods for the efficient post-processing of knowledge discovered. The methodology has been applied to analyze the BAS data retrieved from a real building. The temporal knowledge discovered is valuable to identify dynamics, patterns and anomalies in building operations, derive temporal association rules within and between subsystems, assess building system performance and spot opportunities in energy conservation.",2015,,https://doi.org/10.1016/j.enbuild.2015.09.060
616,Flora D. Salim and Bing Dong and Mohamed Ouf and Qi Wang and Ilaria Pigliautile and Xuyuan Kang and Tianzhen Hong and Wenbo Wu and Yapan Liu and Shakila Khan Rumi and Mohammad Saiedur Rahaman and Jingjing An and Hengfang Deng and Wei Shao and Jakub Dziedzic and Fisayo Caleb Sangogboye and Mikkel Baun Kjærgaard and Meng Kong and Claudia Fabiani and Anna Laura Pisello and Da Yan,"Modelling urban-scale occupant behaviour, mobility, and energy in buildings: A survey","Big data, Occupant behaviour, Energy modelling, Mobility, Urban data, Sensors, Machine learning, Energy in buildings, Energy in cities","The proliferation of urban sensing, IoT, and big data in cities provides unprecedented opportunities for a deeper understanding of occupant behaviour and energy usage patterns at the urban scale. This enables data-driven building and energy models to capture the urban dynamics, specifically the intrinsic occupant and energy use behavioural profiles that are not usually considered in traditional models. Although there are related reviews, none investigated urban data for use in modelling occupant behaviour and energy use at multiple scales, from buildings to neighbourhood to city. This survey paper aims to fill this gap by providing a critical summary and analysis of the works reported in the literature. We present the different sources of occupant-centric urban data that are useful for data-driven modelling and categorise the range of applications and recent data-driven modelling techniques for urban behaviour and energy modelling, along with the traditional stochastic and simulation-based approaches. Finally, we present a set of recommendations for future directions in data-driven modelling of occupant behaviour and energy in buildings at the urban scale.",2020,,https://doi.org/10.1016/j.buildenv.2020.106964
617,Jiashun Mao and Javed Akhtar and Xiao Zhang and Liang Sun and Shenghui Guan and Xinyu Li and Guangming Chen and Jiaxin Liu and Hyeon-Nae Jeon and Min Sung Kim and Kyoung Tai No and Guanyu Wang,Comprehensive strategies of machine-learning-based quantitative structure-activity relationship models,"Data analysis in structural biology, Machine learning, Structural biology","Summary
Early quantitative structure-activity relationship (QSAR) technologies have unsatisfactory versatility and accuracy in fields such as drug discovery because they are based on traditional machine learning and interpretive expert features. The development of Big Data and deep learning technologies significantly improve the processing of unstructured data and unleash the great potential of QSAR. Here we discuss the integration of wet experiments (which provide experimental data and reliable verification), molecular dynamics simulation (which provides mechanistic interpretation at the atomic/molecular levels), and machine learning (including deep learning) techniques to improve QSAR models. We first review the history of traditional QSAR and point out its problems. We then propose a better QSAR model characterized by a new iterative framework to integrate machine learning with disparate data input. Finally, we discuss the application of QSAR and machine learning to many practical research fields, including drug development and clinical trials.",2021,,https://doi.org/10.1016/j.isci.2021.103052
618,Ghasan Fahim Huseien and Kwok Wei Shah,A review on 5G technology for smart energy management and smart buildings in Singapore,"5G technology, Sustainability, Smart building, Facilities management, Build environment","Sustainable and smart building is a recent concept that is gaining momentum in public opinion, and thus, it is making its way into the agendas of researchers and city authorities all over the world. To move towards sustainable development goals, 5G technology would make significant impacts are building construction, operation, and management by facilitating high-class services, providing efficient functionalities. It's well known that the Singapore is one of top smart cities in this world and from the first counties that adopted of 5G technology in various sectors including smart buildings. Based on these facts, this paper discusses the international trends in 5G applications for smart buildings, and R&D and test bedding works conducted in 5G labs. As well as, the manuscript widely reviewed and discussed the 5G technology development, use cases, applications and future projects which supported by Singapore government. Finally, the 5G use cases for smart buildings and build environment improvement application were discussed. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.",2022,,https://doi.org/10.1016/j.egyai.2021.100116
619,Leah Burns and Nadege Le Roux and Robert Kalesnik-Orszulak and Jennifer Christian and Mathias Hukkelhoven and Frank Rockhold and John O'Donnell,Real-World Evidence for Regulatory Decision-Making: Guidance From Around the World,"Efficiency, Product effectiveness, Real-world evidence, Regulatory decision making","Purpose
Interest in leveraging real-world evidence (RWE) to support regulatory decision making for product effectiveness has been increasing globally as evident by the increasing number of regulatory frameworks and guidance documents. However, acceptance of RWE, especially before marketing for regulatory approval, differs across countries. In addition, guidance on the design and conduct of innovative clinical trials, such as randomized controlled registry studies, pragmatic trials, and other hybrid studies, is lacking.
Methods
We assessed the global regulatory environment with regard to RWE based on regional availability of the following 3 key regulatory elements: (1) RWE regulatory framework, (2) data quality and standards guidance. and (3) study methods guidance.
Findings
This article reviews the available frameworks and existing guidance from across the globe and discusses the observed gaps and opportunities for further development and harmonization.
Implications
Cross-country collaborations are encouraged to further shape and align RWE policies and help establish frameworks in countries without current policies with the goal of creating efficiencies when considering RWE to support regulatory decision-making globally.",2022,,https://doi.org/10.1016/j.clinthera.2022.01.012
620,Julien Minet and Yannick Curnel and Anne Gobin and Jean-Pierre Goffart and François Mélard and Bernard Tychon and Joost Wellens and Pierre Defourny,Crowdsourcing for agricultural applications: A review of uses and opportunities for a farmsourcing approach,"Crowdsourcing, Citizen science, Smart farming, Participatory approaches, Big data, ICT, Data collection","Crowdsourcing, understood as outsourcing tasks or data collection by a large group of non-professionals, is increasingly used in scientific research and operational applications. In this paper, we reviewed crowdsourcing initiatives in agricultural science and farming activities and further discussed the particular characteristics of this approach in the field of agriculture. On-going crowdsourcing initiatives in agriculture were analysed and categorised according to their crowdsourcing component. We identified eight types of agricultural data and information that can be generated from crowdsourcing initiatives. Subsequently we described existing methods of quality control of the crowdsourced data. We analysed the profiles of potential contributors in crowdsourcing initiatives in agriculture, suggested ways for increasing farmers’ participation, and discussed the on-going initiatives in the light of their target beneficiaries. While crowdsourcing is reported to be an efficient way of collecting observations relevant to environmental monitoring and contributing to science in general, we pointed out that crowdsourcing applications in agriculture may be hampered by privacy issues and other barriers to participation. Close connections with the farming sector, including extension services and farm advisory companies, could leverage the potential of crowdsourcing for both agricultural research and farming applications. This paper coins the term of farmsourcing asa professional crowdsourcing strategy in farming activities and provides a source of recommendations and inspirations for future collaborative actions in agricultural crowdsourcing.",2017,,https://doi.org/10.1016/j.compag.2017.08.026
621,Zhigang He and Xiaoyu Shen and Yanyan Sun and Shichao Zhao and Bin Fan and Chaofeng Pan,State-of-health estimation based on real data of electric vehicles concerning user behavior,"Electric vehicles, SOH, User behavior, LWLR, LSTM","State of health (SOH) of lithium-ion battery pack directly determines the driving mileage and output power of the electric vehicle. With the development of big data storage and analysis technology, using big data to off-line estimate battery pack SOH is more feasible than before. This paper proposes a SOH estimation method based on real data of electric vehicles concerning user behavior. The charging capacity is calculated by historical charging data, and locally weighted linear regression (LWLR) algorithm is used to qualitatively characterize the capacity decline trend. The health features are extracted from historical operating data, maximal information coefficient (MIC) algorithm is used to measure the correlation between health features and capacity. Then, long and short-term memory (LSTM)-based neural network will further learn the nonlinear degradation relationship between capacity and health features. Bayesian optimization algorithm is used to ensure the generalization of the model when different electric vehicles produce different user behaviors. The estimation method is validated by the 300 days historical dataset from 100 vehicles with different driving behavior. The results indicates that the maximum relative error of estimating SOH is 0.2%.",2021,,https://doi.org/10.1016/j.est.2021.102867
622,Dhammika Amaratunga and Javier Cabrera and Davit Sargsyan and John B. Kostis and Stavros Zinonos and William J. Kostis,Uses and opportunities for machine learning in hypertension research,"Machine learning, Deep neural networks, Hypertension, Disease management, Personalized disease network","Background
Artificial intelligence (AI) promises to provide useful information to clinicians specializing in hypertension. Already, there are some significant AI applications on large validated data sets.
Methods and results
This review presents the use of AI to predict clinical outcomes in big data i.e. data with high volume, variety, veracity, velocity and value. Four examples are included in this review. In the first example, deep learning and support vector machine (SVM) predicted the occurrence of cardiovascular events with 56%–57% accuracy. In the second example, in a data base of 378,256 patients, a neural network algorithm predicted the occurrence of cardiovascular events during 10 year follow up with sensitivity (68%) and specificity (71%). In the third example, a machine learning algorithm classified 1,504,437 patients on the presence or absence of hypertension with 51% sensitivity, 99% specificity and area under the curve 87%. In example four, wearable biosensors and portable devices were used in assessing a person's risk of developing hypertension using photoplethysmography to separate persons who were at risk of developing hypertension with sensitivity higher than 80% and positive predictive value higher than 90%. The results of the above studies were adjusted for demographics and the traditional risk factors for atherosclerotic disease.
Conclusion
These examples describe the use of artificial intelligence methods in the field of hypertension.",2020,,https://doi.org/10.1016/j.ijchy.2020.100027
623,Lokukaluge P. Perera and Brage Mo,Marine Engine Operating Regions under Principal Component Analysis to evaluate Ship Performance and Navigation Behavior,"Principal Component Analysis, Big Data, Marine Engine Operations, Ship Performance Monitoring, Structured Data","Abstract:
Marine engine operating regions under principal component analysis (PCA) to evaluate ship performance and navigation behavior are presented in this study. A data set with ship performance and navigation information (i.e. a selected vessel) is considered to identify its hidden structure with respect to a selected operating region of the marine engine. Firstly, the data set is classified with respect to the engine operating points (i.e. operating modes), identifying three operating regions for the main engine. Secondly, one engine operating region (i.e. a data cluster) is analyzed to calculate the respective principal components (PCs). These PCs represent various relationships among ship performance and navigation parameters of the vessel and those relationships with respect to the marine engine operating region are used to evaluate ship performance and navigation behavior. Furthermore, such knowledge (i.e. PCs and parameter behavior) can also be used for sensor fault identification and data compression/expansion types of applications as a big data solution in shipping.",2016,,https://doi.org/10.1016/j.ifacol.2016.10.487
624,Jordan Gacutan and Emma L. Johnston and Heidi Tait and Wally Smith and Graeme F. Clark,Continental patterns in marine debris revealed by a decade of citizen science,"Environmental monitoring, Plastic pollution, Bioregional management, Litter, Citizen science, Marine debris","Anthropogenic marine debris is a persistent threat to oceans, imposing risks to ecosystems and the communities they support. Whilst an understanding of marine debris risks is steadily advancing, monitoring at spatial and temporal scales relevant to management remains limited. Citizen science projects address this shortcoming but are often critiqued on data accuracy and potential bias in sampling efforts. Here we present 10-years of Australia's largest marine debris database - the Australian Marine Debris Initiative (AMDI), in which we perform systematic data filtering, test for differences between collecting groups, and report patterns in marine debris. We defined five stages of data filtering to address issues in data quality and to limit inference to ocean-facing sandy beaches. Significant differences were observed in the average accumulation of items between filtered and remaining data. Further, differences in sampling were compared between collecting groups at the same site (e.g., government, NGOs, and schools), where no significant differences were observed. The filtering process removed 21% of events due to data quality issues and a further 42% of events to restrict analyses to ocean-facing sandy beaches. The remaining 7275 events across 852 sites allowed for an assessment of debris patterns at an unprecedented spatial and temporal resolution. Hard plastics were the most common material found on beaches both nationally and regionally, consisting of up to 75% of total debris. Nationally, land and sea-sourced items accounted for 48% and 7% of debris, respectively, with most debris found on the east coast of Australia. This study demonstrates the value of citizen science datasets with broad spatial and temporal coverage, and the importance of data filtering to improve data quality. The citizen science presented provides an understanding of debris patterns on Australia's ocean beaches and can serve as a foundation for future source reduction plans.",2022,,https://doi.org/10.1016/j.scitotenv.2021.150742
625,Wenwei Che and Yumiao Zhang and Changqing Lin and Yik Him Fung and Jimmy C.H. Fung and Alexis K.H. Lau,Impacts of pollution heterogeneity on population exposure in dense urban areas using ultra-fine resolution air quality data,"Particulate matter, Nitrogen dioxide, Ozone, Pollution heterogeneity, Urban area","Traditional air quality data have a spatial resolution of 1 km or above, making it challenging to resolve detailed air pollution exposure in complex urban areas. Combining urban morphology, dynamic traffic emission, regional and local meteorology, physicochemical transformations in air quality models using big data fusion technology, an ultra-fine resolution modeling system was developed to provide air quality data down to street level. Based on one-year ultra-fine resolution data, this study investigated the effects of pollution heterogeneity on the individual and population exposure to particulate matter (PM2.5 and PM10), nitrogen dioxide (NO2), and ozone (O3) in Hong Kong, one of the most densely populated and urbanized cities. Sharp fine-scale variabilities in air pollution were revealed within individual city blocks. Using traditional 1 km average to represent individual exposure resulted in a positively skewed deviation of up to 200% for high-end exposure individuals. Citizens were disproportionally affected by air pollution, with annual pollutant concentrations varied by factors of 2 to 5 among 452 District Council Constituency Areas (DCCAs) in Hong Kong, indicating great environmental inequities among the population. Unfavorable city planning resulted in a positive spatial coincidence between pollution and population, which increased public exposure to air pollutants by as large as 46% among districts in Hong Kong. Our results highlight the importance of ultra-fine pollutant data in quantifying the heterogeneity in pollution exposure in the dense urban area and the critical role of smart urban planning in reducing exposure inequities.",2023,,https://doi.org/10.1016/j.jes.2022.02.041
626,Lijing Yao and Hengyuan Zhang and Mengqin Zhang and Xing Chen and Jun Zhang and Jiyi Huang and Lu Zhang,Application of artificial intelligence in renal disease,"Artificial intelligence (AI), Machine learning (ML), Artificial neural network (ANN), Convolution neural network (CNN), Nephrology","Artificial intelligence (AI) has been applied widely in almost every area of our daily lives, due to the growth of computing power, advances in methods and techniques, and the explosion of data, it also plays a critical role in academic disciplines, medicine is not an exception. AI can augment the intelligence of clinicians in diagnosis, prognosis, and treatment decisions.Kidney disease causes great economic burden worldwide, with both acute kidney injury and chronic kidney disease bringing about high morbidity and mortality. Outstanding challenges in nephrology may be addressed by leveraging big data and AI.In this review, we summarized advances in machine learning (ML), artificial neural network (ANN), convolution neural network (CNN) and deep learning (DL), with a special focus on acute kidney injury (AKI), chronic kidney disease (CKD), end-stage renal disease (ESRD), dialysis, kidney transplantation and nephropathology. AI may not be anticipated to replace the nephrologists’ medical decision-making for now, but instead assisting them in providing optimal personalized therapy for patients.",2021,,https://doi.org/10.1016/j.ceh.2021.11.003
627,Sarwar Kamal and Shamim Hasnat Ripon and Nilanjan Dey and Amira S. Ashour and V. Santhi,A MapReduce approach to diminish imbalance parameters for big deoxyribonucleic acid dataset,"MapReduce, K-nearest neighbor, Big data, DNA (deoxyribonucleic acid), Computational biology, Imbalance data","Background
In the age of information superhighway, big data play a significant role in information processing, extractions, retrieving and management. In computational biology, the continuous challenge is to manage the biological data. Data mining techniques are sometimes imperfect for new space and time requirements. Thus, it is critical to process massive amounts of data to retrieve knowledge. The existing software and automated tools to handle big data sets are not sufficient. As a result, an expandable mining technique that enfolds the large storage and processing capability of distributed or parallel processing platforms is essential.
Method
In this analysis, a contemporary distributed clustering methodology for imbalance data reduction using k-nearest neighbor (K-NN) classification approach has been introduced. The pivotal objective of this work is to illustrate real training data sets with reduced amount of elements or instances. These reduced amounts of data sets will ensure faster data classification and standard storage management with less sensitivity. However, general data reduction methods cannot manage very big data sets. To minimize these difficulties, a MapReduce-oriented framework is designed using various clusters of automated contents, comprising multiple algorithmic approaches.
Results
To test the proposed approach, a real DNA (deoxyribonucleic acid) dataset that consists of 90 million pairs has been used. The proposed model reduces the imbalance data sets from large-scale data sets without loss of its accuracy.
Conclusions
The obtained results depict that MapReduce based K-NN classifier provided accurate results for big data of DNA.",2016,,https://doi.org/10.1016/j.cmpb.2016.04.005
628,Juan Yebenes and Marta Zorrilla,Towards a Data Governance Framework for Third Generation Platforms,"Data governance, Industry 4.0, data bus architecture, cloud computing, IoT, big data","The fourth industrial revolution considers data as a business asset and therefore this is placed as a central element of the software architecture (data as a service) that will support the horizontal and vertical digitalization of industrial processes. The large volume of data that the environment generates, its heterogeneity and complexity, as well as its reuse for later processes (e.g. analytics, IA) requires the adoption of policies, directives and standards for its right governance. Furthermore, the issues related to the use of resources in the cloud computing must be taken into account with the aim of meeting the requirements of performance and security of the different processes. This article, in the absence of frameworks adapted to this new architecture, proposes an initial schema for developing an effective data governance programme for third generation platforms, that means, a conceptual tool which guides organizations to define, design, develop and deploy services aligned with its vision and business goals in I4.0 era.",2019,,https://doi.org/10.1016/j.procs.2019.04.082
629,Judy S. LaKind and Cian O’Mahony and Thomas Armstrong and Rosalie Tibaldi and Benjamin C. Blount and Daniel Q. Naiman,ExpoQual: Evaluating measured and modeled human exposure data,"ExpoQual, BEES-C, Exposure, Human, Quality, Fit-for-purpose, Instrument, Biomonitoring, Model uncertainty","Recent rapid technological advances are producing exposure data sets for which there are no available data quality assessment tools. At the same time, regulatory agencies are moving in the direction of data quality assessment for environmental risk assessment and decision-making. A transparent and systematic approach to evaluating exposure data will aid in those efforts. Any approach to assessing data quality must consider the level of quality needed for the ultimate use of the data. While various fields have developed approaches to assess data quality, there is as yet no general, user-friendly approach to assess both measured and modeled data in the context of a fit-for-purpose risk assessment. Here we describe ExpoQual, an instrument developed for this purpose which applies recognized parameters and exposure data quality elements from existing approaches for assessing exposure data quality. Broad data streams such as quantitative measured and modeled human exposure data as well as newer and developing approaches can be evaluated. The key strength of ExpoQual is that it facilitates a structured, reproducible and transparent approach to exposure data quality evaluation and provides for an explicit fit-for-purpose determination. ExpoQual was designed to minimize subjectivity and to include transparency in aspects based on professional judgment. ExpoQual is freely available on-line for testing and user feedback (exposurequality.com).",2019,,https://doi.org/10.1016/j.envres.2019.01.039
630,Dawei Sun and Kelly Robbins and Nicolas Morales and Qingyao Shu and Haiyan Cen,Advances in optical phenotyping of cereal crops,"cereal crops, high-throughput phenotyping, optical sensors, traits","Optical sensors and sensing-based phenotyping techniques have become mainstream approaches in high-throughput phenotyping for improving trait selection and genetic gains in crops. We review recent progress and contemporary applications of optical sensing-based phenotyping (OSP) techniques in cereal crops and highlight optical sensing principles for spectral response and sensor specifications. Further, we group phenotypic traits determined by OSP into four categories – morphological, biochemical, physiological, and performance traits – and illustrate appropriate sensors for each extraction. In addition to the current status, we discuss the challenges of OSP and provide possible solutions. We propose that optical sensing-based traits need to be explored further, and that standardization of the language of phenotyping and worldwide collaboration between phenotyping researchers and other fields need to be established.",2022,,https://doi.org/10.1016/j.tplants.2021.07.015
631,James C.L. Chow,Internet-based computer technology on radiotherapy,"Radiotherapy, Computer technology, Cloud computing, Machine learning, Big data","Recent rapid development of Internet-based computer technologies has made possible many novel applications in radiation dose delivery. However, translational speed of applying these new technologies in radiotherapy could hardly catch up due to the complex commissioning process and quality assurance protocol. Implementing novel Internet-based technology in radiotherapy requires corresponding design of algorithm and infrastructure of the application, set up of related clinical policies, purchase and development of software and hardware, computer programming and debugging, and national to international collaboration. Although such implementation processes are time consuming, some recent computer advancements in the radiation dose delivery are still noticeable. In this review, we will present the background and concept of some recent Internet-based computer technologies such as cloud computing, big data processing and machine learning, followed by their potential applications in radiotherapy, such as treatment planning and dose delivery. We will also discuss the current progress of these applications and their impacts on radiotherapy. We will explore and evaluate the expected benefits and challenges in implementation as well.",2017,,https://doi.org/10.1016/j.rpor.2017.08.005
632,Carlos Granell and Denis Havlik and Sven Schade and Zoheir Sabeur and Conor Delaney and Jasmin Pielorz and Thomas Usländer and Paolo Mazzetti and Katharina Schleidt and Mike Kobernus and Fuada Havlik and Nils Rune Bodsberg and Arne Berre and Jose Lorenzo Mon,Future Internet technologies for environmental applications,"Environmental informatics, Environmental observation web, Future internet, Cloud computing, Internet of things, Big data, Environmental specific enablers, Volunteered geographic information, Crowdtasking","This paper investigates the usability of Future Internet technologies (aka “Generic Enablers of the Future Internet”) in the context of environmental applications. The paper incorporates the best aspects of the state-of-the-art in environmental informatics with geospatial solutions and scalable processing capabilities of Internet-based tools. It specifically targets the promotion of the “Environmental Observation Web” as an observation-centric paradigm for building the next generation of environmental applications. In the Environmental Observation Web, the great majority of data are considered as observations. These can be generated from sensors (hardware), numerical simulations (models), as well as by humans (human sensors). Independently from the observation provenance and application scope, data can be represented and processed in a standardised way in order to understand environmental processes and their interdependencies. The development of cross-domain applications is then leveraged by technologies such as Cloud Computing, Internet of Things, Big Data Processing and Analytics. For example, “the cloud” can satisfy the peak-performance needs of applications which may occasionally use large amounts of processing power at a fraction of the price of a dedicated server farm. The paper also addresses the need for Specific Enablers that connect mainstream Future Internet capabilities with sensor and geospatial technologies. Main categories of such Specific Enablers are described with an overall architectural approach for developing environmental applications and exemplar use cases.",2016,,https://doi.org/10.1016/j.envsoft.2015.12.015
633,Tingting Ye and Naizhuo Zhao and Xuchao Yang and Zutao Ouyang and Xiaoping Liu and Qian Chen and Kejia Hu and Wenze Yue and Jiaguo Qi and Zhansheng Li and Peng Jia,Improved population mapping for China using remotely sensed and points-of-interest data within a random forests model,"Points of interest, Population, Random forests, Nighttime light, China","Remote sensing image products (e.g. brightness of nighttime lights and land cover/land use types) have been widely used to disaggregate census data to produce gridded population maps for large geographic areas. The advent of the geospatial big data revolution has created additional opportunities to map population distributions at fine resolutions with high accuracy. A considerable proportion of the geospatial data contains semantic information that indicates different categories of human activities occurring at exact geographic locations. Such information is often lacking in remote sensing data. In addition, the remarkable progress in machine learning provides toolkits for demographers to model complex nonlinear correlations between population and heterogeneous geographic covariates. In this study, a typical type of geospatial big data, points-of-interest (POIs), was combined with multi-source remote sensing data in a random forests model to disaggregate the 2010 county-level census population data to 100 × 100 m grids. Compared with the WorldPop population dataset, our population map showed higher accuracy. The root mean square error for population estimates in Beijing, Shanghai, Guangzhou, and Chongqing for this method and WorldPop were 27,829 and 34,193, respectively. The large under-allocation of the population in urban areas and over-allocation in rural areas in the WorldPop dataset was greatly reduced in this new population map. Apart from revealing the effectiveness of POIs in improving population mapping, this study promises the potential of geospatial big data for mapping other socioeconomic parameters in the future.",2019,,https://doi.org/10.1016/j.scitotenv.2018.12.276
634,Jonathan Cinnamon,Humanitarian Mapping,"Big data, Cartography, Crisis, Crowdsourcing, Disaster, Emergency, Geospatial web, Relief, Spatial data, Web mapping","Humanitarian mapping refers to the production of spatial data and cartographic products to improve situational awareness and decision-making around humanitarian issues from acute events such as natural disasters and public health emergencies to longer term events such as refugee crises and political unrest. Mapping is a key part of the broader area of humanitarian information management, which has traditionally been undertaken by governments and international humanitarian organizations. As a core aspect of the field of digital humanitarianism, mapping activities are now widely undertaken by smaller organizations and networks of volunteers who produce spatial data and maps on the ground and remotely via the use of Web mapping and mobile phone technologies. Big data based on location and behavioral attributes produced online and through interaction with digital systems and networks can also be exploited to enhance information environments. Together, these new developments signal new possibilities for improved risk and crisis management, based on up-to-date high resolution spatial and temporal evidence. Research in human geography, geographic information science, and related disciplines focuses on tracing benefits such as increased speed and low costs, as well as the risks of relying on distributed volunteers and new sources of data of questionable accuracy and validity.",2020,,https://doi.org/10.1016/B978-0-08-102295-5.10559-1
635,Nasim Sadat Mosavi and Francisco Freitas and Rogério Pires and César Rodrigues and Isabel Silva and Manuel Santos and Paulo Novais,Intelligent energy management using data mining techniques at Bosch Car Multimedia Portugal facilities,"Energy consumption, Prediction, Optimization, Data Mining, Machine Learning, Forecasting, Industry 4.0","The fusion of emerged technologies such as Artificial Intelligence, cloud computing, big data, and the Internet of Things in manufacturing has pioneered this industry to meet the fourth stage of the industrial revolution (industry 4.0). One major approach to keeping this sector sustainable and productive is intelligent energy demand planning. Monitoring and controlling the consumption of energy under industry 4.0, directly results in minimizing the cost of operation and maximizing efficiency. To advance the research on the adoption of industry 4.0, this study examines CRISP-DM methodology to project data mining approach over data from 2020 to 2021 which was collected from industrial sensors to predict/forecast future electrical consumption at Bosch car multimedia facilities located at Braga, Portugal. Moreover, the influence of indicators such as humidity and temperature on electrical energy consumption was investigated. This study employed five promising regression algorithms and FaceBook prophet (FB prophet) to apply over data belonging to two HVAC (heating, ventilation, and air conditioning) sensors (E333, 3260). Results indicate Random Forest (RF) algorithms as a potential regression approach for prediction and the outcome of FB prophet to forecast the demand of future usage of electrical energy associated with HVAC presented. Based on that, it was concluded that predicting the usage of electrical energy for both data points requires time series techniques. Where “timestamp” was identified as the most effective feature to predict consume of electrical energy by regression technique (RF). The result of this study was integrated with Intelligent Industrial Management System (IIMS) at Bosch Portugal.",2022,,https://doi.org/10.1016/j.procs.2022.03.065
636,Pak Ho Leung and Kwok Tai Chui and Kenneth Lo and Patricia Ordóñez {de Pablos},Chapter 13 - A support vector machine–based voice disorders detection using human voice signal,"Artificial intelligence, big data, human voice, imbalanced classification, machine learning, medical screening, smart city, smart healthcare, support vector machine, voice disorders","Voice disorders are common diseases; most of the people have had experienced in their life. Voice disorder sufferers are usually not seeking medical consultation attributable to time-consuming and costly medical expenditure. Recently, researchers have proposed various machine learning algorithms for rapid detection of voice disorders based on the analysis of human voice. In this chapter, we have taken the pronunciation of vowel /a/ as the input of support vector machine algorithm. The research problem is formulated as binary classification which output will be either healthy or pathological status. Our work achieves an accuracy of 69.3% (sensitivity of 83.3% and specificity of 33.3%) which improves by 6.4%–19.3% compared with existing works. The implication of research work suggests tackling the imbalanced classification by adding penalty or generating new training data to class of smaller size. Everybody could contribute the voice signal of vowel /a/ and serving as big data pool.",2021,,https://doi.org/10.1016/B978-0-12-822060-3.00014-0
637,Manu Sharma and Sunil Luthra and Sudhanshu Joshi and Anil Kumar,Implementing challenges of artificial intelligence: Evidence from public manufacturing sector of an emerging economy,"Artificial intelligence, Implementing challenges, Public manufacturing sector, AI enabled systems, Emerging economies","The growing Artificial Intelligence (AI) age has been flooded with several innovations in algorithmic machine learning that may bring significant impacts to industries such as healthcare, agriculture, education, manufacturing, retail etc. But challenges such as data quality, privacy and lack of a skilled workforce limit the scope of AI implementation in emerging economies, particularly in the Public Manufacturing Sector (PMS). Therefore, to enhance the body of relevant literature, this study examines the existing challenges of AI implementation in PMS of India and explores the inter-relationships among them. The study has utilized the DEMATEL method for identification of the cause-and-effect group factors. The findings reveal that poor data quality, managers' lack of understanding of cognitive technologies, data privacy, problems in integrating cognitive projects and expensive technologies are the main challenges for AI implementation in PMS of India. Moreover, a model is proposed for industrial decision-makers and managers to take appropriate decisions to develop intelligent AI enabled systems for manufacturing organizations in emerging economies.",2021,,https://doi.org/10.1016/j.giq.2021.101624
638,Paweł Macias and Damian Stelmasiak and Karol Szafranek,Nowcasting food inflation with a massive amount of online prices,"Inflation nowcasting, Online prices, Big data, Nowcasting competition, Web scraping","The consensus in the literature on providing accurate inflation forecasts underlines the importance of precise nowcasts. In this paper, we focus on this issue by employing a unique, extensive dataset of online food and non-alcoholic beverages prices gathered automatically from the webpages of major online retailers in Poland since 2009. We perform a real-time nowcasting experiment by using a highly disaggregated framework among popular, simple univariate approaches. We demonstrate that pure estimates of online price changes are already effective in nowcasting food inflation, but accounting for online food prices in a simple, recursively optimized model delivers further gains in the nowcast accuracy. Our framework outperforms various other approaches, including judgmental methods, traditional benchmarks, and model combinations. After the outbreak of the COVID-19 pandemic, its nowcasting quality has improved compared to other approaches and remained comparable with judgmental nowcasts. We also show that nowcast accuracy increases with the volume of online data, but their quality and relevance are essential for providing accurate in-sample fit and out-of-sample nowcasts. We conclude that online prices can markedly aid the decision-making process at central banks.",2022,,https://doi.org/10.1016/j.ijforecast.2022.02.007
639,Karine Zeitouni and Mariem Brahem and Laurent Yeh and Atanas Hristov,Chapter 8 - Query Processing and Access Methods for Big Astro and Geo Databases,"Spatial databases, Spatial access methods, Query optimization, Big Data, System architecture","In spite of their development in different communities, either astro-informatics or geo-informatics, data management and analytics of astronomical and geospatial data share the same characteristics, and raise the same challenges when it comes to access, query, or analysis of the spatial features over Big Data. The very first challenge is to deal with the data volume, which is tremendous in many geo and astro datasets. In this chapter, we highlight their main specificity and outline the main steps of query processing in big geospatial and astronomical data servers. Through the review of the state of the art, we show the advance in the topic of Big Data management in both contexts of geospatial and sky surveying, while highlighting their similarity. This progress notwithstanding, several issues remain to deal with the variety (such as multidimensional arrays) of the data.",2020,,https://doi.org/10.1016/B978-0-12-819154-5.00018-7
640,Patricia Eckardt and Donald Bailey and Holli A. DeVon and Cynthia Dougherty and Pamela Ginex and Cheryl A. Krause-Parello and Rita H. Pickler and Therese S. Richmond and Eleanor Rivera and Carol F. Roye and Nancy Redeker,Opioid use disorder research and the Council for the Advancement of Nursing Science priority areas,"Precision health, Big data and Data analytics, Determinants of health, Global health, Opioid use disorder research","Background
Chronic diseases, such as opioid use disorder (OUD) require a multifaceted scientific approach to address their evolving complexity. The Council for the Advancement of Nursing Science's (Council) four nursing science priority areas (precision health; global health, determinants of health, and big data/data analytics) were established to provide a framework to address current complex health problems.
Purpose
To examine OUD research through the nursing science priority areas and evaluate the appropriateness of the priority areas as a framework for research on complex health conditions.
Method
OUD was used as an exemplar to explore the relevance of the nursing science priorities for future research.
Findings
Research in the four priority areas is advancing knowledge in OUD identification, prevention, and treatment. Intersection of OUD research population focus and methodological approach was identified among the priority areas.
Discussion
The Council priorities provide a relevant framework for nurse scientists to address complex health problems like OUD.",2020,,https://doi.org/10.1016/j.outlook.2020.02.001
641,Chengyuan Zhang and Shouyang Wang and Shaolong Sun and Yunjie Wei,Knowledge mapping of tourism demand forecasting research,"Bibliometric analysis, Tourist arrival, Hospitality demand, Knowledge map, CiteSpace, Infographic","Utilizing a scientometric review of global trends and structure from 388 bibliographic records over two decades (1999–2018), this study seeks to advance the building of comprehensive knowledge maps that draw upon global travel demand studies. The study, using the techniques of co-citation analysis, collaboration network and emerging trends analysis, identified major disciplines that provide knowledge and theories for tourism demand forecasting, many trending research topics, the most critical countries, institutions, publications, and articles, and the most influential researchers. The increasing interest and output for big data and machine learning techniques in the field were visualized via comprehensive knowledge maps. This research provides meaningful guidance for researchers, operators and decision makers who wish to improve the accuracy of tourism demand forecasting.",2020,,https://doi.org/10.1016/j.tmp.2020.100715
642,Devarshi Shah and Jin Wang and Q. Peter He,An Internet-of-things Enabled Smart Manufacturing Testbed,"Internet-of-things, smart manufacturing, big data, data analytics, statistical analysis, vibration, soft sensor, process monitoring","The emergence of the industrial Internet of Things (IoT) and ever advancing computing and communication technologies have fueled a new industrial revolution which is happening worldwide to make current manufacturing systems smarter, safer, and more efficient. Although many general frameworks have been proposed for IoT enabled systems for industrial application, there is limited literature on demonstrations or testbeds of such systems. In addition, there is a lack of systematic study on the characteristics of IoT sensors and data analytics challenges associated with IoT sensor data. This study is an attempt to help fill this gap by exploring the characteristics of IoT vibration sensors and show how IoT sensors and big data analytics can be used to develop real time monitoring frameworks.",2019,,https://doi.org/10.1016/j.ifacol.2019.06.122
643,Ilias Pasidis,Congestion by accident? A two-way relationship for highways in England,"Accidents, Traffic congestion, Big data, Highways, England","This paper aims to estimate the causal effect of accidents on traffic congestion and vice versa. In order to identify both effects of this two-way relationship, I use dynamic panel data techniques and open access ‘big data’ of highway traffic and accidents in England for the period 2012–2014. The research design is based on the daily-and-hourly specific mean reversion pattern of highway traffic, which can be used to define a recurrent congestion benchmark. Using this benchmark, I am able to identify the causal effect of accidents on non-recurrent traffic congestion. A positive relationship between traffic congestion and road accidents would yield multiplicative benefits for policies that aim at reducing either of these issues. Additionally, I explore the duration of the effect of an accident on congestion, the ‘rubbernecking’ effect, as well as heterogeneous effects in the most congested highway segments. Then, I test the use of methods which employ the bulk of information in big data and other methods using a very reduced sample. In my application, both approaches produce similar results. Finally, I find a non-linear negative effect of traffic congestion on the probability of an accident.",2019,,https://doi.org/10.1016/j.jtrangeo.2017.10.006
644,Jules J. Berman,1 - Introduction,"Big data definition, Small data, Data filtering, Data reduction","Big Data is not synonymous with lots and lots of data. Useful Big Data resources adhere to a set of data management principles that are fundamentally different from the traditional practices followed for small data projects. The areas of difference include: data collection; data annotation (including metadata and identifiers); location and distribution of stored data; classification of data; data access rules; data curation; data immutability; data permanence; verification and validity methods for the contained data; analytic methods; costs; and incumbent legal, social, and ethical issues. Skilled professionals who are adept in the design and management of small data resources may be unprepared for the unique challenges posed by Big Data. This chapter is an introduction to topics that will be fully explained in later chapters.",2018,,https://doi.org/10.1016/B978-0-12-815609-4.00001-7
645,Jian Zhang and Alessandro Simeone and Peihua Gu and Bo Hong,Product features characterization and customers’ preferences prediction based on purchasing data,"Design, Product development, Big data","Big data of online product purchases is an emerging source for obtaining customers’ preferences of product features for new product development. This paper proposes a framework and associated method for product features characterization and customers’ preference prediction based on online product purchase data. Specifications and components of products are firstly analyzed and the relationships between product specifications and components are then established for features characterization. The customers preferred specifications, features and their combinations are predicted for development of new products. The features characterization and customers’ preferences prediction of toy cars were used as an example of illustrating the proposed method.",2018,,https://doi.org/10.1016/j.cirp.2018.04.020
646,Danette McGilvray,Chapter 5 - Structuring Your Project,"Solution Development Life Cycle (SDLC), Agile, Scrum, sequential, waterfall, project objectives, project team, project roles, project timing, project approach","It is essential that those using the Ten Steps do a good job of organizing their work. This chapter guides readers’ choices when setting up their projects and assembling a project team. Three general types of projects are detailed: 1) Focused data quality improvement project, 2) Data quality activities in another project, such as application development, data migration or integration of any kind, and 3) Ad hoc use of data quality steps, activities, or techniques from the Ten Steps. Additional information is given for incorporating data quality activities into another project using various SDLCs (solution/system/software life cycles). Relevant data quality activities from the Ten Steps can be incorporated into any SDLC that is the basis for the larger project (Agile, sequential, hybrid, etc.). To that end, several tables list data governance, stewardship, data quality and readiness activities and where they would take place in typical SDLC phases. A table with Agile Scrum activities are cross-referenced to the same SDLC phases. The chapter concludes with general tips for project timing, communication, and engagement.",2021,,https://doi.org/10.1016/B978-0-12-818015-0.00001-3
647,Cinzia Cappiello and Marco Comuzzi and Pierluigi Plebani and Matheus Fim,Assessing and improving measurability of process performance indicators based on quality of logs,"Business process, Event log, Data quality assessment, Data quality improvement","The efficiency and effectiveness of business processes are usually evaluated by Process Performance Indicators (PPIs), which are computed using process event logs. PPIs can be insightful only when they are measurable, i.e., reliable. This paper proposes to define PPI measurability on the basis of the quality of the data in the process logs. Then, based on this definition, a framework for PPI measurability assessment and improvement is presented. For the assessment, we propose novel definitions of PPI accuracy, completeness, consistency, timeliness and volume that contextualise the traditional definitions in the data quality literature to the case of process logs. For the improvement, we define a set of guidelines for improving the measurability of a PPI. These guidelines may concern improving existing event logs, for instance through data imputation, implementation or enhancement of the process monitoring systems, or updating the PPI definitions. A case study in a large-sized institution is discussed to show the feasibility and the practical value of the proposed framework.",2022,,https://doi.org/10.1016/j.is.2021.101874
648,Guy {De Tré} and Robin {De Mol} and Antoon Bronselaer,Handling veracity in multi-criteria decision-making: A multi-dimensional approach,"Multi-criteria decision-making, Veracity in ‘big’ data, Data quality assessment, Data quality handling","Decision support systems aim to help a decision maker with selecting the option from a set of available options that best meets her or his needs. In multi-criteria based decision support approaches, a suitability degree is computed for each option, reflecting how suitable that option is considering the preferences of the decision maker. Nowadays, it becomes more and more common that data of different quality, originating from different data sets and different data providers have to be integrated and processed in order to compute the suitability degrees. Also, data sets can be very large such that their data become commonly prone to incompleteness, imprecision and uncertainty. Hence, not all data used for decision making can be trusted to the same extent and consequently, neither the results of computations with such data can be trusted to the same extent. For this reason, data quality assessment becomes an important aspect of a decision making process. To correctly inform the users, it is essential to communicate not only the computed suitability degrees of the available options, but also the confidence about these suitability degrees as can be derived from data quality assessment. In this paper a novel multi-dimensional approach for data quality assessment in multi-criteria decision making, supporting the computation of associated confidence degrees, is presented. Providing confidence information adds an extra dimension to the decision making process and leads to more soundly decisions. The added value of the approach is illustrated with aspects of a geographic decision making process.",2018,,https://doi.org/10.1016/j.ins.2017.09.008
649,Younes Oulad Sayad and Hajar Mousannif and Hassan {Al Moatassime},Predictive modeling of wildfires: A new dataset and machine learning approach,"Big data, Remote sensing, Machine learning, Wildfire prediction, Data mining, Artificial intelligence","Wildfires, whether natural or caused by humans, are considered among the most dangerous and devastating disasters around the world. Their complexity comes from the fact that they are hard to predict, hard to extinguish and cause enormous financial losses. To address this issue, many research efforts have been conducted in order to monitor, predict and prevent wildfires using several Artificial Intelligence techniques and strategies such as Big Data, Machine Learning, and Remote Sensing. The latter offers a rich source of satellite images, from which we can retrieve a huge amount of data that can be used to monitor wildfires. The method used in this paper combines Big Data, Remote Sensing and Data Mining algorithms (Artificial Neural Network and SVM) to process data collected from satellite images over large areas and extract insights from them to predict the occurrence of wildfires and avoid such disasters. For this reason, we implemented a methodology that serves this purpose by building a dataset based on Remote Sensing data related to the state of the crops (NDVI), meteorological conditions (LST), as well as the fire indicator “Thermal Anomalies”, these data, were acquired from “MODIS” (Moderate Resolution Imaging Spectroradiometer), a key instrument aboard the Terra and Aqua satellites. This dataset is available on GitHub via this link (https://github.com/ouladsayadyounes/Wildfires). Experiments were made using the big data platform “Databricks”. Experimental results gave high prediction accuracy (98.32%). These results were assessed using several validation strategies (e.g., classification metrics, cross-validation, and regularization) as well as a comparison with some wildfire early warning systems.",2019,,https://doi.org/10.1016/j.firesaf.2019.01.006
650,Yuri Demchenko and Fatih Turkmen and Cees {de Laat} and Ching-Hsien Hsu and Christophe Blanchet and Charles Loomis,Chapter 2 - Cloud Computing Infrastructure for Data Intensive Applications,"Big Data, Big Data Architecture Framework (BDAF), Big data infrastructure, NIST Big Data Architecture (BDRA), Cloud computing, Intercloud Architecture Framework (ICAF), Cloud powered design, SlipStream cloud automation platform","This chapter describes the general architecture and functional components of the cloud-based big data infrastructure (BDI). The chapter starts with the analysis of emerging Big Data and data intensive technologies and provides the general definition of the Big Data Architecture Framework (BDAF) that includes the following components: Big Data definition, Data Management including data lifecycle and data structures, generically cloud based BDI, Data Analytics technologies and platforms, and Big Data security, compliance, and privacy. The chapter refers to NIST Big Data Reference Architecture (BDRA) and summarizes general requirements to Big Data systems described in NIST documents. The proposed BDI and its cloud-based components are defined in accordance with the NIST BDRA and BDAF. This chapter provides detailed analysis of the two bioinformatics use cases as typical example of the Big Data applications that have being developed by the authors in the framework of the CYCLONE project. The effective use of cloud for bioinformatics applications requires maximum automation of the applications deployment and management that may include resources from multiple clouds and providers. The proposed CYCLONE platform for multicloud multiprovider applications deployment and management is based on the SlipStream cloud automation platform and includes all necessary components to build and operate complex scientific applications. The chapter discusses existing platforms for cloud powered applications development and deployment automation, in particularly referring to the SlipStream cloud automation platform, which allows multicloud applications deployment and management. The chapter also includes a short overview of the existing Big Data platforms and services provided by the major cloud services providers which can be used for fast deployment of customer Big Data applications using the benefits of cloud technologies and global cloud infrastructure.",2017,,https://doi.org/10.1016/B978-0-12-809393-1.00002-7
651,Erisa Karafili and Konstantina Spanaki and Emil C. Lupu,An argumentation reasoning approach for data processing,"Data processing, Data quality, Usage control, Argumentation reasoning, Data manufacturing, Case scenarios","Data-intensive environments enable us to capture information and knowledge about the physical surroundings, to optimise our resources, enjoy personalised services and gain unprecedented insights into our lives. However, to obtain these endeavours extracted from the data, this data should be generated, collected and the insight should be exploited. Following an argumentation reasoning approach for data processing and building on the theoretical background of data management, we highlight the importance of data sharing agreements (DSAs) and quality attributes for the proposed data processing mechanism. The proposed approach is taking into account the DSAs and usage policies as well as the quality attributes of the data, which were previously neglected compared to existing methods in the data processing and management field. Previous research provided techniques towards this direction; however, a more intensive research approach for processing techniques should be introduced for the future to enhance the value creation from the data and new strategies should be formed around this data generated daily from various devices and sources.",2018,,https://doi.org/10.1016/j.compind.2017.09.002
652,Arsalan Ghorbanian and Mohammad Kakooei and Meisam Amani and Sahel Mahdavi and Ali Mohammadzadeh and Mahdi Hasanlou,Improved land cover map of Iran using Sentinel imagery within Google Earth Engine and a novel automatic workflow for land cover classification using migrated training samples,"Land cover classification, Sentinel, Google Earth Engine, Big data, Remote sensing, Iran","Accurate information about the location, extent, and type of Land Cover (LC) is essential for various applications. The only recent available country-wide LC map of Iran was generated in 2016 by the Iranian Space Agency (ISA) using Moderate Resolution Imaging Spectroradiometer (MODIS) images with a considerably low accuracy. Therefore, the production of an up-to-date and accurate Iran-wide LC map using the most recent remote sensing, machine learning, and big data processing algorithms is required. Moreover, it is important to develop an efficient method for automatic LC generation for various time periods without the need to collect additional ground truth data from this immense country. Therefore, this study was conducted to fulfill two objectives. First, an improved Iranian LC map with 13 LC classes and a spatial resolution of 10 m was produced using multi-temporal synergy of Sentinel-1 and Sentinel-2 satellite datasets applied to an object-based Random forest (RF) algorithm. For this purpose, 2,869 Sentinel-1 and 11,994 Sentinel-2 scenes acquired in 2017 were processed and classified within the Google Earth Engine (GEE) cloud computing platform allowing big geospatial data analysis. The Overall Accuracy (OA) and Kappa Coefficient (KC) of the final Iran-wide LC map for 2017 was 95.6% and 0.95, respectively, indicating the considerable potential of the proposed big data processing method. Second, an efficient automatic method was developed based on Sentinel-2 images to migrate ground truth samples from a reference year to automatically generate an LC map for any target year. The OA and KC for the LC map produced for the target year 2019 were 91.35% and 0.91, respectively, demonstrating the efficiency of the proposed method for automatic LC mapping. Based on the obtained accuracies, this method can potentially be applied to other regions of interest for LC mapping without the need for ground truth data from the target year.",2020,,https://doi.org/10.1016/j.isprsjprs.2020.07.013
653,Laura Sebastian-Coleman,Chapter 4 - The Data Challenge: The Mechanics of Meaning,"History of data, statistics, scientific data, organizational data, relational data, characteristics of data","This chapter presents an extended definition of the concept of data, through the lens of history. All forms of data encode information about the real world. Using data always involves interpretation, so it is important to understand how data works, to understand “data as data.” But what we mean by data and how we create and use it in science, statistics, and commerce have changed over time. Many assumptions about data quality are rooted in this evolution. A better understanding of the evolution of data helps us define and manage specific expectations related to data quality.",2022,,https://doi.org/10.1016/B978-0-12-821737-5.00004-3
654,Colin Robertson and Chiranjib Chaudhuri and Majid Hojati and Steven A. Roberts,An integrated environmental analytics system (IDEAS) based on a DGGS,"DGGS, Data model, Big data, Spatial data, Analytics, Environment","Discrete global grid systems (DGGS) have been proposed as a data model for a digital earth framework. We introduce a new data model and analytics system called IDEAS – integrated discrete environmental analysis system to create an operational DGGS-based GIS which is suitable for large scale environmental modelling and analysis. Our analysis demonstrates that DGGS-based GIS is feasible within a relational database environment incorporating common data analytics tools. Common GIS operations implemented in our DGGS data model outperformed the same operations computed using traditional geospatial data types. A case study into wildfire modelling demonstrates the capability for data integration and supporting big data geospatial analytics. These results indicate that DGGS data models have significant capability to solve some of the key outstanding problems related to geospatial data analytics, providing a common representation upon which fast and scalable algorithms can be built.",2020,,https://doi.org/10.1016/j.isprsjprs.2020.02.009
655,Naresh Kumar Gundla and Zhengxin Chen,Creating NoSQL Biological Databases with Ontologies for Query Relaxation,"NoSQL databases, Query Relaxation, Ontology, MongoDB, AllgroGraph","The complexity of building biological databases is well-known and ontologies play an extremely important role in biological databases. However, much of the emphasis on the role of ontologies in biological databases has been on the construction of databases. In this paper, we explore a somewhat overlooked aspect regarding ontologies in biological databases, namely, how ontologies can be used to assist better database retrieval. In particular, we show how ontologies can be used to revise user submitted queries for query relaxation. In addition, since our research is conducted at today's “big data” era, our investigation is centered on NoSQL databases which serve as a kind of “representatives” of big data. This paper contains two major parts: First we describe our methodology of building two NoSQL application databases (MongoDB and AllegroGraph) using GO ontology, and then discuss how to achieve query relaxation through GO ontology. We report our experiments and show sample queries and results. Our research on query relaxation on NoSQL databases is complementary to existing work in big data and in biological databases and deserves further exploration.",2016,,https://doi.org/10.1016/j.procs.2016.07.120
656,Mohammad Saeid Mahdavinejad and Mohammadreza Rezvan and Mohammadamin Barekatain and Peyman Adibi and Payam Barnaghi and Amit P. Sheth,Machine learning for internet of things data analysis: a survey,"Machine learning, Internet of Things, Smart data, Smart City","Rapid developments in hardware, software, and communication technologies have facilitated the emergence of Internet-connected sensory devices that provide observations and data measurements from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As these numbers grow and technologies become more mature, the volume of data being published will increase. The technology of Internet-connected devices, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interactions between the physical and cyber worlds. In addition to an increased volume, the IoT generates big data characterized by its velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this big data are the key to developing smart IoT applications. This article assesses the various machine learning methods that deal with the challenges presented by IoT data by considering smart cities as the main use case. The key contribution of this study is the presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying a Support Vector Machine (SVM) to Aarhus smart city traffic data is presented for a more detailed exploration.",2018,,https://doi.org/10.1016/j.dcan.2017.10.002
657,Fadi Al-Turjman,Intelligence and security in big 5G-oriented IoNT: An overview,"IoNT, Security, Big data, Design factors","Internet of Nano-Things (IoNT) overcomes critical difficulties and additionally open doors for wearable sensor based huge information examination. Conventional computing and/or communication systems do not offer enough flexibility and adaptability to deal with the gigantic amount of assorted information nowadays. This creates the need for legitimate components that can efficiently investigate and communicate the huge data while maintaining security and quality of service. In addition, while developing the ultra-wide Heterogeneous Networks (HetNets) associated with the ongoing Big Data project and 5G-based IoNT, it is required to resolve the emerging difficulties as well. Accordingly, these difficulties and other relevant design issues have been comprehensively reported in this survey. It mainly focuses on security issues and associated intelligence to be considered while managing these issues.",2020,,https://doi.org/10.1016/j.future.2019.08.009
658,Jipan Xie and Eric Q. Wu and Shan Wang and Tao Cheng and Zhou Zhou and Jia Zhong and Larry Liu,Real-World Data for Healthcare Research in China: Call for Actions,"administrative claims, data access, electronic health records, real-world data","Objectives
This study aimed to provide an overview of major data sources in China that can be potentially used for epidemiology, health economics, and outcomes research; compare them with similar data sources in other countries; and discuss future directions of healthcare data development in China.
Methods
The study was conducted in 2 phases. First, various data sources were identified through a targeted literature review and recommendations by experts. Second, an in-depth assessment was conducted to evaluate the strengths and limitations of administrative claims and electronic health record data, which were further compared with similar data sources in developed countries.
Results
Secondary databases, including administrative claims and electronic health records, are the major types of real-world data in China. There are substantial variations in available data elements even within the same type of databases. Compared with similar databases in developed countries, the secondary databases in China have some general limitations such as variations in data quality, unclear data usage mechanism, and lack of longitudinal follow-up data. In contrast, the large sample size and the potential to collect additional data based on research needs present opportunities to further improve real-world data in China.
Conclusions
Although healthcare data have expanded substantially in China, high-quality real-world evidence that can be used to facilitate decision making remains limited in China. To support the generation of real-world evidence, 2 fundamental issues in existing databases need to be addressed—data access/sharing and data quality.",2022,,https://doi.org/10.1016/j.vhri.2021.05.002
659,Sateesh.V. Shet and Tanuj Poddar and Fosso {Wamba Samuel} and Yogesh K. Dwivedi,Examining the determinants of successful adoption of data analytics in human resource management – A framework for implications,"Human resource analytics, HRM analytics, People analytics, Adoption of HR analytics, Challenges, Implementation of HR analytics, Big data, Data analytics, Framework synthesis","Data analytics has gained importance in human resource management (HRM) for its ability to provide insights based on data-driven decision-making processes. However, integrating an analytics-based approach in HRM is a complex process, and hence, many organizations are unable to adopt HR Analytics (HRA). Using a framework synthesis approach, we first identify the challenges that hinder the practice of HRA and then develop a framework to explain the different factors that impact the adoption of HRA within organizations. This study identifies the key aspects related to the technological, organizational, environmental, data governance, and individual factors that influence the adoption of HRA. In addition, this paper determines 23 sub-dimensions of these five factors as the crucial aspects for successfully implementing and practicing HRA within organizations. We also discuss the implications of the framework for HR leaders, HR Managers, CEOs, IT Managers and consulting practitioners for effective adoption of HRA in organization.",2021,,https://doi.org/10.1016/j.jbusres.2021.03.054
660,Ricardo J. Pais and R. Zmuidinaite and S.A. Butler and R.K. Iles,An automated workflow for MALDI-ToF mass spectra pattern identification on large data sets: An application to detect aneuploidies from pregnancy urine,"MALDI-ToF, Pattern recognition, Quality control, Comparative intensity data, Automated processing","Urine from first trimester pregnancies has been found to be rich in information related to aneuploidies and other clinical conditions. Mass spectral analysis derived from matrix assisted laser desorption ionization (MALDI) time of flight (ToF) data has been proven to be a cost effective method for clinical diagnostics. However, urine mass spectra are complex and require data modelling frameworks. Therefore, computational approaches that systematically analyse big data generated from MALDI-ToF mass spectra are essential. To address this issue, we developed an automated workflow that successfully processed large data sets from MALDI-ToF which is 100-fold faster than using a common software tool. Our method performs accurate data quality control decisions, and generates a comparative analysis to extract peak intensity patterns from a data set. We successfully applied our framework to the identification of peak intensity patterns for Trisomy 21 and Trisomy 18 gestations on data sets from maternal pregnancy urines obtained in the UK and China. The results from our automated comparative analysis have shown characteristic patterns associated with aneuploidies in the first trimester pregnancy. Moreover, we have shown that the intensity patterns depended on the population origin, gestational age, and MALDI-ToF instrument.",2019,,https://doi.org/10.1016/j.imu.2019.100194
661,Kyoung Sook Jeong and Suejin Kim and Woo Jin Kim and Hwan-Cheol Kim and Jisuk Bae and Yun-Chul Hong and Mina Ha and Kangmo Ahn and Ji-Young Lee and Yangho Kim and Eunhee Ha,Cohort profile: Beyond birth cohort study – The Korean CHildren's ENvironmental health Study (Ko-CHENS),"Ko-CHENS, Children, Environment, Cohort profile, Birth cohort","The Korean CHildren's ENvironmental health Study (Ko-CHENS) is a nationwide prospective birth cohort showing the correlation between the environmental exposures and the health effects to prevent the environmental diseases in children, and it provides the guidelines for the environmental hazardous factors, applying the life-course approach to the environmental-health management system. The Ko-CHENS consists of 5000 Core and 65,000 Main Cohorts. The children in the Core Cohort are followed up at 6 months, every year before their admission into the elementary school, and every 3 years from the first year after this admission. The children in the Cohort will be followed up through the data links (Statistics Korea, National Health Insurance Service [NHIS], and Ministry of Education). The individual biospecimens will be analyzed for 19 substances. The long-term-storage biological samples will be used for the further substance analysis. The Ko-CHENS will investigate whether the environmental variables including the perinatal outdoor and indoor factors and the greenness contribute causally to the health outcomes in the children and adolescents. In addition to the individual surveys, the assessments of the outdoor exposures and health outcomes will use the national air-quality monitoring data and claim data of the NHIS, respectively. The two big-data forms of the Ko-CHENS are as follows: The Ko-CHENS data that can be linked with the nationally registered NHIS health-related database, including the medical utilization and the periodic health screening, and the birth/mortality database in the Statistics; the other is the Big-CHENS dataset that is based on the NHIS mother delivery code, for which the follow-up of almost 97% of the total birth population is expected. The Ko-CHENS is a very cost-effective study that fully exploits the existing national big-data systems with the data linkage.",2019,,https://doi.org/10.1016/j.envres.2018.12.009
662,Serkan Ayvaz and Koray Alpay,Predictive maintenance system for production lines in manufacturing: A machine learning approach using IoT data in real-time,"Predictive maintenance, Internet of things, Manufacturing systems, Artificial intelligence, Machine learning, Big data","In this study, a data driven predictive maintenance system was developed for production lines in manufacturing. By utilizing the data generated from IoT sensors in real-time, the system aims to detect signals for potential failures before they occur by using machine learning methods. Consequently, it helps address the issues by notifying operators early such that preventive actions can be taken prior to a production stop. In current study, the effectiveness of the system was also assessed using real-world manufacturing system IoT data. The evaluation results indicated that the predictive maintenance system was successful in identifying the indicators of potential failures and it can help prevent some production stops from happening. The findings of comparative evaluations of machine learning algorithms indicated that models of Random Forest, a bagging ensemble algorithm, and XGBoost, a boosting method, appeared to outperform the individual algorithms in the assessment. The best performing machine learning models in this study have been integrated into the production system in the factory.",2021,,https://doi.org/10.1016/j.eswa.2021.114598
663,Humberto Ferreira and Pedro Ruivo and Carolina Reis,How do data scientists and managers influence machine learning value creation?,"machine learning, business value, data scientists, managers, people factor","Corporations are leveraging machine learning (ML) to create business value (BV). So, it becomes relevant to not only ponder the antecedents that influence the ML BV process but also, the main actors that influence the creation of such value within organizations: data scientists and managers. Grounded in the dynamic-capabilities theory, a model is proposed and tested with 319 responses to a survey. While for both groups, platform maturity and data quality are equally important factors for financial performance, information intensity is an equally important factor for organizational performance. On one hand, data scientists care more about the catalytic effect of data quality on the relationship between platform maturity and financial performance, and the compatibility factor for organizational performance. On the other hand, managers care more about the feasibility factor for financial performance. The findings presented here offer insights on how data scientists and managers perceive the ML BV creation process.",2021,,https://doi.org/10.1016/j.procs.2021.01.228
664,Artem Polyvyanyy and Anastasiia Pika and Moe T. Wynn and Arthur H.M. {ter Hofstede},A systematic approach for discovering causal dependencies between observations and incidents in the health and safety domain,"Big data, Data mining, Process mining, Proximity of events, Causality, Health and safety, Cause of incidents","The paper at hand motivates, proposes, demonstrates, and evaluates a novel systematic approach to discovering causal dependencies between events encoded in large arrays of data, called causality mining. The approach has emerged in the discussions with our project partner, an Australian public energy company. It was successfully evaluated in a case study with the project partner to extract valuable, and otherwise unknown, information on the causal dependencies between observations reported by the company’s employees as part of the organizational health and safety management practices and incidents that had occurred at the organization’s sites. The dependencies were derived based on the notion of proximity of the observations and incidents. The setup and results of the evaluation are reported in this paper. The new approach and the delivered insights aim at improving the overall health and safety culture of the project partner practices, as they can be applied to caution and, thus, prevent future incidents.",2019,,https://doi.org/10.1016/j.ssci.2019.04.045
665,Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar,Uncertainty of key performance indicators for Industry 4.0: A methodology based on the theory of belief functions,"Industry 4.0, Performance management, Decision support, Big Data, Uncertainty modeling","For the past few years, we have been hearing about Industry 4.0 (or the fourth industrial revolution), which promises to improve productivity, flexibility, quality, customer satisfaction and employee well-being. To assess whether these goals are achieved, it is necessary to implement a performance management system (PMS). However, a PMS must take into account the various challenges associated with Industry 4.0, including the availability of large amounts of data. While it represents an opportunity for companies to improve performance, big data does not necessarily mean good data. It can be uncertain, imprecise, ambiguous, etc. Uncertainty is one of the major challenges and it is essential to take it into account when computing performance indicators to increase confidence in decision making. To address this issue, we propose a method to model uncertainty in key performance indicators (KPIs). Our work allows associating with each indicator an uncertainty noted m, computed on the basis of the theory of belief functions. The KPI and its associated uncertainty form a pair (KP I, m). The method developed allows calculating this uncertainty m for the input data of the performance management system. We show how these modeled uncertainties should be propagated to the KPIs. For these KPI uncertainties, we have defined rules to support decision-making. The method developed, based on the theory of belief functions, is part of a methodology we propose to define and extract smart data from massive data. To our knowledge, this is the first attempt to use this theory to model uncertain performance indicators. Our work has shown its effectiveness and its applicability to a case of bottle filling line simulation. In addition to these results, this work opens up new perspectives, particularly for taking uncertainty into account in expert opinions and in industrial risk assessment.",2022,,https://doi.org/10.1016/j.compind.2022.103666
666,Michal Moran and Goren Gordon,Curious Feature Selection,"Intrinsic motivation learning, Curiosity loop, Reinforcement learning, Big data, Data science, Feature selection","In state-of-the-art big-data applications, the process of building machine learning models can be very challenging due to continuous changes in data structures and the need for human interaction to tune the variables and models over time. Hence, expedited learning in rapidly changing environments is required. In this work, we address this challenge by implementing concepts from the field of intrinsically motivated computational learning, also known as artificial curiosity (AC). In AC, an autonomous agent acts to optimize its learning about itself and its environment by receiving internal rewards based on prediction errors. We present a novel method of intrinsically motivated learning, based on the curiosity loop, to learn the data structures in large and varied datasets. An autonomous agent learns to select a subset of relevant features in the data, i.e., feature selection, to be used later for model construction. The agent optimizes its learning about the data structure over time without requiring external supervision. We show that our method, called the Curious Feature Selection (CFS) algorithm, positively impacts the accuracy of learning models on three public datasets.",2019,,https://doi.org/10.1016/j.ins.2019.02.009
667,Andrea {Fronzetti Colladon} and Barbara Guardabascio and Rosy Innarella,Using social network and semantic analysis to analyze online travel forums and forecast tourism demand,"Tourism forecasting, Social network analysis, Semantic analysis, Online community, Text mining, Big data","Forecasting tourism demand has important implications for both policy makers and companies operating in the tourism industry. In this research, we applied methods and tools of social network and semantic analysis to study user-generated content retrieved from online communities which interacted on the TripAdvisor travel forum. We analyzed the forums of 7 major European capital cities, over a period of 10 years, collecting more than 2,660,000 posts, written by about 147,000 users. We present a new methodology of analysis of tourism-related big data and a set of variables which could be integrated into traditional forecasting models. We implemented Factor Augmented Autoregressive and Bridge models with social network and semantic variables which often led to a better forecasting performance than univariate models and models based on Google Trend data. Forum language complexity and the centralization of the communication network – i.e. the presence of eminent contributors – were the variables that contributed more to the forecasting of international airport arrivals.",2019,,https://doi.org/10.1016/j.dss.2019.113075
668,Mouzhi Ge and Włodzimierz Lewoniewski,Developing the Quality Model for Collaborative Open Data,"Data Quality, Quality Assessment, Collaborative Open Data, Wikipedia, Quality Model","Nowadays, the development of data sharing technologies allows to involve more people to collaboratively contribute knowledge on the Web. The shared knowledge is usually represented as Collaborative Open Data (COD), for example, Wikipedia is one of the well-known sources for COD. The Wikipedia articles can be written in different languages, updated in real time, and originated from a vast variety of editors. However, COD also bring different data quality problems such as data inconsistency and low data objectiveness due to the crowd-based and dynamic nature. These data quality problems such as biased information may lead to sentimental changes or social impacts. This paper therefore proposes a new measurement model to assess the quality of COD. In order to evaluate the proposed model, A preliminary experiment is conducted with a large scale of Wikipedia articles to validate the applicability and efficiency of this proposed quality model in the real-world scenario.",2020,,https://doi.org/10.1016/j.procs.2020.09.228
669,Jérôme Béranger,2 - Ethical Development of the Medical Datasphere,"Algorithmic ethics, Architecture, Complex data, Ethical data mining, Ethical issues, Ethical-technical guidance, Evaluation, Medical datasphere, Neo-Platonic modeling","Abstract:
As Lucy Suchmann observed in 2011, through Lévi-Strauss, “…we are our tools…” and our personal health data are an integral part of us. In these circumstances, it becomes necessary to question the value of Big Data in the health sphere.",2016,,https://doi.org/10.1016/B978-1-78548-025-6.50002-6
670,Yuquan Xu and Xiaobin Ran and Yuewen Liu and Wei Huang,Comparing differences in the spatiotemporal patterns between resident tourists and non-resident tourists using hotel check-in registers,"Big data, Spatiotemporal patterns, Resident tourists and non-resident tourists, Multiple city travel, Hotel check-in registers","Previous research studied the spatiotemporal patterns in different visitor segments but lacks evidence of the segmentation of resident tourists and non-resident tourists in multi-city travel. To fill this gap, this study conducts a big data study using hotel check-in registers. The exploratory data analysis visualizes the spatiotemporal patterns and the differences between resident tourists and non-resident tourists. Then, the spatiotemporal patterns are measured by the length of stay and the number of visited cities. The regression shows that both the length of stay and the number of visited cities of non-resident tourists are higher than those of resident tourists. Moreover, non-resident tourists reduce their length of stay and their number of visited cities more than resident tourists on three-day holidays, while they increase their number of visited cities less than resident tourists on seven-day holidays. This study has significant implications for understanding spatiotemporal patterns and visitors' segmentations.",2021,,https://doi.org/10.1016/j.tmp.2021.100860
671,Marie-Aude Aufaure and Raja Chiky and Olivier Curé and Houda Khrouf and Gabriel Kepeklian,From Business Intelligence to semantic data stream management,"Data stream, Linked Data, Business Intelligence, Stream reasoning","The Semantic Web technologies are being increasingly used for exploiting relations between data. In addition, new tendencies of real-time systems, such as social networks, sensors, cameras or weather information, are continuously generating data. This implies that data and links between them are becoming extremely vast. Such huge quantity of data needs to be analyzed, processed, as well as stored if necessary. In this position paper, we will introduce recent work on Real-Time Business Intelligence combined with semantic data stream management. We will present underlying approaches such as continuous queries, data summarization and matching, and stream reasoning.",2016,,https://doi.org/10.1016/j.future.2015.11.015
672,Cuili Shao and Yonggang Yang and Sapna Juneja and Tamizharasi GSeetharam,IoT data visualization for business intelligence in corporate finance,"IoT, Data visualization, Business intelligence, Corporate finance","Business intelligence (BI) incorporates business research, data mining, data visualization, data tools,infrastructure, and best practices to help businesses make more data-driven choices.Business intelligence's challenging characteristics include data breaches, difficulty in analyzing different data sources, and poor data quality is consideredessential factors. In this paper, IoT-based Efficient Data Visualization Framework (IoT- EDVF) has been proposed to strengthen leaks' risk, analyze multiple data sources, and data quality management for business intelligence in corporate finance.Corporate analytics management is introduced to enhance the data analysis system's risk, and the complexity of different sources can allow accessing Business Intelligence. Financial risk analysis is implemented to improve data quality management initiative helps use main metrics of success, which are essential to the individual needs and objectives. The statistical outcomes of the simulation analysis show the increasedperformance with a lower delay response of 5ms and improved revenue analysis with the improvement of 29.42% over existing models proving the proposed framework's reliability.",2022,,https://doi.org/10.1016/j.ipm.2021.102736
673,Kerem Kayabay and Mert Onuralp Gökalp and Ebru Gökalp and P. {Erhan Eren} and Altan Koçyiğit,Data science roadmapping: An architectural framework for facilitating transformation towards a data-driven organization,"Technology roadmapping, Technology management, Data science, Digital transformation, Data-driven organization, Big data","Leveraging data science can enable businesses to exploit data for competitive advantage by generating valuable insights. However, many industries cannot effectively incorporate data science into their business processes, as there is no comprehensive approach that allows strategic planning for organization-wide data science efforts and data assets. Accordingly, this study explores the Data Science Roadmapping (DSR) to guide organizations in aligning their business strategies with data-related, technological, and organizational resources. The proposed approach is built on the widely adopted technology roadmapping framework and customizes its context, architecture, and process by synthesizing data science, big data, and data-driven organization literature. Based on industry collaborations, the framework provides a hybrid and agile methodology comprising the recommended steps. We applied DSR with a research group with sector experience to create a comprehensive data science roadmap to validate and refine the framework. The results indicate that the framework facilitates DSR initiatives by creating a comprehensive roadmap capturing strategy, data, technology, and organizational perspectives. The contemporary literature illustrates prebuilt roadmaps to help businesses become data-driven. However, becoming data-driven also necessitates significant social change toward openness and trust. The DSR initiative can facilitate this social change by opening communication channels, aligning perspectives, and generating consensus among stakeholders.",2022,,https://doi.org/10.1016/j.techfore.2021.121264
674,Xun Wang and Yahan Yang and Yuxuan Wu and Wenbin Wei and Li Dong and Yang Li and Xingping Tan and Hankun Cao and Hong Zhang and Xiaodan Ma and Qin Jiang and Yunfan Zhou and Weihua Yang and Chaoyu Li and Yu Gu and Lin Ding and Yanli Qin and Qi Chen and Lili Li and Mingyue Lian and Jin Ma and Dongmei Cui and Yuanzhou Huang and Wenyan Liu and Xiao Yang and Shuiming Yu and Jingjing Chen and Dongni Wang and Zhenzhe Lin and Pisong Yan and Haotian Lin,The national multi-center artificial intelligent myopia prevention and control project,"Myopia prevention and control, Artificial intelligent, National multicenter project","In recent years, the incidence of myopia has increased at an alarming rate among children and adolescents in China. The exploration of an effective prevention and control method for myopia is in urgent need. With the development of information technology in the past decade, artificial intelligence with the Internet of Things technology (AIoT) is characterized by strong computing power, advanced algorithm, continuous monitoring, and accurate prediction of long-term progression. Therefore, big data and artificial intelligence technology have the potential to be applied to data mining of myopia etiology and prediction of myopia occurrence and development. More recently, there has been a growing recognition that myopia study involving AIoT needs to undergo a rigorous evaluation to demonstrate robust results.",2021,,https://doi.org/10.1016/j.imed.2021.05.001
675,Hermanus J Smidt and Osden Jokonya,The challenge of privacy and security when using technology to track people in times of COVID-19 pandemic,"COVID 19, tracking, society, technology, privacy","Since the start of the Coronavirus disease 2019 (COVID-19) governments and health authorities across the world have find it very difficult in controlling infections. Digital technologies such as artificial intelligence (AI), big data, cloud computing, blockchain and 5G have effectively improved the efficiency of efforts in epidemic monitoring, virus tracking, prevention, control and treatment. Surveillance to halt COVID-19 has raised privacy concerns, as many governments are willing to overlook privacy implications to save lives. The purpose of this paper is to conduct a focused Systematic Literature Review (SLR), to explore the potential benefits and implications of using digital technologies such as AI, big data and cloud to track COVID-19 amongst people in different societies. The aim is to highlight the risks of security and privacy to personal data when using technology to track COVID-19 in societies and identify ways to govern these risks. The paper uses the SLR approach to examine 40 articles published during 2020, ultimately down selecting to the most relevant 24 studies. In this SLR approach we adopted the following steps; formulated the problem, searched the literature, gathered information from studies, evaluated the quality of studies, analysed and integrated the outcomes of studies while concluding by interpreting the evidence and presenting the results. Papers were classified into different categories such as technology use, impact on society and governance. The study highlighted the challenge for government to balance the need of what is good for public health versus individual privacy and freedoms. The findings revealed that although the use of technology help governments and health agencies reduce the spread of the COVID-19 virus, government surveillance to halt has sparked privacy concerns. We suggest some requirements for government policy to be ethical and capable of commanding the trust of the public and present some research questions for future research.",2021,,https://doi.org/10.1016/j.procs.2021.01.281
676,Pilar Escobar and Gustavo Candela and Juan Trujillo and Manuel Marco-Such and Jesús Peral,Adding value to Linked Open Data using a multidimensional model approach based on the RDF Data Cube vocabulary,"Linked Open Data, Multidimensional modelling, Conceptual modelling, RDF Data Cube vocabulary, Semantic web, Big data","Most organisations using Open Data currently focus on data processing and analysis. However, although Open Data may be available online, these data are generally of poor quality, thus discouraging others from contributing to and reusing them. This paper describes an approach to publish statistical data from public repositories by using Semantic Web standards published by the W3C, such as RDF and SPARQL, in order to facilitate the analysis of multidimensional models. We have defined a framework based on the entire lifecycle of data publication including a novel step of Linked Open Data assessment and the use of external repositories as knowledge base for data enrichment. As a result, users are able to interact with the data generated according to the RDF Data Cube vocabulary, which makes it possible for general users to avoid the complexity of SPARQL when analysing data. The use case was applied to the Barcelona Open Data platform and revealed the benefits of the application of our approach, such as helping in the decision-making process.",2020,,https://doi.org/10.1016/j.csi.2019.103378
677,Venkat N. Gudivada,Chapter 2 - Data Analytics: Fundamentals,"Data analytics, data science, data mining, clustering, classification, model building","This chapter provides a comprehensive and unified view of data analytics fundamentals. Four functional facets of data analytics—descriptive, diagnostic, predictive, and prescriptive—are described. The evolution of data analytics from SQL analytics, business analytics, visual analytics, big data analytics, to cognitive analytics is presented. Data science as the foundational discipline for the current generation of data analytics systems is explored in this chapter. Data lifecycle and data quality issues are outlined. Open source tools and resources for developing data analytics systems are listed. Future directions in data analytics are indicated. The chapter concludes by providing a summary. To reinforce and enhance the reader’s data analytics knowledge and tools, questions and exercise problems are provided at the end of the chapter.",2017,,https://doi.org/10.1016/B978-0-12-809715-1.00002-X
678,Deanne Larson and Victor Chang,"A review and future direction of agile, business intelligence, analytics and data science","Agile methodologies, Business intelligence (BI), Analytics and big data, Lifecycle for BI and Big Data","Agile methodologies were introduced in 2001. Since this time, practitioners have applied Agile methodologies to many delivery disciplines. This article explores the application of Agile methodologies and principles to business intelligence delivery and how Agile has changed with the evolution of business intelligence. Business intelligence has evolved because the amount of data generated through the internet and smart devices has grown exponentially altering how organizations and individuals use information. The practice of business intelligence delivery with an Agile methodology has matured; however, business intelligence has evolved altering the use of Agile principles and practices. The Big Data phenomenon, the volume, variety, and velocity of data, has impacted business intelligence and the use of information. New trends such as fast analytics and data science have emerged as part of business intelligence. This paper addresses how Agile principles and practices have evolved with business intelligence, as well as its challenges and future directions.",2016,,https://doi.org/10.1016/j.ijinfomgt.2016.04.013
679,Evelyn Hovenga and Heather Grain,"Chapter 9 - Quality data, design, implementation, and governance","Data quality, Data design, System implementation, Information governance, Leadership, Data supply chain","Data is the core for digital health systems; that data needs to be accurate, consistent, and available. The health workforce needs to understand how to define and govern data quality throughout the data supply chain, from origin through sharing, aggregated reporting, and eventually to enable the discovery of new knowledge based upon that data. Data quality applies to the data supply chain as a whole. Data needs to be collectable and useful at origin and able to represent and explain things that may not be known at the time of collection. Consistency of concept representation throughout the data supply chain needs to be clearly specified and transparent. Professional bodies need to provide leadership into the specification and governance of data, information, and computable knowledge, augmenting their traditional role of knowledge acquisition and publication based upon evidence. Throughout all levels of healthcare, the necessity of data quality needs to be better understood and managed.",2022,,https://doi.org/10.1016/B978-0-12-823413-6.00013-6
680,Igor Švab,Complexity of Patient Data in Primary Care Practice,"Big data, Digital health, Family medicine, Genomics, Primary care","The chapter deals with the practical implications of big data in clinical practice, especially primary care. Family medicine has always advocated individualized approach to patient care. Medicine is changing rapidly for different reasons. One of the reasons is the development of new technologies which is going to radically change medical practice in the future. One of the key changes will involve the importance and practice of data management. The traditional data management that was based on paper records is being changed to the electronic medical record which offers great potential for patient management. This transition will also give rise to new challenges to the practising physician. We are facing the challenge of new sources of data, their increase and variety. Currently, all these data is stored in different locations and there is no consensus whether one single profession is going to take responsibility for managing the data of the patient. If this is decided, the primary care practice would seem a logical solution. In order to do this would involve challenges to healthcare policy and infrastructure. The big data approach to medical care gives rise to new ethical challenges that we would have to address. Existing and future physicians will have to be educated in order to address all these issues for the benefit of their patients. Nevertheless, the physicians should still remember that even with the vast development of precision medicine, the patient will still be more than just a collection of data.",2021,,https://doi.org/10.1016/B978-0-12-801238-3.11590-9
681,Ching-Chan Cheng and Ya-Yuan Chang and Cheng-Ta Chen,Construction of a service quality scale for the online food delivery industry,"Online food delivery, Service quality, Big data analytic, OFD service quality scale","The main purpose of this study is based on qualitative and quantitative research procedures, and integrates the key service factors for the online food delivery (OFD) industry extracted by Internet Big Data Analytics (IBDA) to construct a OFD service quality scale (OFD-SERV). This study takes OFD customers in Taipei City as the objects. The results show that 20 key service factors for the OFD industry are extracted through IBDA. The OFD-SERV scale contains six dimensions including reliability, maintenance of meal quality and hygiene, assurance, security, system operation and traceability, a total of 28 items. The results from the structural equation modeling showed that the reliability, assurance and system operation have a positive impact on customer satisfaction. Finally, the findings provide knowledge and inspiration for the current OFD, and enable OFD operators and future researchers to more accurately identify the deficiency of service quality.",2021,,https://doi.org/10.1016/j.ijhm.2021.102938
682,Richard Vidgen and Sarah Shaw and David B. Grant,Management challenges in creating value from business analytics,"Analytics, Delphi, Management challenges, Value creation, Ecosystem","The popularity of big data and business analytics has increased tremendously in the last decade and a key challenge for organizations is in understanding how to leverage them to create business value. However, while the literature acknowledges the importance of these topics little work has addressed them from the organization's point of view. This paper investigates the challenges faced by organizational managers seeking to become more data and information-driven in order to create value. Empirical research comprised a mixed methods approach using (1) a Delphi study with practitioners through various forums and (2) interviews with business analytics managers in three case organizations. The case studies reinforced the Delphi findings and highlighted several challenge focal areas: organizations need a clear data and analytics strategy, the right people to effect a data-driven cultural change, and to consider data and information ethics when using data for competitive advantage. Further, becoming data-driven is not merely a technical issue and demands that organizations firstly organize their business analytics departments to comprise business analysts, data scientists, and IT personnel, and secondly align that business analytics capability with their business strategy in order to tackle the analytics challenge in a systemic and joined-up way. As a result, this paper presents a business analytics ecosystem for organizations that contributes to the body of scholarly knowledge by identifying key business areas and functions to address to achieve this transformation.",2017,,https://doi.org/10.1016/j.ejor.2017.02.023
683,Krish Krishnan,9 - Governance,"Big data application, Data management, Data-driven architecture, Governance, Machine learning, Master data, Metadata","Building the big data application is very interesting and can provide multiple users with multiple perspectives, data discovery to end state analytics and beyond is very much what everybody wants to achieve. Enterprises are ready to spend millions of dollars to get a share of your wallet, they want to be a part of your life and be present at every event that gets your attention. They want to leverage their partnerships and influence you, how do they make this all happen? The most successful companies will tell you their story is built on governance. The aspect of governance is very critical to the success of this journey whether internal or external. What governance are we talking about? How do we implement the same? This chapter will focus on those aspects.",2020,,https://doi.org/10.1016/B978-0-12-815746-6.00009-0
684,Diego Corrales-Garay and Marta Ortiz-de-Urbina-Criado and Eva-María Mora-Valentín,"Knowledge areas, themes and future research on open data: A co-word analysis","Open data, Bibliometric analysis, Co-word analysis, Science map, Knowledge areas, Most-studied themes, Future trends","This paper aims to contribute to a better understanding of the literature on open data in three ways. The first is to develop a descriptive analysis of journals and authors to identify the knowledge areas in which open data are applied. The second is to analyse the conceptual structure of the field using a bibliometric technique. The co-word analysis enabled us to create a map of the main themes that have been studied, identifying their importance and relevance. These themes were analysed and grouped. The third is to propose future research trends. According to our results, the main knowledge areas are Engineering, Health, Public Administration, Management and Education. The main themes are big data, open-linked data and data reuse. Finally, several research questions are proposed according to knowledge area and theme.",2019,,https://doi.org/10.1016/j.giq.2018.10.008
685,Yan Li and Manoj A. Thomas and Kweku-Muata Osei-Bryson,A snail shell process model for knowledge discovery via data analytics,"Knowledge discovery via data analytics, Snail shell process model, KDDA, Big data analytics, Data-driven decision making","The rapid growth of big data environment imposes new challenges that traditional knowledge discovery and data mining process (KDDM) models are not adequately suited to address. We propose a snail shell process model for knowledge discovery via data analytics (KDDA) to address these challenges. We evaluate the utility of the KDDA process model using real-world analytic case studies at a global multi-media company. By comparing against traditional KDDM models, we demonstrate the need and relevance of the snail shell model, particularly in addressing faster turnaround and frequent model updates that characterize knowledge discovery in the big data environment.",2016,,https://doi.org/10.1016/j.dss.2016.07.003
686,Marçal Farré and Federico Todeschini and Didier Grimaldi and Carlos Carrasco-Farré,Chapter 7 - Data-driven policy evaluation,"Survey data, Administrative data, Big data, Policy evaluation, Impact evaluation, Process evaluation","Public policies should be designed and implemented, whenever possible, using evidence as rigorous as possible. Urban interventions then should be no exception. In recent times, we have witnessed increasing efforts to transform information into knowledge, and thus help policymakers make better decisions. In this chapter, we will explore how public policy evaluation helps municipal governments tackle social problems and how big data can improve the design and implementation of more effective, efficient, and transparent policies.",2022,,https://doi.org/10.1016/B978-0-12-821122-9.00002-6
687,Abraham Zhang and Ray Y Zhong and Muhammad Farooque and Kai Kang and V G Venkatesh,Blockchain-based life cycle assessment: An implementation framework and system architecture,"Blockchain, Life cycle assessment, Supply chain sustainability, Environmental sustainability, Operational excellence","Life cycle assessment (LCA) is widely used for assessing the environmental impacts of a product or service. Collecting reliable data is a major challenge in LCA due to the complexities involved in the tracking and quantifying inputs and outputs at multiple supply chain stages. Blockchain technology offers an ideal solution to overcome the challenge in sustainable supply chain management. Its use in combination with internet-of-things (IoT) and big data analytics and visualization can help organizations achieve operational excellence in conducting LCA for improving supply chain sustainability. This research develops a framework to guide the implementation of Blockchain-based LCA. It proposes a system architecture that integrates the use of Blockchain, IoT, and big data analytics and visualization. The proposed implementation framework and system architecture were validated by practitioners who were experienced with Blockchain applications. The research also analyzes system implementation costs and discusses potential issues and solutions, as well as managerial and policy implications.",2020,,https://doi.org/10.1016/j.resconrec.2019.104512
688,Michael Butterworth,The ICO and artificial intelligence: The role of fairness in the GDPR framework,"Artificial intelligence (AI), Big data analytics, General Data Protection Regulation (GDPR), Fairness, Regulations, Collective rights, Data ethics","The year 2017 has seen many EU and UK legislative initiatives and proposals to consider and address the impact of artificial intelligence on society, covering questions of liability, legal personality and other ethical and legal issues, including in the context of data processing. In March 2017, the Information Commissioner's Office (UK) updated its big data guidance to address the development of artificial intelligence and machine learning, and to provide (GDPR), which will apply from 25 May 2018. This paper situates the ICO's guidance in the context of wider legal and ethical considerations and provides a critique of the position adopted by the ICO. On the ICO's analysis, the key challenge for artificial intelligence processing personal data is in establishing that such processing is fair. This shift reflects the potential for artificial intelligence to have negative social consequences (whether intended or unintended) that are not otherwise addressed by the GDPR. The question of ‘fairness’ is an important one, to address the imbalance between big data organisations and individual data subjects, with a number of ethical and social impacts that need to be evaluated.",2018,,https://doi.org/10.1016/j.clsr.2018.01.004
689,Deniz Appelbaum and Alexander Kogan and Miklos Vasarhelyi and Zhaokai Yan,Impact of business analytics and enterprise systems on managerial accounting,"Managerial accounting, Business analytics, Big data, Enterprise systems, Business intelligence","The nature of management accountants' responsibility is evolving from merely reporting aggregated historical value to also including organizational performance measurement and providing management with decision related information. Corporate information systems such as enterprise resource planning (ERP) systems have provided management accountants with both expanded data storage power and enhanced computational power. With big data extracted from both internal and external data sources, management accountants now could utilize data analytics techniques to answer the questions including: what has happened (descriptive analytics), what will happen (predictive analytics), and what is the optimized solution (prescriptive analytics). However, research shows that the nature and scope of managerial accounting has barely changed and that management accountants employ mostly descriptive analytics, some predictive analytics, and a bare minimum of prescriptive analytics. This paper proposes a Managerial Accounting Data Analytics (MADA) framework based on the balanced scorecard theory in a business intelligence context. MADA provides management accountants the ability to utilize comprehensive business analytics to conduct performance measurement and provide decision related information. With MADA, three types of business analytics (descriptive, predictive, and prescriptive) are implemented into four corporate performance measurement perspectives (financial, customer, internal process, and learning and growth) in an enterprise system environment. Other related issues that affect the successful utilization of business analytics within a corporate-wide business intelligence (BI) system, such as data quality and data integrity, are also discussed. This paper contributes to the literature by discussing the impact of business analytics on managerial accounting from an enterprise systems and BI perspective and by providing the Managerial Accounting Data Analytics (MADA) framework that incorporates balanced scorecard methodology.",2017,,https://doi.org/10.1016/j.accinf.2017.03.003
690,Carlos A. Escobar and Debejyo Chakraborty and Megan McGovern and Daniela Macias and Ruben Morales-Menendez,"Quality 4.0 — Green, Black and Master Black Belt Curricula","Quality 4.0, Certification, Smart manufacturing, Artificial intelligence, Big data","Industrial Big Data (IBD) and Artificial Intelligence (AI) are propelling the new era of manufacturing - smart manufacturing. Manufacturing companies can competitively position themselves amongst the most advanced and influential companies by successfully implementing Quality 4.0 practices. Despite the global impact of COVID-19 and the low deployment success rate, industrialization of the AI mega-trend has dominated the business landscape in 2020. Although these technologies have the potential to advance quality standards, it is not a trivial task. A significant portion of quality leaders do not yet have a clear deployment strategy and universally cite difficulty in harnessing such technologies. The lack of people power is one of the biggest challenges. From a career development standpoint, the higher-educated employees (such as engineers) are the most exposed to, and thus affected by, these new technologies. 79% of young professionals have reported receiving training outside of formal schooling to acquire the necessary skills for Industry 4.0. Strategically investing in training is thus important for manufacturing companies to generate value from IBD and AI. Following the path traced by Six Sigma, this article presents a certification curricula for Green, Black, and Master Black Belts. The proposed curriculum combines six areas of knowledge: statistics, quality, manufacturing, programming, learning, and optimization. These areas, along with an ad hoc 7-step problem solving strategy, must be mastered to obtain a certification. Certified professionals will be well positioned to deploy Quality 4.0 technologies and strategies. They will have the capacity to identify engineering intractable problems that can be formulated as machine learning problems and successfully solve them. These certifications are an efficient and effective way for professionals to advance in their career and thrive in Industry 4.0.",2021,,https://doi.org/10.1016/j.promfg.2021.06.085
691,Zhengxin Chen,Towards Integrated Study of Data Management and Data Mining,"Big Data, granular computing, granularity, databases, rough set theory, database keyword search (DBKWS)","From very beginning, research and practice of database management systems (DBMSs) have been cantered on handling granulation and granularities at various levels, thus sharing common interests with granular computing (GrC). Although DBMS and GrC have different focuses, the advent of Big Data has brought these two research areas closer to each other, because Big Data requires integrated study of data storage and analysis. In this paper, we explore this issue. Starting with an examination of granularities from a database perspective, we discuss new challenges of Big Data. We then turn to data management issues related to GrC. As an example of possible cross-fertilization of these two fields, we examine the recent development of database keyword search (DBKWS). Even research in DBKWS is largely independent to GrC, DBKWS has to handle various issues related to granularity handling. In particular, aggregation of DBKWS results is closely related to studies in granularities and granulation, which echoes L. Zadeh's famous formula: Granulation = Summarization. We present our proposed approach, termed as extended keyword search, which illustrates that an integrated study of data management and data mining/analysis is not restricted to GrC or rough set theory",2015,,https://doi.org/10.1016/j.procs.2015.07.117
692,Elisabetta Raguseo and Federico Pigni and Claudio Vitari,Streams of digital data and competitive advantage: The mediation effects of process efficiency and product effectiveness,"Streams of big data, Process efficiency, Product effectiveness, Competitive advantage","Firms can achieve a competitive advantage by leveraging real-time Digital Data Streams (DDSs). The ability to profit from DDSs is emerging as a critical competency for firms and a novel area for Information Technology (IT) investments. We examine the relationship between DDS readiness and competitive advantage by studying the mediation effect of product effectiveness and process efficiency. The research model is tested with data obtained from 302 companies, and the results confirm the existence of the mediation effects. Interestingly, we confirm that competitive advantage is more significantly impacted by IT investments affecting product effectiveness than those affecting process efficiency.",2021,,https://doi.org/10.1016/j.im.2021.103451
693,João de Abreu e Silva and Mark Davis,Workshop Synthesis: Respondent/Survey Interaction in a World of Web and Smartphone Apps,"web based surveys, smartphone surveys, respondent interaction, respondent burden","Web and smartphone surveys are increasingly being used to collect travel information. This workshop explored respondent interaction with these tools, covering a range of research concerns. While smartphone surveys facilitate real-time passive collection of continuous data, thereby reducing respondent burden, their use raises many issues common with those present in web surveys. These include survey design, sample representativeness, privacy, respondent burden, data quality and validation. Workshop participants considered possible areas for future research on these issues and others such as provision of feedback to respondents, linking with big data and focusing on attitudinal and behavioural motivations.",2015,,https://doi.org/10.1016/j.trpro.2015.12.025
694,Turgut Ozkan,Criminology in the age of data explosion: New directions,"Social science, Big data, Crime, Social media, Data-driven social science","This review discusses practical benefits and limitations of novel data-driven research for social scientists in general and criminologists in particular by providing a comprehensive examination of the matter. Specifically, this study is an attempt to critically evaluate ‘big data’, data-driven perspectives, and their epistemological value for both scholars and practitioners, particularly those working on crime. It serves as guidance for those who are interested in data-driven research by pointing out new research avenues. In addition to the benefits, the drawbacks associated with data-driven approaches are also discussed. Finally, critical problems that are emerging in this era, such as privacy and ethical concerns are highlighted.",2019,,https://doi.org/10.1016/j.soscij.2018.10.010
695,Adeel Anjum and Tahir Ahmed and Abid Khan and Naveed Ahmad and Mansoor Ahmad and Muhammad Asif and Alavalapati Goutham Reddy and Tanzila Saba and Nayma Farooq,Privacy preserving data by conceptualizing smart cities using MIDR-Angelization,"Big data, IoT data management, Disclosure risk, HIPAA, Patient privacy, Re-identification risk, Smart city","Smart City and IoT improves the performance of health, transportation, energy and reduce the consumption of resources. Among the smart city services, Big Data analytics is one of the imperative technologies that have a vast perspective to reach sustainability, enhanced resilience, effective quality of life and quick management of resources. This paper focuses on the privacy of big data in the context of smart health to support smart cities. Furthermore, the trade-off between the data privacy and utility in big data analytics is the foremost concern for the stakeholders of a smart city. The majority of smart city application databases focus on preserving the privacy of individuals with different disease data. In this paper, we propose a trust-based hybrid data privacy approach named as “MIDR-Angelization” to assure privacy and utility in big data analytics when sharing same disease data of patients in IoT industry. Above all, this study suggests that privacy-preserving policies and practices to share disease and health information of patients having the same disease should consider detailed disease information to enhance data utility. An extensive experimental study performed on a real-world dataset to measure instance disclosure risk which shows that the proposed scheme outperforms its counterpart in terms of data utility and privacy.",2018,,https://doi.org/10.1016/j.scs.2018.04.014
696,Stefan Reis and Edmund Seto and Amanda Northcross and Nigel W.T. Quinn and Matteo Convertino and Rod L. Jones and Holger R. Maier and Uwe Schlink and Susanne Steinle and Massimo Vieno and Michael C. Wimberly,Integrating modelling and smart sensors for environmental and human health,"Integrated modelling, Environmental sensors, Population health, Environmental health, Big data","Sensors are becoming ubiquitous in everyday life, generating data at an unprecedented rate and scale. However, models that assess impacts of human activities on environmental and human health, have typically been developed in contexts where data scarcity is the norm. Models are essential tools to understand processes, identify relationships, associations and causality, formalize stakeholder mental models, and to quantify the effects of prevention and interventions. They can help to explain data, as well as inform the deployment and location of sensors by identifying hotspots and areas of interest where data collection may achieve the best results. We identify a paradigm shift in how the integration of models and sensors can contribute to harnessing ‘Big Data’ and, more importantly, make the vital step from ‘Big Data’ to ‘Big Information’. In this paper, we illustrate current developments and identify key research needs using human and environmental health challenges as an example.",2015,,https://doi.org/10.1016/j.envsoft.2015.06.003
697,Tian Wu and Xiao Han and M. Mocarlo Zheng and Xunmin Ou and Hongbo Sun and Xiong Zhang,Impact factors of the real-world fuel consumption rate of light duty vehicles in China,"Real-world fuel consumption rate, Energy consumption, Private passenger vehicles, Big data, China","Measuring real-world fuel consumption of light duty vehicles can be challenging due to the limited collection of actual data. In this paper, we use big data retrieved from the record of real-world fuel consumptions of different brands of vehicles in different areas (n = 106,809 samples from 201 brands of vehicles and 34 cities) in China to build up a real-world fuel consumption rate (RFCR) model to estimate the fuel consumption given the driving conditions and figure out the main factors that affect actual fuel consumption in the real world. We find the average deviation of actual fuel consumptions and the fitting results of RFCR model is 4.22% , which does not significantly differ from zero, and the fuel consumptions calculated by RFCR model tend to be 1.40 L/100 km (about 25%) higher than the official reported data. Furthermore, we find that annual average temperature and altitude factors significantly influence the fuel consumption rate. The results indicate that there is a real world performance discrepancy between the theoretical fuel consumption released by authorities and that in the real world, and some green behaviors (choose light duty vehicles, reduce the use of air conditioning and change to manual transmission type) can reduce energy consumption of vehicles.",2020,,https://doi.org/10.1016/j.energy.2019.116388
698,Pramod Kumar and Jaiprakash Bhamu and Kuldip Singh Sangwan,Analysis of Barriers to Industry 4.0 adoption in Manufacturing Organizations: an ISM Approach,"Industry 4.0, interpretive structural modeling, digital manufacturing, barriers, MICMAC analysis","Industry 4.0 has enabled technological integration of cyber physical systems and internet based communication in manufacturing value creation processes. As of now, many people use it as a collective term for advanced technologies, i.e. advanced robotics, artificial intelligence, machine learning, big data analytics, cloud computing, smart sensors, internet of things, augmented reality, etc. This substantially improves flexibility, quality, productivity, cost, and customer satisfaction by transforming existing centralized manufacturing systems towards digital and decentralized one. Despite having potential benefits of industry 4.0, the organizations are facing typical obstacles and challenges in adopting new technologies and successful implementation in their business models. This paper aims to identify potential barriers which may hinder the implementation of industry 4.0 in manufacturing organizations. The identified barriers, through comprehensive literature review and on the basis of opinions collected from industry experts, are: poor value-chain integration, cyber-security challenges, uncertainty about economic benefits, lack of adequate skills in workforce, high investment requirements, lack of infrastructure, jobs disruptions, challenges in data management and data quality, lack of secure standards and norms, and resistance to change. Interpretive Structural Modeling (ISM) is used to establish relationships among these barriers to develop a hierarchical model and MICMAC analysis for further classification of identified barriers for better understanding. An analysis of driving and dependence of the barriers may help in clear understanding of these for successful implementation of Industry 4.0 practices in the organizations.",2021,,https://doi.org/10.1016/j.procir.2021.01.010
699,Samiya Khan and Xiufeng Liu and Syed Arshad Ali and Mansaf Alam,"Bivariate, cluster, and suitability analysis of NoSQL solutions for big graph applications","NoSQL, Big data system, Storage solution, Bivariate analysis, Cluster analysis, Classification","With the explosion of social media, the Web, Internet of Things, and the proliferation of smart devices, large amounts of data are being generated each day. However, traditional data management technologies are increasingly inadequate to cope with this growth in data. NoSQL has become increasingly popular as this technology can provide consistent, scalable and available solutions for the ever-growing heterogeneous data. Recent years have seen growing applications shifting from traditional data management systems to NoSQL solutions. However, there is limited in-depth literature reporting on NoSQL storage technologies for big graph and their applications in various fields. This chapter fills this gap by conducting a comprehensive study of 80 state-of-the-art NoSQL technologies. In this chapter, we first present a feature analysis of the NoSQL solutions and then generate a data set of the investigated solutions for further analysis in order to better understand and select the technologies. We perform a clustering analysis to segment the NoSQL solutions, compare the classified solutions based on their storage data models and Brewer's CAP theorem, and examine big graph applications in six specific domains. To help users select appropriate NoSQL solutions, we have developed a decision tree model and a web-based user interface to facilitate this process. In addition, the significance, challenges, applications and categories of storage technologies are discussed as well.",2022,,https://doi.org/10.1016/bs.adcom.2021.09.006
700,Martijn Groot,Chapter 5 - Data Management Tools and Techniques,"data management technology, databases, big data, financial analytics, IT management","In this chapter we look at the technology and tooling available for data management. We start with a discussion on the different components of an IT infrastructure and how to describe them. We provide a short taxonomy of tools from analytics and data distribution to data governance and end-user tools. This is followed by a discussion on data models and storage models, from traditional relational databases to different challenger technologies including NoSQL databases. We discuss data quality management and curation processes and data storage at different scales from data marts and warehouses to data lakes. We then discuss data analytics and big data technologies and what some of the use cases and implications for financial services firms are. After discussing privacy and security aspects, and the promising application areas of blockchain technology in master data, we discuss cloud storage models and what the cloud trend means for banks and asset managers. We end with a discussion on IT sourcing options, IT management, and IT maturity models before concluding with a look ahead.",2017,,https://doi.org/10.1016/B978-0-12-809776-2.00005-3
701,Jiaping Wu and Junyu He and George Christakos,Chapter 3 - CTDA methodology,"Methodological chain, Knowledge bases, Big data, Scales, Visualization, Chronotopologic statistics","Abstract The methodological characteristics of the chronotopologic data analysis chain are discussed. Various kinds of knowledge are considered and properly classified, and several illustrative examples in applied sciences are presented. Big data and data-driven analyses are critically reviewed, and their implementation carefully assessed. Data scale types are classifications considered in property- and attribute-oriented settings. Classical statistics inadequacies are pointed out and the need of a chronotopology-dependent statistics is outlined. The chronotopologic visualization thinking mode and techniques are briefly reviewed.",2022,,https://doi.org/10.1016/B978-0-12-816341-2.00010-1
702,Christian Dalheim Øien and Sebastian Dransfeld,An approach to data structuring and predictive analysis in discrete manufacturing,"Anomaly Detection, Predictive Maintenance, Discrete Manufacturing, Big Data Analytics, Adaptive Self-learning Systems","In discrete manufacturing the variation in process parameters and duration is often large. Common data storage and analytics systems primarily store data in univariate time series, and when analysing machine components of strongly varying lifetime and behaviour this causes a challenge. This paper presents a data structure and an analysis method for outlier detection which intends to deal with this challenge, as an alternative to predictive maintenance which often requires more data with higher quality than what is available. A case study in aluminium extrusion billet manufacturing is used to demonstrate the approach, predominantly detecting anomalies at the end of a critical component’s lifetime.",2021,,https://doi.org/10.1016/j.procir.2021.11.224
703,Zhuangye Luo and Jia Xu and Pengcheng Zhao and Dejun Yang and Lijie Xu and Jian Luo,Towards high quality mobile crowdsensing: Incentive mechanism design based on fine-grained ability reputation,"Mobile crowdsensing, Incentive mechanism, Quality-aware, Reputation system","Mobile crowdsensing has become an efficient paradigm for performing large-scale sensing tasks. Many quality-aware incentive mechanisms for mobile crowdsensing have been proposed. However, most of them measure the data quality by one single metric from a specific perspective. Moreover, they usually use the real-time quality, which cannot provide sufficient incentive for the workers with long-term high quality. In this paper, we refine the generalized data quality into the fine-grained ability requirement. We present a mobile crowdsensing system to achieve the fine-grained quality control, and formulate the problem of maximizing the social cost such that the fine-grained ability requirement of all sensing tasks can be satisfied. To stimulate the workers with long-term high quality, we design two ability reputation systems to assess workers’ fine-grained abilities online. The incentive mechanism based on the reverse auction and fine-grained ability reputation system is proposed. We design a greedy algorithm to select the winners and determine the payment based on the bids and fine-grained ability reputation of workers. Through both rigorous theoretical analysis and extensive simulations, we demonstrate that the proposed mechanisms achieve computational efficiency, individual rationality, truthfulness, whitewashing proof, and guaranteed approximation. Moreover, the designed mechanisms show prominent advantage in terms of social cost and average ability achievement ratio.",2021,,https://doi.org/10.1016/j.comcom.2021.09.026
704,Yonggui Guo and Ibrahim Mohamed and Ali Zidane and Yashesh Panchal and Omar Abou-Sayed and Ahmed Abou-Sayed,Automated pressure transient analysis: A cloud-based approach,"Pressure transient analysis, Cloud computing, ISIP, Flow regime, G-function, Web-based application","Pressure transient analysis provides essential information to evaluate the dimensions of injection induced fractures, permeability damage near the wellbore, and pressure elevation in the injection horizon. For injection wells, shut-in data can be collected and analyzed after each injection cycle to evaluate the well injectivity and predict the well longevity. However, any interactive analysis of the pressure data could be subjective and time-consuming. In this study a novel cloud-based approach to automatically analyzing pressure data is presented, which aims to improve the reliability and efficiency of pressure transient analysis. There are two fundamental requirements for automated pressure transient analysis: 1) Pressure data needs to be automatically retrieved from field sites and fed to the analyzer; 2) The engineer can automatically select instantaneous shut-in pressure (ISIP), identify flow regimes, and determine the fracture closure point if any. To meet these requirements as well as to take advantage of cloud storage and computing technologies, a web-based application has been developed to pull real time injection data from any field sites and push it to a cloud database. A built-in pressure transient workflow has been also proposed to detect any stored or real-time pressure data and perform pressure analysis automatically if the required data is available. The automated pressure transient analysis technology has been applied to multiple injection projects. In general, the analysis results including formation and fracture properties (i.e. permeability, fracture half length, skin factor, and fracture closure pressure) are comparable to results from interactive analysis. Any discrepancies are mainly caused by poor data quality. Issues such as inconsistent selections of ISIP and different slopes defined for pre and after closure analyses also contribute to the divergence. Overall, the automated pressure transient analysis provides consistent results as the exact same criteria are applied to the pressure data, and analysis results are independent of the analyzer's experience and knowledge. As data from oil/gas industry increases exponentially over time, automated data transmission, storage, analysis and access are becoming necessary to maximize the value of the data and reduce operation cost. The automated pressure transient analysis presented here demonstrates that cloud storage and computing combined with automated analysis tools is a viable way to overcome big data challenges faced by oil/gas industry professionals.",2021,,https://doi.org/10.1016/j.petrol.2020.107627
705,Ming Zhang and Danyun Dai and Siliang Hou and Wei Liu and Feng Gao and Dong Xu and Yu Hu,Thinking on the informatization development of China's healthcare system in the post-COVID-19 era,"Coronavirus disease 2019, Healthcare system, Informatization","With the application of Internet of Things, big data, cloud computing, artificial intelligence, and other cutting-edge technologies, China's medical informatization is developing rapidly. In this paper, we summaried the role of information technology in healthcare sector's battle against the coronavirus disease 2019 (COVID-19) from the perspectives of early warning and monitoring, screening and diagnosis, medical treatment and scientific research, analyzes the bottlenecks of the development of information technology in the post-COVID-19 era, and puts forward feasible suggestions for further promoting the construction of medical informatization from the perspectives of sharing, convenience, and safety.",2021,,https://doi.org/10.1016/j.imed.2021.03.004
706,Jun Wang and Yun-si Li and Wei Song and Ai-hua Li,Research on the Theory and Method of Grid Data Asset Management,"big data, grid data asset, asset management, data governance","In the era of Big Data, data assets have become a strategic resource which cannot be overlooked by both society and enterprises. However, data is not equal to the assets. This paper first introduces the necessary conditions of data assetization and discriminates the concepts of data governance, data management and data asset management. Then it focuses on the unique connotation and characteristics of grid data assets. With reference to the mainstream theory of data management, the framework for the grid data asset management is set up in the combination of the characteristics of data assets, business needs and the actual situation in the power supply enterprises. Finally, this paper puts forward higher system requirements and technical requirements for China’s power supply enterprises to conduct data asset management.",2018,,https://doi.org/10.1016/j.procs.2018.10.258
707,Shahriar Akter and Ruwan Bandara and Umme Hani and Samuel {Fosso Wamba} and Cyril Foropon and Thanos Papadopoulos,Analytics-based decision-making for service systems: A qualitative study and agenda for future research,"Big data analytics, Decision-making, Service systems","While the use of big data tends to add value for business throughout the entire value chain, the integration of big data analytics (BDA) to the decision-making process remains a challenge. This study, based on a systematic literature review, thematic analysis and qualitative interview findings, proposes a set of six-steps to establish both rigor and relevance in the process of analytics-driven decision-making. Our findings illuminate the key steps in this decision process including problem definition, review of past findings, model development, data collection, data analysis as well as actions on insights in the context of service systems. Although findings have been discussed in a sequence of steps, the study identifies them as interdependent and iterative. The proposed six-step analytics-driven decision-making process, practical evidence from service systems, and future research agenda, provide altogether the foundation for future scholarly research and can serve as a step-wise guide for industry practitioners.",2019,,https://doi.org/10.1016/j.ijinfomgt.2019.01.020
708,Christina G. Skarpathiotaki and Konstantinos E. Psannis,Cross-Industry Process Standardization for Text Analytics,"Big data analytics, Advanced analysis, Artificial intelligence, Machine learning, Text analytics, Cross-industry processes","We are living in a world where everything computes, everyone and everything is connected and sharing data. Going beyond just capturing and managing data, enterprises are tapping into IoT and Artificial Intelligence (AI) to create insights and intelligence in a revolutionary way that was not possible before. For instance, by analyzing unstructured data (such as text), call centers can extract entities, concepts, themes which can enable them to get faster insights that only few years back was not feasible. Public safety and law enforcement are only few of the examples that benefit from text analytics used to strengthen crime investigation. Sentiment Analysis, Content Classification, Language Detection and Intent Detection are just some of the Text Classification applications. The overall process model of such applications considering the complexity of the unstructured data, can be definitely challenging. In response to the chaotic emerging science of unstructured data analysis, the main goal of this paper is to first contribute to the gap of no existing methodology approach for Text Analytics projects, by introducing a methodology approach based on one of the most widely accepted and used methodology approach of CRISP-DM.",2022,,https://doi.org/10.1016/j.bdr.2021.100274
709,Haitham Yaish and Madhu Goyal and George Feuerlicht,Multi-tenant Elastic Extension Tables Data Management,"Cloud Computing, Software as a Service, Big Data, Elastic Extension Tables, Multi-tenant Database, Relational Tables, Virtual Relational Tables.","Multi-tenant database is a new database solution which is significant for Software as a service (SaaS) and Big Data applications in the context of cloud computing paradigm. This multi-tenant database has significant design challenges to develop a solution that ensures a high level of data quality, accessibility, and manageability for the tenants using this database. In this paper, we propose a multi-tenant data management service called Elastic Extension Tables Schema Handler Service (EETSHS), which is based on a multi-tenant database schema called Elastic Extension Tables (EET). This data management service satisfies tenants’ different business requirements, by creating, managing, organizing, and administratinglarge volumes of structured, semi-structured, and unstructured data. Furthermore, it combines traditional relational data with virtual relational data in a single database schema and allows tenants to manage this data by calling functions from this service. We present algorithms for frequently used functions of this service, and perform several experiments to measure the feasibility and effectiveness of managing multi-tenant data using these functions. We report experimental results of query execution timesfor managing tenants’ virtual and traditional relational data showing that EET schema is a good candidate for the management of multi-tenant data for SaaS and Big Data applications.",2014,,https://doi.org/10.1016/j.procs.2014.05.202
710,Li-Minn Ang and Kah Phooi Seng,Big Sensor Data Applications in Urban Environments,"Big data, Sensor-based systems, Survey, Application, Challenges","The emergence of new technologies such as Internet/Web/Network-of-Things and large scale wireless sensor systems enables the collection of data from an increasing volume and variety of networked sensors for analysis. In this review article, we summarize the latest developments of big sensor data systems (a term to conceptualize the application of the big data model towards networked sensor systems) in various representative studies for urban environments, including for air pollution monitoring, assistive living, disaster management systems, and intelligent transportation. An important focus is the inclusion of how value is extracted from the big data system. We also discuss some recent techniques for big data acquisition, cleaning, aggregation, modeling, and interpretation in large scale sensor-based systems. We conclude the paper with a discussion on future perspectives and challenges of sensor-based data systems in the big data era.",2016,,https://doi.org/10.1016/j.bdr.2015.12.003
711,Siqi Liu and Tianyu Wang and Shaowei Wang,Toward intelligent wireless communications: Deep learning - based physical layer technologies,"Data-driven, Deep learning, Physical layer, Wireless communications","Advanced technologies are required in future mobile wireless networks to support services with highly diverse requirements in terms of high data rate and reliability, low latency, and massive access. Deep Learning (DL), one of the most exciting developments in machine learning and big data, has recently shown great potential in the study of wireless communications. In this article, we provide a literature review on the applications of DL in the physical layer. First, we analyze the limitations of existing signal processing techniques in terms of model accuracy, global optimality, and computational scalability. Next, we provide a brief review of classical DL frameworks. Subsequently, we discuss recent DL-based physical layer technologies, including both DL-based signal processing modules and end-to-end systems. Deep neural networks are used to replace a single or several conventional functional modules, whereas the objective of the latter is to replace the entire transceiver structure. Lastly, we discuss the open issues and research directions of the DL-based physical layer in terms of model complexity, data quality, data representation, and algorithm reliability.",2021,,https://doi.org/10.1016/j.dcan.2021.09.014
712,Mikiharu Arimura and Tran Vinh Ha and Kota Okumura and Takumi Asada,"Changes in urban mobility in Sapporo city, Japan due to the Covid-19 emergency declarations","Covid-19, Moving pattern, Mobile spatial statistics, Population concentration, Big data","At the time of writing, the world is facing the new coronavirus pandemic, which has been declared one of the most dangerous disasters of the 21st century. All nations and communities have applied many countermeasures to control the spread of the epidemic. In terms of countermeasures, lockdowns and reductions of social activities are meant to flatten the curve of infection. Nevertheless, to date, there has been no evaluation of the effectiveness of these methods. Thus, the present study aims to interpret the change in the population density of Sapporo city in the emergency's period declaration using big data obtained from mobile spatial statistics. The results indicate that, in the time of refraining from traveling, the city's residents have been more likely to stay home and less likely to travel to the center area. This has led to a decrease of up to 90% of the population density in crowded areas. The study's outcomes partly explain the statement of reducing 70%–80% of contact between people in line with the purpose of the emergency declaration. Moreover, these findings establish the primary step for further analysis of estimating the efficiency of policy in controlling the epidemic.",2020,,https://doi.org/10.1016/j.trip.2020.100212
713,S. Thomas Ng and Frank J. Xu and Yifan Yang and Mengxue Lu,"A Master Data Management Solution to Unlock the Value of Big Infrastructure Data for Smart, Sustainable and Resilient City Planning","Big data, integrated infrastructure asset management, master data management, smart city, smart infrastructure","In recent years, many governments have launched various smart city or smart infrastructure initiatives to improve the quality of citizens’ life and help city managers / planners optimize the operation and management of urban infrastructures. By deploying internet of things (IoT) to infrastructure systems, high-volume and high-variety of data pertinent to the condition and performance of infrastructure systems along with the behaviors of citizens can be gathered, processed, integrated and analyzed through cloud-based infrastructure asset management systems, ubiquitous mobile applications and big data analytics platforms. Nonetheless, how to fully exploit the value of ‘big infrastructure data’ is still a key challenge facing most stakeholders. Unless data is shared by different infrastructure systems in an interoperable and consistent manner, it is difficult to realize the smart infrastructure concept for efficient smart city planning, not to mention about developing appropriate resilience and sustainable programs. To unlock the value of big infrastructure data for smart, sustainable and resilient city planning, a master data management (MDM) solution is proposed in this paper. MDM has been adopted in the business sector to orchestrate operational and analytical big data applications. In order to derive a suitable MDM solution for smart, sustainable and resilient city planning, commercial and open source MDM systems, smart city standards, smart city concept models, smart community infrastructure frameworks, semantic web technologies will be critically reviewed, and feedback and requirements will be gathered from experts who are responsible for developing smart, sustainable and resilient city programs. A case study which focuses on the building and transportation infrastructures of a selected community in Hong Kong will be conducted to pilot the proposed MDM solution.",2017,,https://doi.org/10.1016/j.proeng.2017.08.034
714,Yanqing Wang and Fuhai Song and Junwei Zhu and Sisi Zhang and Yadong Yang and Tingting Chen and Bixia Tang and Lili Dong and Nan Ding and Qian Zhang and Zhouxian Bai and Xunong Dong and Huanxin Chen and Mingyuan Sun and Shuang Zhai and Yubin Sun and Lei Yu and Li Lan and Jingfa Xiao and Xiangdong Fang and Hongxing Lei and Zhang Zhang and Wenming Zhao,GSA: Genome Sequence Archive*,"Genome Sequence Archive, GSA, Big data, Raw sequence data, INSDC","With the rapid development of sequencing technologies towards higher throughput and lower cost, sequence data are generated at an unprecedentedly explosive rate. To provide an efficient and easy-to-use platform for managing huge sequence data, here we present Genome Sequence Archive (GSA; http://bigd.big.ac.cn/gsa or http://gsa.big.ac.cn), a data repository for archiving raw sequence data. In compliance with data standards and structures of the International Nucleotide Sequence Database Collaboration (INSDC), GSA adopts four data objects (BioProject, BioSample, Experiment, and Run) for data organization, accepts raw sequence reads produced by a variety of sequencing platforms, stores both sequence reads and metadata submitted from all over the world, and makes all these data publicly available to worldwide scientific communities. In the era of big data, GSA is not only an important complement to existing INSDC members by alleviating the increasing burdens of handling sequence data deluge, but also takes the significant responsibility for global big data archive and provides free unrestricted access to all publicly available data in support of research activities throughout the world.",2017,,https://doi.org/10.1016/j.gpb.2017.01.001
715,Lucy Fortson,Chapter 10 - From Green Peas to STEVE: Citizen Science Engagement in Space Science,"Citizen Science, Crowdsourcing, Machine learning, Volunteer engagement","The past two decades has seen a tremendous rise in citizen science and crowdsourcing techniques as a means to carry out ground-breaking research while at the same time engage the general public in the wonders of space science. This article reviews some of the recent advances made in this realm as well as lessons learned from the unique perspective of the author’s role as a cofounder of the Zooniverse citizen science platform and practicing astrophysics researcher. I briefly describe the factors that led to the recent rise of citizen science including the formation of governance bodies at national and international levels, and the adoption by Federal Agencies within the United States government. I address concerns raised by research colleagues on the validity of citizen science as a research methodology, and then describe several key metrics for the success of citizen science including the link between data quality and publications, and the critical role that motivation and engagement of volunteer participants play in project success. I use the Green Pea galaxies discovered by Galaxy Zoo volunteers and an aurora-like phenomenon known as STEVE discovered by Aurorasaurus volunteers as examples of how, with the right tools and support, non-professional volunteers can make key contributions to space science. I then describe the role that machine learning can play when judiciously teamed with citizen scientists to tackle the ever-growing challenge of big data and close with some reflections on what it takes to support and manage a large platform like the Zooniverse.",2021,,https://doi.org/10.1016/B978-0-12-817390-9.00009-9
716,Nathalie T.M. Demoulin and Kristof Coussement,Acceptance of text-mining systems: The signaling role of information quality,"Technology acceptance model (TAM), Text mining, Big data, Information quality, Top management support","The popularity of the big data domain has boosted corporate interest in collecting and storing tremendous amounts of consumers’ textual information. However, decision makers are often overwhelmed by the abundance of information, and the usage of text mining (TM) tools is still at its infancy. This study validates an extended technology acceptance model integrating information quality (IQ) and top management support. Results confirm that IQ influences behavioral intentions and TM tools usage, through perceptions of external control, perceived ease of use, and perceived usefulness; top management support also has a key role in determining the usage of TM tools.",2020,,https://doi.org/10.1016/j.im.2018.10.006
717,Demetris T. Christopoulos,Extraction of the global absolute temperature for Northern Hemisphere using a set of 6190 meteorological stations from 1800 to 2013,"Absolute temperature, Northern Hemisphere, Valid station, Data quality, Seasonal bias, Extreme values distribution, Missing records, Big data analysis","Starting from a set of 6190 meteorological stations we are choosing 6130 of them and only for Northern Hemisphere we are computing average values for absolute annual Mean, Minimum, Q1, Median, Q3, Maximum temperature plus their standard deviations for years 1800–2013, while we use 4887 stations and 389467 rows of complete yearly data. The data quality and the seasonal bias indices are defined and used in order to evaluate our dataset. After the year 1969 the data quality is monotonically decreasing while the seasonal bias is positive in most of the cases. An Extreme Value Distribution estimation is performed for minimum and maximum values, giving some upper bounds for both of them and indicating a big magnitude for temperature changes. Finally suggestions for improving the quality of meteorological data are presented.",2015,,https://doi.org/10.1016/j.jastp.2015.03.009
718,Hui Lin and Jia Hu and Chuanfeng Xu and Jianfeng Ma and Mengyang Yu,DTRM: A new reputation mechanism to enhance data trustworthiness for high-performance cloud computing,"Cloud computing, Reputation mechanism, Trustworthiness, Data veracity","Cloud computing and the mobile Internet have been the two most influential information technology revolutions, which intersect in mobile cloud computing (MCC). The burgeoning MCC enables the large-scale collection and processing of big data, which demand trusted, authentic, and accurate data to ensure an important but often overlooked aspect of big data — data veracity. Troublesome internal attacks launched by internal malicious users is one key problem that reduces data veracity and remains difficult to handle. To enhance data veracity and thus improve the performance of big data computing in MCC, this paper proposes a Data Trustworthiness enhanced Reputation Mechanism (DTRM) which can be used to defend against internal attacks. In the DTRM, the sensitivity-level based data category, Metagraph theory based user group division, and reputation transferring methods are integrated into the reputation query and evaluation process. The extensive simulation results based on real datasets show that the DTRM outperforms existing classic reputation mechanisms under bad mouthing attacks and mobile attacks.",2018,,https://doi.org/10.1016/j.future.2018.01.026
719,Dimas Cassimiro Nascimento and Carlos Eduardo Santos Pires and Tiago Brasileiro Araujo and Demetrio Gomes Mestre,Estimating record linkage costs in distributed environments,"Record linkage, Theoretical model, Data quality, Cloud computing","Record Linkage (RL) is the task of identifying duplicate entities in a dataset or multiple datasets. In the era of Big Data, this task has gained notorious attention due to the intrinsic quadratic complexity of the problem in relation to the size of the dataset. In practice, this task can be outsourced to a cloud service, and thus, a service customer may be interested in estimating the costs of a record linkage solution before executing it. Since the execution time of a record linkage solution depends on a combination of various algorithms, their respective parameter values and the employed cloud infrastructure, in practice it is hard to perform an a priori estimation of infrastructure costs for executing a record linkage task. Besides estimating customer costs, the estimation of record linkage costs is also important to evaluate whether (or not) the application of a set of RL parameter values will satisfy predefined time and budget restrictions. Aiming to tackle these challenges, we propose a theoretical model for estimating RL costs taking into account the main steps that may influence the execution time of the RL task. We also propose an algorithm, denoted as TBF, for evaluating the feasibility of RL parameter values, given a set of predefined customer restrictions. We evaluate the efficacy of the proposed model combined with regression techniques using record linkage results processed in real distributed environments. Based on the experimental results, we show that the employed regression technique has significant influence over the estimated record linkage costs. Moreover, we conclude that specific regression techniques are more suitable for estimating record linkage costs, depending on the evaluated scenario.",2020,,https://doi.org/10.1016/j.jpdc.2020.05.003
720,Monika Solanki and Bojan Božić and Christian Dirschl and Rob Brennan,Towards a knowledge driven framework for bridging the gap between software and data engineering,"Ontologies, Data engineering, Software engineering, Alignment, Integration","In this paper we present a collection of ontologies specifically designed to model the information exchange needs of combined software and data engineering. Effective, collaborative integration of software and big data engineering for Web-scale systems, is now a crucial technical and economic challenge. This requires new combined data and software engineering processes and tools. Our proposed models have been deployed to enable: tool-chain integration, such as the exchange of data quality reports; cross-domain communication, such as interlinked data and software unit testing; mediation of the system design process through the capture of design intents and as a source of context for model-driven software engineering processes. These ontologies are deployed in web-scale, data-intensive, system development environments in both the commercial and academic domains. We exemplify the usage of the suite on case-studies emerging from two complex collaborative software and data engineering scenarios: one from the legal sector and the other from the Social sciences and Humanities domain.",2019,,https://doi.org/10.1016/j.jss.2018.12.017
721,Zehua Xiang and Minli Xu,Dynamic cooperation strategies of the closed-loop supply chain involving the internet service platform,"Big data marketing, Differential game, Closed-loop supply chain, Internet service platform","In the age of “Internet+”, many Internet service platforms (ISPs) in China have been widely introduced to the closed-loop supply chain (CLSC). To further study the role of the Internet service platform, this paper considers a CLSC composed of a manufacturer, a retailer and an Internet service platform who invests in research and development (R&D), advertising and Big Data marketing, and develops the goodwill dynamic model based on the differential game theory. The construction of a goodwill dynamic model has two purposes, namely, to increase sales and the return rate. The optimal decisions for 3 players under two different cooperative scenarios are obtained, namely, the retailer payment scenario (scenario D) and the manufacturer cost-sharing scenario (scenario S). The supply chain members gain more profit or achieve a higher level of goodwill for products under certain conditions, i.e., a high residual value from remanufacturing, a high sharing rate of residual value from the retailer's recycled products, and a low recycling cost. Interestingly, the wholesale price increases with the residual value of recycled products when goodwill effectiveness is low, while the price declines when goodwill effectiveness is high. After comparing two cooperative scenarios, the result shows that an Internet service platform will invest more in Big Data marketing under the manufacturer cost-sharing scenario, and cooperation between the manufacturer and the Internet service platform can help improve the goodwill of enterprises or products. Moreover, the manufacturer cost-sharing scenario is payoff-Pareto-improving in most cases through the coordination of a cost-sharing rate, and the effectiveness of Big Data marketing exerts a positive effect on goodwill and the development of the industry. In addition, the retailer has “free rider” tendencies in the manufacturer cost-sharing scenario. The results encourage more enterprises to enhance the value of goodwill through cooperation with Internet service platforms because Internet service platforms conveniently utilize Big Data marketing to increase the sales of products and the collecting rate of used products, which in turn helps environmental sustainability.",2019,,https://doi.org/10.1016/j.jclepro.2019.01.310
722,Mehran {Eskandari Torbaghan} and Manu Sasidharan and Louise Reardon and Leila C.W. Muchanga-Hvelplund,Understanding the potential of emerging digital technologies for improving road safety,"Safety, Road, Transport, Digital technology, Information","Each year, 1.35 million people are killed on the world’s roads and another 20–50 million are seriously injured. Morbidity or serious injury from road traffic collisions is estimated to increase to 265 million people between 2015 and 2030. Current road safety management systems rely heavily on manual data collection, visual inspection and subjective expert judgment for their effectiveness, which is costly, time-consuming, and sometimes ineffective due to under-reporting and the poor quality of the data. A range of innovations offers the potential to provide more comprehensive and effective data collection and analysis to improve road safety. However, there has been no systematic analysis of this evidence base. To this end, this paper provides a systematic review of the state of the art. It identifies that digital technologies - Artificial Intelligence (AI), Machine-Learning, Image-Processing, Internet-of-Things (IoT), Smartphone applications, Geographic Information System (GIS), Global Positioning System (GPS), Drones, Social Media, Virtual-reality, Simulator, Radar, Sensor, Big Data – provide useful means for identifying and providing information on road safety factors including road user behaviour, road characteristics and operational environment. Moreover, the results show that digital technologies such as AI, Image processing and IoT have been widely applied to enhance road safety, due to their ability to automatically capture and analyse data while preventing the possibility of human error. However, a key gap in the literature remains their effectiveness in real-world environments. This limits their potential to be utilised by policymakers and practitioners.",2022,,https://doi.org/10.1016/j.aap.2021.106543
723,Feng Ming Tsai and Tat-Dat Bui and Ming-Lang Tseng and Mohd Helmi Ali and Ming K. Lim and Anthony SF Chiu,Sustainable supply chain management trends in world regions: A data-driven analysis,"Sustainable supply chain management, Data-driven analysis, Fuzzy Delphi method, Entropy weight method, Fuzzy decision-making trial and evaluation laboratory","This study proposes a data-driven analysis that describes the overall situation and reveals the factors hindering improvement in the sustainable supply chain management field. The literature has presented a summary of the evolution of sustainable supply chain management across attributes. Prior studies have evaluated different parts of the supply chain as independent entities. An integrated systematic assessment is absent in the extant literature and makes it necessary to identify potential opportunities for research direction. A hybrid of data-driven analysis, the fuzzy Delphi method, the entropy weight method and fuzzy decision-making trial and evaluation laboratory is adopted to address uncertainty and complexity. This study contributes to locating the boundary of fundamental knowledge to advance future research and support practical execution. Valuable direction is provided by reviewing the existing literature to identify the critical indicators that need further examination. The results show that big data, closed-loop supply chains, industry 4.0, policy, remanufacturing, and supply chain network design are the most important indicators of future trends and disputes. The challenges and gaps among different geographical regions is offered that provides both a local viewpoint and a state-of-the-art advanced sustainable supply chain management assessment.",2021,,https://doi.org/10.1016/j.resconrec.2021.105421
724,Suchitra Kataria and Vinod Ravindran,Emerging role of eHealth in the identification of very early inflammatory rheumatic diseases,"Artificial intelligence, Big data, Machine learning, Data analytics, Wearable devices, Robotics, Digital health","Digital health or eHealth technologies, notably pervasive computing, robotics, big-data, wearable devices, machine learning, and artificial intelligence (AI), have opened unprecedented opportunities as to how the diseases are diagnosed and managed with active patient engagement. Patient-related data have provided insights (real world data) into understanding the disease processes. Advanced analytics have refined these insights further to draw dynamic algorithms aiding clinicians in making more accurate diagnosis with the help of machine learning. AI is another tool, which, although is still in the evolution stage, has the potential to help identify early signs even before the clinical features are apparent. The evolving digital developments pose challenges on allowing access to health-related data for further research but, at the same time, protecting each patient's privacy. This review focuses on the recent technological advances and their applications and highlights the immense potential to enable early diagnosis of rheumatological diseases.",2019,,https://doi.org/10.1016/j.berh.2019.101429
725,Jodie Moll and Ogan Yigitbasioglu,The role of internet-related technologies in shaping the work of accountants: New directions for accounting research,"Accounting profession, Cloud, Big data, Blockchain, Artificial intelligence","This paper reviews the accounting literature that focuses on four Internet-related technologies that have the potential to dramatically change and disrupt the work of accountants and accounting researchers in the near future. These include cloud, big data, blockchain, and artificial intelligence (AI). For instance, access to distributed ledgers (blockchain) and big data supported by cloud-based analytics tools and AI will automate decision making to a large extent. These technologies may significantly improve financial visibility and allow more timely intervention due to the perpetual nature of accounting. However, given the number of tasks technology has relieved of accountants, these technologies may also lead to concerns about the profession's legitimacy. The findings suggest that scholars have not given sufficient attention to these technologies and how these technologies affect the everyday work of accountants. Research is urgently needed to understand the new kinds of accounting required to manage firms in the changing digital economy and to determine the new skills and competencies accountants may need to master to remain relevant and add value. The paper outlines a set of questions to guide future research.",2019,,https://doi.org/10.1016/j.bar.2019.04.002
726,Mohammad I. Merhi,Evaluating the critical success factors of data intelligence implementation in the public sector using analytical hierarchy process,"Data intelligence, Systems implementation, Data analytics, Success factors, Public sector, AHP","This study aims to fill a gap in the literature by identifying, defining, and evaluating the critical success factors that impact the implementation of data intelligence in the public sector. Fourteen factors were identified, and then divided into three categories: organization, process, and technology. We used the analytical hierarchy process, a quantitative method of decision-making, to evaluate the importance of the factors presented in the study using data collected from nine experts. The results showed that technology, as a category, is the most important. The analysis also indicated that project management, information systems & data, and data quality are the most important factors among all fourteen critical success factors. We discuss the implications of the analysis for practitioners and researchers in the paper.",2021,,https://doi.org/10.1016/j.techfore.2021.121180
727,Jonathan Cinnamon and Sarah K. Jones and W. Neil Adger,Evidence and future potential of mobile phone data for disease disaster management,"Mobile phone, Call detail records, SMS, Disaster, Disease, Big data","Global health threats such as the recent Ebola and Zika virus outbreaks require rapid and robust responses to prevent, reduce and recover from disease dispersion. As part of broader big data and digital humanitarianism discourses, there is an emerging interest in data produced through mobile phone communications for enhancing the data environment in such circumstances. This paper assembles user perspectives and critically examines existing evidence and future potential of mobile phone data derived from call detail records (CDRs) and two-way short message service (SMS) platforms, for managing and responding to humanitarian disasters caused by communicable disease outbreaks. We undertake a scoping review of relevant literature and in-depth interviews with key informants to ascertain the: (i) information that can be gathered from CDRs or SMS data; (ii) phase(s) in the disease disaster management cycle when mobile data may be useful; (iii) value added over conventional approaches to data collection and transfer; (iv) barriers and enablers to use of mobile data in disaster contexts; and (v) the social and ethical challenges. Based on this evidence we develop a typology of mobile phone data sources, types, and end-uses, and a decision-tree for mobile data use, designed to enable effective use of mobile data for disease disaster management. We show that mobile data holds great potential for improving the quality, quantity and timing of selected information required for disaster management, but that testing and evaluation of the benefits, constraints and limitations of mobile data use in a wider range of mobile-user and disaster contexts is needed to fully understand its utility, validity, and limitations.",2016,,https://doi.org/10.1016/j.geoforum.2016.07.019
728,Michael Riesener and Christian Dölle and Günther Schuh and Christian Tönnes,Framework for defining information quality based on data attributes within the digital shadow using LDA,"Digital Shadow, Information quality, Data quality, Latent dirichlet allocation (LDA)","The amount of data, which is created in companies is increasing due to modern communication technologies and decreasing costs for storing data. This leads to an advancement of methods for data analyses as well as to an increasing awareness of benefits resulting from data-based knowledge. In the context of product service systems and product development, there are two major concepts for providing product information. The digital twin collects every information possible, while the digital shadow provides a sufficient and content-related picture of the product. Since these concepts merge data from different sources, comprehension about information quality and its relation to the data quality becomes immanently important. This paper introduces a framework to determine information quality with respect to data-related and system-related attributes. An extensive literature review with focus on “information quality” and “data quality” identifies the important approaches for describing information and data quality. A latent dirichlet allocation (LDA) algorithm is applied on 371 definitions and identify 12 data-related and system-related attributes for information quality. Those attributes are assigned to six dimensions for information quality. So the proposed framework depicts the relationships between data attributes and the influence on information quality.",2019,,https://doi.org/10.1016/j.procir.2019.03.131
729,Kathryn B. Newhart and Ryan W. Holloway and Amanda S. Hering and Tzahi Y. Cath,Data-driven performance analyses of wastewater treatment plants: A review,"Wastewater treatment, Big data, Statistical process control, Process optimization, Monitoring","Recent advancements in data-driven process control and performance analysis could provide the wastewater treatment industry with an opportunity to reduce costs and improve operations. However, big data in wastewater treatment plants (WWTP) is widely underutilized, due in part to a workforce that lacks background knowledge of data science required to fully analyze the unique characteristics of WWTP. Wastewater treatment processes exhibit nonlinear, nonstationary, autocorrelated, and co-correlated behavior that (i) is very difficult to model using first principals and (ii) must be considered when implementing data-driven methods. This review provides an overview of data-driven methods of achieving fault detection, variable prediction, and advanced control of WWTP. We present how big data has been used in the context of WWTP, and much of the discussion can also be applied to water treatment. Due to the assumptions inherent in different data-driven modeling approaches (e.g., control charts, statistical process control, model predictive control, neural networks, transfer functions, fuzzy logic), not all methods are appropriate for every goal or every dataset. Practical guidance is given for matching a desired goal with a particular methodology along with considerations regarding the assumed data structure. References for further reading are provided, and an overall analysis framework is presented.",2019,,https://doi.org/10.1016/j.watres.2019.03.030
730,Diana Moise and Denis Shestakov,Chapter 12 - Terabyte-Scale Image Similarity Search,"Big Data, Hadoop, MapReduce, Image search, Multimedia retrieval, Smart deployment, SIFT, HDFS, Hadoop deployment, Hadoop configuration, Hadoop performance, Map waves","While the past decade has witnessed an unprecedented growth of data generated and collected all over the world, existing data management approaches lack the ability to address the challenges of Big Data. One of the most promising tools for Big Data processing is the MapReduce paradigm. Although it has its limitations, the MapReduce programming model has laid the foundations for answering some of the Big Data challenges. In this chapter, we focus on Hadoop, the open-source implementation of the MapReduce paradigm. Using as case study a Hadoop-based application, i.e., image similarity search, we present our experiences with the Hadoop framework when processing terabytes of data. The scale of the data and the application workload allowed us to test the limits of Hadoop and the efficiency of the tools it provides. We present a wide collection of experiments and the practical lessons we have drawn from our experience with the Hadoop environment. Our findings can be shared as best practices and recommendations to the Big Data researchers and practitioners.",2015,,https://doi.org/10.1016/B978-0-444-63492-4.00012-5
731,Marie-Isabelle Muninger and Dominik Mahr and Wafa Hammedi,Social media use: A review of innovation management practices,"Social media, Innovation, Systematic review, Framework and research agenda","The use of social media for innovation requires firms to manage rapid information transfers, big data, and multiway communication. Yet managers lack clear insights on the way social media should be managed and current literature is dispersed across various research streams. In this article, the authors aim to develop a better understanding of how social media use should be leveraged for innovation. To achieve this objective, they build a systematic review of evidence from 177 scientific articles across four key management disciplines. They analyze research perspectives and conceptualizations of social media use for innovation and provide a framework of the drivers, contingencies and outcomes related to this topic. Next, they attempt to identify what is currently known about social media use for innovation. Last, they suggest critical areas for future inquiry on this important subject.",2022,,https://doi.org/10.1016/j.jbusres.2022.01.039
732,Marilex Rea Llave,Data lakes in business intelligence: reporting from the trenches,"Business intelligence, big data, data lake, BI architecture","The data lake approach has emerged as a promising way to handle large volumes of structured and unstructured data. This big data technology enables enterprises to profoundly improve their Business Intelligence. However, there is a lack of empirical studies on the use of the data lake approach in enterprises. This paper provides the results of an exploratory study designed to improve the understanding of the use of the data lake approach in enterprises. I interviewed 12 experts who had implemented this approach in various enterprises and identified three important purposes of implementing data lakes: (1) as staging areas or sources for data warehouses, (2) as a platform for experimentation for data scientists and analysts, and (3) as a direct source for self-service business intelligence. The study also identifies several perceived benefits and challenges of the data lake approach. The results may be beneficial for both academics and practitioners. Further, suggestions for future research is presented.",2018,,https://doi.org/10.1016/j.procs.2018.10.071
733,Hossein Hassani and Christina Beneki and Emmanuel Sirimal Silva and Nicolas Vandeput and Dag Øivind Madsen,The science of statistics versus data science: What is the future?,"Perspective, Science, Statistics, Data science, Similarities, Differences, Pragmatism","The importance and relevance of the discipline of statistics with the merits of the evolving field of data science continues to be debated in academia and industry. Following a narrative literature review with over 100 scholarly and practitioner-oriented publications from statistics and data science, this article generates a pragmatic perspective on the relationships and differences between statistics and data science. Some data scientists argue that statistics is not necessary for data science as statistics delivers simple explanations and data science delivers results. Therefore, this article aims to stimulate debate and discourse among both academics and practitioners in these fields. The findings reveal the need for stakeholders to accept the inherent advantages and disadvantages within the science of statistics and data science. The science of statistics enables data science (aiding its reliability and validity), and data science expands the application of statistics to Big Data. Data scientists should accept the contribution and importance of statistics and statisticians must humbly acknowledge the novel capabilities made possible through data science and support this field of study with their theoretical and pragmatic expertise. Indeed, the emergence of data science does pose a threat to statisticians, but the opportunities for synergies are far greater.",2021,,https://doi.org/10.1016/j.techfore.2021.121111
734,Lei Cao and Jun Chen,Online investigation of vibration serviceability limitations using smartphones,"Vibration serviceability, Online sampling, Big data, Data cleaning","Vibration serviceability issue has attracted increasing attentions recently. Many studies on vibration serviceability limitations have been performed in labs using simulation. The proposed limits were incompatible and lacked details about the physiological and environmental factors because of small sample sizes and unrealistic environments. This study proposes a novel online big data approach for investigating vibration serviceability limits in real environment. A smartphone-based application (App) was designed and spread to volunteers to collect multi-source heterogeneous data including questionnaires of personal judgement on vibration level, vibration signals, environmental and biological factors in their daily life. So far, 8521 records have been received. Data cleaning was performed and a qualified database with large volume and various types of factor information was produced. Analysis of the database showed that vibration limits given by the new method were compatible with previous results, but with more abundant details that were ignored in previous studies.",2020,,https://doi.org/10.1016/j.measurement.2020.107850
735,Ricardo Matheus and Marijn Janssen and Devender Maheshwari,Data science empowering the public: Data-driven dashboards for transparent and accountable decision-making in smart cities,"Data science, Dashboards, E-government, Open government, Open data, Big data, Smart City, Design principles, Transparency, Accountability, Trust, Policy-making, Decision-making","Dashboards visualize a consolidated set data for a certain purpose which enables users to see what is happening and to initiate actions. Dashboards can be used by governments to support their decision-making and policy processes or to communicate and interact with the public. The objective of this paper is to understand and to support the design of dashboards for creating transparency and accountability. Two smart city cases are investigated showing that dashboards can improve transparency and accountability, however, realizing these benefits was cumbersome and encountered various risks and challenges. Challenges include insufficient data quality, lack of understanding of data, poor analysis, wrong interpretation, confusion about the outcomes, and imposing a pre-defined view. These challenges can easily result in misconceptions, wrong decision-making, creating a blurred picture resulting in less transparency and accountability, and ultimately in even less trust in the government. Principles guiding the design of dashboards are presented. Dashboards need to be complemented by mechanisms supporting citizens' engagement, data interpretation, governance and institutional arrangements.",2020,,https://doi.org/10.1016/j.giq.2018.01.006
736,Vasileios C. Pezoulas and Themis P. Exarchos and Dimitrios I. Fotiadis,Chapter 2 - Types and sources of medical and other related data,"Big data, Cohorts, Medical data acquisition, Sources of medical data, Types of medical data","This chapter presents the origin of medical data that comprises the core of any federated cloud platform that deals with medical data sharing and analytics. The different types and sources of medical data are extensively described along with applications and standard data acquisition protocols. Emphasis is given on the definition and the impact of the cohorts in clinical research, as well as the importance of the big data in medicine. The impact of the big data in medicine is also discussed along with emerging opportunities and challenges. The need to develop data standardization protocols across heterogeneous and dispersed data sources is finally highlighted, to enable the analysis of different types of medical big data.",2020,,https://doi.org/10.1016/B978-0-12-816507-2.00002-5
737,Raoudha Gaha and Alexandre Durupt and Benoit Eynard,"Towards the implementation of the Digital Twin in CMM inspection process: opportunities, challenges and proposals","Digital Twin, CMM, inspection, Model-based-defintion, Digital thread","The use of Digital Twin (DT) is adopted by manufacturers and have positive effects on the product manufacturing process. The aim of this paper is to define a Coordinate Measuring Machine (CMM) inspection DT model, based on inspection process digitalized functionalities, from one side, and Industry 4.0 opportunities (Digital thread, Big data, etc.), from the other side. A review about DT definition, is firstly presented. Secondly, we review related studies based on existing DT orientations and usages for CMM inspection. Thirdly, challenges related to the variation management are presented. Finally, a discussion about possible DT functionalities and opportunities is conducted then a CMM inspection DT model is presented.",2021,,https://doi.org/10.1016/j.promfg.2021.07.033
738,Tanveer Ahmad and Rafal Madonski and Dongdong Zhang and Chao Huang and Asad Mujeeb,"Data-driven probabilistic machine learning in sustainable smart energy/smart energy systems: Key developments, challenges, and future research opportunities in the context of smart grid paradigm","Data-driven probabilistic machine learning, Energy distribution, Discovery and design of energy materials, Big data analytics and smart grid, Strategic energy planning and smart manufacturing, Energy demand-side response","The current trend indicates that energy demand and supply will eventually be controlled by autonomous software that optimizes decision-making and energy distribution operations. New state-of-the-art machine learning (ML) technologies are integral in optimizing decision-making in energy distribution networks and systems. This study was conducted on data-driven probabilistic ML techniques and their real-time applications to smart energy systems and networks to highlight the urgency of this area of research. This study focused on two key areas: i) the use of ML in core energy technologies and ii) the use cases of ML for energy distribution utilities. The core energy technologies include the use of ML in advanced energy materials, energy systems and storage devices, energy efficiency, smart energy material manufacturing in the smart grid paradigm, strategic energy planning, integration of renewable energy, and big data analytics in the smart grid environment. The investigated ML area in energy distribution systems includes energy consumption and price forecasting, the merit order of energy price forecasting, and the consumer lifetime value. Cybersecurity topics for power delivery and utilization, grid edge systems and distributed energy resources, power transmission, and distribution systems are also briefly studied. The primary goal of this work was to identify common issues useful in future studies on ML for smooth energy distribution operations. This study was concluded with many energy perspectives on significant opportunities and challenges. It is noted that if the smart ML automation is used in its targeting energy systems, the utility sector and energy industry could potentially save from $237 billion up to $813 billion.",2022,,https://doi.org/10.1016/j.rser.2022.112128
739,Javier Garcia-Bernardo and Frank W. Takes,The effects of data quality on the analysis of corporate board interlock networks,,"Nowadays, social network data of ever increasing size is gathered, stored and analyzed by researchers from a range of disciplines. This data is often automatically gathered from API’s, websites or existing databases. As a result, the quality of this data is typically not manually validated, and the resulting social networks may be based on false, biased or incomplete data. In this paper, we investigate the effect of data quality issues on the analysis of large networks. We focus on the global board interlock network, in which nodes represent firms across the globe, and edges model social ties between firms – shared board members holding a position at both firms. First, we demonstrate how we can automatically assess the completeness of a large dataset of 160 million firms, in which data is missing not at random. Second, we present a novel method to increase the accuracy of the entries in our data. By comparing the expected and empirical characteristics of the resulting network topology, we develop a technique that automatically prunes and merges duplicate nodes and edges. Third, we use a case study of the board interlock network of Sweden to show how poor quality data results in distorted network topologies, incorrect community division, biased centrality values and abnormal influence spread under a well-known diffusion model. Finally, we demonstrate how the proposed data quality assessment methods help restore the network structure, ultimately allowing us to derive meaningful and correct results from the analysis of the network.",2018,,https://doi.org/10.1016/j.is.2017.10.005
740,Foyez Ahmed Prodhan and Jiahua Zhang and Shaikh Shamim Hasan and Til Prasad {Pangali Sharma} and Hasiba Pervin Mohana,"A review of machine learning methods for drought hazard monitoring and forecasting: Current research trends, challenges, and future research directions","Machine learning, Deep learning, Forecasting, Drought, Big data","Machine learning is a dynamic field with wide-ranging applications, including drought modeling and forecasting. Drought is a complex, devastating natural disaster for which it is challenging to develop effective prediction models. Therefore, our review focuses on basic information about machine learning methods (MLMs) and their potential applications in developing efficient and effective drought forecasting models. We observed that MLMs have achieved significant advances in the robustness, effectiveness, and accuracy of the algorithms for drought modelling in recent years. The performance comparison of MLMs with other models provides a comprehensive conception of different model evaluation metrics. Further challenges of MLMs, such as inadequate training data sets, noise, outliers, and observation bias for spatial data sets, are explored. Finally, our review conveys in-depth understanding to researchers on machine learning applications in forecasting and modeling and provides drought mitigation strategy guidance for policymakers.",2022,,https://doi.org/10.1016/j.envsoft.2022.105327
741,Canh Xuan Do and Makoto Tsukai and Akimasa Fujiwara,Data quality analysis of interregional travel demand: Extracting travel patterns using matrix decomposition,"Interregional travel survey, Web-based survey, Mobile phone data, Data quality, Nonnegative matrix factorization","The Interregional Travel Survey in Japan (formerly the Net Passenger Transportation Survey [NPTS]) still has some limitations. New data sources have recently emerged, e.g., massive data from web-based surveys (WEB) or collecting passive mobile phone data (MOBI). Using or not using these data sources have been questioned for data integration or model estimation and validation. Therefore, as an initial step, the data quality of new data sources was evaluated to identify the potential for data integration with NPTS or new data collection methods to replace NPTS. This study focused on finding out the similarities in travel patterns extracted from these data sources using a nonnegative matrix factorization method. This study found that origin–destination pairs in the MOBI travel patterns were significantly different from those of NPTS and WEB, while there were some similarities between NPTS and WEB. However, some issues have been remaining and should be resolved in the future.",2020,,https://doi.org/10.1016/j.eastsj.2020.100018
742,Jules J. Berman,Chapter 13 - Legalities,"Feist Publishing, Inc. v. Rural Telephone Service Co., Data Quality Act, Freedom of Information Act, limited data use agreements, tort, patents, intellectual property, informed consent, data ownership, copyright, infringement, fair use","Big Data projects always incur some legal risk. It is impossible to know all the data contained in a Big Data project, and it is impossible to know every purpose to which Big Data is used. Hence, the entities that produce Big Data may unknowingly contribute to a variety of illegal activities, chiefly copyright and other intellectual property infringements, breaches of confidentiality, and privacy invasions. In addition, issues of data quality, data availability, and data documentation may contribute to the legal or regulatory disqualification of Big Data as a resource suitable for its intended purposes. In this chapter, four issues will be discussed in detail: (1) responsibility for the accuracy of the contained data; (2) rights to create, use, and share the data held in the resource; (3) intellectual property encumbrances incurred from the use of standards required for data representation and data exchange; and (4) protections for individuals whose personal information is used in the resource. Big Data managers contend with a wide assortment of legal issues, but these four problems never seem to go away.",2013,,https://doi.org/10.1016/B978-0-12-404576-7.00013-7
743,Deepak Puthal and Surya Nepal and Rajiv Ranjan and Jinjun Chen,A dynamic prime number based efficient security mechanism for big sensing data streams,"Security, Sensor networks, Big data stream, Key exchange, Security verification","Big data streaming has become an important paradigm for real-time processing of massive continuous data flows in large scale sensing networks. While dealing with big sensing data streams, a Data Stream Manager (DSM) must always verify the security (i.e. authenticity, integrity, and confidentiality) to ensure end-to-end security and maintain data quality. Existing technologies are not suitable, because real time introduces delay in data stream. In this paper, we propose a Dynamic Prime Number Based Security Verification (DPBSV) scheme for big data streams. Our scheme is based on a common shared key that updated dynamically by generating synchronized prime numbers. The common shared key updates at both ends, i.e., source sensing devices and DSM, without further communication after handshaking. Theoretical analyses and experimental results of our DPBSV scheme show that it can significantly improve the efficiency of verification process by reducing the time and utilizing a smaller buffer size in DSM.",2017,,https://doi.org/10.1016/j.jcss.2016.02.005
744,Karthika D.,A study on artificial intelligence for monitoring smart environments,"Big data analytics, Machine learning, Internet of things, Smart cities, Wireless technologies","Wireless networking has made enormous improvements. These developments have brought about new paradigms of wireless networking and communications Environmental protection has been in recent years a more intelligent and linked system for all facets of a global city. With the rise in data gathering, machine learning (ML) approaches can be used to boost the knowledge and the skill of an application. As the numbers increase and technology develops, the number of available data increases. Smart collection and interpretation of these Big Data is the underground to the rising of smart Internet of Things IoT apps. This study discovers the diverse methods of machine learning that resolve data difficulties in smart cities. The discussion takes place on applications such as air quality, water pollution, radiation pollution, smart buildings, smart transport, etc., which pose genuine environmental challenges. Adequate monitoring is needed to ensure sustainable growth in the world by safeguarding a healthy society. The potential and challenges in particular the role of machine learning technology for the Internet of Things and Big Data Analytics.",2021,,https://doi.org/10.1016/j.matpr.2021.06.046
745,Matthias Fuchs and Wolfram Höpken and Maria Lexhagen,Big data analytics for knowledge generation in tourism destinations – A case from Sweden,"Big data analytics, Tourism destination, Destination management information system, Business intelligence, Data mining, Online Analytical Processing (OLAP)","This paper presents a knowledge infrastructure which has recently been implemented as a genuine novelty at the leading Swedish mountain tourism destination, Åre. By applying a Business Intelligence approach, the Destination Management Information System Åre (DMIS-Åre) drives knowledge creation and application as a precondition for organizational learning at tourism destinations. Schianetz, Kavanagh, and Lockington’s (2007) concept of the ‘Learning Tourism Destination’ and the ‘Knowledge Destination Framework’ introduced by Höpken, Fuchs, Keil, and Lexhagen (2011) build the theoretical fundament for the technical architecture of the presented Business Intelligence application. After having introduced the development process of indicators measuring destination performance as well as customer behaviour and experience, the paper highlights how DMIS-Åre can be used by tourism managers to gain new knowledge about customer-based destination processes focused on pre- and post-travel phases, like “Web-Navigation”, “Booking” and “Feedback”. After a concluding discussion about the various components building the prototypically implemented BI-based DMIS infrastructure with data from destination stakeholders, the agenda of future research is sketched. The agenda considers, for instance, the application of real-time Business Intelligence to gain real-time knowledge on tourists’ on-site behaviour at tourism destinations.",2014,,https://doi.org/10.1016/j.jdmm.2014.08.002
746,Ahmed O. Oyedele and Anuoluwapo O. Ajayi and Lukumon O. Oyedele,Machine learning predictions for lost time injuries in power transmission and distribution projects,"Hazard assessment, Deep learning, Big data predictive analytics, Power infrastructure, Zero count data","Although advanced machine learning algorithms are predominantly used for predicting outcomes in many fields, their utilisation in predicting incident outcome in construction safety is still relatively new. This study harnesses Big Data with Deep Learning to develop a robust safety management system by analysing unstructured incident datasets consisting of 168,574 data points from power transmission and distribution projects delivered across the UK from 2004 to 2016. This study compared Deep Learning performance with popular machine learning algorithms (support vector machine, random forests, multivariate adaptive regression splines, generalised linear model, and their ensembles) concerning lost time injury and risk assessment in power utility projects. Deep Learning gave the best prediction for safety outcomes with high skills (AUC = 0.95, R2 = 0.88, and multi-class ROC = 0.93), thus outperforming the other algorithms. The results from this study also highlight the significance of quantitative analysis of empirical data in safety science and contribute to an enhanced understanding of injury patterns using predictive analytics in conjunction with safety experts’ perspectives. Additionally, the results will enhance the skills of safety managers in the power utility domain to advance safety intervention efforts.",2021,,https://doi.org/10.1016/j.mlwa.2021.100158
747,Carolina Reis and Pedro Ruivo and Tiago Oliveira and Paulo Faroleiro,Assessing the drivers of machine learning business value,"Machine learning, Business value, Competitive advantage, Dynamic capabilities theory","Machine learning (ML) is expected to transform the business landscape in the near future completely. Hitherto, some successful ML case-stories have emerged. However, how organizations can derive business value (BV) from ML has not yet been substantiated. We assemble a conceptual model, grounded on the dynamic capabilities theory, to uncover key drivers of ML BV, in terms of financial and strategic performance. The proposed model was assessed by surveying 319 corporations. Our findings are that ML use, big data analytics maturity, platform maturity, top management support, and process complexity are, to some extent, drivers of ML BV. We also find that platform maturity has, to some degree, a moderator influence between ML use and ML BV, and between big data analytics maturity and ML BV. To the best of our knowledge, this is the first research to deliver such findings in the ML field.",2020,,https://doi.org/10.1016/j.jbusres.2020.05.053
748,Chengxu Li and Wenmin Fei and Yang Han and Xiaoli Ning and Ziyi Wang and Keke Li and Ke Xue and Jingkai Xu and Ruixing Yu and Rusong Meng and Feng Xu and Weimin Ma and Yong Cui,Construction of an artificial intelligence system in dermatology: effectiveness and consideration of Chinese Skin Image Database (CSID),"Artificial intelligence, Dermatology, Skin image, Chinese Skin Image Database","After more than 60 years of development, artificial intelligence (AI) has been widely used in various fields. Especially in recent years, with the development of deep learning, AI has made many remarkable achievements in the medical field. Dermatology, as a clinical discipline with morphology as its main feature, is particularly suitable for the development of AI. The rapid development of skin imaging technology has helped dermatologists to assist in the diagnosis of diseases and has greatly improved the accuracy of diagnosis. Skin imaging data have natural big data attributes, which is important for AI research. The establishment of the Chinese Skin Image Database (CSID) has solved many problems such as isolated data islands and inconsistent data quality. Based on the CSID, many pioneering achievements have been made in the research and development of AI-assisted decision-making software, the establishment of expert organizations, personnel training, scientific research, and so on. At present, there are still many problems with AI in the field of dermatology, such as clinical validation, medical device licensing, interdisciplinary, and standard formulation, which urgently need to be solved by joint efforts of all parties.",2021,,https://doi.org/10.1016/j.imed.2021.04.003
749,Boris Huard,The data quality paradox,,"After its people, data is arguably an organisation's most valuable asset. According to recent figures by the Networked Systems and Services department at SINTEF, some 90% of all the data in the world has been created in the past two years. That shouldn't be too much of a surprise to us given the data-driven world we now live in; but what is promising is that companies are increasingly switching on to its strategic and commercial value. The challenge is how do we extract value from this data in a way that empowers people and organisations alike?",2015,,https://doi.org/10.1016/S1353-4858(15)30051-9
750,Jaehan Cho and Timothy DeStefano and Hanhin Kim and Inchul Kim and Jin Hyun Paik,What's driving the diffusion of next-generation digital technologies?,,"The recent development and diffusion of next-generation digital technologies (NGDTs) such as artificial intelligence, the Internet of Things, big data, 3D printing, and so on are expected to have an immense impact on businesses, innovation, and society. While we know from extant research that a firm's R&D investment, intangible assets, and productivity are factors that influence technology use more generally, to date there is little known about the factors that determine how these emerging tools are used, and by who. Using Probit and OLS modeling on a survey of 12,579 South Korean firms in 2017, we conduct one of the first comprehensive examinations highlighting various firm characteristics that drive NGDT implementation. While much of the literature assesses the use of individual technologies, our research attempts to unveil the extent to which firms implement NGDTs in bundles. Our investigation shows that more than half of the firms that use NGDTs deployed multiple technologies simultaneously. One of the insightful complementarities identified in this research exists amongst technologies that generate, facilitate and demand large sums of data, including big data, IoT, cloud computing and AI. Such technologies also appear important for innovative tools such as 3D printing and robotics.",2022,,https://doi.org/10.1016/j.technovation.2022.102477
751,Yong Zhou and Huanghui Gu and Teng Su and Xuebing Han and Languang Lu and Yuejiu Zheng,Remaining useful life prediction with probability distribution for lithium-ion batteries based on edge and cloud collaborative computation,"Battery life prediction, RVM optimization prediction, Parameter transfer: RUL probability prediction","This paper proposes the architecture of the combination of the battery management system (BMS) and the cloud big data platform. Firstly, BMS measures and extracts the mean voltage falloff (MVF). A regression model of capacity and MVF based on historical data is established with generalized Box-Cox Transformation and least squares. The capacity and MVF are uploaded to the cloud big data platform, and then the mean and variance of the MVF is predicted based on the relevance vector machine, thereby realizing the 2σ range prediction of the lithium battery's state of health and the probability density function prediction of the remaining useful life. This paper makes two contributions to the data-driven prediction method. First, the edge-cloud collaborative computing architecture combining BMS and cloud is proposed, which effectively utilizes the advantages of BMS data quality and cloud computing power. Second, through the combination of relevance vector machine with particle swarm optimization and horizontal parameter transfer, the number of samples required for model learning is reduced to 30% and has better accuracy and robustness. Through the verification of NASA data, the results show that the average error is less than 2.18%.",2021,,https://doi.org/10.1016/j.est.2021.103342
752,Khanh Q. Bui and Lokukaluge P. Perera,Advanced data analytics for ship performance monitoring under localized operational conditions,"Big data analytics, Machine learning, Ship performance monitoring, Energy efficiency, Emission control, Data anomaly detection","Improving the operational energy efficiency of existing ships is attracting considerable interests to reduce the environmental footprint due to air emissions. As the shipping industry is entering into Shipping 4.0 with digitalization as a disruptive force, an intriguing area in the field of ship’s operational energy efficiency is big data analytics. This paper proposes a big data analytics framework for ship performance monitoring under localized operational conditions with the help of appropriate data analytics together with domain knowledge. The proposed framework is showcased through a data set obtained from a bulk carrier pertaining the detection of data anomalies, the investigation of the ship’s localized operational conditions, the identification of the relative correlations among parameters and the quantification of the ship’s performance in each of the respective conditions. The novelty of this study is to provide a KPI (i.e. key performance indicator) for ship performance quantification in order to identify the best performance trim-draft mode under the engine modes of the case study ship. The proposed framework has the features to serve as an operational energy efficiency measure to provide data quality evaluation and decision support for ship performance monitoring that is of value for both ship operators and decision-makers.",2021,,https://doi.org/10.1016/j.oceaneng.2021.109392
753,Bibiano Rivas and Jorge Merino and Ismael Caballero and Manuel Serrano and Mario Piattini,Towards a service architecture for master data exchange based on ISO 8000 with support to process large datasets,"Master data, Data quality, ISO 8000, Big data","During the execution of business processes involving various organizations, Master Data is usually shared and exchanged. It is necessary to keep appropriate levels of quality in these Master Data in order to prevent problems in the business processes. Organizations can be benefitted from having information about the level of quality of master data along with the master data to support decision about the usage of data in business processes is to include information about the level of quality alongside the Master Data. ISO 8000-1x0 specifies how to add this information to the master data messages. From the clauses stated in the various part of standard we developed a reference architecture, enhanced with big data technologies to better support the management of large datasets The main contribution of this paper is a service architecture for Master Data Exchange supporting the requirements stated by the different parts of the standard like the development of a data dictionary with master data terms; a communication protocol; an API to manage the master data messages; and the algorithms in MapReduce to measure the data quality.",2017,,https://doi.org/10.1016/j.csi.2016.10.004
754,Richard Knepper and Matthew Standish,Forward Observer system for radar data workflows: Big data management in the field,"Microcomputers, Information storage, Physical sciences and engineering","There are unique challenges in managing data collection and management from instruments in the field in general. These issues become extreme when “in the field” means “in a plane over the Antarctic”. In this paper we present the design and function of the Forward Observer a computer cluster and data analysis system that flies in a plane in the Arctic and Antarctic to collect, analyze in real time, and store Synthetic Aperture Radar (SAR) data. SAR is used to analyze the thickness and structure of polar ice sheets. We also discuss the processing of data once it is returned to the continental US and made available via data grids. The needs for in-flight data analysis and storage in the Antarctic and Arctic are highly unusual, and we have developed a novel system to meet those needs. We describe the constraints and requirements that led to the creation of this system and the general functionality which it applies to any instrument. We discuss the main means for handling replication and creating checksum information to ensure that data collected in polar regions are returned safely to mainland US for analysis. So far, not a single byte of data collected in the field has failed to make it home to the US for analysis (although many particular data storage devices have failed or been damaged due to the challenges of the extreme environments in which this system is used). While the Forward Observer system is developed for the extreme situation of data management in the field in the Antarctic, the technology and solutions we have developed are applicable and potentially usable in many situations where researchers wish to do real time data management in the field in areas that are constrained in terms of electrical supply.",2017,,https://doi.org/10.1016/j.future.2017.05.031
755,Nisreen Ameen and Sameer Hosany and Ali Tarhini,Consumer interaction with cutting-edge technologies: Implications for future research,"Consumer interaction, Cutting-edge technologies, Artificial intelligence, Virtual reality and augmented reality, Robotics, Wearable technology, Big data analytics","This article provides an overview of extant literature addressing consumer interaction with cutting-edge technologies. Six focal cutting-edge technologies are identified: artificial intelligence, augmented reality, virtual reality, wearable technology, robotics and big data analytics. Our analysis shows research on consumer interaction with cutting-edge technologies is at a nascent stage, and there are several gaps requiring attention. To further advance knowledge, our article offers avenues for future interdisciplinary research addressing implications of consumer interaction with cutting-edge technologies. More specifically, we propose six main areas for future research namely: rethinking consumer behaviour models, identifying behavioural differences among different generations of consumers, understanding how consumers interact with automated services, ethics, privacy and the blackbox, consumer security concerns and consumer interaction with new-age technologies during and after a major global crisis such as the COVID-19 pandemic.",2021,,https://doi.org/10.1016/j.chb.2021.106761
756,Wen-Long Shang and Haoran Zhang and Yi Sui,Chapter 7 - Data mining technologies for Mobility-as-a-Service (MaaS),"MaaS, Big data, Data mining, Support vector machine, Linear regression, Decision tree, Clustering, Bike-sharing, COVID-19","This chapter mainly introduces big data technologies for MaaS. Firstly, the development, definition, and purpose of MaaS and the significance of data mining technologies for MaaS are introduced briefly. Following this, the definition of data mining, its processing objects, classical steps and processes, and types of traffic big data are reviewed. Afterward, data mining technologies such as support vector machine, linear regression, decision tree, and clustering analysis are introduced. Finally, a case study of data mining technology used for bike-sharing in Beijing during the Covid-19 pandemic is presented to demonstrate the role of data mining technologies in travel behaviors. This chapter mainly provides a clue or reference for the exploration of big data analysis of MaaS.",2022,,https://doi.org/10.1016/B978-0-323-90169-7.00008-7
757,James R. Marsden and David E. Pingry and Jason B. Thatcher,Perspectives on numerical data quality in IS research,,,2019,,https://doi.org/10.1016/j.dss.2019.113172
758,Ewa Szymańska,Modern data science for analytical chemical data – A comprehensive review,"Chemometrics, Data science, Big data, Chemical analytical data, Methodology","Efficient and reliable analysis of chemical analytical data is a great challenge due to the increase in data size, variety and velocity. New methodologies, approaches and methods are being proposed not only by chemometrics but also by other data scientific communities to extract relevant information from big datasets and provide their value to different applications. Besides common goal of big data analysis, different perspectives and terms on big data are being discussed in scientific literature and public media. The aim of this comprehensive review is to present common trends in the analysis of chemical analytical data across different data scientific fields together with their data type-specific and generic challenges. Firstly, common data science terms used in different data scientific fields are summarized and discussed. Secondly, systematic methodologies to plan and run big data analysis projects are presented together with their steps. Moreover, different analysis aspects like assessing data quality, selecting data pre-processing strategies, data visualization and model validation are considered in more detail. Finally, an overview of standard and new data analysis methods is provided and their suitability for big analytical chemical datasets shortly discussed.",2018,,https://doi.org/10.1016/j.aca.2018.05.038
759,Hyun Ae Jung and Jong-Mu Sun and Se-Hoon Lee and Jin Seok Ahn and Myung-Ju Ahn and Keunchil Park,"Ten-year patient journey of stage III non-small cell lung cancer patients: A single-center, observational, retrospective study in Korea (Realtime autOmatically updated data warehOuse in healTh care; UNIVERSE-ROOT study)","Real-time updated system, Big data, Real-world data, NSCLC, Treatment","Introduction
Until the recent approval of immunotherapy after completing concurrent chemoradiotherapy (CCRT), there has been little progress in treating unresectable stage III non-small cell lung cancer (NSCLC). This prompted us to search real-world data (RWD) to better understand diagnosis and treatment patterns, and outcomes.
Methods
This non-interventional observational study used a unique, novel algorithm for big data analysis to collect and assess anonymized patient electronic medical records from a clinical data warehouse (CDW) over a 10-year period to capture real-world patterns of diagnosis, treatment, and outcomes of stage III NSCLC patients. We describe real-world patterns of diagnosis and treatment of patients with newly-diagnosed stage III NSCLC, and patients’ characteristics, and assessment of treatment outcomes.
Results
We analyzed clinical variables from 23,735 NSCLC patients. Stage III patients (N = 4138, 18.2 %) were diagnosed as IIIA (N = 2,547, 11.2 %) or IIIB (N = 1,591. 7.0 %). Treated stage III patients (N = 2530, 61.1 %) had a median age of 64.2 years, were mostly male (78.5 %) and had an ECOG performance status of 1 (65.2 %). Treatment comprised curative-intent surgery (N = 1,254, 49.6 %) with 705 receiving neoadjuvant therapy; definitive CRT (N = 648, 25.6 %); palliative CT (N = 270, 10.7 %), or thoracic RT (N = 170, 6.7 %). Median OS (range) for neoadjuvant, surgery, CRT, palliative chemotherapy, lung RT alone, and supportive care was 49.2 (42.0–56.5), 52.5 (43.1–61.9), 30.3 (26.6–34.0), 14.7 (13.0–16.4), 8.8 (6.2–11.3), and 2.0 (1.0–3.0) months, respectively.
Conclusions
This unique in-house algorithm enabled a rapid and comprehensive analysis of big data through a CDW, with daily automatic updates that documented real-world PFS and OS consistent with the published literature, and real-world treatment patterns and clinical outcomes in stage III NSCLC patients.",2020,,https://doi.org/10.1016/j.lungcan.2020.05.033
760,Bing Wang,Safety intelligence as an essential perspective for safety management in the era of Safety 4.0: From a theoretical to a practical framework,"Safety intelligence (SI), Safety big data, Safety 4.0, Safety management, Safety decision-making","In the age of big data, intelligence, and Industry 4.0, intelligence plays an increasingly significant role in management or, more specifically, decision making; thus, it becomes a popular topic and is recognised as an important discipline. Hence, safety intelligence (SI) as a new safety concept and term was proposed. SI aims to transform raw safety data and information into meaningful and actionable information for safety management; it is considered an essential perspective for safety management in the era of Safety 4.0 (computational safety science—a new paradigm for safety science in the age of big data, intelligence, and Industry 4.0). However, thus far, no existing research provides a framework that comprehensively describes SI and guides the implementation of SI practices in organisations. To address this research gap and to provide a framework for SI and its practice in the context of safety management, based on a systematic and comprehensive explanation on SI from different perspectives, this study attempts to propose a theoretical framework for SI from a safety management perspective and then presents an SI practice model aimed at supporting safety management in organisations.",2021,,https://doi.org/10.1016/j.psep.2020.10.008
761,Matteo Francia and Enrico Gallinucci and Matteo Golfarelli and Anna Giulia Leoni and Stefano Rizzi and Nicola Santolini,Making data platforms smarter with MOSES,"Data lake, Metadata, Big data, Data platform","The rise of data platforms has enabled the collection and processing of huge volumes of data, but has opened to the risk of losing their control. Collecting proper metadata about raw data and transformations can significantly reduce this risk. In this paper we propose MOSES, a technology-agnostic, extensible, and customizable framework for metadata handling in big data platforms. The framework hinges on a metadata repository that stores information about the objects in the big data platform and the processes that transform them. MOSES provides a wide range of functionalities to different types of users of the platform. Differently from previous high-level proposals, MOSES is fully implemented and it was not conceived for a specific technology. Besides discussing the rationale and the features of MOSES, in this paper we describe its implementation and we test it on a real case study. The ultimate goal is to take a significant step forward towards proving that metadata handling in big data platforms is feasible and beneficial.",2021,,https://doi.org/10.1016/j.future.2021.06.031
762,Aimad Karkouch and Hajar Mousannif and Hassan {Al Moatassime} and Thomas Noel,Data quality in internet of things: A state-of-the-art survey,"Internet of things, Data quality, Data cleaning, Outlier detection","In the Internet of Things (IoT), data gathered from a global-scale deployment of smart-things, are the base for making intelligent decisions and providing services. If data are of poor quality, decisions are likely to be unsound. Data quality (DQ) is crucial to gain user engagement and acceptance of the IoT paradigm and services. This paper aims at enhancing DQ in IoT by providing an overview of its state-of-the-art. Data properties and their new lifecycle in IoT are surveyed. The concept of DQ is defined and a set of generic and domain-specific DQ dimensions, fit for use in assessing IoT's DQ, are selected. IoT-related factors endangering the DQ and their impact on various DQ dimensions and on the overall DQ are exhaustively analyzed. DQ problems manifestations are discussed and their symptoms identified. Data outliers, as a major DQ problem manifestation, their underlying knowledge and their impact in the context of IoT and its applications are studied. Techniques for enhancing DQ are presented with a special focus on data cleaning techniques which are reviewed and compared using an extended taxonomy to outline their characteristics and their fitness for use for IoT. Finally, open challenges and possible future research directions are discussed.",2016,,https://doi.org/10.1016/j.jnca.2016.08.002
763,Honghui Dong and Mingchao Wu and Xiaoqing Ding and Lianyu Chu and Limin Jia and Yong Qin and Xuesong Zhou,Traffic zone division based on big data from mobile phone base stations,"Mobile telephones, Call detail record (CDR) data, Traffic semantic analysis, Traffic zone division, Traffic zone attribute index, Travel patterns","Call detail record (CDR) data from mobile communication carriers offer an emerging and promising source of information for analysis of traffic problems. To date, research on insights and information to be gleaned from CDR data for transportation analysis has been slow, and there has been little progress on development of specific applications. This paper proposes the traffic semantic concept to extract traffic commuters’ origins and destinations information from the mobile phone CDR data and then use the extracted data for traffic zone division. A K-means clustering method was used to classify a cell-area (the area covered by a base stations) and tag a certain land use category or traffic semantic attribute (such as working, residential, or urban road) based on four feature data (including real-time user volume, inflow, outflow, and incremental flow) extracted from the CDR data. By combining the geographic information of mobile phone base stations, the roadway network within Beijing’s Sixth Ring Road was divided into a total of 73 traffic zones using another K-means clustering algorithm. Additionally, we proposed a traffic zone attribute-index to measure tendency of traffic zones to be residential or working. The calculated attribute-index values of 73 traffic zones in Beijing were consistent with the actual traffic and land-use data. The case study demonstrates that effective traffic and travel data can be obtained from mobile phones as portable sensors and base stations as fixed sensors, providing an opportunity to improve the analysis of complex travel patterns and behaviors for travel demand modeling and transportation planning.",2015,,https://doi.org/10.1016/j.trc.2015.06.007
764,M. Kebisek and P. Tanuska and L. Spendla and J. Kotianova and P. Strelec,Artificial Intelligence Platform Proposal for Paint Structure Quality Prediction within the Industry 4.0 Concept,"artificial intelligence, automotive, big data analytics, industry 4.0, knowledge discovery, neural networks, prediction, principal component analysis","This article provides an artificial intelligence platform proposal for paint structure quality prediction using Big Data analytics methodologies. The whole proposal fits into the current trends that are outlined in the Industry 4.0 concept. The painting process is very complex, producing huge volumes of data, but the main problem is that the data comes from different data sources, often heterogeneous, and it is necessary to propose a way to collect and integrate them into a common repository. The motivation for this work were the industry requirements to solve specific problems that cannot be solved by standard methods but require a sophisticated and holistic approach. It is the application of artificial intelligence that suggests a solution that is not otherwise visible, and the use of standard methods would not give any satisfactory results. The result is the design of an artificial intelligence platform that has been deployed in a real manufacturing process, and the initial results confirm the correctness and validity of this step. We also present a data collection and integration architecture, which is an integral part of every big data analytics solution, and a principal component analysis that was used to reduce the dimensionality of the large number of production process data.",2020,,https://doi.org/10.1016/j.ifacol.2020.12.299
765,Dominic Evangelista and France Thouzé and Manpreet Kaur Kohli and Philippe Lopez and Frédéric Legendre,Topological support and data quality can only be assessed through multiple tests in reviewing Blattodea phylogeny,"Phylogenetic signal, mtDNA, Termite, Dictyoptera, SAMS, Rogue taxa, Long branch attraction, Signal analysis","Assessing support for molecular phylogenies is difficult because the data is heterogeneous in quality and overwhelming in quantity. Traditionally, node support values (bootstrap frequency, Bayesian posterior probability) are used to assess confidence in tree topologies. Other analyses to assess the quality of phylogenetic data (e.g. Lento plots, saturation plots, trait consistency) and the resulting phylogenetic trees (e.g. internode certainty, parameter permutation tests, topological tests) exist but are rarely applied. Here we argue that a single qualitative analysis is insufficient to assess support of a phylogenetic hypothesis and relate data quality to tree quality. We use six molecular markers to infer the phylogeny of Blattodea and apply various tests to assess relationship support, locus quality, and the relationship between the two. We use internode-certainty calculations in conjunction with bootstrap scores, alignment permutations, and an approximately unbiased (AU) test to assess if the molecular data unambiguously support the phylogenetic relationships found. Our results show higher support for the position of Lamproblattidae, high support for the termite phylogeny, and low support for the position of Anaplectidae, Corydioidea and phylogeny of Blaberoidea. We use Lento plots in conjunction with mutation-saturation plots, calculations of locus homoplasy to assess locus quality, identify long branch attraction, and decide if the tree’s relationships are the result of data biases. We conclude that multiple tests and metrics need to be taken into account to assess tree support and data robustness.",2018,,https://doi.org/10.1016/j.ympev.2018.05.007
766,Antonio Kolossa and Bruno Kopp,Data quality over data quantity in computational cognitive neuroscience,"Computational modeling, Functional brain imaging, Signal-to-noise ratio, Reliability, Replicability","We analyzed factors that may hamper the advancement of computational cognitive neuroscience (CCN). These factors include a particular statistical mindset, which paves the way for the dominance of statistical power theory and a preoccupation with statistical replicability in the behavioral and neural sciences. Exclusive statistical concerns about sampling error occur at the cost of an inadequate representation of the problem of measurement error. We contrasted the manipulation of data quantity (sampling error, by varying the number of subjects) against the manipulation of data quality (measurement error, by varying the number of data per subject) in a simulated Bayesian model identifiability study. The results were clear-cut in showing that - across all levels of signal-to-noise ratios - varying the number of subjects was completely inconsequential, whereas the number of data per subject exerted massive effects on model identifiability. These results emphasize data quality over data quantity, and they call for the integration of statistics and measurement theory.",2018,,https://doi.org/10.1016/j.neuroimage.2018.01.005
767,Suresh Neethirajan and Bas Kemp,Digital Livestock Farming,"Precision Livestock Farming, digitalization, Digital Technologies in Livestock Systems, sensor technology, big data, blockchain, data models, livestock agriculture","As the global human population increases, livestock agriculture must adapt to provide more livestock products and with improved efficiency while also addressing concerns about animal welfare, environmental sustainability, and public health. The purpose of this paper is to critically review the current state of the art in digitalizing animal agriculture with Precision Livestock Farming (PLF) technologies, specifically biometric sensors, big data, and blockchain technology. Biometric sensors include either noninvasive or invasive sensors that monitor an individual animal’s health and behavior in real time, allowing farmers to integrate this data for population-level analyses. Real-time information from biometric sensors is processed and integrated using big data analytics systems that rely on statistical algorithms to sort through large, complex data sets to provide farmers with relevant trending patterns and decision-making tools. Sensors enabled blockchain technology affords secure and guaranteed traceability of animal products from farm to table, a key advantage in monitoring disease outbreaks and preventing related economic losses and food-related health pandemics. Thanks to PLF technologies, livestock agriculture has the potential to address the abovementioned pressing concerns by becoming more transparent and fostering increased consumer trust. However, new PLF technologies are still evolving and core component technologies (such as blockchain) are still in their infancy and insufficiently validated at scale. The next generation of PLF technologies calls for preventive and predictive analytics platforms that can sort through massive amounts of data while accounting for specific variables accurately and accessibly. Issues with data privacy, security, and integration need to be addressed before the deployment of multi-farm shared PLF solutions becomes commercially feasible.",2021,,https://doi.org/10.1016/j.sbsr.2021.100408
768,Morris A. Davis and Stephen D. Oliner and Edward J. Pinto and Sankar Bokka,"Residential land values in the Washington, DC metro area: New insights from big data","Land, Housing, House prices, Housing boom and bust, Financial crisis","We use a new property-level data set and an innovative methodology to estimate the price of land from 2000 to 2013 for nearly the universe of detached single-family homes in the Washington, DC metro area and to characterize the boom-bust cycle in land and house prices at a fine geography. The results show that land prices were more volatile than house prices everywhere, but especially so in the areas where land was inexpensive in 2000. We demonstrate that the change in the land share of house value during the boom was a significant predictor of the decline in house prices during the bust, highlighting the value of focusing on land in assessing house-price risk.",2017,,https://doi.org/10.1016/j.regsciurbeco.2017.06.006
769,John R. Talburt and Yinle Zhou,Chapter 11 - ISO Data Quality Standards for Master Data,"ISO, ANSI, ISO 8000, ISO 22745, Semantic Encoding","This chapter provides a discussion of the new International Organization for Standardization (ISO) standards related to the exchange of master data. It includes an in-depth look at the ISO 8000 family of standards, including ISO 8000-110, -120, -130, and -140, and their relationship to the ISO 22745-10, -30, and -40 standards. Also an explanation is given of simple versus strong ISO 8000-110 compliance, and the value proposition for ISO 8000 compliance is discussed.",2015,,https://doi.org/10.1016/B978-0-12-800537-8.00011-9
770,Alastair Faulkner and Mark Nicholson,4 - Data Fundamentals,"Data ecosystems, Data context, Data architectures, Data Integrity, Data Quality","Data is one component in a system. It has value. The economics of increasingly data-centric systems is explored. There is a growing reliance on data to create systems that are larger scale, have wider scope and are more complex. As reliance on the data component increases, a self-reinforcing problem of implementing checks and balances necessary to enforce appropriate levels of risk reduction arises. This chapter introduces data architecture elements of container and content. It places this architecture within an appropriate data context. It introduces the concepts of data quality and data integrity. Data integrity is placed within the Safety Management and Safety Assurance processes. Big data and machine learning are considered in this context.",2020,,https://doi.org/10.1016/B978-0-12-820790-1.00017-6
771,Paul Harrigan and Timothy M. Daly and Kristof Coussement and Julie A. Lee and Geoffrey N. Soutar and Uwana Evers,Identifying influencers on social media,"Influencers, Market mavens, Big data, Social media, Twitter","The increased availability of social media big data has created a unique challenge for marketing decision-makers; turning this data into useful information. One of the significant areas of opportunity in digital marketing is influencer marketing, but identifying these influencers from big data sets is a continual challenge. This research illustrates how one type of influencer, the market maven, can be identified using big data. Using a mixed-method combination of both self-report survey data and publicly accessible big data, we gathered 556,150 tweets from 370 active Twitter users. We then proposed and tested a range of social-media-based metrics to identify market mavens. Findings show that market mavens (when compared to non-mavens) have more followers, post more often, have less readable posts, use more uppercase letters, use less distinct words, and use hashtags more often. These metrics are openly available from public Twitter accounts and could integrate into a broad-scale decision support system for marketing and information systems managers. These findings have the potential to improve influencer identification effectiveness and efficiency, and thus improve influencer marketing.",2021,,https://doi.org/10.1016/j.ijinfomgt.2020.102246
772,Dan J. Kim and John Hebeler and Victoria Yoon and Fred Davis,"Exploring Determinants of Semantic Web Technology Adoption from IT Professionals' Perspective: Industry Competition, Organization Innovativeness, and Data Management Capability","Semantic web, IT professionals' perspective technology adoption, Technology-organization-environment framework, Innovation diffusion theory","The scale and complexity of big data quickly exceed the reach of direct human comprehension and increasingly require machine assistance to semantically analyze, organize, and interpret vast and diverse sources of big data in order to unlock its strategic value. Due to its volume, velocity, variety, and veracity, big data integration challenges overwhelm traditional integration approaches leaving many integration possibilities out of reach. Unlocking the value of big data requires innovative technology. Organizations must have the innovativeness and data capability to adopt the technology and harness its potential value. The Semantic Web (SW) technology has demonstrated its potential for integrating big data and has become important technology for tackling big data. Despite its importance to manage big data, little research has examined the determinants affecting SW adoption. Drawing upon the technology–organization–environment framework as a theory base, this study develops a research model explaining the factors affecting the adoption of SW technology from IT professionals' perspective, specifically in the context of corporate computing enterprises. We validate the proposed model using a set of empirical data collected from IT professionals including IT managers, system architects, software developers, and web developers. The findings suggest that perceived usefulness, perceived ease of use, organization's innovativeness, organization's data capability, and applicability to data management are important drivers of SW adoption. This study provides new insights on theories of organizational IT adoption from IT professionals' perspectives tailored to the context of SW technology.",2018,,https://doi.org/10.1016/j.chb.2018.04.014
773,Bernd Heinrich and Mathias Klier and Alexander Schiller and Gerit Wagner,Assessing data quality – A probability-based metric for semantic consistency,"Data quality, Data quality assessment, Data quality metric, Data consistency","We present a probability-based metric for semantic consistency using a set of uncertain rules. As opposed to existing metrics for semantic consistency, our metric allows to consider rules that are expected to be fulfilled with specific probabilities. The resulting metric values represent the probability that the assessed dataset is free of internal contradictions with regard to the uncertain rules and thus have a clear interpretation. The theoretical basis for determining the metric values are statistical tests and the concept of the p-value, allowing the interpretation of the metric value as a probability. We demonstrate the practical applicability and effectiveness of the metric in a real-world setting by analyzing a customer dataset of an insurance company. Here, the metric was applied to identify semantic consistency problems in the data and to support decision-making, for instance, when offering individual products to customers.",2018,,https://doi.org/10.1016/j.dss.2018.03.011
774,Karl Stöger and David Schneeberger and Peter Kieseberg and Andreas Holzinger,Legal aspects of data cleansing in medical AI,"Data cleansing, Data quality, Medical AI, Medical devices","Data quality is of paramount importance for the smooth functioning of modern data-driven AI applications with machine learning as a core technology. This is also true for medical AI, where malfunctions due to ""dirty data"" can have particularly dramatic harmful implications. Consequently, data cleansing is an important part in improving the usability of (Big) Data for medical AI systems. However, it should not be overlooked that data cleansing can also have negative effects on data quality if not performed carefully. This paper takes an interdisciplinary look at some of the technical and legal challenges of data cleansing against the background of European medical device law, with the key message that technical and legal aspects must always be considered together in such a sensitive context.",2021,,https://doi.org/10.1016/j.clsr.2021.105587
775,Jiangchuan Fan and Ying Zhang and Weiliang Wen and Shenghao Gu and Xianju Lu and Xinyu Guo,The future of Internet of Things in agriculture: Plant high-throughput phenotypic platform,"Internet of things in agriculture, Big data, High-throughput phenotype, Data mining","With continuous collaborative research in sensor technology, communication technology, plant science, computer science and engineering science, Internet of Things (IoT) in agriculture has made a qualitative leap through environmental sensor networks, non-destructive imaging, spectral analysis, robotics, machine vision and laser radar technology. Physical and chemical analysis can continuously obtain environmental data, experimental metadata (including text, image and spectral, 3D point cloud and real-time growth data) through integrated automation platform equipment and technical means. Based on data on multi-scale, multi-environmental and multi-mode plant traits that constitute big data on plant phenotypes, genotype–phenotype–envirotype relationship in the omics system can be explored deeply. Detailed information on the formation mechanism of specific biological traits can promote the process of functional genomics, plant molecular breeding and efficient cultivation. This study summarises the development background, research process and characteristics of high-throughput plant phenotypes. A systematic review of the research progress of IoT in agriculture and plant high-throughput phenotypes is conducted, including the acquisition and analysis of plant phenotype big data, phenotypic trait prediction and multi-recombination analysis based on plant phenomics. This study proposes key techniques for current plant phenotypes, and looks forward to the research on plant phenotype detection technology in the field environment, fusion and data mining of plant phenotype multivariate data, simultaneous observation of multi-scale phenotype platform and promotion of a comprehensive high-throughput phenotype technology.",2021,,https://doi.org/10.1016/j.jclepro.2020.123651
776,Tanveer Ahmad and Dongdong Zhang and Chao Huang and Hongcai Zhang and Ningyi Dai and Yonghua Song and Huanxin Chen,"Artificial intelligence in sustainable energy industry: Status Quo, challenges and opportunities","Artificial intelligence, Renewable energy, Energy demand, Decision making, Big data, Energy digitization","The energy industry is at a crossroads. Digital technological developments have the potential to change our energy supply, trade, and consumption dramatically. The new digitalization model is powered by the artificial intelligence (AI) technology. The integration of energy supply, demand, and renewable sources into the power grid will be controlled autonomously by smart software that optimizes decision-making and operations. AI will play an integral role in achieving this goal. This study focuses on the use of AI techniques in the energy sector. This study aims to present a realistic baseline that allows researchers and readers to compare their AI efforts, ambitions, new state-of-the-art applications, challenges, and global roles in policymaking. We covered three major aspects, including: i) the use of AI in solar and hydrogen power generation; (ii) the use of AI in supply and demand management control; and (iii) recent advances in AI technology. This study explored how AI techniques outperform traditional models in controllability, big data handling, cyberattack prevention, smart grid, IoT, robotics, energy efficiency optimization, predictive maintenance control, and computational efficiency. Big data, the development of a machine learning model, and AI will play an important role in the future energy market. Our study’s findings show that AI is becoming a key enabler of a complex, new and data-related energy industry, providing a key magic tool to increase operational performance and efficiency in an increasingly cut-throat environment. As a result, the energy industry, utilities, power system operators, and independent power producers may need to focus more on AI technologies if they want meaningful results to remain competitive. New competitors, new business strategies, and a more active approach to customers would require informed and flexible regulatory engagement with the associated complexities of customer safety, privacy, and information security. Given the pace of development in information technology, AI and data analysis, regulatory approvals for new services and products in the new Era of digital energy markets can be enforced as quickly and efficiently as possible.",2021,,https://doi.org/10.1016/j.jclepro.2021.125834
777,Manik Madhikermi and Sylvain Kubler and Jérémy Robert and Andrea Buda and Kary Främling,Data quality assessment of maintenance reporting procedures,"Data quality, Information quality, Multi-criteria decision making, Analytic hierarchy process, Decision support systems, Maintenance","Today’s largest and fastest growing companies’ assets are no longer physical, but rather digital (software, algorithms...). This is all the more true in the manufacturing, and particularly in the maintenance sector where quality of enterprise maintenance services are closely linked to the quality of maintenance data reporting procedures. If quality of the reported data is too low, it can results in wrong decision-making and loss of money. Furthermore, various maintenance experts are involved and directly concerned about the quality of enterprises’ daily maintenance data reporting (e.g., maintenance planners, plant managers...), each one having specific needs and responsibilities. To address this Multi-Criteria Decision Making (MCDM) problem, and since data quality is hardly considered in existing expert maintenance systems, this paper develops a maintenance reporting quality assessment (MRQA) dashboard that enables any company stakeholder to easily – and in real-time – assess/rank company branch offices in terms of maintenance reporting quality. From a theoretical standpoint, AHP is used to integrate various data quality dimensions as well as expert preferences. A use case describes how the proposed MRQA dashboard is being used by a Finnish multinational equipment manufacturer to assess and enhance reporting practices in a specific or a group of branch offices.",2016,,https://doi.org/10.1016/j.eswa.2016.06.043
778,Valentina Bellini and Alberto Petroni and Giuseppina Palumbo and Elena Bignami,Data quality and blockchain technology,"Machine learning, Artificial intelligence, Blockchain technology",,2019,,https://doi.org/10.1016/j.accpm.2018.12.015
779,Nima Hoseinzadeh and Yuandong Liu and Lee D. Han and Candace Brakewood and Amin Mohammadnazar,"Quality of location-based crowdsourced speed data on surface streets: A case study of Waze and Bluetooth speed data in Sevierville, TN","Location-based data, Crowdsourced data, Waze, Bluetooth, Big data, Smart cities, Surface streets","Obtaining accurate speed and travel time information is a challenge for researchers, geographers, and transportation agencies. In the past, traffic data were usually acquired and disseminated by government agencies through fixed-location sensors. High costs, infrastructure demands, and low coverage levels of these sensor devices require agencies and researchers to look beyond the traditional approaches. With the emergence of smartphones and navigation apps, location-based and crowdsourced Big Data are receiving increased attention. In this regard, location-based big data (LocBigData) collected from probe vehicles and road users can be used to provide speed and travel time information in different locations. Examining the quality of crowdsourced data is essential for researchers and agencies before using them. This study assessed the quality of Waze speed data from surface streets and conducted a case study in Sevierville, Tennessee. Typically, examining the quality of these data in surface streets and arterials is more challenging than freeways data. This research used Bluetooth speed data as the ground truth, which is independent of Waze data. In this study, three steps of methodology were used. In the first step, Waze speed data was compared to Bluetooth data in terms of accuracy, mean difference, and distribution similarity. In the second step, a k-means algorithm was used to categorize Waze data quality, and a multinomial logistics regression model was performed to explore the significant factors that impact data quality. Finally, in the third step, machine learning techniques were conducted to predict the data quality in different conditions. The result of the comparison showed a similar pattern and a slight difference between datasets, which verified the quality of Waze speed data. The statistical model indicates that that Waze speed data are more accurate in peak hours than in night hours. Also, the traffic speed, traffic volume, and segment length have a significant association on the accuracy of Waze data on surface streets. Finally, the result of machine learning prediction showed that a KNN method performed the highest prediction accuracy of 84.5% and 82.9% of the time for training and test datasets, respectively. Overall, the study results suggest that Waze speed data is a promising data source for surface streets.",2020,,https://doi.org/10.1016/j.compenvurbsys.2020.101518
780,Fa Li and Zhipeng Gui and Huayi Wu and Jianya Gong and Yuan Wang and Siyu Tian and Jiawen Zhang,Big enterprise registration data imputation: Supporting spatiotemporal analysis of industries in China,"Geocoding, Missing values imputation, High Performance Computing, Industrial spatial distribution, Urban spatial structure, Short text classification","Big, fine-grained enterprise registration data that includes time and location information enables us to quantitatively analyze, visualize, and understand the patterns of industries at multiple scales across time and space. However, data quality issues like incompleteness and ambiguity, hinder such analysis and application. These issues become more challenging when the volume of data is immense and constantly growing. High Performance Computing (HPC) frameworks can tackle big data computational issues, but few studies have systematically investigated imputation methods for enterprise registration data in this type of computing environment. In this paper, we propose a big data imputation workflow based on Apache Spark as well as a bare-metal computing cluster, to impute enterprise registration data. We integrated external data sources, employed Natural Language Processing (NLP), and compared several machine-learning methods to address incompleteness and ambiguity problems found in enterprise registration data. Experimental results illustrate the feasibility, efficiency, and scalability of the proposed HPC-based imputation framework, which also provides a reference for other big georeferenced text data processing. Using these imputation results, we visualize and briefly discuss the spatiotemporal distribution of industries in China, demonstrating the potential applications of such data when quality issues are resolved.",2018,,https://doi.org/10.1016/j.compenvurbsys.2018.01.010
781,Daowen Liu and Liqi Lei and Tong Ruan and Ping He,Constructing Large Scale Cohort for Clinical Study on Heart Failure with Electronic Health Record in Regional Healthcare Platform: Challenges and Strategies in Data Reuse,"electronic health records, clinical terminology knowledge graph, clinical special disease case repository, evaluation of data quality, large scale cohort study","Regional healthcare platforms collect clinical data from hospitals in specific areas for the purpose of healthcare management. It is a common requirement to reuse the data for clinical research. However, we have to face challenges like the inconsistence of terminology in electronic health records (EHR) and the complexities in data quality and data formats in regional healthcare platform. In this paper, we propose methodology and process on constructing large scale cohorts which forms the basis of causality and comparative effectiveness relationship in epidemiology. We firstly constructed a Chinese terminology knowledge graph to deal with the diversity of vocabularies on regional platform. Secondly, we built special disease case repositories (i.e., heart failure repository) that utilize the graph to search the related patients and to normalize the data. Based on the requirements of the clinical research which aimed to explore the effectiveness of taking statin on 180-days readmission in patients with heart failure, we built a large-scale retrospective cohort with 29647 cases of heart failure patients from the heart failure repository. After the propensity score matching, the study group (n=6346) and the control group (n=6346) with parallel clinical characteristics were acquired. Logistic regression analysis showed that taking statins had a negative correlation with 180-days readmission in heart failure patients. This paper presents the workflow and application example of big data mining based on regional EHR data.",2019,,https://doi.org/10.24920/003579
782,Sandya De Alwis and Ziwei Hou and Yishuo Zhang and Myung Hwan Na and Bahadorreza Ofoghi and Atul Sajjanhar,"A survey on smart farming data, applications and techniques","Smart farming, Data analysis, Big data, Machine learning, Digital farming, Predictive farming, Farming industry","The Internet of Things (IoT) and the relevant technologies have had a significant impact on smart farming as a major sub-domain within the field of agriculture. Modern technology supports data collection from IoT devices through several farming processes. The extensive amount of collected smart farming data can be utilized for daily decision making and analysis such as yield prediction, growth analysis, quality maintenance, animal and aquaculture, as well as farm management. This survey focuses on three major aspects of contemporary smart farming. First, it highlights various types of big data generated through smart farming and makes a broad categorization of such data. Second, this paper discusses a comprehensive set of typical applications of big data in smart farming. Third, it identifies and introduces the principal big data and machine learning techniques that are utilized in smart farming data analysis. In doing so, this survey also identifies some of the major, current challenges in smart farming big data analysis.This paper provides a discussion on potential pathways toward more effective smart farming through relevant analytics-guided decision making.",2022,,https://doi.org/10.1016/j.compind.2022.103624
783,Laura Sebastian-Coleman,Chapter 13 - Directives for Data Quality Strategy,,,2013,,https://doi.org/10.1016/B978-0-12-397033-6.00014-6
784,Tao Zhang and Weixi Ji and Yongtao Qiu,A framework of energy-consumption driven discrete manufacturing system,"Energy-efficient optimization, Discrete manufacturing system, Data preprocessing, Data mining","Because of big data on energy consumption, there is a lack of research on the discrete manufacturing system. The discrete manufacturing system has plenty of multi-source and heterogeneous data; it was challenging to collect real-time data. Recently, low carbon and green manufacturing is a hot field; especially, it can save electrical energy. This paper proposes a significant energy consumption data of a data-driven analysis framework, which promoting the energy efficiency of discrete manufacturing plant, equipment, and workshop production process. Firstly, put forward the evaluation standards of energy efficiency for discrete manufacturing shops. Then make energy-consumption data preprocessing. Efficiency optimization of big data mining method is put forward based on grid computing function. Design the discrete manufacturing system energy-consumption parameter values, then summarizes prediction algorithms and models in order to predict the results and the trends. Finally, introduce the application of a mobile phone shell manufacturing shop to verify the proposed framework. Further research will focus on energy-consumption data mining processing.",2021,,https://doi.org/10.1016/j.seta.2021.101336
785,Ruo-Qian Wang,Chapter 13 - Artificial Intelligence for Flood Observation,"artificial intelligence, natural language processing, computer vision, machine learning, Big Data, Internet of Things, crowdsourcing, citizen science, surveillance video","Artificial intelligence (AI) is fundamentally changing our society, benefiting from the big data revolution and dramatical  declination in the Internet of Things (IoT) costs. Flood research and applications will progress with this emerging technology, as AI is creating new flood data sources, enhancing our capability to analyze the data, and improving our accuracy of flood predictions. This chapter introduces the basic concepts of AI and its technical frontier. Using the method of “review of the reviews” with example highlight the emerging AI applications in the field of flood hazards is summarized in terms of the data sources, including crowdsourcing and surveillance camera videos. The use of the AI-enabled big data is also discussed. The opportunities and barriers of this new technology are summarized. At the end of the chapter, the trend and the research gaps are identified in this field.",2021,,https://doi.org/10.1016/B978-0-12-819412-6.00013-4
786,Yahan Yang and Ruiyang Li and Yifan Xiang and Duoru Lin and Anqi Yan and Wenben Chen and Zhongwen Li and Weiyi Lai and Xiaohang Wu and Cheng Wan and Wei Bai and Xiucheng Huang and Qiang Li and Wenrui Deng and Xiyang Liu and Yucong Lin and Pisong Yan and Haotian Lin,"Standardization of collection, storage, annotation, and management of data related to medical artificial intelligence","Artificial intelligence, Big data, Intelligent medicine, Data collection, Data storage, Data annotation, Data management","Medical artificial intelligence (AI) and big data technology have rapidly advanced in recent years, and they are now routinely used for image-based diagnosis. China has a massive amount of medical data. However, a uniform criteria for medical data quality have yet to be established. Therefore, this review aimed to develop a standardized and detailed set of quality criteria for medical data collection, storage, annotation, and management related to medical AI. This will greatly improve the process of medical data resource sharing and the use of AI in clinical medicine.",2021,,https://doi.org/10.1016/j.imed.2021.11.002
787,Diego Vidaurre and Romesh Abeysuriya and Robert Becker and Andrew J. Quinn and Fidel Alfaro-Almagro and Stephen M. Smith and Mark W. Woolrich,Discovering dynamic brain networks from big data in rest and task,,"Brain activity is a dynamic combination of the responses to sensory inputs and its own spontaneous processing. Consequently, such brain activity is continuously changing whether or not one is focusing on an externally imposed task. Previously, we have introduced an analysis method that allows us, using Hidden Markov Models (HMM), to model task or rest brain activity as a dynamic sequence of distinct brain networks, overcoming many of the limitations posed by sliding window approaches. Here, we present an advance that enables the HMM to handle very large amounts of data, making possible the inference of very reproducible and interpretable dynamic brain networks in a range of different datasets, including task, rest, MEG and fMRI, with potentially thousands of subjects. We anticipate that the generation of large and publicly available datasets from initiatives such as the Human Connectome Project and UK Biobank, in combination with computational methods that can work at this scale, will bring a breakthrough in our understanding of brain function in both health and disease.",2018,,https://doi.org/10.1016/j.neuroimage.2017.06.077
788,Shazia Sadiq and Marta Indulska,Open data: Quality over quantity,"Open data, Data quality","Open data aims to unlock the innovation potential of businesses, governments, and entrepreneurs, yet it also harbours significant challenges for its effective use. While numerous innovation successes exist that are based on the open data paradigm, there is uncertainty over the data quality of such datasets. This data quality uncertainty is a threat to the value that can be generated from such data. Data quality has been studied extensively over many decades and many approaches to data quality management have been proposed. However, these approaches are typically based on datasets internal to organizations, with known metadata, and domain knowledge of the data semantics. Open data, on the other hand, are often unfamiliar to the user and may lack metadata. The aim of this research note is to outline the challenges in dealing with data quality of open datasets, and to set an agenda for future research to address this risk to deriving value from open data investments.",2017,,https://doi.org/10.1016/j.ijinfomgt.2017.01.003
789,J.W. Wang and M. Williams,"Registries, Databases and Repositories for Developing Artificial Intelligence in Cancer Care","Artificial intelligence, Big Data, database, deep learning, registries, repository","Modern artificial intelligence techniques have solved some previously intractable problems and produced impressive results in selected medical domains. One of their drawbacks is that they often need very large amounts of data. Pre-existing datasets in the form of national cancer registries, image/genetic depositories and clinical datasets already exist and have been used for research. In theory, the combination of healthcare Big Data with modern, data-hungry artificial intelligence techniques should offer significant opportunities for artificial intelligence development, but this has not yet happened. Here we discuss some of the structural reasons for this, barriers preventing artificial intelligence from making full use of existing datasets, and make suggestions as to enable progress. To do this, we use the framework of the 6Vs of Big Data and the FAIR criteria for data sharing and availability (Findability, Accessibility, Interoperability, and Reuse). We share our experience in navigating these barriers through The Brain Tumour Data Accelerator, a Brain Tumour Charity-supported initiative to integrate fragmented patient data into an enriched dataset. We conclude with some comments as to the limits of such approaches.",2022,,https://doi.org/10.1016/j.clon.2021.11.040
790,Vincent Looten and Liliane {Kong Win Chang} and Antoine Neuraz and Marie-Anne Landau-Loriot and Benoit Vedie and Jean-Louis Paul and Laëtitia Mauge and Nadia Rivet and Angela Bonifati and Gilles Chatellier and Anita Burgun and Bastien Rance,What can millions of laboratory test results tell us about the temporal aspect of data quality? Study of data spanning 17 years in a clinical data warehouse,"Quality control, Computational biology/methods*, Information storage and retrieval, Humans, Clinical laboratory information systems","Objective
To identify common temporal evolution profiles in biological data and propose a semi-automated method to these patterns in a clinical data warehouse (CDW).
Materials and Methods
We leveraged the CDW of the European Hospital Georges Pompidou and tracked the evolution of 192 biological parameters over a period of 17 years (for 445,000 + patients, and 131 million laboratory test results).
Results
We identified three common profiles of evolution: discretization, breakpoints, and trends. We developed computational and statistical methods to identify these profiles in the CDW. Overall, of the 192 observed biological parameters (87,814,136 values), 135 presented at least one evolution. We identified breakpoints in 30 distinct parameters, discretizations in 32, and trends in 79.
Discussion and conclusion
our method allowed the identification of several temporal events in the data. Considering the distribution over time of these events, we identified probable causes for the observed profiles: instruments or software upgrades and changes in computation formulas. We evaluated the potential impact for data reuse. Finally, we formulated recommendations to enable safe use and sharing of biological data collection to limit the impact of data evolution in retrospective and federated studies (e.g. the annotation of laboratory parameters presenting breakpoints or trends).",2019,,https://doi.org/10.1016/j.cmpb.2018.12.030
791,Senthil Kumar Jagatheesaperumal and Preeti Mishra and Nour Moustafa and Rahul Chauhan,A holistic survey on the use of emerging technologies to provision secure healthcare solutions,"Healthcare, Security, Internet of Things, Artificial intelligence, Machine learning, Deep learning, 5G networks","Healthcare applications demand systematic approaches to eradicate inevitable human errors to design a framework that systematically eliminates cyber-threats. The key focus of this paper is to provide a comprehensive survey on the use of modern enabling technologies, such as the Internet of Things (IoT), 5G networks, artificial intelligence (AI), and big data analytics, for providing secure and resilient healthcare solutions. A detailed taxonomy of existing technologies has been demonstrated for tackling various healthcare problems, along with their security-related issues in handling healthcare data. The application areas of each of the emerging technologies, along with their security aspects, are explained. Furthermore, an IoT-enabled smart pill bottle prototype is designed and illustrated as a case study for providing better understanding of the subject. Finally, various key research challenges are summarized with future research directions.",2022,,https://doi.org/10.1016/j.compeleceng.2022.107691
792,Gifty R. and Bharathi R.,Weibull Cumulative Distribution based real-time response and performance capacity modeling of Cyber–Physical Systems through software defined networking,"Cyber–Physical Systems (CPS), Weibull Cumulative Distribution, Big data, Response time","Huge volumes of data are generated at rates faster than the speed of computing resources and executing processors available in market place. This anticipates a draft of information challenges associated with the performance capacity and the ability of big data processing systems to retort in real-time. Moreover, the elapsed time between probabilistic failures drops as the scale of information increases. An error occurred at a specific cluster node of a large Cyber–Physical System influences the overall computation requires to unfold big data transactions. Numerous failure characteristics, statistical response time and lifetime evaluation can be modeled through Weibull Distribution. In this paper, to scrutinize the latency for a data infrastructure, the three-parameter Weibull Cumulative Distribution is used through software defined networking in cyber–physical system. This speculation predicts that the shape of the response time distribution confide in the shape of the learning curve and depicts its parameters to the criterion of the input distribution.",2020,,https://doi.org/10.1016/j.comcom.2019.11.018
793,René Buffat and Andreas Froemelt and Niko Heeren and Martin Raubal and Stefanie Hellweg,Big data GIS analysis for novel approaches in building stock modelling,"Building heat demand, Big data, Large scale modelling, Bottom-up modelling, GIS, Climate data, Spatio-temporal modelling","Building heat demand is responsible for a significant share of the total global final energy consumption. Building stock models with a high spatio-temporal resolution are a powerful tool to investigate the effects of new building policies aimed at increasing energy efficiency, the introduction of new heating technologies or the integration of buildings within an energy system based on renewable energy sources. Therefore, building stock models have to be able to model the improvements and variation of used materials in buildings. In this paper, we propose a method based on generalized large-scale geographic information system (GIS) to model building heat demand of large regions with a high temporal resolution. In contrast to existing building stock models, our approach allows to derive the envelope of all buildings from digital elevation models and to model location dependent effects such as shadowing due to the topography and climate conditions. We integrate spatio-temporal climate data for temperature and solar radiation to model climate effects of complex terrain. The model is validated against a database containing the measured energy demand of 1845 buildings of the city of St. Gallen, Switzerland and 120 buildings of the Alpine village of Zernez, Switzerland. The proposed model is able to assess and investigate large regions by using spatial data describing natural and anthropogenic land features. The validation resulted in an average goodness of fit (R2) of 0.6.",2017,,https://doi.org/10.1016/j.apenergy.2017.10.041
794,A. Bronselaer and J. Nielandt and G. {De Tré},An incremental approach for data quality measurement with insufficient information,"Data quality measurement, Uncertainty modelling, Insufficient information, Possibility theory","Recently, a fundamental study on measurement of data quality introduced an ordinal-scaled procedure of measurement. Besides the pure ordinal information about the level of quality, numerical information is induced when considering uncertainty involved during measurement. In the case where uncertainty is modelled as probability, this numerical information is ratio-scaled. An essential property of the mentioned approach is that the application of a measure on a large collection of data can be represented efficiently in the sense that (i) the representation has a low storage complexity and (ii) it can be updated incrementally when new data are observed. However, this property only holds when the evaluation of predicates is clear and does not deal with uncertainty. For some dimensions of quality, this assumption is far too strong and uncertainty comes into play almost naturally. In this paper, we investigate how the presence of uncertainty influences the efficiency of a measurement procedure. Hereby, we focus specifically on the case where uncertainty is caused by insufficient information and is thus modelled by means of possibility theory. It is shown that the amount of data that reaches a certain level of quality, can be summarized as a possibility distribution over the set of natural numbers. We investigate an approximation of this distribution that has a controllable loss of information, allows for incremental updates and exhibits a low space complexity.",2018,,https://doi.org/10.1016/j.ijar.2018.03.007
795,Ari Wibisono and Wisnu Jatmiko and Hanief Arief Wisesa and Benny Hardjono and Petrus Mursanto,Traffic big data prediction and visualization using Fast Incremental Model Trees-Drift Detection (FIMT-DD),"Intelligent traffic systems, Data stream, Traffic prediction, Traffic visualization","Information extraction using distributed sensors has been widely used to obtain information knowledge from various regions or areas. Vehicle traffic data extraction is one of the ways to gather information in order to get the traffic condition information. This research intends to predict and visualize the traffic conditions in a particular road region. Traffic data was obtained from Department of Transport UK. These data are collected using hundreds of sensors for 24 h. Thus, the size of data is very huge. In order to get the behavior of the traffic condition, we need to analyze the huge dataset which was obtained from the sensors. The uses of conventional data mining methods are not sufficient to use, due to the process of knowledge building that should store data temporary in the memory. The fact that data is continuously becoming larger over time, therefore we need to find a method that could automatically adapt to process data in the form of streams. We use method called FIMT-DD (Fast Incremental Model Trees-Drift Detection) to analyze and predict the very large traffic dataset. Based on the prediction system that we have developed, we also visualize the prediction of traffic flow condition within generated sensor point in the real map simulation.",2016,,https://doi.org/10.1016/j.knosys.2015.10.028
796,James C. Thomas and Kathy Doherty and Stephanie Watson-Grant and Manish Kumar,Advances in monitoring and evaluation in low- and middle-income countries,"Monitoring and evaluation, Health information systems, Developing countries","Data to inform and improve health care systems in low- and middle-income countries (LMICs) has been facilitated by the development of monitoring and evaluation (M&E) systems. The drivers of change in M&E systems over the last 50 years have included a series of health concerns that have animated global donors (e.g., family planning, vaccination campaigns, and HIV/AIDS); the data requirements of donors; improved national economies enabling LMICs to invest more in M&E systems; and rapid advances in digital technologies. Progress has included the training and expansion of an M&E workforce, the creation of systems for data collection and use, and processes for assessing and ensuring data quality. Controversies have included the development of disease-specific systems that do not coordinate with each other, and a growing burden on health care deliverers to collect data for a proliferating number of health and process indicators. Digital technologies offer the promise of real time data and quick adaptation but also raise ethical and privacy concerns. The desire for speed can cast large-scale evaluations, considered by some to be the gold standard, in an unfavorable light as slow and expensive. Accordingly, there is a growing demand for speedy evaluations that rely on routine health information systems and privately collected “big data” from electronic health records and social media.",2021,,https://doi.org/10.1016/j.evalprogplan.2021.101994
797,Gabriel Valença and Filipe Moura and Ana {Morais de Sá},Main challenges and opportunities to dynamic road space allocation: From static to dynamic urban designs,"Dynamic road space allocation, Big data, Smart cities, Intelligent transportation systems, Information and communication technology, Urban planning","Urban planning has focused on reallocating road space from automobile to more sustainable transport modes in many cities worldwide. Mostly in urban areas, road space (from façade to façade) is highly disputed by different urban activities and functions. Nonetheless, there are varying demand periods during the day in which road space is underutilized due to its static design. Underutilized spaces could be used for other mobility or access purposes to improve efficiency. Sensing road space, using big data and transport demand management tools, may characterize different demand patterns, adapt the road space dynamically and, ultimately, promote efficiency in using a scarce resource, such as urban road space. This approach also reinforces short-term flexibility in urban planning, allowing for better responses to unpredictable events. This paper defines the concept of dynamic road space allocation by discussing the previous literature on dynamic allocation of space. We propose a methodological framework and discuss the technological solutions as well as the many challenges of implementing dynamic road space allocation.",2021,,https://doi.org/10.1016/j.urbmob.2021.100008
798,Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang,Technology in the 21st century: New challenges and opportunities,"Business intelligence, Big data, Big data analytics, Advanced techniques, Decision-making","Although big data, big data analytics (BDA) and business intelligence have attracted growing attention of both academics and practitioners, a lack of clarity persists about how BDA has been applied in business and management domains. In reflecting on Professor Ayre's contributions, we want to extend his ideas on technological change by incorporating the discourses around big data, BDA and business intelligence. With this in mind, we integrate the burgeoning but disjointed streams of research on big data, BDA and business intelligence to develop unified frameworks. Our review takes on both technical and managerial perspectives to explore the complex nature of big data, techniques in big data analytics and utilisation of big data in business and management community. The advanced analytics techniques appear pivotal in bridging big data and business intelligence. The study of advanced analytics techniques and their applications in big data analytics led to identification of promising avenues for future research.",2019,,https://doi.org/10.1016/j.techfore.2018.06.009
799,Hayat Khaloufi and Karim Abouelmehdi and Abderrahim Beni-hssane and Mostafa Saadi,Security model for Big Healthcare Data Lifecycle,"Big data Security, Big data in healthcare, Big data lifecycle, Security threat model","Big data is a concept that aimed at collecting, storing, processing and transforming large amount of data into value using new combination of strategies and technologies. Big data is characterized by data that have a large volume, massive velocity, numerous variety, useful value, and veracity. Big Data Analytics offers tremendous insights to different organizations especially in healthcare. Currently, Big healthcare data has the highest potential for improving patient outcomes, gaining valuable insights, predicting outbreaks of epidemics, avoiding preventable diseases and effectively minimizing the cost of healthcare delivery. However, the dynamic nature of health data presents various conceptual, technical, legal and ethical challenges associated with the data processing and analysis activities. The big data security and privacy concepts are some of the most pertinent issues and have become increasingly significant associated with big healthcare data in the modern world. In this paper, we give an overview of big data characteristics and challenges in healthcare and present big healthcare data lifecycle integrated with security threats and attacks to provide encompass policies and mechanisms that aim at solving the various security challenges in each step of big data lifecycle. The focus is also placed on the description of the recently proposed techniques related to authentication, encryption, anonymization, access control, and privacy. We finally propose an approach to secure threat model for big healthcare data lifecycle as a main contribution of this paper.",2018,,https://doi.org/10.1016/j.procs.2018.10.199
800,Caroline Duvier and P.B. Anand and Crina Oltean-Dumbrava,Data quality and governance in a UK social housing initiative: Implications for smart sustainable cities,"Data quality, Data interoperability, Social housing, Smart sustainable cities, Business intelligence","Smart Sustainable Cities (SSC) consist of multiple stakeholders, who must cooperate in order for SSCs to be successful. Housing is an important challenge and in many cities, therefore, a key stakeholder are social housing organisations. This paper introduces a qualitative case study of a social housing provider in the UK who implemented a business intelligence project (a method to assess data networks within an organisation) to increase data quality and data interoperability. Our analysis suggests that creating pathways for different information systems within an organisation to ‘talk to’ each other is the first step. Some of the issues during the project implementation include the lack of training and development, organisational reluctance to change, and the lack of a project plan. The challenges faced by the organisation during this project can be helpful for those implementing SSCs. Currently, many SSC frameworks and models exist, yet most seem to neglect localised challenges faced by the different stakeholders. This paper hopes to help bridge this gap in the SSC research agenda.",2018,,https://doi.org/10.1016/j.scs.2018.02.015
801,I. Kregel and D. Stemann and J. Koch and A. Coners,Process Mining for Six Sigma: Utilising Digital Traces,"process mining, six sigma, DMAIC, big data analytics, data science, project management","Six Sigma is one of the most successful quality management philosophies of the past 20 years. However, the current challenges facing companies, such as rising process and supply chain complexity, as well as high volumes of unstructured data, cannot easily be answered by relying on traditional Six Sigma tools. Instead, the Process Mining (PM) technology using big data analytics promises valuable support for 6S and its data analysis capabilities. The article presents a design science research project in which a method for the integration of PM in Six Sigma’s DMAIC project structure was developed. This method could be extended, refined and tested during three evaluation cycles: an expert evaluation with Six Sigma professionals, a technical experiment and finally a multi case study in a company. The method therefore was eventually endorsed by 6S experts and successfully applied in a first pilot setting. This article presents the first developed method for the integration of PM and Six Sigma. It follows the recommendations of many researchers to test Six Sigma as an application field of PM as well as using the potential of big data analytics. The method can be used by researchers and practitioners alike to implement, test and verify its design in organisations.",2021,,https://doi.org/10.1016/j.cie.2020.107083
802,Jose Luis Araya Lopez and Anna V. Kalyuzhnaya and Sergey S. Kosukhin and Sergey V. Ivanov,Data Quality Control for St. Petersburg Flood Warning System,"outliers, quality-control, principal components, gap filling","This paper focuses on techniques for dealing with imperfect data in a frame of early warning system (EWS). Despite the fact that data may be technically damaged by presenting noise, outliers or missing values, met-ocean simulation systems have to deal with them to provide data transaction between models, real time data assimilation, calibration, etc. In this context data quality-control becomes one of the most important parts of EWS. St. Petersburg FWS was considered as an example of EWS. Quality control in St. Petersburg FWS contains blocks of technical control, human mistakes control, statistical control of simulated fields, statistical control and restoration of measurements and control using alternative models. Domain specific quality control was presented as two types of procedures based on theoretically proved methods were applied. The first procedure is based on probabilistic model of dynamical system, where processes are spatially interrelated and could be implemented in a form of multivariate regression (MRM). The second procedure is based on principal component analysis extended for taking into account temporal relations in data set (ePCA).",2016,,https://doi.org/10.1016/j.procs.2016.05.532
803,William McKnight,Chapter Four - Data Quality: Passing the Standard,"data quality, referential integrity, system of origination, data profiling",We might as well not do any data storage if we are not storing and passing high quality data. This chapter defines data quality and a program to maintain high standards throughout the enterprise.,2014,,https://doi.org/10.1016/B978-0-12-408056-0.00004-7
804,Jonathan Liono and Prem Prakash Jayaraman and A.K. Qin and Thuong Nguyen and Flora D. Salim,QDaS: Quality driven data summarisation for effective storage management in Internet of Things,"Quality of data, Storage management, Internet of Things (IoT), Cloud computing, Quality of service, Data summarisation","The proliferation of Internet of Things (IoT) has led to the emergence of enabling many interesting applications within the realm of several domains including smart cities. However, the accumulation of data from smart IoT devices poses significant challenges for data storage while there are needs to deliver relevant and high quality services to consumers. In this paper, we propose QDaS, a novel domain agnostic framework as a solution for effective data storage and management of IoT applications. The framework incorporates a novel data summarisation mechanism that uses an innovative data quality estimation technique. This proposed data quality estimation technique computes the quality of data (based on their utility) without requiring any feedback from users of this IoT data or domain awareness of the data. We evaluate the effectiveness of the proposed QDaS framework using real world datasets.",2019,,https://doi.org/10.1016/j.jpdc.2018.03.013
805,Aydin Farrokhi and Farid Shirazi and Nick Hajli and Mina Tajvidi,Using artificial intelligence to detect crisis related to events: Decision making in B2B by artificial intelligence,"Big data, Artificial intelligence, Machine learning, Data mining, Sentiment analytics","Artificial Intelligence (AI) could be an important foundation of competitive advantage in the market for firms. As such, firms use AI to achieve deep market engagement when the firm's data are employed to make informed decisions. This study examines the role of computer-mediated AI agents in detecting crises related to events in a firm. A crisis threatens organizational performance; therefore, a data-driven strategy will result in an efficient and timely reflection, which increases the success of crisis management. The study extends the situational crisis communication theory (SCCT) and Attribution theory frameworks built on big data and machine learning capabilities for early detection of crises in the market. This research proposes a structural model composed of a statistical and sentimental big data analytics approach. The findings of our empirical research suggest that knowledge extracted from day-to-day data communications such as email communications of a firm can lead to the sensing of critical events related to business activities. To test our model, we use a publicly available dataset containing 517,401 items belonging to 150 users, mostly senior managers of Enron during 1999 through the 2001 crisis. The findings suggest that the model is plausible in the early detection of Enron's critical events, which can support decision making in the market.",2020,,https://doi.org/10.1016/j.indmarman.2020.09.015
806,T. Devi and K. Alice and N. Deepa,Traffic management in smart cities using support vector machine for predicting the accuracy during peak traffic conditions,"Big data, Mobility, Linear and logistic regression, Support vector machine (SVM), Traffic management","Mobility is the main key for smart living, where navigation and automatic suggestions are also a strategy for a successful life in smart cities. Big Data analytics are behind urban changes in the mobility of smart cities to bring sustainable life. By the year 2025 all over Indian states can reach the expected lifestyle by providing high security and mobility which can grow the opportunities also high. As the population is rapidly increasing, the needs of people are also increasing such that necessitating real-time apps for daily needs, communication devices, and so on. We focus our idea on the benefits of traffic and safety measures which are becoming a huge challenge nowadays. Many are preferred with sophistication when traveling for short distances. In such a way the big data analytics tools R studio and weka are used on the dataset smart city from the Kaggle website for traffic patterns during the high traffic duration. Using the dataset, the data are classified using a Support vector machine (SVM) and applied with regression such as linear, logistic regression to find the accuracy of traffic peak situations. The proposed work aims to compare the efficiency of big data technologies which can be applied using various classification and regression that can be shown on various tools such as R, Weka, map-reduce which can produce accurate results to visualize the smart cities and their traffic analysis.",2022,,https://doi.org/10.1016/j.matpr.2022.03.722
807,Zhiting Song and Yanming Sun and Jiafu Wan and Peipei Liang,Data quality management for service-oriented manufacturing cyber-physical systems,"Data quality, Cyber-physical systems, Service-oriented manufacturing, Workflow nets","Service-oriented manufacturing (SOM) is a new worldwide manufacturing paradigm, and a cyber-physical system (CPS) is accepted as a strategic choice of SOM enterprises looking to provide bundles of satisfying products and services to customers. The issue of data quality is common in any CPS and poses great challenges to its efficient operation. This paper focuses on defective data generated by the improper operation of physical and cyber components of a service-oriented manufacturing CPS (SMCPS), and develops effective managerial policies to deal with such data. First, formal semantics of workflow nets (WF-nets) are employed to construct process-oriented ontology for the SMCPS. Second, a two-stage optimization model together with algorithms is designed to find optimal policies that balance local and global management objectives. Finally, our model is illustrated through a case. Results show that the proposed control strategy outperforms one-stage control and random control in guaranteeing data quality and saving control costs.",2017,,https://doi.org/10.1016/j.compeleceng.2016.08.010
808,Mauro A. Encinas and Andrzej T. Tunkiel and Dan Sui,Downhole data correction for data-driven rate of penetration prediction modeling,"Drilling, Machine learning, Rate of penetration, Drilling data quality improvement, Recurrent neural networks","In recent years, machine learning has been adopted in the Oil and Gas industry as a promising technology for solutions to the most demanding problems like downhole parameters estimations and incidents detection. A big amount of available data makes this technology an attractive option for solving a wide variety of drilling problems, as well as a reliable candidate for performing big-data analysis and interpretation. Nevertheless, this approach may cause, in some cases, that petroleum engineering concepts are disregarded in favor of more data-intensive approaches. This study aims to evaluate the impact of drilling data measurement correction on data-driven model performance. In our study, besides using the standard data processing technologies, like gap filling, outlier removal, noise reduction etc., the physics-based drilling models are also implemented for data quality improvement and data correction in consideration of the measurement physics, rarely mentioned in most of publications. In our case study, recurrent neural networks (RNN) that are able to capture temporal natures of a signal are employed for the rate of penetration (ROP) estimation with an adjustable predictive window. The results show that the RNN model produces the best results when using the drilling data recovered through analytical methods. Moreover, the comprehensive data-driven model evaluation and engineering interpretation are conducted to facilitate better understanding of the data-driven models and their applications.",2022,,https://doi.org/10.1016/j.petrol.2021.109904
809,Oane Visser and Sarah Ruth Sippel and Louis Thiemann,Imprecision farming? Examining the (in)accuracy and risks of digital agriculture,"Digital agriculture, Smart farming, Precision agriculture, Accuracy, Big data","The myriad potential benefits of digital farming hinge on the promise of increased accuracy, which allows ‘doing more with less’ through precise, data-driven operations. Yet, precision farming's foundational claim of increased accuracy has hardly been the subject of comprehensive examination. Drawing on social science studies of big data, this article examines digital agriculture's (in)accuracies and their repercussions. Based on an examination of the daily functioning of the various components of yield mapping, it finds that digital farming is often ‘precisely inaccurate’, with the high volume and granularity of big data erroneously equated with high accuracy. The prevailing discourse of ‘ultra-precise’ digital technologies ignores farmers' essential efforts in making these technologies more accurate, via calibration, corroboration and interpretation. We suggest that there is the danger of a ‘precision trap’. Namely, an exaggerated belief in the precision of big data that over time leads to an erosion of checks and balances (analogue data, farmer observation et cetera) on farms. The danger of ‘precision traps’ increases with the opacity of algorithms, with shifts from real-time measurement and advice towards forecasting, and with farmers' increased remoteness from field operations. Furthermore, we identify an emerging ‘precision divide’: unequally distributed precision benefits resulting from the growing algorithmic divide between farmers focusing on staple crops, catered well by technological innovation on the one hand, and farmers cultivating other crops, who have to make do with much less advanced or applicable algorithms on the other. Consequently, for the latter farms digital farming may feel more like ‘imprecision farming’.",2021,,https://doi.org/10.1016/j.jrurstud.2021.07.024
810,Roger Clarke,Guidelines for the responsible application of data analytics,"Big data, Data science, Data quality, Decision quality, Regulation","The vague but vogue notion of ‘big data’ is enjoying a prolonged honeymoon. Well-funded, ambitious projects are reaching fruition, and inferences are being drawn from inadequate data processed by inadequately understood and often inappropriate data analytic techniques. As decisions are made and actions taken on the basis of those inferences, harm will arise to external stakeholders, and, over time, to internal stakeholders as well. A set of Guidelines is presented, whose purpose is to intercept ill-advised uses of data and analytical tools, prevent harm to important values, and assist organisations to extract the achievable benefits from data, rather than dreaming dangerous dreams.",2018,,https://doi.org/10.1016/j.clsr.2017.11.002
811,David Salvetat and Jean-Sébastien Lacam,Data determinants of the activity of SMEs automobile dealers,"Big data, Smart data, Development, Automobile, SME","Many SMEs still seem reluctant to accept the management of large datasets, which still appear to be too complex for them. However, our study reveals that the majority of small French car dealers are developing Big data and Smart data policies to improve the quality of their offers, the dynamism of their sales and their access to new opportunities. However, not every policy has the same effects on the development of their business. Whereas Big data improves all the components of SME development in a global, short-term and operational way, Smart data presents itself as a more targeted, prospective and strategic approach.",2020,,https://doi.org/10.1016/j.jengtecman.2020.101602
812,Camilla Schaefer and Ana Makatsaria,Framework of Data Analytics and Integrating Knowledge Management,"Data analytics, Knowledge management, Big data, Business intelligence, Data discovery","Big data is significantly dependent on technologies such as cloud computing, machine learning and statistical models. However, its significance is becoming more dependent on human qualities e.g. judgment, value, intuition and experience. Therefore, the human knowledge presents a basis for knowledge management and big data, which are a major element of data analytics. This research contribution applies the process of Data, Information, Knowledge and Perception hierarchy as a structure to evaluate the end-users’ process. The framework in incorporating data analytics and display a conceptual data analytics process (with three phases) evaluated as knowledge management, including the creation, discovery and application of knowledge. Knowledge conversion theories are applicable in data analytics to emphasize on the typically overlooked organizational and human aspects, which are critical to the efficiency of data analytics. The synergy and alignment between knowledge management and data analytics is fundamental in fostering innovations and collaboration.",2021,,https://doi.org/10.1016/j.ijin.2021.09.004
813,Shivani Sharma and Durga Toshniwal,MR-I MaxMin-scalable two-phase border based knowledge hiding technique using MapReduce,"Big data, Computational cost, Knowledge hiding techniques, MapReduce, Privacy preservation, Scalability","Border based Knowledge hiding techniques (BB-KHT) are widely adopted form of privacy preservation techniques of data mining. These approaches are used to hide sensitive knowledge (confidential information) present in a dataset before sharing or analyzing it. BB-KHT primarily rely on border theory and maximum criterion method for preserving privacy and perpetuating good data quality of sanitized dataset but costs high computational complexity. Further, due to sequential nature, these approaches are particularly felicitous for small datasets and become infeasible while dealing with large scale datasets. Therefore, to subjugate the identified challenges of infeasibility and high computational complexity, a scalable two-phase improved MaxMin BB-KHT using MapReduce framework (MR-I MaxMin) is proposed. The proposed scheme requires only two database scans throughout the hiding process and hence, is computationally inexpensive. Moreover, the scheme also commits to preserve good data quality of sanitized dataset. The MapReduce version of proposed approach helps in achieving the feasibility by processing large voluminous data in a parallel fashion. Quantitative experiments and evaluations have been performed over a number of real and synthetically generated large-scale datasets. It is shown that the proposed MR-I MaxMin technique outperforms the similar existing approaches and vanquishes the identified challenges along with much-needed privacy preservation.",2020,,https://doi.org/10.1016/j.future.2018.05.063
814,Wei Xiong and Li Xiong,Anti-collusion data auction mechanism based on smart contract,"Data auction mechanism, Anti-collusion, Smart contract, Blockchain, Ethereum","Due to the uncertainty of the value of big data, it is difficult to directly give a reasonable price for big data. Auction is an effective method of distributing goods to the bidder with the highest valuation. Hence, the use of auction strategy can not only guarantee the interests of data sellers, but also conform to market principles. However, existing data auction mechanisms are centralized. It is hard to build trust among sellers, buyers and auctioneers. An open and anonymous online environment may cause entities involved in data auctions to collude to manipulate the results of data auctions. This will cause the price of auction data to fail to reach a fair and truthful level. Therefore, the first anti-collusion data auction mechanism based on smart contract is proposed. Through a well-designed anti-collusion data auction algorithm, mutual distrust and rational buyers and sellers safely participate in the data auction without a trusted third party. The data auction mechanism designed in the smart contract can effectively prevent collusion and realize the fairness and truthfulness of data auction. The webpack in the Truffle Boxes is used to implement the data auction mechanism, and the anti-collusion property of the mechanism has been verified. The source code of the smart contract has been uploaded to GitHub.",2021,,https://doi.org/10.1016/j.ins.2020.10.053
815,Assunta {Di Vaio} and Rohail Hassan and Claude Alavoine,Data intelligence and analytics: A bibliometric analysis of human–Artificial intelligence in public sector decision-making effectiveness,"Ambidexterity, Industry 4.0, Business intelligence, Big data, Intellectual capital, Human intellect, Accountability and performance","This study investigates the literary corpus of the role and potential of data intelligence and analytics through the lenses of artificial intelligence (AI), big data, and the human–AI interface to improve overall decision-making processes. It investigates how data intelligence and analytics improve decision-making processes in the public sector. A bibliometric analysis of a database containing 161 English-language articles published between 2017 and 2021 is performed, providing a map of the knowledge produced and disseminated in previous studies. It provides insights into key topics, citation patterns, publication activities, the status of collaborations between contributors over past studies, aggregated data intelligence, and analytics research contributions. The study provides a retrospective review of published content in the field of data intelligence and analytics. The findings indicate that field research has been concentrated mainly on emerging technologies' intelligence capabilities rather than on human–artificial intelligence in decision-making performance in the public sector. This study extends an ambidexterity theory in decision support, which enlightens how this ambidexterity can be encouraged and how it affects decision outcomes. The study emphasises the importance of the public sector adoption of data intelligence and analytics, as well as its efficiency. Furthermore, this study expands how researchers and practitioners interpret and understand data intelligence and analytics, AI, and big data for effective public sector decision-making.",2022,,https://doi.org/10.1016/j.techfore.2021.121201
816,Emily Tat and Mark Rabbat,Chapter 17 - Ethical and legal challenges,"Artificial intelligence, Autonomy, Big data, Black box, Ethics, Informed consent, Liability, Privacy, Safety","As the technology of artificial intelligence (AI) grows in cardiovascular medicine, so do the ethical and legal challenges that come with it. Currently, the medical community is ill-informed of what these challenges entail, and policy and ethical guidelines are lacking. Physicians and policy makers should be informed of these issues to minimize harm and promote patient care. Three overarching themes relating to the data, the algorithms, and the results comprise the foundation of these challenges and will be discussed in this chapter. The introduction of big data raises concern for patient privacy and security, with issues of data quality and inconsistent medical records. There is also risk for biases in the algorithms that could worsen health disparities or skew results for financial gain. Finally, the archetypal “black box” algorithm, questions of legal liability, and what happens when humans and machine disagree are discussed in depth. Ultimately, a code of ethics in the coming integration of AI is needed to ensure the preservation of human rights.",2021,,https://doi.org/10.1016/B978-0-12-820273-9.00017-8
817,Ziqian Xia and Yurong Liu,Aiding pro-environmental behavior measurement by Internet of Things,"Pro-environmental behavior, Internet of Things, Measurement, Big data, Environmental psychology","Promoting pro-environmental behavior is an effective means of reducing carbon emissions at the individual end, but the measurement of behavior has long been a problem for scholars. Especially in environmental psychology community, the complexity of social policies and habitat implies greater difficulty in measuring. Due to the limitations of traditional questionnaire, laboratory, and naturalistic observation methods, environmental psychologists need more realistic, accurate, and cost-effective ways to measure behavior. The rapid development of IoT technology lights up the hope for achieving this goal, and its large-scale popularization will bring great changes to the research community. This paper reviews the current methods and their limitations, proposes a framework for measuring behavior using IoT devices, and points out its future research directions.",2021,,https://doi.org/10.1016/j.crbeha.2021.100055
818,Maira E. Ezerins and Timothy D. Ludwig and Tara O'Neil and Anne M. Foreman and Yalçın Açıkgöz,Advancing safety analytics: A diagnostic framework for assessing system readiness within occupational safety and health,"Safety analytics, Data analytics, Readiness assessment, Occupational health","Big data and analytics have shown promise in predicting safety incidents and identifying preventative measures directed towards specific risk variables. However, the safety industry is lagging in big data utilization due to various obstacles, which may include lack of data readiness (e.g., disparate databases, missing data, low validity) and personnel competencies. This paper provides a primer on the application of big data to safety. We then describe a safety analytics readiness assessment framework that highlights system requirements and the challenges that safety professionals may encounter in meeting these requirements. The proposed framework suggests that safety analytics readiness depends on (a) the quality of the data available, (b) organizational norms around data collection, scaling, and nomenclature, (c) foundational infrastructure, including technological platforms and skills required for data collection, storage, and analysis of health and safety metrics, and (d) measurement culture, or the emergent social patterns between employees, data acquisition, and analytic processes. A safety-analytics readiness assessment can assist organizations with understanding current capabilities so measurement systems can be matured to accommodate more advanced analytics for the ultimate purpose of improving decisions that mitigate injury and incidents.",2022,,https://doi.org/10.1016/j.ssci.2021.105569
819,Jules J. Berman,19 - Legalities,"Data Quality Act, Freedom of Information Act, FOIA, Limited Data Use Agreements, and Madey v. Duke, Tort, Patents, Intellectual property, Informed consent, Data ownership, Copyright, Infringement, Fair use","Big Data projects always incur some legal risk. It is impossible to know all the data contained in a Big Data project, and it is impossible to know every purpose to which Big Data is used. Hence, the entities that produce Big Data may unknowingly contribute to a variety of illegal activities, chiefly: copyright and other intellectual property infringements, breaches of confidentiality, and privacy invasions. In addition, issues of data quality, data availability, and data documentation may contribute to the legal or regulatory disqualification of a Big Data resource. In this chapter, four issues will be discussed in detail: (1) responsibility for the accuracy of the contained data; (2) obtaining the rights to create, use, and share the data held in the resource; (3) intellectual property encumbrances incurred from the use of standards required for data representation and data exchange; and (4) protections for individuals whose personal information is used in the resource. Big Data managers contend with a wide assortment of legal issues, but these four issues will never go away.",2018,,https://doi.org/10.1016/B978-0-12-815609-4.00019-4
820,Madhavi Vaidya and Shweta Katkar,Chapter 24 - Exploring performance and predictive analytics of agriculture data,"Agriculture, Big data, Weka, J48, Talend, Crops, Analytics, Fertilizers, Prediction","The exponential growth and ubiquity of both structured and unstructured data have led us into the big data era. Big data analytics is increasingly becoming a trending practice that many organizations are adopting to construct valuable information from big data. This field has substantially attracted academics, practitioners, and industries. But there are some challenges for big data processing and analytics that include integration of data, volume ofdata, the rate of transformation of data, and the veracity and validity of data. The history of griculture in India dates back to the Indus Valley civilization. Due to variations in climatic conditions, it has become challenging to achieve the desired results in crop yields. The use of technology in agriculture has increased in recent years and data analytics is one such trend that has penetrated the agriculture field. The main challenge in using big data in agriculture is identifying the effectiveness of big data analytics. Big data analysis can be processed and analyzed using parallel databases such as Talend or analytical paradigms like MapReduce on a Hadoop distributed file system. There are other mechanisms such as Weka and R, which are two of the most popular data analytical and statistical computing tools produced by the open source community, but there are certain challenges compared to the other techniques mentioned. In this chapter, the comparative studies of various mechanisms will be provided that will give an insight to process and analyze big data generated from farms and the grains obtained from it according to the seasons, the soil health, and the location. In addition, various case studies are shown for data processing in context with planting, agricultural growth, and smart farming. From the experimentation, the authors have shown which is the right fertilizer for a specific state and soil. In addition, the authors have worked on the analysis of crop production per state and per year.",2022,,https://doi.org/10.1016/B978-0-12-823694-9.00030-X
821,Gonzalo Sirgo and Federico Esteban and Josep Gómez and Gerard Moreno and Alejandro Rodríguez and Lluis Blanch and Juan José Guardiola and Rafael Gracia and Lluis {De Haro} and María Bodí,Validation of the ICU-DaMa tool for automatically extracting variables for minimum dataset and quality indicators: The importance of data quality assessment,"Electronic medical record, Quality indicators, Critical care, Information processing, Data quality, Verification","Background
Big data analytics promise insights into healthcare processes and management, improving outcomes while reducing costs. However, data quality is a major challenge for reliable results. Business process discovery techniques and an associated data model were used to develop data management tool, ICU-DaMa, for extracting variables essential for overseeing the quality of care in the intensive care unit (ICU).
Objective
To determine the feasibility of using ICU-DaMa to automatically extract variables for the minimum dataset and ICU quality indicators from the clinical information system (CIS).
Methods
The Wilcoxon signed-rank test and Fisher’s exact test were used to compare the values extracted from the CIS with ICU-DaMa for 25 variables from all patients attended in a polyvalent ICU during a two-month period against the gold standard of values manually extracted by two trained physicians. Discrepancies with the gold standard were classified into plausibility, conformance, and completeness errors.
Results
Data from 149 patients were included. Although there were no significant differences between the automatic method and the manual method, we detected differences in values for five variables, including one plausibility error and two conformance and completeness errors. Plausibility: 1) Sex, ICU-DaMa incorrectly classified one male patient as female (error generated by the Hospital’s Admissions Department). Conformance: 2) Reason for isolation, ICU-DaMa failed to detect a human error in which a professional misclassified a patient’s isolation. 3) Brain death, ICU-DaMa failed to detect another human error in which a professional likely entered two mutually exclusive values related to the death of the patient (brain death and controlled donation after circulatory death). Completeness: 4) Destination at ICU discharge, ICU-DaMa incorrectly classified two patients due to a professional failing to fill out the patient discharge form when thepatients died. 5) Length of continuous renal replacement therapy, data were missing for one patient because the CRRT device was not connected to the CIS.
Conclusions
Automatic generation of minimum dataset and ICU quality indicators using ICU-DaMa is feasible. The discrepancies were identified and can be corrected by improving CIS ergonomics, training healthcare professionals in the culture of the quality of information, and using tools for detecting and correcting data errors.",2018,,https://doi.org/10.1016/j.ijmedinf.2018.02.007
822,Daniel E. O'Leary,Technology life cycle and data quality: Action and triangulation,"Technology life cycle, Data quality, Non-stationary data, Hype cycle, Data biases, Data phase triangulation","Where a technology is its life cycle can make a difference in the data generated by or about adopting, using or implementing that technology. As a result, it is arguable that adoption, usage or implementation data generated early in a technology's life cycle is directly comparable to data generated later in the life cycle. In particular, comparisons of early and late data can result in a number of disparate results and limit replicability, because of biases in the data and non-stationary data. This paper suggests that it can be important for information systems researchers to disclose an estimate of the location in the technology's life cycle, as part of their analysis. The data life cycle discussion is then extended to the notion of “data phase triangulation,” which is contrasted with “methodology triangulation” and “data (collection) triangulation.” In addition, we discuss the importance of being able to use the findings from life cycle-based research to “push” a technology from one phase to another phase.",2019,,https://doi.org/10.1016/j.dss.2019.113139
823,Yang Wang and Hong Zhang and Libing Liu,Does city construction improve life quality?-evidence from POI data of China,"Quality of life, Point of interest, Happiness","To explore the construction of a big data indicator system is conducive to a comprehensive, scientific, timely and accurate grasp of the quality of life of our residents and its evolutionary trends. This paper systematically sorts out the performance dimensions of the residents' quality of life, and integrates two types of methods of objective observation and subjective evaluation commercial POI(Point of Interest) data. From the aspects of life, entertainment, transportation, etc., preliminary development has been made including 8 first-level indicators, 16 second-level indicators, and 27 third-level indicators Big data indicator system, and measure the ""clogging point"" of the improvement of residents' quality of life, with a view to providing a scientific and feasible decision-making reference for ""meeting the people's increasing needs for a better life"".",2022,,https://doi.org/10.1016/j.iref.2022.01.004
824,Tibor Koltay,Chapter 5 - Digital Research Data: Where are we Now?,"data citation, data curation, data literacy, data management, data quality, data sharing, research data","The key topic of digital research data raises a multitude of issues: big data, data sharing, data quality, data management, data curation, data citation, data literacy. This chapter addresses questions related to the definition of these concepts, to the frameworks constructed for a better understanding and treatment of the different phenomena, as well as ethical considerations. The potential of libraries and information professionals in fulfilling data-related activities is outlined, together with the associated requirements of them.",2016,,https://doi.org/10.1016/B978-0-08-100251-3.00005-6
825,Anja {du Plessis},"Necessity of making water smart for proactive informed decisive actions: A case study of the upper vaal catchment, South Africa","Data quality, Decisive action, Smart water management, Water quality, South Africa, Upper vaal catchment","The need for informed management of water resources has been continuously highlighted worldwide. Societies are increasingly faced with water quality challenges globally which directly translate into multifaceted challenges. South Africa has acknowledged that water is not receiving the attention and status it deserves. Wastage is rife and degradation widespread. The sustainability of South Africa's freshwater resources has reached a critical point and requires decisive action. Vast amounts of water quality data, varying in quality, is available however the seemingly lack of integrative data management has led to reactive planning and questionable decisions. The paper highlights the necessity for making water smart through a case study of the Upper Vaal catchment. The quality of available government data is mostly of an acceptable standard according to the evaluated data dimensions and elements. The practical application of determining hydrological responses to predict possible water quality changes towards land cover change in the Vaal river catchment emphasises that there is suitable data available and highlights the value of Smart Water Management (SWM). SWM can enable improved integrated water resource management by increasing sharing and effective use of real-time data of acceptable quality to promote proactive unambiguous strategies and decisions focused on overall improved water management and the evasion of a future water predicament.",2021,,https://doi.org/10.1016/j.envc.2021.100100
826,Inês Araújo Machado and Carlos Costa and Maribel Yasmina Santos,Data Mesh: Concepts and Principles of a Paradigm Shift in Data Architectures,"Big Data, Data Mesh, Data Architectures, Data Lake","Inherent to the growing use of the most varied forms of software (e.g., social applications), there is the creation and storage of data that, due to its characteristics (volume, variety, and velocity), make the concept of Big Data emerge. Big Data Warehouses and Data Lakes are concepts already well established and implemented by several organizations, to serve their decision-making needs. After analyzing the various problems demonstrated by those monolithic architectures, it is possible to conclude about the need for a paradigm shift that will make organizations truly data-oriented. In this new paradigm, data is seen as the main concern of the organization, and the pipelining tools and the Data Lake itself are seen as a secondary concern. Thus, the Data Mesh consists in the implementation of an architecture where data is intentionally distributed among several Mesh nodes, in such a way that there is no chaos or data silos, since there are centralized governance strategies and the guarantee that the core principles are shared throughout the Mesh nodes. This paper presents the motivation for the appearance of the Data Mesh paradigm, its features, and approaches for its implementation.",2022,,https://doi.org/10.1016/j.procs.2021.12.013
827,Marta Zorrilla and Juan Yebenes,A reference framework for the implementation of data governance systems for industry 4.0,"Data governance, Data-Centric architecture, Industry 4.0, Big data, IoT","The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This is mainly based on the digitalization of the industrial environment by means of the convergence of Information Technologies (IT) and operational Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT) and the use of data generated in real time for gaining insights and making decisions. Therefore data becomes a critical asset for Industry 4.0 and must be managed and governed like a strategic asset. We rely on Data Governance (DG) as a key instrument for carrying out this transformation. This paper presents the design of a specific governance framework for Industry 4.0. First, this contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, the cloud and edge computing, the artificial intelligence and the current regulations. Next, we formally define a reference framework for the implementation of Data Governance Systems for Industry 4.0 using international standards and providing several examples of architecture building blocks.",2022,,https://doi.org/10.1016/j.csi.2021.103595
828,J. Ramsingh and V. Bhuvaneswari,An integrated multi-node Hadoop framework to predict high-risk factors of Diabetes Mellitus using a Multilevel MapReduce based Fuzzy Classifier (MMR-FC) and Modified DBSCAN algorithm,"Fuzzy classifier, MDBSCAN, MapReduce, Hadoop, Diabetes mellitus","In the era of data deluge, the world is experiencing an intensive growth of Big data with complex structures. While processing of these data is a complex and labor-intensive process, a proper analysis of Big data leads to greater knowledge extraction. In this paper, Big data is used to predict high-risk factors of Diabetes Mellitus using a new integrated framework with four Hadoop clusters, which are developed to classify the data based on Multi-level MapReduce Fuzzy Classifier (MMR-FC) and MapReduce-Modified Density-Based Spatial Clustering of Applications with Noise (MR-MDBSCAN) algorithm. Big data concerning people’s food habits, physical activity are extracted from social media using the API’s provided. The MMR-FC takes place at three levels of index (Glycemic Index, Physical activity Index, Sleeping Pattern) values. The fuzzy rules are generated by the MMR-FC algorithm to predict the risk of Diabetes Mellitus using the data extracted. The result from MMR-FC is used as an input to the semantic location prediction framework to predict the high-risk zones of Diabetes Mellitus using the MR-MDBSCAN algorithm. The analysis shows that more than 55% of people are in a high-risk group with positive sentiments on the data extracted. More than 70% of food with a high Glycemic Index is usually consumed during Night and Early Evenings, which reveals that people consume food that has a high Glycemic Index during their sedentary slot and have irregular sleep practices. Around 70% of the unhealthiest dietary patterns are retrieved from urban hotspots such as Delhi, Cochin, Kolkata, and Chennai. From the results, it is evident that 55% of younger generations, users of social networking sites having high possibilities of Type II Diabetes Mellitus at large.",2021,,https://doi.org/10.1016/j.asoc.2021.107423
829,Ruo-Qian Wang and Huina Mao and Yuan Wang and Chris Rae and Wesley Shaw,Hyper-resolution monitoring of urban flooding with social media and crowdsourcing data,,"Hyper-resolution datasets for urban flooding are rare. This problem prevents detailed flooding risk analysis, urban flooding control, and the validation of hyper-resolution numerical models. We employed social media and crowdsourcing data to address this issue. Natural Language Processing and Computer Vision techniques are applied to the data collected from Twitter and MyCoast (a crowdsourcing app). We found these big data based flood monitoring approaches can complement the existing means of flood data collection. The extracted information is validated against precipitation data and road closure reports to examine the data quality. The two data collection approaches are compared and the two data mining methods are discussed. A series of suggestions is given to improve the data collection strategy.",2018,,https://doi.org/10.1016/j.cageo.2017.11.008
830,Iñigo Martinez and Elisabeth Viles and Igor {G. Olaizola},Data Science Methodologies: Current Challenges and Future Approaches,"Data science, Big data, Data science methodology, Project life-cycle, Organizational impacts, Knowledge management","Data science has employed great research efforts in developing advanced analytics, improving data models and cultivating new algorithms. However, not many authors have come across the organizational and socio-technical challenges that arise when executing a data science project: lack of vision and clear objectives, a biased emphasis on technical issues, a low level of maturity for ad-hoc projects and the ambiguity of roles in data science are among these challenges. Few methodologies have been proposed on the literature that tackle these type of challenges, some of them date back to the mid-1990, and consequently they are not updated to the current paradigm and the latest developments in big data and machine learning technologies. In addition, fewer methodologies offer a complete guideline across team, project and data & information management. In this article we would like to explore the necessity of developing a more holistic approach for carrying out data science projects. We first review methodologies that have been presented on the literature to work on data science projects and classify them according to the their focus: project, team, data and information management. Finally, we propose a conceptual framework containing general characteristics that a methodology for managing data science projects with a holistic point of view should have. This framework can be used by other researchers as a roadmap for the design of new data science methodologies or the updating of existing ones.",2021,,https://doi.org/10.1016/j.bdr.2020.100183
831,M.S. Reis and P.M. Saraiva and B.R. Bakshi,3.10 - Data Quality and Denoising: A Review☆,"Bayesian estimation, Data quality, Data rectification, Filtering, Fourier analysis, Gaussian and non-Gaussian noise, Kalman filtering, Model-based denoising, Multiscale analysis, Off-line and online denoising, Outliers, Smoothing, Wavelet analysis, Wavelet thresholding, Windowed Fourier analysis","This article introduces the methods of Fourier and wavelet analysis for enhancing data quality in typical chemometric and process analytics applications. Fourier analysis has been popular for many decades but is best suited for enhancing signals where most features are localized in frequency. In contrast, wavelet analysis is appropriate for signals that contain features localized in both time and frequency domains. It also retains the benefits of Fourier analysis such as orthonormality and computational efficiency. Practical algorithms for off-line and on-line denoising are described and compared via simple examples. These algorithms can be used for off-line or on-line applications in order to mitigate the impact of additive Gaussian as well as non-Gaussian noise.",2020,,https://doi.org/10.1016/B978-0-12-409547-2.14874-7
832,Carlos Sáez and Siaw-Teng Liaw and Eizen Kimura and Pascal Coorevits and Juan M Garcia-Gomez,Guest editorial: Special issue in biomedical data quality assessment methods,,,2019,,https://doi.org/10.1016/j.cmpb.2019.06.013
833,Kwok Tai Chui and Pandian Vasant and Ryan Wen Liu,Chapter 7 - Smart city is a safe city: information and communication technology–enhanced urban space monitoring and surveillance systems: the promise and limitations,"Cyber security, Ethics, Policy-making, Security, Surveillance","Urban space monitoring and surveillance systems are present almost everywhere in various forms of sensing devices such as closed-circuit television, smartphone, and camera. This requires a robust and easy-to-manage information and communication technology (ICT) infrastructure that is generally comprises sensors, protocols, networks, and steps. Smart adoption of such systems could influence, manage, direct, and protect human beings and property. Nevertheless, it may create problems of government support, data quality, privacy, and security. Today's computational world allows implementation of artificial intelligence models for big data analytics to bring cities smart (with intelligence and optimal improvement). This chapter will discuss the applications of urban space monitoring and surveillance systems via ICT. The typical limitations of the current research are discussed in detail.",2019,,https://doi.org/10.1016/B978-0-12-816639-0.00007-7
834,Joanna Kazmierska and Andrew Hope and Emiliano Spezi and Sam Beddar and William H. Nailon and Biche Osong and Anshu Ankolekar and Ananya Choudhury and Andre Dekker and Kathrine Røe Redalen and Alberto Traverso,From multisource data to clinical decision aids in radiation oncology: The need for a clinical data science community,"Artificial intelligence, Big data, Data science, Personalized treatment, Radiotherapy, Shared decision making","Big data are no longer an obstacle; now, by using artificial intelligence (AI), previously undiscovered knowledge can be found in massive data collections. The radiation oncology clinic daily produces a large amount of multisource data and metadata during its routine clinical and research activities. These data involve multiple stakeholders and users. Because of a lack of interoperability, most of these data remain unused, and powerful insights that could improve patient care are lost. Changing the paradigm by introducing powerful AI analytics and a common vision for empowering big data in radiation oncology is imperative. However, this can only be achieved by creating a clinical data science community in radiation oncology. In this work, we present why such a community is needed to translate multisource data into clinical decision aids.",2020,,https://doi.org/10.1016/j.radonc.2020.09.054
835,T. Gant,The importance of data quality to enhance the impact of omics sciences,,,2015,,https://doi.org/10.1016/j.toxlet.2015.08.097
836,Thomas Steckler and Katja Brose and Magali Haas and Martien J. Kas and Elena Koustova and Anton Bespalov,The preclinical data forum network: A new ECNP initiative to improve data quality and robustness for (preclinical) neuroscience,"Reproducibility, Robustness, Relevance, Quality assurance, Neuroscience, Pre-clinical","Current limitations impeding on data reproducibility are often poor statistical design, underpowered studies, lack of robust data, lack of methodological detail, biased reporting and lack of open data sharing, coupled with wrong research incentives. To improve data reproducibility, robustness and quality for brain disease research, a Preclinical Data Forum Network was formed under the umbrella of the European College of Neuropsychopharmacology (ECNP). The goal of this network, members of which met for the first time in October 2014, is to establish a forum to collaborate in precompetitive space, to exchange and develop best practices, and to bring together the members from academia, pharmaceutical industry, publishers, journal editors, funding organizations, public/private partnerships and non-profit advocacy organizations. To address the most pertinent issues identified by the Network, it was decided to establish a data sharing platform that allows open exchange of information in the area of preclinical neuroscience and to develop an educational scientific program. It is also planned to reach out to other organizations to align initiatives to enhance efficiency, and to initiate activities to improve the clinical relevance of preclinical data. Those Network activities should contribute to scientific rigor and lead to robust and relevant translational data. Here we provide a synopsis of the proceedings from the inaugural meeting.",2015,,https://doi.org/10.1016/j.euroneuro.2015.05.011
837,Rutian Liu and Eric Simon and Bernd Amann and Stéphane Gançarski,Discovering and merging related analytic datasets,"Schema augmentation, Schema complement, Data quality, SAP HANA","The production of analytic datasets is a significant big data trend and has gone well beyond the scope of traditional IT-governed dataset development. Analytic datasets are now created by data scientists and data analysts using big data frameworks and agile data preparation tools. However, despite the profusion of available datasets, it remains quite difficult for a data analyst to start from a dataset at hand and customize it with additional attributes coming from other existing datasets. This article describes a model and algorithms that exploit automatically extracted and user-defined semantic relationships for extending analytic datasets with new atomic or aggregated attribute values. Our framework is implemented as a REST service in SAP HANA and includes a careful theoretical analysis and practical solutions for several complex data quality issues.",2020,,https://doi.org/10.1016/j.is.2020.101495
838,Sascha Eichstädt and Anke Keidel and Julia Tesch,Metrology for the digital age,"Digital transformation, Digital certificates, Systemic metrology, Big data, Industry 4.0, Open science, FAIR","Based on digital technologies, big data, artificial intelligence and machine-readable information, the digital transformation rapidly changes society, industries, and economies. Metrology as a central element of international trade, for confidence in measurements and part of the quality infrastructure is facing several challenges and opportunities in these developments. In this contribution we discuss some of the key challenges and a potential future role of metrology in the digital age. We address metrological principles for confidence in data and Algorithms, cyber-physical systems, FAIR data and metrology, and the role of metrology in the digital transformation in the quality infrastructure.",2021,,https://doi.org/10.1016/j.measen.2021.100232
839,Alan Simon,Chapter 8 - Considerations for the Big Data Era,"Big Data, business intelligence, BI, analytics, predictive analytics, program manager, program management, project manager, project management, challenges, data warehouse, data warehousing, enterprise data warehouse, EDW","The Big Data era is creating seismic shifts in how we approach enterprise business intelligence and data warehousing. This final chapter discusses considerations related to technology and architecture, analytics-oriented requirements collection, and organizational structure.",2015,,https://doi.org/10.1016/B978-0-12-801540-7.00008-1
840,Wen Chen and Kaile Zhou and Shanlin Yang and Cheng Wu,Data quality of electricity consumption data in a smart grid environment,"Electricity consumption data, Data quality, Outlier detection, Outlier data, Smart grid","With the increasing penetration of traditional and emerging information technologies in the electric power industry, together with the rapid development of electricity market reform, the electric power industry has accumulated a large amount of data. Data quality issues have become increasingly prominent, which affect the accuracy and effectiveness of electricity data mining and energy big data analytics. It is also closely related to the safety and reliability of the power system operation and management based on data-driven decision support. In this paper, we study the data quality of electricity consumption data in a smart grid environment. First, we analyze the significance of data quality. Also, the definition and classification of data quality issues are explained. Then we analyze the data quality of electricity consumption data and introduce the characteristics of electricity consumption data in a smart grid environment. The data quality issues of electricity consumption data are divided into three types, namely noise data, incomplete data and outlier data. We make a detailed discussion on these three types of data quality issues. In view of that outlier data is one of the most prominent issues in electricity consumption data, so we mainly focus on the outlier detection of electricity consumption data. This paper introduces the causes of electricity consumption outlier data and illustrates the significance of the electricity consumption outlier data from the negative and positive aspects respectively. Finally, the focus of this paper is to provide a review on the detection methods of electricity consumption outlier data. The methods are mainly divided into two categories, namely the data mining-based and the state estimation-based methods.",2017,,https://doi.org/10.1016/j.rser.2016.10.054
841,Zhong Wang and Qian Yu,Privacy trust crisis of personal data in China in the era of Big Data: The survey and countermeasures,"Personal data, Privacy trust, Questionnaires, Interview, Big data","Privacy trust directly affects the personal willingness to share data and thus influences the quality and size of the data, thus affecting the development of big data technology and industry. As China is probably the largest personal data pool and vastest application market of big data, the situation of Chinese privacy trust plays a significant role. Based on the 17 most common data collection scenarios, the following aspects have been observed through 508 questionnaires and interviews of 20 samples. To start with, there is a severe privacy trust crisis in China, both in the field of enterprise services such as online shopping and social networks, etc. and in some public services like medical care and education, etc. Besides, there are also doubts about data collected by the government since individuals refuse to offer personal information or give false information as much as possible. Some people even buy two phone numbers, one is in use, while the other is not carried around or used by them, which is only bought to be offered to data collectors. Secondly, in terms of gender, females have lower trust in enterprises and social associations than males, especially in the fields of social networks and personal consumption. However, there is no obvious difference in fields of government and public services. Females possess stronger awareness but less skilled in precautions than males. Thirdly, people between the ages of 18 and 50 are more suspicious of data collected by enterprises, while age exerts little obvious influence on the credibility of data collected by the government, social associations and public services. Older people are less aware of precautions than people at other ages. In addition, from the perspective of education background, people with higher degrees possess stronger awareness of precautions and thus lower degree of trust. Therefore, it is suggested that more education on privacy consciousness should be given, and relative laws as well as regulations need improving. Besides, innovation in privacy protection technologies should be encouraged. What is more, we need to reinforce the management of the internet industry and strictly regulate personal data collection of the government.",2015,,https://doi.org/10.1016/j.clsr.2015.08.006
842,Richard Knepper and Matthew Standish and Matthew Link,Big Data on Ice: The Forward Observer System for In-flight Synthetic Aperture Radar Processing,"Big Data, Network Filesystems, Synthetic Aperture Radar, Ice Sheet Data","We introduce the Forward Observer system, which is designed to provide data assurance in field data acquisition while receiving significant amounts (several terabytes per flight) of Synthetic Aperture Radar data during flights over the polar regions, which provide unique requirements for developing data collection and processing systems. Under polar conditions in the field and given the difficulty and expense of collecting data, data retention is absolutely critical. Our system provides a storage and analysis cluster with software that connects to field instruments via standard protocols, replicates data to multiple stores automatically as soon as it is written, and provides pre-processing of data so that initial visualizations are available immediately after collection, where they can provide feedback to researchers in the aircraft during the flight.",2015,,https://doi.org/10.1016/j.procs.2015.05.340
843,Rui Zhao and Yiyun Liu and Ning Zhang and Tao Huang,An optimization model for green supply chain management by using a big data analytic approach,"Hazardous materials, Inherent risk, Carbon emissions, Multi-objective optimization, Green supply chain management, Big data analysis","This paper presents a multi-objective optimization model for a green supply chain management scheme that minimizes the inherent risk occurred by hazardous materials, associated carbon emission and economic cost. The model related parameters are capitalized on a big data analysis. Three scenarios are proposed to improve green supply chain management. The first scenario divides optimization into three options: the first involves minimizing risk and then dealing with carbon emissions (and thus economic cost); the second minimizes both risk and carbon emissions first, with the ultimate goal of minimizing overall cost; and the third option attempts to minimize risk, carbon emissions, and economic cost simultaneously. This paper provides a case study to verify the optimization model. Finally, the limitations of this research and approach are discussed to lay a foundation for further improvement.",2017,,https://doi.org/10.1016/j.jclepro.2016.03.006
844,Krish Krishnan,Chapter 10 - Integration of Big Data and Data Warehousing,"Big Data, Big Data appliances, Hadoop, NoSQL, RDBMS, data virtualization, semantic framework","The focus of this chapter is to discuss the integration of Big Data and the data warehouse, the possible techniques and pitfalls, and where we leverage a technology. How do we deal with complexity and heterogeneity of technologies? What are the performance and scalabilities of each technology, and how can we sustain performance for the new environment?",2013,,https://doi.org/10.1016/B978-0-12-405891-0.00010-6
845,Benjamin T. Hazen and Joseph B. Skipper and Jeremy D. Ezell and Christopher A. Boone,Big data and predictive analytics for supply chain sustainability: A theory-driven research agenda,"Big data, Predictive analytics, Supply chain management","Big data and predictive analytics (BDPA) tools and methodologies are leveraged by businesses in many ways to improve operational and strategic capabilities, and ultimately, to positively impact corporate financial performance. BDPA has become crucial for managing supply chain functions, where data intensive processes can be vastly improved through its effective use. BDPA has also become a competitive necessity for the management of supply chains, with practitioners and scholars focused almost entirely on how BDPA is used to increase economic measures of performance. There is limited understanding, however, as to how BDPA can impact other aspects of the triple bottom-line, namely environmental and social sustainability outcomes. Indeed, this area is in immediate need of attention from scholars in many fields including industrial engineering, supply chain management, information systems, business analytics, as well as other business and engineering disciplines. The purpose of this article is to motivate such research by proposing an agenda based in well-established theory. This article reviews eight theories that can be used by researchers to examine and clarify the nature of BDPA’s impact on supply chain sustainability, and presents research questions based upon this review. Scholars can leverage this article as the basis for future research activity, and practitioners can use this article as a means to understand how company-wide BDPA initiatives might impact measures of supply chain sustainability.",2016,,https://doi.org/10.1016/j.cie.2016.06.030
846,Timothy M.D. Ebbels and Jake T.M. Pearce and Noureddin Sadawi and Jianliang Gao and Robert C. Glen,Chapter 11 - Big Data and Databases for Metabolic Phenotyping,"Metabolomics, Metabonomics, Metabolic phenotyping, Big data, Cloud computing, High-performance computing, Software tools, Databases, PhenoMeNal, Ethical, Legal, Social implications, ELSI","Metabolic phenotyping is entering the era of Big Data, leading to new opportunities and challenges. Cloud computing has been proposed as a novel paradigm, but as yet is not widely understood or used. In this chapter we introduce the concepts of Big Data and cloud computing, and discuss how they might change the landscape of metabolic phenotyping and analysis. We highlight some of the reasons for the increase in data size and explain advantages and disadvantages of large-scale computing in this context. We illustrate the area with a survey of software tools and databases currently available, and describe the newly developed cloud infrastructure “PhenoMeNal,” which will enable widespread use of these approaches. We conclude the chapter with a discussion of the important ethical, legal, and social implications (ELSI) of large-scale computing in this rapidly developing field.",2019,,https://doi.org/10.1016/B978-0-12-812293-8.00011-6
847,Uthayasankar Sivarajah and Muhammad Mustafa Kamal and Zahir Irani and Vishanth Weerakkody,Critical analysis of Big Data challenges and analytical methods,"Big Data, Big Data Analytics, Challenges, Methods, Systematic literature review","Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized/proposed/employed by organizations to help others understand this landscape with the objective of making robust investment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 – What are the different types of BD challenges theorized/proposed/confronted by organizations? and Q2 – What are the different types of BDA methods theorized/proposed/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implications and potential further research avenues to support the academic community in exploring research themes/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.",2017,,https://doi.org/10.1016/j.jbusres.2016.08.001
848,Ralph Hughes,Chapter 13 - Surface Solutions Using Data Virtualization and Big Data,"Agile enterprise data warehousing, surface solutions, backfilling the architecture, shadow IT, data virtualization, big data, Hadoop, HDFS, Map/Reduce, Hive","Without investing in exotic data modeling techniques, EDW teams can achieve fast delivery using “surface solutions.” Surface solutions allow developers to first solve business problems with data taken from landing areas and then steadily “backfill” the DW/BI reference architecture to provide progressively more complete and robust solutions. Teams can create surface solutions by leveraging shadow IT, using data virtualization, and tapping a big data platform. When leveraging shadow IT, the EDW team delivers progressively richer data sets to departmental staff members, who build their own temporary BI solutions using that information. The data virtualization strategy relies on a “superoptimizer” that can create views across databases and data types, even including semistructured data as needed. The big data strategy employs a new category of products such as Hadoop’s HDFS, MapReduce, and Hive to provide access to new data, whether it be very large, poorly structured, and/or just unfamiliar to IT and the business users.",2016,,https://doi.org/10.1016/B978-0-12-396464-9.00013-8
849,Marius Fieschi,"16 - Data for Epidemiology and Public Health, and Big Data11The questions posed by data processing for epidemiology and public health are often similar to those discussed in the chapters on clinical research (Chapter 18) and bioinformatics data (Chapter 17). For the sake of clarity, we address these questions in different chapters, although the problems are of the same nature and the solutions are isomorphic. In order to avoid too much repetition, the issue of big data is discussed here without going into the content of the other chapters.","Data processing, Data-sharing, e-health, Epidemiology, Health security, Monitoring systems, Preventive action, Public health, SurSaUD system","Abstract:
The approaches used by epidemiologists are diverse: they range from “field studies” for modeling and healthcare monitoring, to methods developed for researching and combating the emergence of diseases. Their analytical tools focus on the bio-statistics used as a tool to objectify phenomena studied in well-defined populations.",2018,,https://doi.org/10.1016/B978-1-78548-287-8.50016-X
850,Carol Isaacson Barash and Keith O. Elliston and W. {Andrew Faucett} and Jonathan Hirsch and Gauri Naik and Alice Rathjen and Grant Wood,Harnessing big data for precision medicine: A panel of experts elucidates the data challenges and proposes key strategic decisions points,,"A group of disparate translational bioinformatics experts convened at the 6th Annual Precision Medicine Partnership Meeting, October 29–30, 2014 to discuss big data challenges and key strategic decisions needed to advance precision medicine, emerging solutions, and the anticipated path to success. This article reports the panel discussion.",2015,,https://doi.org/10.1016/j.atg.2015.02.002
851,Ingunn Björnsdottir and Guri Birgitte Verne,Exhibiting caution with use of big data: The case of amphetamine in Iceland's prescription registry,,"Background
Data from large electronic databases are increasingly used in epidemiological research, but golden standards for database validation remain elusive. The Prescription Registry (IPR) and the National Health Service (NHS) databases in Iceland have not undergone formal validation, and gross errors have repeatedly been found in Icelandic statistics on pharmaceuticals. In 2015, new amphetamine tablets entered the Icelandic market, but were withdrawn half a year later due to being substandard. Return of unused stocks provided knowledge of the exact number of tablets used and hence a case where quality of the data could be assessed.
Objective
A case study of the quality of statistics in a national database on pharmaceuticals.
Methods
Data on the sales of the substandard amphetamine were obtained from the Prescription Registry and the pharmaceuticals statistics database. Upon the revelation of discrepancies, explanations were sought from the respective institutions, the producer, and dose dispensing companies.
Results
The substandard amphetamine was available from 1.9.2015 until 15.3.2016. According to NHS, 73990 tablets were sold to consumers in that period, whereas IPR initially stated 82860 tablets to have been sold, correcting to 74796 upon being notified about errors. The producer stated 72811 tablets to have been sold, and agreed with the dose dispensing companies on sales to those. The producer’s numbers were confirmed by the Medicines Agency.
Conclusion
Over-registration in the IPR was 13.8% before correction, 2.7% after correction, and 1.6% in the NHS. This case provided a unique opportunity for external validation of sales data for pharmaceuticals in Iceland, revealing enormous quality problems. The case has implications regarding database integrity beyond Iceland.",2018,,https://doi.org/10.1016/j.sapharm.2018.02.009
852,Antonella D. Pontoriero and Giovanna Nordio and Rubaida Easmin and Alessio Giacomel and Barbara Santangelo and Sameer Jahuar and Ilaria Bonoldi and Maria Rogdaki and Federico Turkheimer and Oliver Howes and Mattia Veronese,Automated Data Quality Control in FDOPA brain PET Imaging using Deep Learning,"FDOPA, PET, quality control, QC, convolutional neural networks","ABSTRACT
Introduction. With biomedical imaging research increasingly using large datasets, it becomes critical to find operator-free methods to quality control the data collected and the associated analysis. Attempts to use artificial intelligence (AI) to perform automated quality control (QC) for both single-site and multi-site datasets have been explored in some neuroimaging techniques (e.g. EEG or MRI), although these methods struggle to find replication in other domains. The aim of this study is to test the feasibility of an automated QC pipeline for brain [18F]-FDOPA PET imaging as a biomarker for the dopamine system. Methods. Two different Convolutional Neural Networks (CNNs) were used and combined to assess spatial misalignment to a standard template and the signal-to-noise ratio (SNR) relative to 200 static [18F]-FDOPA PET images that had been manually quality controlled from three different PET/CT scanners. The scans were combined with an additional 400 scans, in which misalignment (200 scans) and low SNR (200 scans) were simulated. A cross-validation was performed, where 80% of the data were used for training and 20% for validation. Two additional datasets of [18F]-FDOPA PET images (50 and 100 scans respectively with at least 80% of good quality images) were used for out-of-sample validation. Results. The CNN performance was excellent in the training dataset (accuracy for motion: 0.86 ± 0.01, accuracy for SNR: 0.69 ± 0.01), leading to 100% accurate QC classification when applied to the two out-of-sample datasets. Data dimensionality reduction affected the generalizability of the CNNs, especially when the classifiers were applied to the out-of-sample data from 3D to 1D datasets. Conclusions. This feasibility study shows that it is possible to perform automatic QC of [18F]-FDOPA PET imaging with CNNs. The approach has the potential to be extended to other PET tracers in both brain and non-brain applications, but it is dependent on the availability of large datasets necessary for the algorithm training.",2021,,https://doi.org/10.1016/j.cmpb.2021.106239
853,Oscar Romero and Victor Herrero and Alberto Abelló and Jaume Ferrarons,Tuning small analytics on Big Data: Data partitioning and secondary indexes in the Hadoop ecosystem,"Big Data, OLAP, Multidimensional model, Indexes, Partitioning, Cost estimation","In the recent years the problems of using generic storage (i.e., relational) techniques for very specific applications have been detected and outlined and, as a consequence, some alternatives to Relational DBMSs (e.g., HBase) have bloomed. Most of these alternatives sit on the cloud and benefit from cloud computing, which is nowadays a reality that helps us to save money by eliminating the hardware as well as software fixed costs and just pay per use. On top of this, specific querying frameworks to exploit the brute force in the cloud (e.g., MapReduce) have also been devised. The question arising next tries to clear out if this (rather naive) exploitation of the cloud is an alternative to tuning DBMSs or it still makes sense to consider other options when retrieving data from these settings. In this paper, we study the feasibility of solving OLAP queries with Hadoop (the Apache project implementing MapReduce) while benefiting from secondary indexes and partitioning in HBase. Our main contribution is the comparison of different access plans and the definition of criteria (i.e., cost estimation) to choose among them in terms of consumed resources (namely CPU, bandwidth and I/O).",2015,,https://doi.org/10.1016/j.is.2014.09.005
854,Holger Maier and Stefanie Leuchtenberger and Helmut Fuchs and Valerie Gailus-Durner and Martin {Hrabe de Angelis},Big data in large-scale systemic mouse phenotyping,,"Systemic phenotyping of mutant mice has been established at large scale in the last decade as a new tool to uncover the relations between genotype, phenotype and environment. Recent advances in that field led to the generation of a valuable open access data resource that can be used to better understanding the underlying causes for human diseases. From an ethical perspective, systemic phenotyping significantly contributes to the reduction of experimental animals and the refinement of animal experiments by enforcing standardisation efforts. There are particular logistical, experimental and analytical challenges of systemic large-scale mouse phenotyping. On all levels, IT solutions are critical to implement and efficiently support breeding, phenotyping and data analysis processes that lead to the generation of high-quality systemic phenotyping data accessible for the scientific community.",2017,,https://doi.org/10.1016/j.coisb.2017.07.012
855,Niki Sadat Abbasian and Afshin Salajegheh and Henrique Gaspar and Per Olaf Brett,Improving early OSV design robustness by applying ‘Multivariate Big Data Analytics’ on a ship's life cycle,"External data, Internal data, Abnormality, Missing data, Outliers, Randomness, Multivariate analysis, Data integration, Clustering","Typically, only a smaller portion of the monitorable operational data (e.g. from sensors and environment) from Offshore Support Vessels (OSVs) are used at present. Operational data, in addition to equipment performance data, design and construction data, creates large volumes of data with high veracity and variety. In most cases, such data richness is not well understood as to how to utilize it better during design and operation. It is, very often, too time consuming and resource demanding to estimate the final operational performance of vessel concept design solution in early design by applying simulations and model tests. This paper argues that there is a significant potential to integrate ship lifecycle data from different phases of its operation in large data repository for deliberate aims and evaluations. It is disputed discretely in the paper, evaluating performance of real similar type vessels during early stages of the design process, helps substantially improving and fine-tuning the performance criterion of the next generations of vessel design solutions. Producing learning from such a ship lifecycle data repository to find useful patterns and relationships among design parameters and existing fleet real performance data, requires the implementation of modern data mining techniques, such as big data and clustering concepts, which are introduced and applied in this paper. The analytics model introduced suggests and reviews all relevant steps of data knowledge discovery, including pre-processing (integration, feature selection and cleaning), processing (data analyzing) and post processing (evaluating and validating results) in this context.",2018,,https://doi.org/10.1016/j.jii.2018.02.002
856,Christel Daniel and Patricia Serre and Nina Orlova and Stéphane Bréant and Nicolas Paris and Nicolas Griffon,Initializing a hospital-wide data quality program. The AP-HP experience.,"Data accuracy, Data quality, Electronic health records, Data warehousing, Observational Studies as Topic","Background and Objectives
Data Quality (DQ) programs are recognized as a critical aspect of new-generation research platforms using electronic health record (EHR) data for building Learning Healthcare Systems. The AP-HP Clinical Data Repository aggregates EHR data from 37 hospitals to enable large-scale research and secondary data analysis. This paper describes the DQ program currently in place at AP-HP and the lessons learned from two DQ campaigns initiated in 2017.
Materials and Methods
As part of the AP-HP DQ program, two domains - patient identification (PI) and healthcare services (HS) - were selected for conducting DQ campaigns consisting of 5 phases: defining the scope, measuring, analyzing, improving and controlling DQ. Semi-automated DQ profiling was conducted in two data sets – the PI data set containing 8.8 M patients and the HS data set containing 13,099 consultation agendas and 2122 care units. Seventeen DQ measures were defined and DQ issues were classified using a unified DQ reporting framework. For each domain, actions plans were defined for improving and monitoring prioritized DQ issues.
Results
Eleven identified DQ issues (8 for the PI data set and 3 for the HS data set) were categorized into completeness (n = 6), conformance (n = 3) and plausibility (n = 2) DQ issues. DQ issues were caused by errors from data originators, ETL issues or limitations of the EHR data entry tool. The action plans included sixteen actions (9 for the PI domain and 7 for the HS domain). Though only partial implementation, the DQ campaigns already resulted in significant improvement of DQ measures.
Conclusion
DQ assessments of hospital information systems are largely unpublished. The preliminary results of two DQ campaigns conducted at AP-HP illustrate the benefit of the engagement into a DQ program. The adoption of a unified DQ reporting framework enables the communication of DQ findings in a well-defined manner with a shared vocabulary. Dedicated tooling is needed to automate and extend the scope of the generic DQ program. Specific DQ checks will be additionally defined on a per-study basis to evaluate whether EHR data fits for specific uses.",2019,,https://doi.org/10.1016/j.cmpb.2018.10.016
857,Krish Krishnan,Chapter 11 - Data-Driven Architecture for Big Data,"metadata, master data, machine learning, algorithms, semantic libraries, data governance",The goal of this chapter is to provide readers with data governance in the age of Big Data. We will discuss the goals of what managing data means with respect to the next generation of data warehousing and the role of metadata and master data in integrating Big Data into the data warehouse.,2013,,https://doi.org/10.1016/B978-0-12-405891-0.00011-8
858,John R. Talburt and Yinle Zhou,Chapter 1 - The Value Proposition for MDM and Big Data,"Master data, master data management, MDM, Big Data, reference data management, RDM",This chapter gives a definition of master data management (MDM) and describes how it generates value for organizations. It also provides an overview of Big Data and the challenges it brings to MDM.,2015,,https://doi.org/10.1016/B978-0-12-800537-8.00001-6
859,Bong-Chul Seo and Munsung Keem and Raymond Hammond and Ibrahim Demir and Witold F. Krajewski,A pilot infrastructure for searching rainfall metadata and generating rainfall product using the big data of NEXRAD,"NEXRAD, Rainfall, Cloud computing, Level II data, Hydrology","The Iowa Flood Center (IFC) developed a pilot infrastructure to explore rainfall metadata (descriptive statistics) and generate rainfall products over the Iowa domain based on the NEXRAD Level II data directly accessible through cloud storage (e.g., Amazon Web Services). Known as IFC-Cloud-NEXRAD, it resembles the Hydro-NEXRAD portal that provided researchers with ready access to NEXRAD radar data. Taking advantage of the cloud storage benefits (unlimited storage and instant access), IFC-Cloud-NEXRAD reduces the common challenges of most data exploration systems, which often lead to massive data acquisition/ingestion and rapid filling of limited system storage. Its map-based interface allows researchers to select a space-time domain of interest, retrieve and visualize pre-calculated rainfall metadata, and generate radar-derived rainfall products. Because the system provides generalized approaches to compute metadata and process data for rainfall estimation, the framework presented in this study would be readily transferrable to other geographic regions and larger scale applications.",2019,,https://doi.org/10.1016/j.envsoft.2019.03.008
860,Roberto {Álvarez Sánchez} and Andoni {Beristain Iraola} and Gorka {Epelde Unanue} and Paul Carlin,"TAQIH, a tool for tabular data quality assessment and improvement in the context of health data","Data quality, Exploratory data analysis, Data pre-processing","Background and Objectives
Data curation is a tedious task but of paramount relevance for data analytics and more specially in the health context where data-driven decisions must be extremely accurate. The ambition of TAQIH is to support non-technical users on 1) the exploratory data analysis (EDA) process of tabular health data, and 2) the assessment and improvement of its quality.
Methods
A web-based tool has been implemented with a simple yet powerful visual interface. First, it provides interfaces to understand the dataset, to gain the understanding of the content, structure and distribution. Then, it provides data visualization and improvement utilities for the data quality dimensions of completeness, accuracy, redundancy and readability.
Results
It has been applied in two different scenarios. (1) The Northern Ireland General Practitioners (GPs) Prescription Data, an open data set containing drug prescriptions. (2) A glucose monitoring tele health system dataset. Findings on (1) include: Features that had significant amount of missing values (e.g. AMP_NM variable 53.39%); instances that have high percentage of variable values missing (e.g. 0.21% of the instances with > 75% of missing values); highly correlated variables (e.g. Gross and Actual cost almost completely correlated (∼ + 1.0)). Findings on (2) include: Features that had significant amount of missing values (e.g. patient height, weight and body mass index (BMI) (> 70%), date of diagnosis 13%)); highly correlated variables (e.g. height, weight and BMI). Full detail of the testing and insights related to findings are reported.
Conclusions
TAQIH enables and supports users to carry out EDA on tabular health data and to assess and improve its quality. Having the layout of the application menu arranged sequentially as the conventional EDA pipeline helps following a consistent analysis process. The general description of the dataset and features section is very useful for the first overview of the dataset. The missing value heatmap is also very helpful in visually identifying correlations among missing values. The correlations section has proved to be supportive as a preliminary step before further data analysis pipelines, as well as the outliers section. Finally, the data quality section provides a quantitative value to the dataset improvements.",2019,,https://doi.org/10.1016/j.cmpb.2018.12.029
861,Neil T. Bendle and Xin (Shane) Wang,Uncovering the message from the mess of big data,"Big data, User-Generated content, Latent Dirichlet Allocation, Topic modeling, Market research, Qualitative data","User-generated content, such as online product reviews, is a valuable source of consumer insight. Such unstructured big data is generated in real-time, is easily accessed, and contains messages consumers want managers to hear. Analyzing such data has potential to revolutionize market research and competitive analysis, but how can the messages be extracted? How can the vast amount of data be condensed into insights to help steer businesses’ strategy? We describe a non-proprietary technique that can be applied by anyone with statistical training. Latent Dirichlet Allocation (LDA) can analyze huge amounts of text and describe the content as focusing on unseen attributes in a specific weighting. For example, a review of a graphic novel might be analyzed to focus 70% on the storyline and 30% on the graphics. Aggregating the content from numerous consumers allows us to understand what is, collectively, on consumers’ minds, and from this we can infer what consumers care about. We can even highlight which attributes are seen positively or negatively. The value of this technique extends well beyond the CMO's office as LDA can map the relative strategic positions of competitors where they matter most: in the minds of consumers.",2016,,https://doi.org/10.1016/j.bushor.2015.10.001
862,Thilo Schuler and John Kipritidis and Thomas Eade and George Hruby and Andrew Kneebone and Mario Perez and Kylie Grimberg and Kylie Richardson and Sally Evill and Brooke Evans and Blanca Gallego,Big Data Readiness in Radiation Oncology: An Efficient Approach for Relabeling Radiation Therapy Structures With Their TG-263 Standard Name in Real-World Data Sets,,"Purpose
To prepare for big data analyses on radiation therapy data, we developed Stature, a tool-supported approach for standardization of structure names in existing radiation therapy plans. We applied the widely endorsed nomenclature standard TG-263 as the mapping target and quantified the structure name inconsistency in 2 real-world data sets.
Methods and Materials
The clinically relevant structures in the radiation therapy plans were identified by reference to randomized controlled trials. The Stature approach was used by clinicians to identify the synonyms for each relevant structure, which was then mapped to the corresponding TG-263 name. We applied Stature to standardize the structure names for 654 patients with prostate cancer (PCa) and 224 patients with head and neck squamous cell carcinoma (HNSCC) who received curative radiation therapy at our institution between 2007 and 2017. The accuracy of the Stature process was manually validated in a random sample from each cohort. For the HNSCC cohort we measured the resource requirements for Stature, and for the PCa cohort we demonstrated its impact on an example clinical analytics scenario.
Results
All but 1 synonym group (“Hydrogel”) was mapped to the corresponding TG-263 name, resulting in a TG-263 relabel rate of 99% (8837 of 8925 structures). For the PCa cohort, Stature matched a total of 5969 structures. Of these, 5682 structures were exact matches (ie, following local naming convention), 284 were matched via a synonym, and 3 required manual matching. This original radiation therapy structure names therefore had a naming inconsistency rate of 4.81%. For the HNSCC cohort, Stature mapped a total of 2956 structures (2638 exact, 304 synonym, 14 manual; 10.76% inconsistency rate) and required 7.5 clinician hours. The clinician hours required were one-fifth of those that would be required for manual relabeling. The accuracy of Stature was 99.97% (PCa) and 99.61% (HNSCC).
Conclusions
The Stature approach was highly accurate and had significant resource efficiencies compared with manual curation.",2019,,https://doi.org/10.1016/j.adro.2018.09.013
863,Shixia Liu and Gennady Andrienko and Yingcai Wu and Nan Cao and Liu Jiang and Conglei Shi and Yu-Shuen Wang and Seokhee Hong,Steering data quality with visual analytics: The complexity challenge,"Data quality management, Visual analytics, Data cleansing","Data quality management, especially data cleansing, has been extensively studied for many years in the areas of data management and visual analytics. In the paper, we first review and explore the relevant work from the research areas of data management, visual analytics and human-computer interaction. Then for different types of data such as multimedia data, textual data, trajectory data, and graph data, we summarize the common methods for improving data quality by leveraging data cleansing techniques at different analysis stages. Based on a thorough analysis, we propose a general visual analytics framework for interactively cleansing data. Finally, the challenges and opportunities are analyzed and discussed in the context of data and humans.",2018,,https://doi.org/10.1016/j.visinf.2018.12.001
864,M.A. Mayer and L. Fernández-Luque and A. Leis,Chapter 5 - Big Data For Health Through Social Media,"Big Data, social media, data analysis, public health, Internet","Social Media (SM) can be a complementary channel of information to other official means for the health data collection such as the epidemiological surveillance activities and control carried out by health authorities. For this reason, more and more organizations, professionals, and scientific institutions are seeing the need to make the most of resources of health information based on SM platforms through the use of Big Data tools and analytics. Although there is a consensus on the potential benefits and opportunities that SM may provide when used for healthcare purposes, its use has brought unsuspected drawbacks and challenges related to the protection of personal data, it is essential to promote a wide reflection and that the authorities and governments establish, in collaboration with patients associations and professional institutions, specific ethical, legal guidelines, and use policies to the benefit of the current and future healthcare professional–patient relationship and general public.",2016,,https://doi.org/10.1016/B978-0-12-809269-9.00005-0
865,Daniel Drewer and Vesela Miladinova,The BIG DATA Challenge: Impact and opportunity of large quantities of information under the Europol Regulation,"Europol, Big data, Privacy, Data protection, Data protection impact assessment, Risk assessment, Privacy by design, Advanced technologies, Europol Regulation, Integrated Data Management Concept (IDMC)","In the digital age, the interaction between privacy, data protection and advanced technological developments such as big data analytics has become pertinent to Europol's effectiveness in providing accurate crime analyses. For the purposes of preventing and combating crime falling within the scope of its objectives, it is imperative for Europol to employ the fullest and most up-to-date information and technical capabilities possible whilst respecting fundamental human rights. The present article addresses precisely the “paradox” of on one side protecting fundamental human rights against external terrorist and/or cybercrime intrusions, and on the other providing a privacy-conscious approach to data collection and analytics, so that Europol can even more effectively support and strengthen action in protecting society against internal threats in a proportionate, responsible and legitimate manner. The advantage proposed in this very context of large quantities of data informing strategic analysis at Europol is a purpose-oriented data protection impact assessment. Namely, the evolution from traditional instruments in the fight against organised crime and terrorism to more technologically advanced ones equally requires an alteration of the conventional notions of privacy and investigative and information-sharing methods.",2017,,https://doi.org/10.1016/j.clsr.2017.03.006
866,Lisa C. Günther and Eduardo Colangelo and Hans-Hermann Wiendahl and Christian Bauer,Data quality assessment for improved decision-making: a methodology for small and medium-sized enterprises,"Data quality assessment, Data quality control, Information quality, Benchmarking, Production planning, control","Industrial enterprises rely on prediction of market behavior, monitoring of performance measures, evaluation of production processes and other data analyses to support strategic and operational decisions. However, although an adequate data quality (DQ) is essential for any data analysis and several methodologies for DQ assessment exist, not all organizations consider DQ in decision-making processes. E.g., inaccurate and delayed data acquisition leads to imprecise master data and poor knowledge of machine utilization. While these aspects should influence production planning and control, current approaches to data evaluation are too complex to use them on a-day-to-day basis. In this paper, we propose a methodology that simplifies the execution of DQ evaluations and improves the understandability of its results. One of its main concerns is to make DQ assessment usable to small and medium-sized enterprises (SME). The approach takes selected, context related structured or semi-structured data as input and uses a set of generic test criteria applicable to different tasks and domains. It combines data and domain driven aspects and can be partly executed automated and without context specific domain knowledge. The results of the assessment can be summarized into quality dimensions and used for benchmarking. The methodology is validated using data from the enterprise resource planning (ERP) and manufacturing execution system (MES) of a sheet metal manufacturer covering a year of time. The particular application aims at calculating logistic key performance indicators. Based on these conditions, data requirements are defined and the available data is evaluated considering domain specific characteristics.",2019,,https://doi.org/10.1016/j.promfg.2019.02.114
867,Sonal K. Phan and Cathy Chen,9 - Big Data and Monitoring the Grid,"Big data, power quality disturbance detection, intrusion detection, islanding detection, feature extraction, classification, data analytics, forecasting, visualization, smart meters, demand response","A traditional power grid, also known as the legacy grid, collects data at a few locations on the grid to monitor grid performance and forecast energy requirements on a macro level. A smart grid is the next generation of the electric power grid; it includes technologies for real-time data acquisition from various sections of the grid and provides a means for two-way communication between energy suppliers and consumers. Compared to a legacy grid, the smart grid generates large volumes of data that can be exploited for power quality event monitoring, intrusion detection, islanding detection, price forecasting, and energy forecasting at a much more granular level. These large volumes of data have to be analyzed in real-time and with high accuracy in order assist in decision making for power system operations and optimal power flow. This poses a big data challenge, which to be implemented successfully requires changes in infrastructure and data analysis methods. This chapter describes the smart grid and its associated big data and discusses methods for informative feature extraction from raw data, event monitoring, and energy consumption forecasting using these features and visualization methods to assist with data interpretation and decision making.",2017,,https://doi.org/10.1016/B978-0-12-805321-8.00009-4
868,Jean-Emmanuel Bibault and Philippe Giraud and Anita Burgun,Big Data and machine learning in radiation oncology: State of the art and future prospects,"Radiation oncology, Big Data, Predictive model, Machine learning","Precision medicine relies on an increasing amount of heterogeneous data. Advances in radiation oncology, through the use of CT Scan, dosimetry and imaging performed before each fraction, have generated a considerable flow of data that needs to be integrated. In the same time, Electronic Health Records now provide phenotypic profiles of large cohorts of patients that could be correlated to this information. In this review, we describe methods that could be used to create integrative predictive models in radiation oncology. Potential uses of machine learning methods such as support vector machine, artificial neural networks, and deep learning are also discussed.",2016,,https://doi.org/10.1016/j.canlet.2016.05.033
869,Yimeng Song and Bo Huang and Qingqing He and Bin Chen and Jing Wei and Rashed Mahmood,Dynamic assessment of PM2.5 exposure and health risk using remote sensing and geo-spatial big data,"Human mobility, Spatiotemporal heterogeneity, Remote sensing, Big data, Environmental health","In the past few decades, extensive epidemiological studies have focused on exploring the adverse effects of PM2.5 (particulate matters with aerodynamic diameters less than 2.5 μm) on public health. However, most of them failed to consider the dynamic changes of population distribution adequately and were limited by the accuracy of PM2.5 estimations. Therefore, in this study, location-based service (LBS) data from social media and satellite-derived high-quality PM2.5 concentrations were collected to perform highly spatiotemporal exposure assessments for thirteen cities in the Beijing-Tianjin-Hebei (BTH) region, China. The city-scale exposure levels and the corresponding health outcomes were first estimated. Then the uncertainties in exposure risk assessments were quantified based on in-situ PM2.5 observations and static population data. The results showed that approximately half of the population living in the BTH region were exposed to monthly mean PM2.5 concentration greater than 80 μg/m3 in 2015, and the highest risk was observed in December. In terms of all-cause, cardiovascular, and respiratory disease, the premature deaths attributed to PM2.5 were estimated to be 138,150, 80,945, and 18,752, respectively. A comparative analysis between five different exposure models further illustrated that the dynamic population distribution and accurate PM2.5 estimations showed great influence on environmental exposure and health assessments and need be carefully considered. Otherwise, the results would be considerably over- or under-estimated.",2019,,https://doi.org/10.1016/j.envpol.2019.06.057
870,Laura Morán-Fernández and Verónica Bólon-Canedo and Amparo Alonso-Betanzos,How important is data quality? Best classifiers vs best features,"Feature selection, Filters, Preprocessing, High dimensionality, Classification, Data analysis","The task of choosing the appropriate classifier for a given scenario is not an easy-to-solve question. First, there is an increasingly high number of algorithms available belonging to different families. And also there is a lack of methodologies that can help on recommending in advance a given family of algorithms for a certain type of datasets. Besides, most of these classification algorithms exhibit a degradation in the performance when faced with datasets containing irrelevant and/or redundant features. In this work we analyze the impact of feature selection in classification over several synthetic and real datasets. The experimental results obtained show that the significance of selecting a classifier decreases after applying an appropriate preprocessing step and, not only this alleviates the choice, but it also improves the results in almost all the datasets tested.",2022,,https://doi.org/10.1016/j.neucom.2021.05.107
871,Muhammad Babar and Ataur Rahman and Fahim Arif and Gwanggil Jeon,Energy-harvesting based on internet of things and big data analytics for smart health monitoring,"Big data analytics, IoT, Energy harvesting","Current advancements and growth in the arena of the Internet of Things (IoT) is providing great potential in the novel epoch of healthcare. The future of healthcare is expansively promising, as it advances the excellence of life and health of humans, involving several health regulations. Continual increases of multifaceted IoT devices in healthcare is beset by challenges, such as powering IoT terminal nodes used for health monitoring, data processing, smart decisions, and event management. In this paper, we propose a healthcare architecture which is based on an analysis of energy harvesting for health monitoring sensors and the realization of Big Data analytics in healthcare. The rationale of the proposed architecture is two-fold: (1) comprehensive conceptual framework for energy harvesting for health monitoring sensors; and (2) data processing and decision management for healthcare. The proposed architecture is a three-layered architecture that comprises: (1) energy harvesting and data generation; (2) data pre-processing; and (3) data processing and application. The proposed scheme highlights the effectiveness of energy-harvesting based IoT in healthcare. In addition, it also proposes a solution for smart health monitoring and planning. We also utilized consistent datasets on the Hadoop server to validate the proposed architecture based on threshold limit values (TLVs). The study demonstrates that the proposed architecture offers substantial and immediate value to the field of smart health.",2018,,https://doi.org/10.1016/j.suscom.2017.10.009
872,Naresh Sundar Rajan and Ramkiran Gouripeddi and Peter Mo and Randy K. Madsen and Julio C. Facelli,Towards a content agnostic computable knowledge repository for data quality assessment,"Data Quality Metadata Repository, Knowledge representation, Data quality assessment, Data quality dimensions, Data quality framework","Background and objective
In recent years, several data quality conceptual frameworks have been proposed across the Data Quality and Information Quality domains towards assessment of quality of data. These frameworks are diverse, varying from simple lists of concepts to complex ontological and taxonomical representations of data quality concepts. The goal of this study is to design, develop and implement a platform agnostic computable data quality knowledge repository for data quality assessments.
Methods
We identified computable data quality concepts by performing a comprehensive literature review of articles indexed in three major bibliographic data sources. From this corpus, we extracted data quality concepts, their definitions, applicable measures, their computability and identified conceptual relationships. We used these relationships to design and develop a data quality meta-model and implemented it in a quality knowledge repository.
Results
We identified three primitives for programmatically performing data quality assessments: data quality concept, its definition, its measure or rule for data quality assessment, and their associations. We modeled a computable data quality meta-data repository and extended this framework to adapt, store, retrieve and automate assessment of other existing data quality assessment models.
Conclusion
We identified research gaps in data quality literature towards automating data quality assessments methods. In this process, we designed, developed and implemented a computable data quality knowledge repository for assessing quality and characterizing data in health data repositories. We leverage this knowledge repository in a service-oriented architecture to perform scalable and reproducible framework for data quality assessments in disparate biomedical data sources.",2019,,https://doi.org/10.1016/j.cmpb.2019.05.017
873,Rajiv Saran and Diane Steffick and Jennifer Bragg-Gresham,The China Kidney Disease Network (CK-NET): “Big Data—Big Dreams”,,,2017,,https://doi.org/10.1053/j.ajkd.2017.04.008
874,Konstantinos J. Chalvatzis and Hanif Malekpoor and Nishikant Mishra and Fiona Lettice and Sonal Choudhary,Sustainable resource allocation for power generation: The role of big data in enabling interindustry architectural innovation,"Energy innovation, Interindustry architectural innovation, Sustainable energy, Fuel mix, Grey TOPSIS, grey linear programming","Economic, social and environmental requirements make planning for a sustainable electricity generation mix a demanding endeavour. Technological innovation offers a range of renewable generation and energy management options which require fine tuning and accurate control to be successful, which calls for the use of large-scale, detailed datasets. In this paper, we focus on the UK and use Multi-Criteria Decision Making (MCDM) to evaluate electricity generation options against technical, environmental and social criteria. Data incompleteness and redundancy, usual in large-scale datasets, as well as expert opinion ambiguity are dealt with using a comprehensive grey TOPSIS model. We used evaluation scores to develop a multi-objective optimization model to maximize the technical, environmental and social utility of the electricity generation mix and to enable a larger role for innovative technologies. Demand uncertainty was handled with an interval range and we developed our problem with multi-objective grey linear programming (MOGLP). Solving the mathematical model provided us with the electricity generation mix for every 5 min of the period under study. Our results indicate that nuclear and renewable energy options, specifically wind, solar, and hydro, but not biomass energy, perform better against all criteria indicating that interindustry architectural innovation in the power generation mix is key to sustainable UK electricity production and supply.",2019,,https://doi.org/10.1016/j.techfore.2018.04.031
875,V.N. Gudivada and M.T. Irfan and E. Fathi and D.L. Rao,Chapter 5 - Cognitive Analytics: Going Beyond Big Data Analytics and Machine Learning,"Cognitive analytics, Text analytics, Learning analytics, Educational data mining, Cognitive systems, Cognitive computing, Personalized learning, Data science, Machine learning, Big data analytics, Business analytics","This chapter defines analytics and traces its evolution from its origin in 1988 to its current stage—cognitive analytics. We discuss types of learning and describe classes of machine learning algorithms. Given this backdrop, we propose a reference architecture for cognitive analytics and indicate ways to implement the architecture. A few cognitive analytics applications are briefly described. The chapter concludes by indicating current trends and future research direction.",2016,,https://doi.org/10.1016/bs.host.2016.07.010
876,Prasad Calyam and Anup Mishra and Ronny Bazan Antequera and Dmitrii Chemodanov and Alex Berryman and Kunpeng Zhu and Carmen Abbott and Marjorie Skubic,Synchronous Big Data analytics for personalized and remote physical therapy,"Smart health care, Personalized remote physical therapy, Synchronous Big Data, Gigabit networking app","With gigabit networking becoming economically feasible and widely installed at homes, there are new opportunities to revisit in-home, personalized telehealth services. In this paper, we describe a novel telehealth eldercare service that we developed viz., “PhysicalTherapy-as-a-Service” (PTaaS) that connects a remote physical therapist at a clinic to a senior at home. The service leverages a high-speed, low-latency network connection through an interactive interface built on top of Microsoft Kinect motion sensing capabilities. The interface that is built using user-centered design principles for wellness coaching exercises is essentially a ‘Synchronous Big Data’ application due to its: (i) high data-in-motion velocity (i.e., peak data rate is ≈400 Mbps), (ii) considerable variety (i.e., measurements include 3D sensing, network health, user opinion surveys and video clips of RGB, skeletal and depth data), and (iii) large volume (i.e., several GB of measurement data for a simple exercise activity). The successful PTaaS delivery through this interface is dependent on the veracity analytics needed for correlation of the real-time Big Data streams within a session, in order to assess exercise balance of the senior without any bias due to network quality effects. Our experiments with PTaaS in an actual testbed involving senior homes in Kansas City with Google Fiber connections and our university clinic demonstrate the network configuration and time synchronization related challenges in order to perform online analytics. Our findings provide insights on how to: (a) enable suitable resource calibration and perform network troubleshooting for high user experience for both the therapist and the senior, and (b) realize a Big Data architecture for PTaaS and other similar personalized healthcare services to be remotely delivered at a large-scale in a reliable, secure and cost-effective manner.",2016,,https://doi.org/10.1016/j.pmcj.2015.09.004
877,Obinna Anya and Hissam Tawfik,Chapter 5 - Leveraging Big Data Analytics for Personalized Elderly Care: Opportunities and Challenges,"Elderly care, personalized care, independent living, ACTVAGE, Big Data analytics, CAPIM, lifestyle-oriented, context-awareness, framework","Owing to the growing increase in the world’s ageing population, research has focused on developing information and communication technology (ICT)–based services for personalized care, improved health, and quality social life for the elderly. Recent efforts explore Big Data in order to build mathematical models of personal behavior and lifestyle for analytics. Leveraging Big Data analytics holds enormous potential for solving some of the biggest and most intractable challenges in personalized elderly care through quantified modeling of a person’s lifestyle in a way that takes cognizance of their beliefs, values, and preferences, and connects to a history of events, things, and places around which they have progressively built their lives. However, the idea of discovering patterns to personalize care and inform critical health care decisions for the elderly is challenged as data grow exponentially in volume, become faster and increasingly unstructured, and are generated from sociodigital engagements that often may not accurately reflect the real-world entities and contexts they represent. As a result, the idea raises issues along several dimensions, including social, technical, and context-aware challenges. In this chapter, we present an overview of the state of the art in personalized elderly care, and explore the opportunities and inherent sociotechnical challenges in leveraging Big Data analytics to support elderly care and independent living. Based on this discussion, and arguing that analytics need to take account of the contexts that shape the generation and use of data, ACTVAGE, a context-aware lifestyle-oriented framework for personalized elderly care and independent living is proposed.",2016,,https://doi.org/10.1016/B978-0-12-803468-2.00005-9
878,Tomer Gueta and Yohay Carmel,Quantifying the value of user-level data cleaning for big data: A case study using mammal distribution models,"Biodiversity informatics, Data-cleaning, SDM performance, MaxEnt, Australian mammals, Big-data","The recent availability of species occurrence data from numerous sources, standardized and connected within a single portal, has the potential to answer fundamental ecological questions. These aggregated big biodiversity databases are prone to numerous data errors and biases. The data-user is responsible for identifying these errors and assessing if the data are suitable for a given purpose. Complex technical skills are increasingly required for handling and cleaning biodiversity data, while biodiversity scientists possessing these skills are rare. Here, we estimate the effect of user-level data cleaning on species distribution model (SDM) performance. We implement several simple and easy-to-execute data cleaning procedures, and evaluate the change in SDM performance. Additionally, we examine if a certain group of species is more sensitive to the use of erroneous or unsuitable data. The cleaning procedures used in this research improved SDM performance significantly, across all scales and for all performance measures. The largest improvement in distribution models following data cleaning was for small mammals (1g–100g). Data cleaning at the user level is crucial when using aggregated occurrence data, and facilitating its implementation is a key factor in order to advance data-intensive biodiversity studies. Adopting a more comprehensive approach for incorporating data cleaning as part of data analysis, will not only improve the quality of biodiversity data, but will also impose a more appropriate usage of such data.",2016,,https://doi.org/10.1016/j.ecoinf.2016.06.001
879,Krish Krishnan,Chapter 5 - Big Data Driving Business Value,"sensor data, machine data, social media, compliance, safety","The first four chapters provided you an introduction to Big Data, the complexities associated with Big Data, and the processing techniques and technologies for Big Data. This chapter will focus on use cases of Big Data and how real-world companies are implementing Big Data.",2013,,https://doi.org/10.1016/B978-0-12-405891-0.00005-2
880,Dirk U. Pfeiffer and Kim B. Stevens,Spatial and temporal epidemiological analysis in the Big Data era,"Data science, Exploratory analysis, Internet of Things, Modelling, Multi-criteria decision analysis, Spatial analysis, Visualisation","Concurrent with global economic development in the last 50 years, the opportunities for the spread of existing diseases and emergence of new infectious pathogens, have increased substantially. The activities associated with the enormously intensified global connectivity have resulted in large amounts of data being generated, which in turn provides opportunities for generating knowledge that will allow more effective management of animal and human health risks. This so-called Big Data has, more recently, been accompanied by the Internet of Things which highlights the increasing presence of a wide range of sensors, interconnected via the Internet. Analysis of this data needs to exploit its complexity, accommodate variation in data quality and should take advantage of its spatial and temporal dimensions, where available. Apart from the development of hardware technologies and networking/communication infrastructure, it is necessary to develop appropriate data management tools that make this data accessible for analysis. This includes relational databases, geographical information systems and most recently, cloud-based data storage such as Hadoop distributed file systems. While the development in analytical methodologies has not quite caught up with the data deluge, important advances have been made in a number of areas, including spatial and temporal data analysis where the spectrum of analytical methods ranges from visualisation and exploratory analysis, to modelling. While there used to be a primary focus on statistical science in terms of methodological development for data analysis, the newly emerged discipline of data science is a reflection of the challenges presented by the need to integrate diverse data sources and exploit them using novel data- and knowledge-driven modelling methods while simultaneously recognising the value of quantitative as well as qualitative analytical approaches. Machine learning regression methods, which are more robust and can handle large datasets faster than classical regression approaches, are now also used to analyse spatial and spatio-temporal data. Multi-criteria decision analysis methods have gained greater acceptance, due in part, to the need to increasingly combine data from diverse sources including published scientific information and expert opinion in an attempt to fill important knowledge gaps. The opportunities for more effective prevention, detection and control of animal health threats arising from these developments are immense, but not without risks given the different types, and much higher frequency, of biases associated with these data.",2015,,https://doi.org/10.1016/j.prevetmed.2015.05.012
881,Samah Salem and Fouzia Benchikha,LODQuMa: A Free-ontology process for Linked (Open) Data quality management,"Linked Open Data, Quality assessment, Quality improvement, Synonym predicates, Profiling statistics, DBpedia","For many years, data quality is among the most commonly discussed issue in Linked Open Data (LOD) due to the huge volume of integrated datasets that are usually heterogeneous. Several ontology-based approaches dealing with quality problems have been proposed. However, when datasets lack a well-defined schema, these approaches become ineffective because of the lack of metadata. Moreover, the detection of quality problems based on an analysis between RDF (Resource Description Framework) triples without requiring ontology statistical and semantical information is not addressed. Keeping in mind that ontologies are not always available and they may be incomplete or misused. In this paper, a novel free-ontology process called LODQuMa is proposed to assess and improve the quality of LOD. It is mainly based on profiling statistics, synonym relationships between predicates, QVCs (Quality Verification Cases), and SPARQL (SPARQL Protocol and RDF Query Language) query templates. Experiments on the DBpedia dataset demonstrate that the proposed process is effective for increasing the intrinsic quality dimensions, resulting in correct and compact datasets.",2021,,https://doi.org/10.1016/j.jksuci.2021.06.001
882,Olivier Debauche and Sidi Ahmed Mahmoudi and Saïd Mahmoudi and Pierre Manneback,Cloud Platform using Big Data and HPC Technologies for Distributed and Parallels Treatments,"GPU, FPGA, MIC, CPU, TPU, Cloud, Big Data, parallel, distributed processing, heterogeneous cloud architecture","Smart agriculture is one of the most diverse research. In addition, the quantity of data to be stored and the choice of the most efficient algorithms to process are significant elements in this field. The storage of collecting data from Internet of Things (IoT), existing on distributed, local databases and open data need a particular infrastructure to federate all these data to make complex treatments. The storage of this wide range of data that comes at high frequency and variable throughput is particularly difficult. In this paper, we propose the use of distributed databases and high-performance computing architecture in order to exploit multiple re-configurable computing and application specific processing such as CPUs, GPUs, TPUs and FPGAs efficiently. This exploitation allows an accurate training for an application to machine learning, deep learning and unsupervised modeling algorithms. The last ones are used for training supervised algorithms on images when it labels a set of images and unsupervised algorithms on IoT data which are unlabeled with variable qualities. The processing of data is based on Hadoop 3.1 MapReduce to achieve parallel processing and use containerization technologies to distribute treatments on Multi GPU, MIC and FPGA. This architecture allows efficient treatments of data coming from several sources with a cloud high-performance heterogeneous architecture. The proposed 4 layers infrastructure can also implement FPGA and MIC which are now natively supported by recent version of Hadoop. Moreover, with the advent of new technologies like Intel® MovidiusTM; it is now possible to deploy CNN at the Fog level in the IoT network and to make inference with the cloud and therefore limit significantly the network traffic that result in reducing the move of large amounts of data to the cloud.",2018,,https://doi.org/10.1016/j.procs.2018.10.156
883,Tsan-Ming Choi and Suyuan Luo,"Data quality challenges for sustainable fashion supply chain operations in emerging markets: Roles of blockchain, government sponsors and environment taxes","Fashion business operations, Supply chain centralization, Emerging markets, Sustainable operations, Social welfare","In emerging markets, there are data quality problems. In this paper, we establish theoretical models to explore how data quality problems affect sustainable fashion supply chain operations. We start with the decentralized supply chain and find that poor data quality lowers supply chain profit and social welfare. We consider the implementation of blockchain to help and identify the situation in which blockchain helps enhance social welfare but brings harm to supply chain profitability. We propose a government sponsor scheme as well as an environment taxation waiving scheme to help. We further extend the study to the centralized supply chain setting.",2019,,https://doi.org/10.1016/j.tre.2019.09.019
884,Emanuele Fumeo and Luca Oneto and Davide Anguita,Condition Based Maintenance in Railway Transportation Systems Based on Big Data Streaming Analysis,"Big Data Streams, Data Analytics, Condition Based Maintenance, Intelligent Transporta- tion Systems, Online Learning, Model Selection","Streaming Data Analysis (SDA) of Big Data Streams (BDS) for Condition Based Maintenance (CBM) in the context of Rail Transportation Systems (RTS) is a state-of-the-art field of re- search. SDA of BDS is the problem of analyzing, modeling and extracting information from huge amounts of data that continuously come from several sources in real time through com- putational aware solutions. Among others, CBM for Rail Transportation is one of the most challenging SDA problems, consisting of the implementation of a predictive maintenance system for evaluating the future status of the monitored assets in order to reduce risks related to failures and to avoid service disruptions. The challenge is to collect and analyze all the data streams that come from the numerous on-board sensors monitoring the assets. This paper deals with the problem of CBM applied to the condition monitoring and predictive maintenance of train axle bearings based on sensors data collection, with the purpose of maximizing their Remaining Useful Life (RUL). In particular we propose a novel algorithm for CBM based on SDA that takes advantage of the Online Support Vector Regression (OL-SVR) for predicting the RUL. The novelty of our proposal is the heuristic approach for optimizing the trade-off between the accuracy of the OL-SVR models and the computational time and resources needed in order to build them. Results from tests on a real-world dataset show the actual benefits brought by the proposed methodology.",2015,,https://doi.org/10.1016/j.procs.2015.07.321
885,Simon Elias Bibri and John Krogstie,ICT of the new wave of computing for sustainable urban forms: Their big data and context-aware augmented typologies and design concepts,"Sustainable urban forms, Smart sustainable cities, Big data analytics, Context-aware computing, Typologies and design concepts, Technologies and applications, ICT of the new wave of computing","Undoubtedly, sustainable development has inspired a generation of scholars and practitioners in different disciplines into a quest for the immense opportunities created by the development of sustainable urban forms for human settlements that will enable built environments to function in a more constructive and efficient way. However, there are still significant challenges that need to be addressed and overcome. The issue of such forms has been problematic and difficult to deal with, particularly in relation to the evaluation and improvement of their contribution to the goals of sustainable development. As it is an urban world where the informational and physical landscapes are increasingly being merged, sustainable urban forms need to embrace and leverage what current and future ICT has to offer as innovative solutions and sophisticated methods so as to thrive—i.e. advance their contribution to sustainability. The need for ICT of the new wave of computing to be embedded in such forms is underpinned by the recognition that urban sustainability applications are deemed of high relevance to the contemporary research agenda of computing and ICT. To unlock and exploit the underlying potential, the field of sustainable urban planning is required to extend its boundaries and broaden its horizons beyond the ambit of the built form of cities to include technological innovation opportunities. This paper explores and substantiates the real potential of ICT of the new wave of computing to evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. This entails merging big data and context-aware technologies and their applications with the typologies and design concepts of sustainable urban forms to achieve multiple hitherto unrealized goals. In doing so, this paper identifies models of smart sustainable city and their technologies and applications and models of sustainable urban form and their design concepts and typologies. In addition, it addresses the question of how these technologies and applications can be amalgamated with these design concepts and typologies in ways that ultimately evaluate and improve the contribution of sustainable urban forms to the goals of sustainable development. The overall aim of this paper suits a mix of three methodologies: literature review, thematic analysis, and secondary (qualitative) data analysis to achieve different but related objectives. The study identifies four technologies and two classes of applications pertaining to models of smart sustainable city as well as three design concepts and four typologies related to models of sustainable urban form. Finally, this paper proposes a Matrix to help scholars and planners in understanding and analyzing how and to what extent the contribution of sustainable urban forms to sustainability can be improved through ICT of the new wave of computing as to the underlying novel technologies and their applications, as well as a data-centric approach into investigating and evaluating this contribution and a simulation method for strategically optimizing it.",2017,,https://doi.org/10.1016/j.scs.2017.04.012
886,Gurparkash Singh and Duane Schulthess and Nigel Hughes and Bart Vannieuwenhuyse and Dipak Kalra,Real world big data for clinical research and drug development,,"The objective of this paper is to identify the extent to which real world data (RWD) is being utilized, or could be utilized, at scale in drug development. Through screening peer-reviewed literature, we have cited specific examples where RWD can be used for biomarker discovery or validation, gaining a new understanding of a disease or disease associations, discovering new markers for patient stratification and targeted therapies, new markers for identifying persons with a disease, and pharmacovigilance. None of the papers meeting our criteria was specifically geared toward novel targets or indications in the biopharmaceutical sector; the majority were focused on the area of public health, often sponsored by universities, insurance providers or in combination with public health bodies such as national insurers. The field is still in an early phase of practical application, and is being harnessed broadly where it serves the most direct need in public health applications in early, rare and novel disease incidents. However, these exemplars provide a valuable contribution to insights on the use of RWD to create novel, faster and less invasive approaches to advance disease understanding and biomarker discovery. We believe that pharma needs to invest in making better use of Electronic Health Records and the need for more precompetitive collaboration to grow the scale of this ‘big denominator’ capability, especially given the needs of precision medicine research.",2018,,https://doi.org/10.1016/j.drudis.2017.12.002
887,David Loshin,Chapter 11 - Developing the Big Data Roadmap,"Need for big data, organizational buy-in, team building, big data evangelist, application architect, data integration, platform architect, data scientist, proof of concept, big data pilot, technology evaluation, technology selection, data management, appliance, application development, YARN, MapReduce, SDLC, training, project scoping, platform scoping, problem data size, computational complexity, storage configuration, integration plan, maintenance, management, assessment","This final chapter reviews best practices for incrementally adopting big data into the enterprise. The chapter revisits assessing the need and value of big data, organizational buy-in, building the big data team, scoping and piloting a proof of concept, technology evaluation and selection, application development, testing, and implementation, platform and project scoping, the big data integration plan, management and maintenance, assessment of success criteria, and overall summary and considerations.",2013,,https://doi.org/10.1016/B978-0-12-417319-4.00011-9
888,Weitian Tong,Chapter 5 - Machine learning for spatiotemporal big data in air pollution,"Air pollution, Fine particulate matter, Spatiotemporal interpolation, Machine learning, Deep learning","An accurate understanding of air pollutants in a continuous space-time domain is critical for meaningful assessment of the quantitative relationship between the adverse health effects and the concentrations of air pollutants. Traditional interpolation methods, including various statistic and nonstatistic regression models, typically involve restrictive assumptions regarding independence of observations and distributions of outcomes. Moreover, a set of relationships among variables need to be defined strictly in advance. Machine learning opens a new door to understand the air pollution data based on the exposing data-driven relationships and predicting outcomes without empirical models. In this chapter, the state-of-the-art machine learning methods will be introduced to unlock the full potential of the air pollutant data, that is, to estimate the PM2.5 concentration more accurately in the spatiotemporal domain. The methods can be extended to the other air pollutants.",2020,,https://doi.org/10.1016/B978-0-12-815822-7.00005-4
889,Charles S. Mayo and Marc L. Kessler and Avraham Eisbruch and Grant Weyburne and Mary Feng and James A. Hayman and Shruti Jolly and Issam {El Naqa} and Jean M. Moran and Martha M. Matuszak and Carlos J. Anderson and Lynn P. Holevinski and Daniel L. McShan and Sue M. Merkel and Sherry L. Machnak and Theodore S. Lawrence and Randall K. {Ten Haken},The big data effort in radiation oncology: Data mining or data farming?,,"Although large volumes of information are entered into our electronic health care records, radiation oncology information systems and treatment planning systems on a daily basis, the goal of extracting and using this big data has been slow to emerge. Development of strategies to meet this goal is aided by examining issues with a data farming instead of a data mining conceptualization. Using this model, a vision of key data elements, clinical process changes, technology issues and solutions, and role for professional societies is presented. With a better view of technology, process and standardization factors, definition and prioritization of efforts can be more effectively directed.",2016,,https://doi.org/10.1016/j.adro.2016.10.001
890,Thiago Graca Ramos and Jean Cristian Ferreira Machado and Bruna Principe Vieira Cordeiro,Primary Education Evaluation in Brazil Using Big Data and Cluster Analysis,"Big Data, Data Warehouse, Cluster, Education, IDEB","This study aims to understand the assessment of basic education in the perspective of the State Reviewer as a mechanism that generates information regarding the positivity and weaknesses of a school or an educational system to provide improvements. For this reason, a Data Warehouse was created and later some analysis of the indicators were performed through clustering.",2015,,https://doi.org/10.1016/j.procs.2015.07.061
891,John R. Talburt and Yinle Zhou,Chapter 10 - CSRUD for Big Data,"Big Data, Hadoop Map/Reduce, Transitive Closure, Graph Component","This chapter describes how a distributed processing environment such as Hadoop Map/Reduce can be used to support the CSRUD Life Cycle for Big Data. The examples shown in this chapter use the match key blocking described in Chapter 9 as a data partitioning strategy to perform ER on large datasets. The chapter includes an algorithm for finding the transitive closure of multiple match keys in a distributed processing environment using an iterative algorithm that minimizes the amount of local memory required for each processor. It also outlines a structure for an identity knowledge base in a distributed key-value data store, and describes strategies and distributed processing workflows for capture and update phases of the CSRUD life cycle using both record-based and attribute-based cluster-to-cluster structure projections.",2015,,https://doi.org/10.1016/B978-0-12-800537-8.00010-7
892,Roberto Corizzo and Michelangelo Ceci and Nathalie Japkowicz,Anomaly Detection and Repair for Accurate Predictions in Geo-distributed Big Data,"Anomaly detection, Data repair, Geo-distributed big data, Spatial autocorrelation, Neural networks, Gradient-boosting","The increasing presence of geo-distributed sensor networks implies the generation of huge volumes of data from multiple geographical locations at an increasing rate. This raises important issues which become more challenging when the final goal is that of the analysis of the data for forecasting purposes or, more generally, for predictive tasks. This paper proposes a framework which supports predictive modeling tasks from streaming data coming from multiple geo-referenced sensors. In particular, we propose a distance-based anomaly detection strategy which considers objects described by embedding features learned via a stacked auto-encoder. We then devise a repair strategy which repairs the data detected as anomalous exploiting non-anomalous data measured by sensors in nearby spatial locations. Subsequently, we adopt Gradient Boosted Trees (GBTs) to predict/forecast values assumed by a target variable of interest for the repaired newly arriving (unlabeled) data, using the original feature representation or the embedding feature representation learned via the stacked auto-encoder. The workflow is implemented with distributed Apache Spark programming primitives and tested on a cluster environment. We perform experiments to assess the performance of each module, separately and in a combined manner, considering the predictive modeling of one-day-ahead energy production, for multiple renewable energy sites. Accuracy results show that the proposed framework allows reducing the error up to 13.56%. Moreover, scalability results demonstrate the efficiency of the proposed framework in terms of speedup, scaleup and execution time under a stress test.",2019,,https://doi.org/10.1016/j.bdr.2019.04.001
893,Fraser Sampson,Chapter 15 - The Legal Challenges of Big Data Application in Law Enforcement,"Dilemma, Human rights, Jurisdiction, Law enforcement, Privacy, Purpose limitation","This chapter considers the specific issues that Big Data presents for law enforcement agencies (LEAs). In particular, it looks at the dilemmas created for LEAs seeking to use the advantages Big Data gives them while remaining compliant with the developing legal framework governing privacy and the protection of personal data, and how those very advantages can present challenges in law enforcement.",2015,,https://doi.org/10.1016/B978-0-12-801967-2.00015-X
894,Abhaya Kumar Sahoo and Sitikantha Mallik and Chittaranjan Pradhan and Bhabani Shankar Prasad Mishra and Rabindra Kumar Barik and Himansu Das,Chapter 9 - Intelligence-Based Health Recommendation System Using Big Data Analytics,"Big data analytics, Classification, Healthcare, Privacy preservation, Recommendation system","In today's digital world, healthcare is one of the core areas in the medical domain. A healthcare system is required to analyze a large amount of patient data, which helps to derive insights and predictions of disease. This system should be intelligent and able to predict the patient's health condition by analyzing the patient's lifestyle, physical health records, and social activities. The health recommendation system (HRS) is becoming an important platform for healthcare services. In this context, health intelligent systems have become indispensable tools in decision-making processes in the healthcare sector. The main objective is to ensure the availability of valuable information at the right time by ensuring information quality, trustworthiness, authentication, and privacy. As people use social networks to learn about their health condition, so the HRS is very important to derive outcomes such as recommending diagnosis, health insurance, clinical pathway-based treatment methods, and alternative medicines based on the patient's health profile. In this chapter, we discuss recent research that targeted utilization of large volumes of medical data while combining multimodal data from disparate sources, which reduces the workload and cost in healthcare. In the healthcare sector, big data analytics using a recommendation system has an important role in terms of decision-making processes regarding the patient's health. This chapter presents a proposed intelligent HRS that provides an insight into how to use big data analytics for implementing an effective health recommendation engine and shows how to transform the healthcare industry from the traditional scenario to more personalized paradigm in a tele-health environment. Our proposed intelligent HRS resulted in lower MAE value when compared to existing approaches.",2019,,https://doi.org/10.1016/B978-0-12-818146-1.00009-X
895,Yan Ye and Yang Zhao and Jennifer Shang and Liyi Zhang,A hybrid IT framework for identifying high-quality physicians using big data analytics,"Online healthcare communities, Physician identifying, Signaling theory, Machine learning, Topic modeling, Multi-criterion analysis","Patients face difficulties identifying appropriate doctors owing to the sizeable quantity and uneven quality of information in online healthcare communities. In studying physician searches, researchers often focus on expertise similarity matches and sentiment analyses of reviews. However, the quality is often ignored. To address patients' information needs holistically, we propose a four-dimensional IT framework based on signaling theory. The model takes expertise knowledge, online reviews, profile descriptions (e.g., hospital reputation, number of patients, city) and service quality (e.g., response speed, interaction frequency, cost) as signals that distinguish high-quality physicians. It uses machine learning approaches to derive similarity matches and sentiment analysis. It also measures the relative importance of the signals by multi-criterion analysis and derives the physician rankings through the aggregated scores. Our study revealed that the proposed approach performs better compared with the other two recommend techniques. This research expands the boundary of signaling theory to healthcare management and enriches the literature on IT use and inter-organizational systems. The proposed IT model may improve patient care, alleviate the physician-patient relationship and reduce lawsuits against hospitals; it also has practical implications for healthcare management.",2019,,https://doi.org/10.1016/j.ijinfomgt.2019.01.005
896,Krish Krishnan,Chapter 14 - Implementing the Big Data – Data Warehouse – Real-Life Situations,"Hadoop, RDBMS, NoSQL, transformation, architecture",This chapter discusses the real-life implementation of the next-generation platform by three different companies and the direction they each have chosen from a technology and architecture perspective.,2013,,https://doi.org/10.1016/B978-0-12-405891-0.00014-3
897,Ibrahim Alghamdi and Christos Anagnostopoulos and Dimitrios P. Pezaros,Data quality-aware task offloading in Mobile Edge Computing: An Optimal Stopping Theory approach,"Mobile edge computing, Tasks offloading, Data quality, Optimal stopping theory, Sequential decision making","An important use case of the Mobile Edge Computing (MEC) paradigm is task and data offloading. Computational offloading is beneficial for a wide variety of mobile applications on different platforms including autonomous vehicles and smartphones. With the envision deployment of MEC servers along the roads and while mobile nodes are moving and having certain tasks (or data) to be offloaded to edge servers, choosing an appropriate time and an ideally suited MEC server to guarantee the Quality of Service (QoS) is challenging. We tackle the data quality-aware offloading sequential decision making problem by adopting the principles of Optimal Stopping Theory (OST) to minimize the expected processing time. A variety of OST stochastic models and their applications to the offloading decision making problem are investigated and assessed. A performance evaluation is provided using simulation approach and real world data sets together with the assessment of baseline deterministic and stochastic offloading models. The results show that the proposed OST models can significantly minimize the expected processing time for analytics task execution and can be implemented in the mobile nodes efficiently.",2021,,https://doi.org/10.1016/j.future.2020.12.017
898,Shiliang Su and Zekun Li and Mengya Xu and Zhongliang Cai and Min Weng,"A geo-big data approach to intra-urban food deserts: Transit-varying accessibility, social inequalities, and implications for urban planning","Food geography, Healthy food access, Accessibility, Social inequalities, Transport mode, Multilevel regression","Urban studies attempt to identify the geographic areas with restricted access to healthy and affordable foods (defined as food deserts in the literature). While prior publications have reported the socioeconomic disparities in healthy food accessibility, little evidence has been released from developing countries, especially in China. This paper proposes a geo-big data approach to measuring transit-varying healthy food accessibility and applies it to identify the food deserts within Shenzhen, China. In particular, we develop a crawling tool to harvest the daily travel time from each community (8117) to each healthy food store (102) from the Baidu Map under four transport modes (walking, public transit, private car, and bicycle) during 17:30–20:30 in June 2016. Based on the travel time calculations, we develop four travel time indicators to measure the healthy food accessibility: the minimum, the maximum, the weighted average, and the standard deviation. Results show that the four accessibility indicators generate different estimations and the nearest service (minimum time) alone fails to reflect the multidimensional nature of healthy food accessibility. The communities within Shenzhen present quite different typology with respect to healthy food accessibility under different transport modes. Multilevel additive regression is further applied to examine the associations between healthy food accessibility and nested socioeconomic characteristics at two geographic levels (community and district). We discover that the associations are divergent with transport modes and with geographic levels. More specifically, significant social equalities in healthy food accessibility are identified via walking, public transit, and bicycle in Shenzhen. Based on the associations, we finally map the food deserts and propose corresponding planning strategies. The methods demonstrated in this study should offer deeper spatial insights into intra-urban foodscapes and provide more nuanced understanding of food deserts in urban settings of developing countries.",2017,,https://doi.org/10.1016/j.habitatint.2017.04.007
899,April Reeve,Chapter 21 - Big Data Integration,,,2013,,https://doi.org/10.1016/B978-0-12-397167-8.00021-2
900,Norbert Jesse,Internet of Things and Big Data – The Disruption of the Value Chain and the Rise of New Software Ecosystems,"Internet of Things, Smart Factories, Big Data, Software Platforms, Data Science","Abstract:
IoT connects devices, humans, places, and even abstract items like events. Driven by smart sensors, powerful embedded microelectronics, high-speed connectivity and the standards of the internet, IoT is on the brink of disrupting today's value chains. Big Data, characterized by high volume, high velocity and a high variety of formats, is a result of and also a driving force for IoT. The datafication of business presents completely new opportunities and risks. To hedge the technical risks posed by the interaction between “everything”, IoT requires comprehensive modelling tools. Furthermore, new IT platforms and architectures are necessary to process and store the unprecedented flow of structured and unstructured, repetitive and non-repetitive data in real-time. In the end, only powerful analytics tools are able to extract “sense” from the exponentially growing amount of data and, as a consequence, data science becomes a strategic asset. The era of IoT relies heavily on standards for technologies which guarantee the interoperability of everything. This paper outlines some fundamental standardization activities. Big Data approaches for real-time processing are outlined and tools for analytics are addressed. As consequence, IoT is a (fast) evolutionary process whose success in penetrating all dimensions of life heavily depends on close cooperation between standardization organizations, open source communities and IT experts.",2016,,https://doi.org/10.1016/j.ifacol.2016.11.079
901,Martin M. Andreasen and Jens H.E. Christensen and Glenn D. Rudebusch,Term Structure Analysis with Big Data: One-Step Estimation Using Bond Prices,"Extended Kalman filter, Fixed-coupon bond prices, Arbitrage-free Nelson–Siegel model","Nearly all studies that analyze the term structure of interest rates take a two-step approach. First, actual bond prices are summarized by interpolated synthetic zero-coupon yields, and second, some of these yields are used as the source data for further empirical examination. In contrast, we consider the advantages of a one-step approach that directly analyzes the universe of bond prices. To illustrate the feasibility and desirability of the one-step approach, we compare arbitrage-free dynamic term structure models estimated using both approaches. We also provide a simulation study showing that a one-step approach can extract the information in large panels of bond prices and avoid any arbitrary noise introduced from a first-stage interpolation of yields.",2019,,https://doi.org/10.1016/j.jeconom.2019.04.019
902,Nathan Brown and Jean Cambruzzi and Peter J. Cox and Mark Davies and James Dunbar and Dean Plumbley and Matthew A. Sellwood and Aaron Sim and Bryn I. Williams-Jones and Magdalena Zwierzyna and David W. Sheppard,Chapter Five - Big Data in Drug Discovery,"Big Data, Artificial intelligence, Drug discovery, Biology, Chemistry, Clinical trials","Interpretation of Big Data in the drug discovery community should enhance project timelines and reduce clinical attrition through improved early decision making. The issues we encounter start with the sheer volume of data and how we first ingest it before building an infrastructure to house it to make use of the data in an efficient and productive way. There are many problems associated with the data itself including general reproducibility, but often, it is the context surrounding an experiment that is critical to success. Help, in the form of artificial intelligence (AI), is required to understand and translate the context. On the back of natural language processing pipelines, AI is also used to prospectively generate new hypotheses by linking data together. We explain Big Data from the context of biology, chemistry and clinical trials, showcasing some of the impressive public domain sources and initiatives now available for interrogation.",2018,,https://doi.org/10.1016/bs.pmch.2017.12.003
903,Pierre-Francois D’Haese and Peter E. Konrad and Benoit M. Dawant,Chapter 13 - Big Data and Deep Brain Stimulation,"Atlas, Big data, Collaborative, CranialCloud, DBS, Normalization","Surgeons, neurologists, researchers, and patients have lacked the technology-based tools to facilitate sharing the tremendously valuable data about patients’ treatment and research in regard to what is working and what is not. Today, only 9% of patients who could benefit from complex therapies to address neurologic conditions actually receive them, and the medical information for each patient who does is hidden away in disconnected databases. To optimize and accelerate our understanding of the brain, we need to gather intelligence around every case, every research subject, every study while connecting that information through a unified, Health Insurance Portability and Accountability Act of 1996 (HIPAA)-compliant network that leverages technology and harnesses the Internet to drive advancements and better connect patients to their care teams. In this chapter, we highlight the key aspects needed to fulfill the requirements of a robust, HIPAA-compliant archive for brain data and highlight the impact of normalization on the accuracy of statistical analyses.",2018,,https://doi.org/10.1016/B978-0-12-805353-9.00013-9
904,Manel Souibgui and Faten Atigui and Saloua Zammali and Samira Cherfi and Sadok Ben Yahia,Data quality in ETL process: A preliminary study,"Business Intelligence & Analytics, ETL quality, Data, process quality, Talend Data Integration, Talend Data Quality","The accuracy and relevance of Business Intelligence & Analytics (BI&A) rely on the ability to bring high data quality to the data warehouse from both internal and external sources using the ETL process. The latter is complex and time-consuming as it manages data with heterogeneous content and diverse quality problems. Ensuring data quality requires tracking quality defects along the ETL process. In this paper, we present the main ETL quality characteristics. We provide an overview of the existing ETL process data quality approaches. We also present a comparative study of some commercial ETL tools to show how much these tools consider data quality dimensions. To illustrate our study, we carry out experiments using an ETL dedicated solution (Talend Data Integration) and a data quality dedicated solution (Talend Data Quality). Based on our study, we identify and discuss quality challenges to be addressed in our future research.",2019,,https://doi.org/10.1016/j.procs.2019.09.223
905,Paul A. Mathew and Laurel N. Dunn and Michael D. Sohn and Andrea Mercado and Claudine Custudio and Travis Walter,Big-data for building energy performance: Lessons from assembling a very large national database of building energy use,"Buildings Performance Database, Building performance, Big data, Building data collection, Data-driven decision support","Building energy data has been used for decades to understand energy flows in buildings and plan for future energy demand. Recent market, technology and policy drivers have resulted in widespread data collection by stakeholders across the buildings industry. Consolidation of independently collected and maintained datasets presents a cost-effective opportunity to build a database of unprecedented size. Applications of the data include peer group analysis to evaluate building performance, and data-driven algorithms that use empirical data to estimate energy savings associated with building retrofits. This paper discusses technical considerations in compiling such a database using the DOE Buildings Performance Database (BPD) as a case study. We gathered data on over 750,000 residential and commercial buildings. We describe the process and challenges of mapping and cleansing data from disparate sources. We analyze the distributions of buildings in the BPD relative to the Commercial Building Energy Consumption Survey (CBECS) and Residential Energy Consumption Survey (RECS), evaluating peer groups of buildings that are well or poorly represented, and discussing how differences in the distributions of the three datasets impact use-cases of the data. Finally, we discuss the usefulness and limitations of the current dataset and the outlook for increasing its size and applications.",2015,,https://doi.org/10.1016/j.apenergy.2014.11.042
906,Rajiv Ranjan and Dhavalkumar Thakker and Armin Haller and Rajkumar Buyya,A note on exploration of IoT generated big data using semantics,,Welcome to this special issue of the Future Generation Computer Systems (FGCS) journal. The special issue compiles seven technical contributions that significantly advance the state-of-the-art in exploration of Internet of Things (IoT) generated big data using semantic web techniques and technologies.,2017,,https://doi.org/10.1016/j.future.2017.06.032
907,Filip Dabek and Jesus J. Caban,Leveraging Big Data to Model the Likelihood of Developing Psychological Conditions After a Concussion,"Big Data, Machine Learning, Concussion, Informatics mild Traumatic Brain Injury","A concussion is an invisible and poorly understood mild traumatic brain injury (mTBI) that can alter the way the brain functions. Patients who have screened positive for mTBI are at an increased risk of depression, post-traumatic stress disorder (PTSD), headaches, sleep disorders, and other neurological and psychological problems. Early detection of psychological conditions such as PTSD following a concussion might improve the overall outcome of a patient and could potentially reduce the cost associated with intense interventions often required when conditions go untreated for a long time. Statistical and predictive models that leverage large-scale clinical repositories and use pre-existing conditions to determine the probability of a patient developing psychological conditions following a concussion have not been widely studied. This paper presents an SVM-based model that has been trained with a longitudinal dataset of over 5.3 million clinical encounters of 89,840 service members that have sustained a concussion. The model has been tested and validated with over 16,045 patients that developed PTSD and it has shown an accuracy of over 85% (AUC of 86.52%) at predicting the condition within the first year following the injury.",2015,,https://doi.org/10.1016/j.procs.2015.07.303
908,Nadine Levin and Reza M. Salek and Christoph Steinbeck,Chapter 11 - From Databases to Big Data,"Databases, big data, metabolomics, metabolic phenotyping, data exchange, data warehousing","This chapter explains the concept of big data and shows that the analytical data and related meta-data of metabolic phenotyping experiments fall into this category. The various databases that can be used to aid interpretation of such data, comprising general chemical and biochemical data, biochemical pathway specific data, analytical chemistry data to aid metabolite identification, and finally metabolic results, are discussed. The concept of data warehousing is explored in the context of metabolic data sets. Finally, the challenges for successful data storage, data exchange, and data interpretation are discussed.",2016,,https://doi.org/10.1016/B978-0-12-800344-2.00011-2
909,J.G. Enríquez and F.J. Domínguez-Mayo and M.J. Escalona and M. Ross and G. Staples,Entity reconciliation in big data sources: A systematic mapping study,"Systematic mapping study, Entity reconciliation, Heterogeneous databases, Big data","The entity reconciliation (ER) problem aroused much interest as a research topic in today's Big Data era, full of big and open heterogeneous data sources. This problem poses when relevant information on a topic needs to be obtained using methods based on: (i) identifying records that represent the same real world entity, and (ii) identifying those records that are similar but do not correspond to the same real-world entity. ER is an operational intelligence process, whereby organizations can unify different and heterogeneous data sources in order to relate possible matches of non-obvious entities. Besides, the complexity that the heterogeneity of data sources involves, the large number of records and differences among languages, for instance, must be added. This paper describes a Systematic Mapping Study (SMS) of journal articles, conferences and workshops published from 2010 to 2017 to solve the problem described before, first trying to understand the state-of-the-art, and then identifying any gaps in current research. Eleven digital libraries were analyzed following a systematic, semiautomatic and rigorous process that has resulted in 61 primary studies. They represent a great variety of intelligent proposals that aim to solve ER. The conclusion obtained is that most of the research is based on the operational phase as opposed to the design phase, and most studies have been tested on real-world data sources, where a lot of them are heterogeneous, but just a few apply to industry. There is a clear trend in research techniques based on clustering/blocking and graphs, although the level of automation of the proposals is hardly ever mentioned in the research work.",2017,,https://doi.org/10.1016/j.eswa.2017.03.010
910,Alexander Markowetz and Konrad Błaszkiewicz and Christian Montag and Christina Switala and Thomas E. Schlaepfer,Psycho-Informatics: Big Data shaping modern psychometrics,,"For the first time in history, it is possible to study human behavior on great scale and in fine detail simultaneously. Online services and ubiquitous computational devices, such as smartphones and modern cars, record our everyday activity. The resulting Big Data offers unprecedented opportunities for tracking and analyzing behavior. This paper hypothesizes the applicability and impact of Big Data technologies in the context of psychometrics both for research and clinical applications. It first outlines the state of the art, including the severe shortcomings with respect to quality and quantity of the resulting data. It then presents a technological vision, comprised of (i) numerous data sources such as mobile devices and sensors, (ii) a central data store, and (iii) an analytical platform, employing techniques from data mining and machine learning. To further illustrate the dramatic benefits of the proposed methodologies, the paper then outlines two current projects, logging and analyzing smartphone usage. One such study attempts to thereby quantify severity of major depression dynamically; the other investigates (mobile) Internet Addiction. Finally, the paper addresses some of the ethical issues inherent to Big Data technologies. In summary, the proposed approach is about to induce the single biggest methodological shift since the beginning of psychology or psychiatry. The resulting range of applications will dramatically shape the daily routines of researches and medical practitioners alike. Indeed, transferring techniques from computer science to psychiatry and psychology is about to establish Psycho-Informatics, an entire research direction of its own.",2014,,https://doi.org/10.1016/j.mehy.2013.11.030
911,Shastri L Nimmagadda and Torsten Reiners and and {Gary Burke},Big Data Guided Design Science Information System (DSIS) Development for Sustainability Management and Accounting,"Design Science, Digital Ecosystem, Sustainability, Multidimensional Artefacts, Data Interpretation","Sustainability is a dynamic, complex and composite data relationship among geographically distributed human and environment ecosystems. The ecosystems may have strong interactions among their elements and processes, but with dynamic implicit boundaries. Multi-scalable and multidimensional ecosystems have significance based on a commonality of basic structural units and domains. We intend to develop a holistic information system for managing different ecosystems within a sustainability framework/context, using an empirical qualitative and quantitative interpretation and analysis of the measured observations. Design Science Research (DSR) approach is aimed at developing an information system using the volumes of unstructured Big Data observations. Collaborating multiple domains, interpreting and evaluating the commonality, uncovering the connectivity among multiple systems are key aspects of the study. The Design Science Information System (DSIS), evolved from DSR approach is used in solving the ecosystem issues associated with multiple domains, in which the sustainability challenges manifest. In this context, we propose a human-environment-economic ecosystem (HEES) framework consisting of human, environment and economic elements and processes. In broad terms, human, environment and economic domains are conceptualized as different players/agents that operate within a range of sustainability scenarios. This approach recognizes the existing constraints of the systems as well as the emerging knowledge of the boundaries of ecosystems and their connectivity. The connectivity and interaction among the systems are analyzed by data mining, visualization and interpretation artefacts within a sustainability policy framework.",2017,,https://doi.org/10.1016/j.procs.2017.08.233
912,P. Mackie and F. Sim and C. Johnman,Big data! Big deal?,,,2015,,https://doi.org/10.1016/j.puhe.2015.02.013
913,Caroline Duvier and Daniel Neagu and Crina Oltean-Dumbrava and Dave Dickens,Data quality challenges in the UK social housing sector,"Social housing, Data quality","The social housing sector has yet to realise the potential of high data quality. While other businesses, mainly in the private sector, reap the benefits of data quality, the social housing sector seems paralysed, as it is still struggling with recent government regulations and steep revenue reduction. This paper offers a succinct review of relevant literature on data quality and how it relates to social housing. The Housing and Development Board in Singapore offers a great example on how to integrate data quality initiatives in the social housing sector. Taking this example, the research presented in this paper is extrapolating cross-disciplinarily recommendations on how to implement data quality initiatives in social housing providers in the UK.",2018,,https://doi.org/10.1016/j.ijinfomgt.2017.09.008
914,Li Lin and Wang Shuang and Liu Yifang and Wang Shouyang,A New Idea of Study on the Influence Factors of Companies’ Debt Costs in the Big Data Era,"debt cost, big data, quality of accounting information, corporate governance, LASSO method","Under the background of big data era today, once been widely used method – multiple linear regressions can not satisfy people's need to handle big data any more because of its bad characteristics such as multicollinearity, instability, subjectivity in model chosen etc. Contrary to MLR, LASSO method has many good natures. it is stable and can handle multicollinearity and successfully select the best model and do estimation in the same time. LASSO method is an effective improvement of multiple linear regressions. It is a natural change and innovation to introduce LASSO method into the accounting field and use it to deal with the debt costs problems. It helps us join the statistic field and accounting field together step by step. What's more, in order to proof the applicability of LASSO method in dealing with debt costs problems, we take 2301 companies’ data from Shanghai and Shenzhen A-share market in 2012 as samples, and chose 18 indexes to verify that the results of LASSO method is scientific, reasonable and accurate. In the end, we compare LASSO method with traditional multiple linear regressions and ridge regression, finding out that LASSO method can not only offer the most accurate prediction but also simplify the model.",2014,,https://doi.org/10.1016/j.procs.2014.05.299
915,Rupert Hollin,Chapter 2 - Drilling into the Big Data Gold Mine: Data Fusion and High-Performance Analytics for Intelligence Professionals,"Big Data, Fusion, High-performance analytics, Visualization","Threats to local, national, and global public security are continually evolving, and for those tasked with preventing and responding to these threats, the amount of potentially useful data can often seem overwhelming. What compounds this Big Data issue is the fact that we are living in a time of global economic austerity in which national security and law enforcement agencies need to become better at exploiting information while managing the demands of ever-shrinking budgets. This chapter looks at how, by using the latest software tools and techniques for data fusion and high-performance analytics, agencies can automate traditional labor-intensive tasks, gain a holistic view of information that originates from multiple sources, and extract valuable intelligence in a timely and more efficient manner.",2015,,https://doi.org/10.1016/B978-0-12-801967-2.00002-1
916,Zoie S.Y. Wong and Jiaqi Zhou and Qingpeng Zhang,Artificial Intelligence for infectious disease Big Data Analytics,"Infectious diseases modelling, Emergency response, Artificial Intelligence, Machine learning","Background
Since the beginning of the 21st century, the amount of data obtained from public health surveillance has increased dramatically due to the advancement of information and communications technology and the data collection systems now in place.
Methods
This paper aims to highlight the opportunities gained through the use of Artificial Intelligence (AI) methods to enable reliable disease-oriented monitoring and projection in this information age.
Results and Conclusion
It is foreseeable that together with reliable data management platforms AI methods will enable analysis of massive infectious disease and surveillance data effectively to support government agencies, healthcare service providers, and medical professionals to response to disease in the future.",2019,,https://doi.org/10.1016/j.idh.2018.10.002
917,Nauman Sheikh,"Chapter 11 - Big Data, Hadoop, and Cloud Computing","Hadoop, Big Data, cloud computing","When the idea for this book was originally conceived, Big Data and Hadoop were not the most popular themes on the tech circuit, although cloud computing was somewhat more prominent. Some of the reviewer feedback suggested that these topics should be addressed in the context of the conceptual layout of analytics solutions. In this chapter their use in an overall analytics solution will be explained using the previous chapters as a foundation. Big Data, Hadoop, and cloud computing are presented as standalone material, each tying back into the overall analytics solution implementations presented in preceding chapters.",2013,,https://doi.org/10.1016/B978-0-12-401696-5.00011-6
918,Otmane Azeroual and Gunter Saake and Eike Schallehn,Analyzing data quality issues in research information systems via data profiling,"Current research information systems, CRIS, Research information systems, RIS, Research information, Data sources, Data quality, Extraction transformation load, ETL, Data analysis, Data profiling, Science system, Standardization","The success or failure of a RIS in a scientific institution is largely related to the quality of the data available as a basis for the RIS applications. The most beautiful Business Intelligence (BI) tools (reporting, etc.) are worthless when displaying incorrect, incomplete, or inconsistent data. An integral part of every RIS is thus the integration of data from the operative systems. Before starting the integration process (ETL) of a source system, a rich analysis of source data is required. With the support of a data quality check, causes of quality problems can usually be detected. Corresponding analyzes are performed with data profiling to provide a good picture of the state of the data. In this paper, methods of data profiling are presented in order to gain an overview of the quality of the data in the source systems before their integration into the RIS. With the help of data profiling, the scientific institutions can not only evaluate their research information and provide information about their quality, but also examine the dependencies and redundancies between data fields and better correct them within their RIS.",2018,,https://doi.org/10.1016/j.ijinfomgt.2018.02.007
919,Abdul-Nasser El-Kassar and Sanjay Kumar Singh,Green innovation and organizational performance: The influence of big data and the moderating role of management commitment and HR practices,"Green innovation, Corporate environmental ethics, Large scale data, Human resource practices, Management commitment, Environmental and economic performance","Faced with internal and external pressure to adapt and implement environmental friendly business activities, it is becoming crucial for firms to identify practices that enhance their competitive advantage, economic, and environmental performance. Green innovation, green technologies, and the implementation of green supply chain management are examples of such practices. Green innovation and the adoption of the combination of green product innovation and green process innovation involve reduction in consumption of energy and pollution emission, recycling of wastes, sustainable utilization of resources, and green product designs. Although the extent research in this area is substantial, research on the importance of considering corporate environmental ethics, stakeholders view of green product, and demand for green products as drivers of green innovation must be conducted. Moreover, the role of large scale data, management commitment, and human resource practices play to overcome the technological challenges, achieve competitive advantage, and enhance the economic and environmental performance have yet to be addressed. This paper develops and tests a holistic model that depicts and examines the relationships among green innovation, its drivers, as well as factors that help overcome the technological challenges and influence the performance and competitive advantage of the firm. This paper is among the first works to deal with such a complex framework which considers the interrelationships among numerous constructs and their effects on competitive advantage as well as overall organizational performance. A questionnaire was designed to measure the influence of green innovation adoption/implementation and its drivers on performance and competitive advantage while taking into consideration the impact of management commitment and HR practices, as well as the use of large data on these relationships. Data collected from a sample of 215 respondents working in Middle East and North Africa (MENA) region and Golf-Cooperation Countries (GCC) were used to test the proposed relationships. The proposed model proved to be fit. The hypotheses were supported, and implications were discussed.",2019,,https://doi.org/10.1016/j.techfore.2017.12.016
920,,Clinical research and big data,,,2016,,https://doi.org/10.1016/j.denabs.2015.10.005
921,Suzanne McDermott and Margaret A. Turk,What are the implications of the big data paradigm shift for disability and health?,,,2015,,https://doi.org/10.1016/j.dhjo.2015.04.003
922,Haifei Yu and Mengxiao Zhang,Data pricing strategy based on data quality,"Big data, Data marketplace, Data pricing, Production management, Bi-level programming model","This paper presents a bi-level mathematical programming model for the data-pricing problem that considers both data quality and data versioning strategies. Data products and data-related services differ from information products or services in terms of quality assessment methods. For this problem, we consider two aspects of data quality: (1) its multidimensionality and (2) the interaction between the dimensions. We designed a multi-version data strategy and propose a data-pricing bi-level programming model based on the data quality to maximize the profit by the owner of the data platform and the utility to consumers. A genetic algorithm was used to solve the model. The numerical solutions for the data-pricing model indicate that the multi-version strategy achieves a better market segmentation and is more profitable and feasible when the multiple dimensions of data quality are considered. These results also provide managerial guidance on data provision and data pricing for platform owners.",2017,,https://doi.org/10.1016/j.cie.2017.08.008
923,Chung-Feng {Jeffrey Kuo} and Chieh-Hung Lin and Ming-Hao Lee,Analyze the energy consumption characteristics and affecting factors of Taiwan's convenience stores-using the big data mining approach,"Convenience store, Data mining, Machine learning, Energy consumption characteristics, Energy consumption affecting factor","This study applies big data mining, machine learning analysis technique and uses the Waikato Environment for Knowledge Analysis (WEKA) as a tool to discuss the convenience stores energy consumption performance in Taiwan which consists of (a). Influential factors of architectural space environment and geographical conditions; (b). Influential factors of management type; (c). Influential factors of business equipment; (d). Influential factors of local climatic conditions; (e). Influential factors of service area socioeconomic conditions. The survey data of 1,052 chain convenience stores belong to 7-Eleven, Family Mart and Hi-Life groups by Taiwan Architecture and Building Center (TABC) in 2014. The implicit knowledge will be explored in order to improve the traditional analysis technique which is unlikely to build a model for complex, inexact and uncertain dynamic energy consumption system for convenience stores. The analysis process comprises of (a). Problem definition and objective setting; (b). Data source selection; (c). Data collection; (d). Data preprocessing/preparation; (e). Data attributes selection; (f). Data mining and model construction; (g). Results analysis and evaluation; (h). Knowledge discovery and dissemination. The key factors influencing the convenience stores energy consumption and the influence intensity order can be explored by data attributes selection. The numerical prediction model for energy consumption is built by applying regression analysis and classification techniques. The optimization thresholds of various influential factors are obtained. The different cluster data are compared by using clustering analysis to verify the correlation between the factors influencing the convenience stores energy consumption characteristic. The implicit knowledge of energy consumption characteristic obtained by the aforesaid analysis can be used to (a). Provide the owners with accurate predicted energy consumption performance to optimize architectural space, business equipment and operations management mode; (b). The design planners can obtain the optimum design proposal of Cost Performance Ratio (C/P) by planning the thresholds of various key factors and the validation of prediction model; (c). Provide decision support for government energy and environment departments, to make energy saving and carbon emission reduction policies, in order to estimate and set the energy consumption scenarios of convenience store industry.",2018,,https://doi.org/10.1016/j.enbuild.2018.03.021
924,Xian-He Sun and Marc Frincu and Charalampos Chelmis,Special Issue on Scalable Computing Systems for Big Data Applications,,,2017,,https://doi.org/10.1016/j.jpdc.2017.05.020
925,James R. Marsden and David E. Pingry,Numerical data quality in IS research and the implications for replication,,"We argue that there are major, persistent numerical data quality issues in IS academic research. These issues undermine the ability to replicate our research – a critical element of scientific investigation and analysis. In IS empirical and analytics research articles, the amount of space devoted to the details of data collection, validation, and/or quality pales in comparison to the space devoted to the evaluation and selection of relatively sophisticated model form(s) and estimation technique(s). Yet erudite modeling and estimation can yield no immediate value or be meaningfully replicated without high quality data inputs. The purpose of this paper is: 1) to detail potential quality issues with data types currently used in IS research, and 2) to start a wider and deeper discussion of data quality in IS research. No data type is inherently of low quality and no data type guarantees high quality. As researchers, our empirical research must always address data quality issues and provide the information necessary to determine What, When, Where, How, Who, and Which.",2018,,https://doi.org/10.1016/j.dss.2018.10.007
926,Claudia Vitolo and Yehia Elkhatib and Dominik Reusser and Christopher J.A. Macleod and Wouter Buytaert,Web technologies for environmental Big Data,"Web-based modelling, Big Data, Web services, OGC standards","Recent evolutions in computing science and web technology provide the environmental community with continuously expanding resources for data collection and analysis that pose unprecedented challenges to the design of analysis methods, workflows, and interaction with data sets. In the light of the recent UK Research Council funded Environmental Virtual Observatory pilot project, this paper gives an overview of currently available implementations related to web-based technologies for processing large and heterogeneous datasets and discuss their relevance within the context of environmental data processing, simulation and prediction. We found that, the processing of the simple datasets used in the pilot proved to be relatively straightforward using a combination of R, RPy2, PyWPS and PostgreSQL. However, the use of NoSQL databases and more versatile frameworks such as OGC standard based implementations may provide a wider and more flexible set of features that particularly facilitate working with larger volumes and more heterogeneous data sources.",2015,,https://doi.org/10.1016/j.envsoft.2014.10.007
927,Günther Schuh and Eric Rebentisch and Michael Riesener and Thorben Ipers and Christian Tönnes and Merle-Hendrikje Jank,Data quality program management for digital shadows of products,"data quality program, digital shadow, data quality management","Nowadays, companies are facing challenges due to increasingly dynamic market environments, a growing internal and external complexity, as well as globally intensifying competition. To keep pace, companies need to establish extensive knowledge about their business and its surroundings based on insights generated through the analysis of data. The digital shadow is a novel information system concept that integrates data of heterogeneous sources to provide product-related information to stakeholders across the company. The concept aims at improving the results of decision making, enabling advanced data analyses, and increasing information handling efficiency. As insufficient information quality has immediate effects on the utility of the information and induces significant costs, managing the quality of the digital shadow data basis is crucial. However, there are currently no comprehensive methodologies for the assessment and improvement of the data quality of digital shadows. Therefore, this paper introduces a methodology that supports the derivation of data quality projects aimed at optimizing the digital shadow data basis. The proposed methodology comprises four steps: First, digital shadow use cases along the product lifecycle are described. Next, the use cases are prioritized with regard to the expected benefits of applying the digital shadow. Third, quality deficiencies in the digital shadow data basis are assessed with respect to use case specific requirements. Finally, the prioritized use cases in relation with the identified quality deficits allow deriving needs for action, which are addressed by data quality projects. Together, the data quality projects constitute a data quality program. The methodology is applied in an industry case to prove the practical effectivity and efficiency.",2019,,https://doi.org/10.1016/j.procir.2020.01.027
928,Belain Eyob and Marissa A. Boeck and Patrick FaSiOen and Shamir Cawich and Michael D. Kluger,Ensuring safe surgical care across resource settings via surgical outcomes data & quality improvement initiatives,"Surgical outcomes, Developing countries, Caribbean, Safe surgery, Quality improvement, Big data","Staggering statistics regarding the global burden of disease due to lack of surgical care worldwide has been gaining attention in the global health literature over the last 10 years. The Lancet Commission on Global Surgery reported that 16.9 million lives were lost due to an absence of surgical care in 2010, equivalent to 33% of all deaths worldwide. Although data from low- and middle-income countries (LMICs) are limited, recent investigations, such as the African Surgical Outcomes Study, highlight that despite operating on low risk patients, there is increased postoperative mortality in LMICs versus higher-resource settings, a majority of which occur secondary to seemingly preventable complications like surgical site infections. We propose that implementing creative, low-cost surgical outcomes monitoring and select quality improvement systems proven effective in high-income countries, such as surgical infection prevention programs and safety checklists, can enhance the delivery of safe surgical care in existing LMIC surgical systems. While efforts to initiate and expand surgical access and capacity continues to deserve attention in the global health community, here we advocate for creative modifications to current service structures, such as promoting a culture of safety, employing technology and mobile health (mHealth) for patient data collection and follow-up, and harnessing partnerships for information sharing, to create a framework for improving morbidity and mortality in responsible, scalable, and sustainable ways.",2019,,https://doi.org/10.1016/j.ijsu.2019.07.036
929,David Gil and Il-Yeol Song,Modeling and Management of Big Data: Challenges and opportunities,"Conceptual modeling Big Data, Ecosystem, Integrate & analyze & visualize","The term Big Data denotes huge-volume, complex, rapid growing datasets with numerous, autonomous and independent sources. In these new circumstances Big Data bring many attractive opportunities; however, good opportunities are always followed by challenges, such as modelling, new paradigms, novel architectures that require original approaches to address data complexities. The purpose of this special issue on Modeling and Management of Big Data is to discuss research and experience in modelling and to develop as well as deploy systems and techniques to deal with Big Data. A summary of the selected papers is presented, followed by a conceptual modelling proposal for Big Data. Big Data creates new requirements based on complexities in data capture, data storage, data analysis and data visualization. These concerns are discussed in detail in this study and proposals are recommended for specific areas of future research.",2016,,https://doi.org/10.1016/j.future.2015.07.019
930,Krish Krishnan,Chapter 12 - Information Management and Life Cycle for Big Data,"information life-cycle management, governance, program governance, data governance, data quality",This chapter deals with how to implement information life-cycle management principles to Big Data and create a sustainable process that will ensure that business continuity is not interrupted and data is available on demand.,2013,,https://doi.org/10.1016/B978-0-12-405891-0.00012-X
931,Krish Krishnan,Chapter 1 - Introduction to Big Data,"Big Data, data warehousing, sentiments, social media, machine data","Why this book? Why now? The goal of this book is to provide readers with a concise perspective into the biggest buzz in the industry—Big Data—and, more importantly, its impact on data processing, management, decision support, and data warehousing. At the time of this writing, there is a lot of interest to adopt a Big Data solution, but the profound confusion is what is the future of data warehousing and many investments that have been made over the years into building the decision support platform. This book addresses those areas of concern and provides readers an introduction to the next-generation of data management and data warehousing. This chapter provides you a concise and example driven introduction to what is Big Data, and how any organization needs to understand the value of Big Data.",2013,,https://doi.org/10.1016/B978-0-12-405891-0.00001-5
932,Tony O’Brien,‘Accounting’ for Data Quality in Enterprise Systems,"Data Quality, Enterprise Systems, Accounting Information Systems, ERP, SCM, CRM, Big Data","Organisations are facing ever more diverse challenges in managing their enterprise systems as emerging technologies bring both added complexities as well as opportunities to the way they conduct their business. Underpinning this ever-increasing volatility is the importance of having quality data to provide information to make those important enterprise-wide decisions. Numerous studies suggest that many organisations are not paying enough attention to their data and that a major cause of this is their failure to measure its quality and value and/or evaluate the costs of having poor data. This study proposes an integrated framework that organisations can adopt as part of their financial and management control processes to provide a mechanism for quantifying data problems, costing potential solutions and monitoring the on-going costs and benefits, to assist them in improving and then sustaining the quality of their data.",2015,,https://doi.org/10.1016/j.procs.2015.08.539
933,Haluk Demirkan and Dursun Delen,Leveraging the capabilities of service-oriented decision support systems: Putting analytics and big data in cloud,"Cloud computing, Service orientation, Service science, Data-as-a-service, Information-as-a-service, Analytics-as-a-service, Big data","Using service-oriented decision support systems (DSS in cloud) is one of the major trends for many organizations in hopes of becoming more agile. In this paper, after defining a list of requirements for service-oriented DSS, we propose a conceptual framework for DSS in cloud, and discus about research directions. A unique contribution of this paper is its perspective on how to servitize the product oriented DSS environment, and demonstrate the opportunities and challenges of engineering service oriented DSS in cloud. When we define data, information and analytics as services, we see that traditional measurement mechanisms, which are mainly time and cost driven, do not work well. Organizations need to consider value of service level and quality in addition to the cost and duration of delivered services. DSS in CLOUD enables scale, scope and speed economies. This article contributes new knowledge in service science by tying the information technology strategy perspectives to the database and design science perspectives for a broader audience.",2013,,https://doi.org/10.1016/j.dss.2012.05.048
934,Victor Chang and Gary Wills,A model to compare cloud and non-cloud storage of Big Data,"Organizational sustainability modeling (OSM), Comparison between Cloud and non-Cloud storage platforms, Real Cloud case studies, Data analysis and visualization","When comparing Cloud and non-Cloud Storage it can be difficult to ensure that the comparison is fair. In this paper we examine the process of setting up such a comparison and the metric used. Performance comparisons on Cloud and non-Cloud systems, deployed for biomedical scientists, have been conducted to identify improvements of efficiency and performance. Prior to the experiments, network latency, file size and job failures were identified as factors which degrade performance and experiments were conducted to understand their impacts. Organizational Sustainability Modeling (OSM) is used before, during and after the experiments to ensure fair comparisons are achieved. OSM defines the actual and expected execution time, risk control rates and is used to understand key outputs related to both Cloud and non-Cloud experiments. Forty experiments on both Cloud and non-Cloud systems were undertaken with two case studies. The first case study was focused on transferring and backing up 10,000 files of 1 GB each and the second case study was focused on transferring and backing up 1000 files 10 GB each. Results showed that first, the actual and expected execution time on the Cloud was lower than on the non-Cloud system. Second, there was more than 99% consistency between the actual and expected execution time on the Cloud while no comparable consistency was found on the non-Cloud system. Third, the improvement in efficiency was higher on the Cloud than the non-Cloud. OSM is the metric used to analyze the collected data and provided synthesis and insights to the data analysis and visualization of the two case studies.",2016,,https://doi.org/10.1016/j.future.2015.10.003
935,Yu Xiao and Louis Y.Y. Lu and John S. Liu and Zhili Zhou,Knowledge diffusion path analysis of data quality literature: A main path analysis,"Data quality, Main path analysis, Knowledge diffusion, Citation analysis, Social network analysis, Big data","This study presents a unique approach in investigating the knowledge diffusion structure for the field of data quality through an analysis of the main paths. We study a dataset of 1880 papers to explore the knowledge diffusion path, using citation data to build the citation network. The main paths are then investigated and visualized via social network analysis. This paper takes three different main path analyses, namely local, global, and key-route, to depict the knowledge diffusion path and additionally implements the g-index and h-index to evaluate the most important journals and researchers in the data quality domain.",2014,,https://doi.org/10.1016/j.joi.2014.05.001
936,David Loshin,Chapter 4 - Developing a Strategy for Integrating Big Data Analytics into the Enterprise,"Strategic plan, business requirements, technology adoption, massive scalability, data reuse, data repurposing, oversight, governance, mainstreaming technology, enterprise integration","This chapter expands on the previous one by looking at some key issues that often plague new technology adoption and show that the key issues are not new ones, and that there is likely to be organizational knowledge that can help in fleshing out a reasonable strategic plan. We look at the typical hype cycle, and how its flaws can be mitigated by instituting good practices for defining expectations and continuing to measure performance. We help define the acceptability criteria for evaluating the result of a big data pilot that can be used to make a go/no-go decision. We then pose some thoughts about preparing the organization for massive scalability, data reuse, and the need for oversight and governance. The objective is to provide a pathway for mainstreaming big data into the technology infrastructure that is integrated with the existing investment.",2013,,https://doi.org/10.1016/B978-0-12-417319-4.00004-1
937,Joe Celko,Chapter 9 - Big Data and Cloud Computing,"Forrester Research, V-list, cloud computing, Big Data, data mining","Big Data is largely a buzzword in IT right now. It was coined by Forrester Research to put a wrapper around existing database mining, data management, and other extensions of existing technology to the current hardware. The goal is to use mixed tools with larger volumes of several different forms of data being brought together under one roof. Along with this approach to data, we are also concerned with cloud computing, which is a public or private Internet network that replaces the tradition hardwired landlines within a company.",2014,,https://doi.org/10.1016/B978-0-12-407192-6.00009-1
938,Jonathan Dekhtiar and Alexandre Durupt and Matthieu Bricogne and Benoit Eynard and Harvey Rowson and Dimitris Kiritsis,"Deep learning for big data applications in CAD and PLM – Research review, opportunities and case study","Deep learning, Machine learning, Computer vision, Product Lifecycle Management, Digital mock-up, Shape retrieval","With the increasing amount of available data, computing power and network speed for a decreasing cost, the manufacturing industry is facing an unprecedented amount of data to process, understand and exploit. Phenomena such as Big Data, the Internet-of-Things, Closed-Loop Product Lifecycle Management, and the advances of Smart Factories tend to produce humanly unmanageable quantities of data. The paper approaches the aforesaid context by assuming that any data processing automation is not only desirable but rather necessary in order to prevent prohibitive data analytics costs. This study focuses on highlighting the major specificities of engineering data and the data-processing difficulties which are inherent to data coming from the manufacturing industry. The artificial intelligence field of research is able to provide methods and tools to address some of the identified issues. A special attention was paid to provide a literature review of the most recent (in 2017) applications, that could present a high potential for the manufacturing industry, in the fields of machine learning and deep learning. In order to illustrate the proposed work, a case study was conducted on the challenging research question of object recognition in heterogeneous formats (3D models, photos and videos) with deep learning techniques. The DICE project – DMU Imagery Comparison Engine – is presented and has been completely open-sourced in order to encourage reuse and improvements of the proposed case-study. This project also leads to the development of an open-source research dataset of 2000 CAD Models, called DMU-Net available at: https://www.dmu-net.org.",2018,,https://doi.org/10.1016/j.compind.2018.04.005
939,Samuel {Fosso Wamba} and Shahriar Akter and Andrew Edwards and Geoffrey Chopin and Denis Gnanzou,How ‘big data’ can make big impact: Findings from a systematic review and a longitudinal case study,"‘Big data’, Analytics, Business value, Issues, Case study, Emergency services, Literature review","Big data has the potential to revolutionize the art of management. Despite the high operational and strategic impacts, there is a paucity of empirical research to assess the business value of big data. Drawing on a systematic review and case study findings, this paper presents an interpretive framework that analyzes the definitional perspectives and the applications of big data. The paper also provides a general taxonomy that helps broaden the understanding of big data and its role in capturing business value. The synthesis of the diverse concepts within the literature on big data provides deeper insights into achieving value through big data strategy and implementation.",2015,,https://doi.org/10.1016/j.ijpe.2014.12.031
940,Susanna K.P. Lau and Patrick C.Y. Woo,"Pitfalls in big data analysis: next-generation technologies, last-generation data",,,2019,,https://doi.org/10.1016/j.diagmicrobio.2018.12.006
941,Amy Genender-Feltheimer,Visualizing High Dimensional and Big Data,"Data Visualization, Dimensionality Reduction, Parallel Processing, Hadoop, Machine Learning, Big Data","The amount of data created by people, machines and corporations around the world is growing every year. Thanks to innovations such as the Internet of Things, this trend will continue, giving rise to the creation of Big Data. Data visualization leverages principles of visual psychology to help stakeholders identify patterns, trends and correlations that might go undetected in text-based or spreadsheet data. The return on investment (ROI) of big data visualization is well-documented in numerous studies and use cases. However, to achieve ROI from analytics investments, key insights must be uncovered, understood and communicated. Synthesizing huge quantities of data into key insights grows more challenging as data volumes and varieties increase. To address visualization challenges posed by big and high-dimensional data, this paper explores algorithms and techniques that compress the amount of data and/or reduce the number of attributes to be analyzed and visualized. Specifically, this paper examines applying dimensionality reduction and data compression algorithms to reduce attributes, tuples and data points returned to the visualization. By reducing data returned to the visualization, trends, patterns and correlations are easier to view and visualization tool performance is optimized.",2018,,https://doi.org/10.1016/j.procs.2018.10.308
942,Ying Cheng and Ken Chen and Hemeng Sun and Yongping Zhang and Fei Tao,Data and knowledge mining with big data towards smart production,"Big data, Data mining techniques (DMTs), Production management, Smart manufacturing, Statistical analysis, Knowledge discovery","Driven by the innovative improvement of information and communication technologies (ICTs) and their applications into manufacturing industry, the big data era in manufacturing is correspondingly arising, and the developing data mining techniques (DMTs) pave the way for pursuing the aims of smart production with the real-time, dynamic, self-adaptive and precise control. However, lots of factors in the ever-changing environment of manufacturing industry, such as, various of complex production processes, larger scale and uncertainties, more complicated constrains, coupling of operational performance, and so on, make production management face with more and more big challenges. The dynamic inflow of a large number of raw data which is collected from the physical manufacturing sites or generated in various related information systems, caused the heavy information overload problems. Indeed, most of traditional DMTs are not yet sufficient to process such big data for smart production management. Therefore, this paper reviews the development of DMTs in the big data era, and makes discussion on the applications of DMTs in production management, by selecting and analyzing the relevant papers since 2010. In the meantime, we point out limitations and put forward some suggestions about the smartness and further applications of DMTs used in production management.",2018,,https://doi.org/10.1016/j.jii.2017.08.001
943,Christos I. Papanagnou and Omeiza Matthews-Amune,Coping with demand volatility in retail pharmacies with the aid of big data exploration,"Retail pharmacy, Data mining, Time series, Forecasting, Big data, Demand uncertainty","Data management tools and analytics have provided managers with the opportunity to contemplate inventory performance as an ongoing activity by no longer examining only data agglomerated from ERP systems, but also, considering internet information derived from customers’ online buying behaviour. The realisation of this complex relationship has increased interest in business intelligence through data and text mining of structured, semi-structured and unstructured data, commonly referred to as “big data” to uncover underlying patterns which might explain customer behaviour and improve the response to demand volatility. This paper explores how sales structured data can be used in conjunction with non-structured customer data to improve inventory management either in terms of forecasting or treating some inventory as “top-selling” based on specific customer tendency to acquire more information through the internet. A medical condition is considered - namely pain - by examining 129 weeks of sales data regarding analgesics and information seeking data by customers through Google, online newspapers and YouTube. In order to facilitate our study we consider a VARX model with non-structured data as exogenous to obtain the best estimation and we perform tests against several univariate models in terms of best fit performance and forecasting.",2018,,https://doi.org/10.1016/j.cor.2017.08.009
944,C.L. {Philip Chen} and Chun-Yang Zhang,"Data-intensive applications, challenges, techniques and technologies: A survey on Big Data","Big Data, Data-intensive computing, e-Science, Parallel and distributed computing, Cloud computing","It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore’s Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing.",2014,,https://doi.org/10.1016/j.ins.2014.01.015
945,Javier Carnicero and David Rojas,Chapter 8 - Healthcare Decision-Making Support Based on the Application of Big Data to Electronic Medical Records: A Knowledge Management Cycle,"Big Data, Electronic medical record, Practice-based medicine, Learning health system, Semantic interoperability","Any given health system needs to increase efficiency and effectiveness up to the point of requiring a transformation of their current model to ensure their sustainability and continuity. The electronic medical record (EMR) is the main source of knowledge to improve the quality of healthcare, clinical research, epidemiological surveillance, patient empowerment, personalized medicine, and clinical decision-making support systems. There is also a huge amount of available information related to diseases and other medical conditions, such as drugs and therapies, omics data (genetic and proteomic), social networks, and wearable devices. Big Data technologies allow the processing of this data to reach the final goal, which is a learning health system. The great diversity of data, sources, structures, and uses requires a data linkage procedure to integrate and harmonize these data. This generation of knowledge allows the transition from evidence-based medicine, which still prevails, to practice-based medicine. The key points for any Big Data project based on EMRs and other medical information sources are semantic interoperability, data structure and granularity, information quality, patient privacy, legal framework, and bioethics.",2019,,https://doi.org/10.1016/B978-0-12-809556-0.00008-3
946,Yaguang Lin and Xiaoming Wang and Fei Hao and Liang Wang and Lichen Zhang and Ruonan Zhao,An on-demand coverage based self-deployment algorithm for big data perception in mobile sensing networks,"Mobile sensing network, High performance sensing, Big data perception, Node self-deployment, On-demand coverage, Mobile cellular learning automata","Mobile Sensing Networks have been widely applied to many fields for big data perception such as intelligent transportation, medical health and environment sensing. However, in some complex environments and unreachable regions of inconvenience for human, the establishment of the mobile sensing networks, the layout of the nodes and the control of the network topology to achieve high performance sensing of big data are increasingly becoming a main issue in the applications of the mobile sensing networks. To deal with this problem, we propose a novel on-demand coverage based self-deployment algorithm for big data perception based on mobile sensing networks in this paper. Firstly, by considering characteristics of mobile sensing nodes, we extend the cellular automata model and propose a new mobile cellular automata model for effectively characterizing the spatial–temporal evolutionary process of nodes. Secondly, based on the learning automata theory and the historical information of node movement, we further explore a new mobile cellular learning automata model, in which nodes can self-adaptively and intelligently decide the best direction of movement with low energy consumption. Finally, we propose a new optimization algorithm which can quickly solve the node self-adaptive deployment problem, thus, we derive the best deployment scheme of nodes in a short time. The extensive simulation results show that the proposed algorithm in this paper outperforms the existing algorithms by as much as 40% in terms of the degree of satisfaction of network coverage, the iterations of the algorithm, the average moving steps of nodes and the energy consumption of nodes. Hence, we believe that our work will make contributions to large-scale adaptive deployment and high performance sensing scenarios of the mobile sensing networks.",2018,,https://doi.org/10.1016/j.future.2018.01.007
947,Isaac Cano and Akos Tenyi and Emili Vela and Felip Miralles and Josep Roca,Perspectives on Big Data applications of health information,"Digital health, Secondary use of data, Health analytics, Predictive modeling, Health forecasting","Recent advances on prospective monitoring and retrospective analysis of health information at national or regional level are generating high expectations for the application of Big Data technologies that aim to analyze at real time high-volumes and/or complex of data from healthcare delivery (e.g., electronic health records, laboratory and radiology information, electronic prescriptions, etc.) and citizens' lifestyles (e.g., personal health records, personal monitoring devices, social networks, etc.). Along these same lines, advances in the field of genomics are revolutionizing biomedical research, both in terms of data volume and prospects, as well as in terms of the social impact it entails. The potential of Big Data applications that consider all of the above levels of health information lies in the possibility of combining and integrating de-identified health information to allow secondary uses of data. This is the use and re-use of various sources of health information for purposes in addition to the direct clinical care of specific patients or the direct investigation of specific biomedical research hypotheses. Current applications include: epidemiological and pharmacovigilance studies, facilitating recruitment to randomized controlled trials, carrying out audits and benchmarking studies, financial and service planning, and ultimately supporting the generation of novel biomedical research outcomes.",2017,,https://doi.org/10.1016/j.coisb.2017.04.012
948,Roger Bivand and Konstantin Krivoruchko,"Big data sampling and spatial analysis: “which of the two ladles, of fig-wood or gold, is appropriate to the soup and the pot?”","Change of support, Sampling design, Data transformation, Prediction standard error","Following from Krivoruchko and Bivand (2009), we consider some general points related to challenges to the usefulness of big data in spatial statistical applications when data collection is compromised or one or more model assumptions are violated. We look further at the desirability of comparison of new methods intended to handle large spatial and spatio-temporal datasets.",2018,,https://doi.org/10.1016/j.spl.2018.02.012
949,Luciana {Dalla Valle} and Ron Kenett,Social media big data integration: A new approach based on calibration,"Bayesian networks, Calibration, Data integration, Social media, Information quality (InfoQ), Resampling techniques","In recent years, the growing availability of huge amounts of information, generated in every sector at high speed and in a wide variety of forms and formats, is unprecedented. The ability to harness big data is an opportunity to obtain more accurate analyses and to improve decision-making in industry, government and many other organizations. However, handling big data may be challenging and proper data integration is a key dimension in achieving high information quality. In this paper, we propose a novel approach to data integration that calibrates online generated big data with interview based customer survey data. A common issue of customer surveys is that responses are often overly positive, making it difficult to identify areas of weaknesses in organizations. On the other hand, online reviews are often overly negative, hampering an accurate evaluation of areas of excellence. The proposed methodology calibrates the levels of unbalanced responses in different data sources via resampling and performs data integration using Bayesian Networks to propagate the new re-balanced information. In this paper we show, with a case study example, how the novel data integration approach allows businesses and organizations to get a bias corrected appraisal of the level of satisfaction of their customers. The application is based on the integration of online data of review blogs and customer satisfaction surveys from the San Francisco airport. We illustrate how this integration enhances the information quality of the data analytic work in four of InfoQ dimensions, namely, Data Structure, Data Integration, Temporal Relevance and Chronology of Data and Goal.",2018,,https://doi.org/10.1016/j.eswa.2017.12.044
950,Adithya Thaduri and Diego Galar and Uday Kumar,Railway Assets: A Potential Domain for Big Data Analytics,"Big Data, Railways, Maintenance, Transportation","Two concepts currently at the leading edge of todays information technology revolution are Analytics and Big Data. The public transportation industry has been at the forefront in utilizing and implementing Analytics and Big Data, from ridership forecasting to transit operations Rail transit systems have been especially involved with these IT concepts, and tend to be especially amenable to the advantages of Analytics and Big Data because they are generally closed systems that involve sophisticated processing of large volumes of data. The more that public transportation professionals and decision makers understand the role of Analytics and Big Data in their industry in perspective, the more effectively they will be able to utilize its promise. This paper gives an overview of Big Data technologies in context of transportation with specific to Railways. This paper also gives an insight on how the existing data modules from the transport authority combines Big Data and how can be incorporated in providing maintenance decision making.",2015,,https://doi.org/10.1016/j.procs.2015.07.323
951,Antonio Vetrò and Marco Torchiano and Mariachiara Mecati,A data quality approach to the identification of discrimination risk in automated decision making systems,"Automated decision making, Data ethics, Data quality, Data bias, Algorithm fairness, Digital policy, Digital governance","Automated decision-making (ADM) systems may affect multiple aspects of our lives. In particular, they can result in systematic discrimination of specific population groups, in violation of the EU Charter of Fundamental Rights. One of the potential causes of discriminative behavior, i.e., unfairness, lies in the quality of the data used to train such ADM systems. Using a data quality measurement approach combined with risk management, both defined in ISO standards, we focus on balance characteristics and we aim to understand how balance indexes (Gini, Simpson, Shannon, Imbalance Ratio) identify discrimination risk in six large datasets containing the classification output of ADM systems. The best result is achieved using the Imbalance Ratio index. Gini and Shannon indexes tend to assume high values and for this reason they have modest results in both aspects: further experimentation with different thresholds is needed. In terms of policies, the risk-based approach is a core element of the EU approach to regulate algorithmic systems: in this context, balance measures can be easily assumed as risk indicators of propagation – or even amplification – of bias in the input data of ADM systems.",2021,,https://doi.org/10.1016/j.giq.2021.101619
952,Mahfoud Bala and Omar Boussaid and Zaia Alimazighi,A Fine‐Grained Distribution Approach for ETL Processes in Big Data Environments,"Data Warehousing, ETL, Parallel and Distributed Processing, Big Data, MapReduce","Among the so-called “4Vs” (volume, velocity, variety, and veracity) that characterize the complexity of Big Data, this paper focuses on the issue of “Volume” in order to ensure good performance for Extracting-Transforming-Loading (ETL) processes. In this study, we propose a new fine-grained parallelization/distribution approach for populating the Data Warehouse (DW). Unlike prior approaches that distribute the ETL only at coarse-grained level of processing, our approach provides different ways of parallelization/distribution both at process, functionality and elementary functions levels. In our approach, an ETL process is described in terms of its core functionalities which can run on a cluster of computers according to the MapReduce (MR) paradigm. The novel approach allows thereby the distribution of the ETL process at three levels: the “process” level for coarse-grained distribution and the “functionality” and “elementary functions” levels for fine-grained distribution. Our performance analysis reveals that employing 25 to 38 parallel tasks enables the novel approach to speed up the ETL process by up to 33% with the improvement rate being linear.",2017,,https://doi.org/10.1016/j.datak.2017.08.003
953,Hye-Chung Kum and C. {Joy Stewart} and Roderick A. Rose and Dean F. Duncan,Using big data for evidence based governance in child welfare,"Big data, Evidence based governance, Knowledge discovery and data mining (KDD), Data science, Population informatics, Policy informatics, Academic government partnership, Administrative data","Numerous approaches are available for improving governance of the child welfare system, all of which require longitudinal data reporting on child welfare clients. A substantial amount of agency administrative information – big data – can be transformed into knowledge for policy and management actions through a rigorous information generation process. Important properties of the information generation process are that it must generate accurate, timely information while protecting the confidentiality of the clients. In addition, it must be extensible to serve an ever-changing policy and technology environment. Knowledge discovery and data mining (KDD), aka data science, is a method developed in the private sector to mine consumer data and can be used in public settings to support evidence based governance. KDD consists of a rigorous 5-step process that includes a Web-based end-user interface. The relationship between KDD and governance is a continuous feedback cycle that enables ongoing development of new information and knowledge as stakeholders identify emerging needs. In this paper, we synthesis the different frameworks for utilizing big data for public governance, introduce the KDD process, describe the nature of big data in child welfare, and then present an updated KDD architecture that can support these frameworks to utilize big data for governance. We also demonstrate the role KDD plays in child welfare management through 2 case studies. We conclude with a discussion on implications for agency–university partnerships and research-to-practice.",2015,,https://doi.org/10.1016/j.childyouth.2015.09.014
954,Robert K. Perrons and Jesse W. Jensen,Data as an asset: What the oil and gas sector can learn from other industries about “Big Data”,"Big data, Oil and gas, Information technologies, Data","The upstream oil and gas industry has been contending with massive data sets and monolithic files for many years, but “Big Data” is a relatively new concept that has the potential to significantly re-shape the industry. Despite the impressive amount of value that is being realized by Big Data technologies in other parts of the marketplace, however, much of the data collected within the oil and gas sector tends to be discarded, ignored, or analyzed in a very cursory way. This viewpoint examines existing data management practices in the upstream oil and gas industry, and compares them to practices and philosophies that have emerged in organizations that are leading the way in Big Data. The comparison shows that, in companies that are widely considered to be leaders in Big Data analytics, data is regarded as a valuable asset—but this is usually not true within the oil and gas industry insofar as data is frequently regarded there as descriptive information about a physical asset rather than something that is valuable in and of itself. The paper then discusses how the industry could potentially extract more value from data, and concludes with a series of policy-related questions to this end.",2015,,https://doi.org/10.1016/j.enpol.2015.02.020
955,Dong Yang and Lingxiao Wu and Shuaian Wang and Haiying Jia and Kevin X. Li,How big data enriches maritime research – a critical review of Automatic Identification System (AIS) data applications,"AIS data, data mining, navigation safety, ship behaviour analysis, environmental evaluation, advanced applications of AIS data","ABSTRACT
The information-rich vessel movement data provided by the Automatic Identification System (AIS) has gained much popularity over the past decade, during which the employment of satellite-based receivers has enabled wide coverage and improved data quality. The application of AIS data has developed from simply navigation-oriented research to now include trade flow estimation, emission accounting, and vessel performance monitoring. The AIS now provides high frequency, real-time positioning and sailing patterns for almost the whole world's commercial fleet, and therefore, in combination with supplementary databases and analyses, AIS data has arguably kickstarted the era of digitisation in the shipping industry. In this study, we conduct a comprehensive review of the literature regarding AIS applications by dividing it into three development stages, namely, basic application, extended application, and advanced application. Each stage contains two to three application fields, and in total we identified seven application fields, including (1) AIS data mining, (2) navigation safety, (3) ship behaviour analysis, (4) environmental evaluation, (5) trade analysis, (6) ship and port performance, and (7) Arctic shipping. We found that the original application of AIS data to navigation safety has, with the improvement of data accessibility, evolved into diverse applications in various directions. Moreover, we summarised the major methodologies in the literature into four categories, these being (1) data processing and mining, (2) index measurement, (3) causality analysis, and (4) operational research. Undoubtedly, the applications of AIS data will be further expanded in the foreseeable future. This will not only provide a more comprehensive understanding of voyage performance and allow researchers to examine shipping market dynamics from the micro level, but also the abundance of AIS data may also open up the rather opaque aspect of how shipping companies release information to external authorities, including the International Maritime Organization, port states, scientists and researchers. It is expected that more multi-disciplinary AIS studies will emerge in the coming years. We believe that this study will shed further light on the future development of AIS studies.",2019,,https://doi.org/10.1080/01441647.2019.1649315
956,Dong-Hee Shin and Min Jae Choi,Ecological views of big data: Perspectives and issues,"Big data, Data ecosystem, Ecology, South Korea, Socio-technical perspective, Big data policy, Big data for development","From the viewpoint of big data as a socio-technical phenomenon, this study examines the associated assumptions and biases critically and contextually. The research analyzes the big data phenomenon from a socio-technical systems theory perspective: cultural, technological, and scholarly phenomena that rest on the interplay of technology, analysis, and mythology provoking extensive utopian and dystopian rhetoric. It examines the development of big data by reviewing this theory, identifying key components of the big data ecosystem, and explaining how these components are likely to evolve over time. Despite extensive investment and proactive drive, uncertainty exists concerning the evolution of big data and the impact on the new information milieu. Significant concerns recently addressed are in the areas of privacy, data quality, access, curation, preservation, and use. This study provides insight into these challenges and opportunities through the lens of a socio-technical analysis of big data development, which includes social dynamics, political discourse, and technological choices inherent in the design and development of next-generation ICT ecology. The policy implications of big data are addressed using Korean information initiatives to highlight key considerations as the country progresses in this new ecology era.",2015,,https://doi.org/10.1016/j.tele.2014.09.006
957,Rashid Mehmood and Gary Graham,Big Data Logistics: A health-care Transport Capacity Sharing Model,"future city, Big data, transport operation management, healthcare informationsystems, integrated systems, shared resources","The growth of cities in the 21st century has put more pressure on resources and conditions of urban life. There are several reasons why the health-care industry is the focus of this investigation. For instance, in the UK various studies point to the lack of failure of basic quality control procedures and misalignment between customer needs and provider services and duplication of logistics practices. The development of smart cities and big data present unprecedented challenges and opportunities for operations managers; they need to develop new tools and techniques for network planning and control. Our paper aims to make a contribution to big data and city operations theory by exploring how big data can lead to improvements in transport capacity sharing. We explore using Markov models the integration of big data with future city (health-care) transport sharing. A mathematical model was designed to illustrate how sharing transport load (and capacity) in a smart city can improve efficiencies in meeting demand for city services. The results from our analysis of 13 different sharing/demand scenarios are presented. A key finding is that the probability for system failure and performance variance tends to be highest in a scenario of high demand/zero sharing.",2015,,https://doi.org/10.1016/j.procs.2015.08.566
958,Alexander N. Raikov and Z. Avdeeva and A. Ermakov,Big Data Refining on the Base of Cognitive Modeling,"data refining, cognitive modeling, Big Data, intellectual agents, networked expertise","Abstract:
In conditions of rapid external changes the requirement to quality of control of purposeful development of complex system (states, regions, corporations etc.) dramatically increases. Automation support of the key stages of decision making process is one of the ways to cope with the challenges. This paper focuses on the approach based on the Big Data Refining during cognitive modeling that proves the correctness of modeling and decision-making. The approach uses the requests to the Big Data for cognitive model components verification. The requests are created by intelligent agents with feedback from decision makers. Some practical results confirm the adequacy of the proposed approach.",2016,,https://doi.org/10.1016/j.ifacol.2016.12.205
959,,In praise of Meeting the Challenges of Data Quality Management,,,2022,,https://doi.org/10.1016/B978-0-12-821737-5.00016-X
960,Z.G. Hui and Q. Guo and W.Z. Shi and M.C. Gong and C. Liu and H. Xu and H. Li,PCN181 THE NATIONAL CANCER BIG DATA PLATFORM OF CHINA: VISION AND STATUS,,,2019,,https://doi.org/10.1016/j.jval.2019.04.303
961,Lars Richter and Gerhard F. Ecker,Medicinal chemistry in the era of big data,,"In the era of big data medicinal chemists are exposed to an enormous amount of bioactivity data. Numerous public data sources allow for querying across medium to large data sets mostly compiled from literature. However, the data available are still quite incomplete and of mixed quality. This mini review will focus on how medicinal chemists might use such resources and how valuable the current data sources are for guiding drug discovery.",2015,,https://doi.org/10.1016/j.ddtec.2015.06.001
962,Ying Huang and Alan L. Porter and Scott W. Cunningham and Douglas K.R. Robinson and Jianhua Liu and Donghua Zhu,A technology delivery system for characterizing the supply side of technology emergence: Illustrated for Big Data & Analytics,"Technology delivery system, Tech mining, Emerging technology, Big Data, Technology assessment, Impact assessment","While there is a general recognition that breakthrough innovation is non-linear and requires an alignment between producers (supply) and users (demand), there is still a need for strategic intelligence about the emerging supply chains of new technological innovations. This technology delivery system (TDS) is an updated form of the TDS model and provides a promising chain-link approach to the supply side of innovation. Building on early research into supply-side TDS studies, we present a systematic approach to building a TDS model that includes four phases: (1) identifying the macroeconomic and policy environment, including market competition, financial investment, and industrial policy; (2) specifying the key public and private institutions; (3) addressing the core technical complements and their owners, then tracing their interactions through information linkages and technology transfers; and (4) depicting the market prospects and evaluating the potential profound influences on technological change and social developments. Our TDS methodology is illustrated using the field of Big Data & Analytics (“BDA”).",2018,,https://doi.org/10.1016/j.techfore.2017.09.012
963,Marin M. Kress,Big Data for Ecological Models,"Big data, Crowdsourcing or crowdsourced, Data discovery, Data discovery, Data science, Database, Dryad, Environmental health, Interdisciplinary, Machine readable, Metadata, Remote sensing, Social media","The use of data repositories for parameterizing ecological models and storing model runs is becoming more common, yet often these data archives do not contain the appropriate metadata, nor are they maintained for others to use. Data archiving and sharing are additional steps in the scientific process that add value to a researcher׳s work, and more importantly, facilitate transparency and repeatability of a researcher׳s work. Historically, peer-reviewed publications did not allow for the full presentation of underlying datasets, which were only shared through personal contact with a scientist. However, with the expanding use of “supporting online material” (SOM) files that accompany digital publication there is an increased expectation that even large datasets can be made accessible to readers. Thus, researchers are faced with the additional task of becoming their own archivist and depositing data in a repository where it can be used by others. This article introduces basic concepts in data archiving and sharing, including major digital repositories for life science data, commonly used digital file formats, and why metadata is an essential element to successful data sharing when machine-readable data is increasingly used in large-scale studies.",2019,,https://doi.org/10.1016/B978-0-12-409548-9.10557-3
964,Dominik Kozjek and Rok Vrabič and Borut Rihtaršič and Peter Butala,Big data analytics for operations management in engineer-to-order manufacturing,"Engineer-to-order manufacturing, Operations management, Data analytics, Industrial data, Data mining, Big data",Manufacturing data offers big potential for improving management of manufacturing operations. The paper addresses an approach to data analytics in engineer-to-order (ETO) manufacturing systems where the product quality and due-date reliability play a key role in management decisionmaking. The objective of the research is to investigate manufacturing data which are collected by a manufacturing execution system (MES) during operations in an ETO enterprise and to develop tools for supporting scheduling of operations. The developed tools can be used for simulation of production and forecasting of potential resource overloads.,2018,,https://doi.org/10.1016/j.procir.2018.03.098
965,Yiheng Chen and Dawei Han,On Big Data and Hydroinformatics,"Big data, Hydroinformatics","Big data is an increasingly hot concept in the past five years in the area of computer science, e-commence, and bioinformatics, because more and more data has been collected by the internet, remote sensing network, wearable devices and the Internet of Things. The big data technology provides techniques and analytical tools to handle large datasets, so that creative ideas and new values can be extracted from them. However, the hydroinformatics research community are not so familiar with big data. This paper provides readers who are embracing the data-rich era with a timely review on big data and its relevant technology, and then points out the relevance with hydroinformatics in three aspects.",2016,,https://doi.org/10.1016/j.proeng.2016.07.443
966,Ibna Zaman and Kayvan Pazouki and Rose Norman and Shervin Younessi and Shirley Coleman,Challenges and Opportunities of Big Data Analytics for Upcoming Regulations and Future Transformation of the Shipping Industry,"Carbon emission, data-oriented, MRV, big data","Shipping is a heavily regulated industry and responsible for around 3% of global carbon emissions. Global trade is highly dependent on shipping which covers around 90% of commercial demand. Now the industry is expected to navigate through many twists and turns of different situations like upcoming regulations, climate change, energy shortages and technological revolutions. Technological development is apparent across all marine sectors due to the rapid development of sensor technology, IT, automation and robotics. The industry must continue to develop at a rapid pace over the next decade in order to be able to adapt to upcoming regulations and market pressure. Ship intelligence will be the driving force shaping the future of the industry. Ships generate a large volume of data from different sources and in different formats. So big data has become the talk of the industry nowadays. Big data analysis discovers correlations between different measurable or unmeasurable parameters to determine hidden patterns and trends. This analysis will have a significant impact on vessel performance monitoring and provide performance prediction, real-time transparency, and decision-making support to the ship operator. Big data will also bring new opportunities and challenges for the maritime industry. It will increase the capability of performance monitoring, remove human error and increase interdependencies of components. However, the industry will have to face many challenges such as data processing, reliability, and data security. Many regulations rely on ship data including the new EU MRV (Monitoring, Reporting and Verification) regulation to quantify the CO2 emissions for ships above 5000 gross tonnage. As a result, ship operators will have to monitor and report the verified amount of CO2 emitted by their vessels on voyages to, from and between EU ports and will also be required to provide information on energy efficiency parameters. The MRV is a data-oriented regulation requiring ship operators to capture and monitor the ship emissions and other related data and although it is a regional regulation at the moment there is scope for the International Maritime Organisation (IMO) to implement it globally in the near future.",2017,,https://doi.org/10.1016/j.proeng.2017.08.182
967,Muhammad Habib ur Rehman and Victor Chang and Aisha Batool and Teh Ying Wah,Big data reduction framework for value creation in sustainable enterprises,"Sustainable enterprises, Value creation, Big data analytics, Data reduction, Business model","Value creation is a major sustainability factor for enterprises, in addition to profit maximization and revenue generation. Modern enterprises collect big data from various inbound and outbound data sources. The inbound data sources handle data generated from the results of business operations, such as manufacturing, supply chain management, marketing, and human resource management, among others. Outbound data sources handle customer-generated data which are acquired directly or indirectly from customers, market analysis, surveys, product reviews, and transactional histories. However, cloud service utilization costs increase because of big data analytics and value creation activities for enterprises and customers. This article presents a novel concept of big data reduction at the customer end in which early data reduction operations are performed to achieve multiple objectives, such as (a) lowering the service utilization cost, (b) enhancing the trust between customers and enterprises, (c) preserving privacy of customers, (d) enabling secure data sharing, and (e) delegating data sharing control to customers. We also propose a framework for early data reduction at customer end and present a business model for end-to-end data reduction in enterprise applications. The article further presents a business model canvas and maps the future application areas with its nine components. Finally, the article discusses the technology adoption challenges for value creation through big data reduction in enterprise applications.",2016,,https://doi.org/10.1016/j.ijinfomgt.2016.05.013
968,Andrea Damiani and Graziano Onder and Vincenzo Valentini,Large databases (Big Data) and evidence-based medicine,,,2018,,https://doi.org/10.1016/j.ejim.2018.05.019
969,Anna Kobusińska and Kamil Pawluczuk and Jerzy Brzeziński,Big Data fingerprinting information analytics for sustainability,"Big Data, Fingerprinting, Web tracking, Security, Analytics","Web-based device fingerprinting is the process of collecting security information through the browser to perform stateless device identification. Fingerprints may then be used to identify and track computing devices in the web. There are various reasons why device-related information may be needed. Among the others, this technique could help to efficiently analyze security information for sustainability. In this paper we introduce a fingerprinting analytics tool that discovers the most appropriate device fingerprints and their corresponding optimal implementations. The fingerprints selected in the result of the performed analysis are used to enrich and improve an open-source fingerprinting analytics tool Fingerprintjs2, daily consumed by hundreds of websites. As a result, the paper provides a noticeable progress in analytics of dozens of values of device fingerprints, and enhances analysis of fingerprints security information.",2018,,https://doi.org/10.1016/j.future.2017.12.061
970,Zhe Liu and Kim-Kwang Raymond Choo and Minghao Zhao,Practical-oriented protocols for privacy-preserving outsourced big data analysis: Challenges and future research directions,"Big data analysis, Privacy-preserving, Outsourced big data, Oblivious RAM, Security, Practical-oriented, Secure query","With the significant increase in the volume, variety, velocity and veracity of data generated, collected and transmitted through computing and networking systems, it is of little surprise that big data analysis and processing is the subject of focus from enterprise, academia and government. Outsourcing is one popular solution considered in big data processing, although security and privacy are two key concerns often attributed to the underutilization of outsourcing and other promising big data analysis and processing technologies. In this paper, we survey the state-of-the-art literature on cryptographic solutions designed to ensure the security and/or privacy in big data outsourcing. For example, we provide concrete examples to explain how these cryptographic solutions can be deployed. We summarize the existing state-of-play before discussing research opportunities.",2017,,https://doi.org/10.1016/j.cose.2016.12.006
971,M. Olmedilla and M.R. Martínez-Torres and S.L. Toral,Harvesting Big Data in social science: A methodological approach for collecting online user-generated content,"Big Data, User-generated content, e-Social science, Computing, Data gathering","Online user-generated content is playing a progressively important role as information source for social scientists seeking for digging out value. Advances procedures and technologies to enable the capture, storage, management, and analysis of the data make possible to exploit increasing amounts of data generated directly by users. In that regard, Big Data is gaining meaning into social science from quantitative datasets side, which differs from traditional social science where collecting data has always been hard, time consuming, and resource intensive. Hence, the emergent field of computational social science is broadening researchers' perspectives. However, it also requires a multidisciplinary approach involving several and different knowledge areas. This paper outlines an architectural framework and methodology to collect Big Data from an electronic Word-of-Mouth (eWOM) website containing user-generated content. Although the paper is written from the social science perspective, it must be also considered together with other complementary disciplines such as data accessing and computing.",2016,,https://doi.org/10.1016/j.csi.2016.02.003
972,Alexander T. Janke and Daniel L. Overbeek and Keith E. Kocher and Phillip D. Levy,Exploring the Potential of Predictive Analytics and Big Data in Emergency Care,,"Clinical research often focuses on resource-intensive causal inference, whereas the potential of predictive analytics with constantly increasing big data sources remains largely unexplored. Basic prediction, divorced from causal inference, is much easier with big data. Emergency care may benefit from this simpler application of big data. Historically, predictive analytics have played an important role in emergency care as simple heuristics for risk stratification. These tools generally follow a standard approach: parsimonious criteria, easy computability, and independent validation with distinct populations. Simplicity in a prediction tool is valuable, but technological advances make it no longer a necessity. Emergency care could benefit from clinical predictions built using data science tools with abundant potential input variables available in electronic medical records. Patients’ risks could be stratified more precisely with large pools of data and lower resource requirements for comparing each clinical encounter to those that came before it, benefiting clinical decisionmaking and health systems operations. The largest value of predictive analytics comes early in the clinical encounter, in which diagnostic and prognostic uncertainty are high and resource-committing decisions need to be made. We propose an agenda for widening the application of predictive analytics in emergency care. Throughout, we express cautious optimism because there are myriad challenges related to database infrastructure, practitioner uptake, and patient acceptance. The quality of routinely compiled clinical data will remain an important limitation. Complementing big data sources with prospective data may be necessary if predictive analytics are to achieve their full potential to improve care quality in the emergency department.",2016,,https://doi.org/10.1016/j.annemergmed.2015.06.024
973,David Opresnik and Marco Taisch,The value of Big Data in servitization,"Servitization, Big Data, Manufacturing, Competitive advantage, Value, Information","Servitization has become a pervasive business strategy among manufacturers, enabling them to undergird their competitive advantage. However, it has at least one weakness. While it is used worldwide also in economies with lower production costs, services in manufacturing are slowly becoming commoditized and will become a necessary, though not sufficient, condition for reaching an above average competitive advantage. Consequently, in this article we propose a new basis for competitive advantage for manufacturing enterprises called a Big Data Strategy in servitization. We scrutinize how manufacturers can exploit the opportunity arising from combined Big Data and servitization. Therefore, the concept of a Big Data Strategy framework in servitization is proposed. The findings are benchmarked against established frameworks in the Big Data and servitization literature. Its impact on competitive advantage is assessed through three theoretical perspectives that increase the validity of the results. The main finding is that, through the proposed strategy, new revenue streams can be created, while opening the possibility to decrease prices for product–services. Through the proposed strategy manufacturers can differentiate themselves from the ones that are already servitizing. This article introduces the possibility of influencing the most important of the five “Vs” in Big Data–Value, in addition to the other four “Vs”—Volume, Variety, Velocity and Verification. As in regards to servitization, the article adds a third layer of added value— “information”, beside the two existing ones: product and service. The results have strategic implications for managers.",2015,,https://doi.org/10.1016/j.ijpe.2014.12.036
974,A.F. Simpao and L.M. Ahumada and M.A. Rehman,Big data and visual analytics in anaesthesia and health care†,"decision support systems, clinical, electronic health records, integrated advanced information management systems, medical informatics","Advances in computer technology, patient monitoring systems, and electronic health record systems have enabled rapid accumulation of patient data in electronic form (i.e. big data). Organizations such as the Anesthesia Quality Institute and Multicenter Perioperative Outcomes Group have spearheaded large-scale efforts to collect anaesthesia big data for outcomes research and quality improvement. Analytics—the systematic use of data combined with quantitative and qualitative analysis to make decisions—can be applied to big data for quality and performance improvements, such as predictive risk assessment, clinical decision support, and resource management. Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces, and it can facilitate performance of cognitive activities involving big data. Ongoing integration of big data and analytics within anaesthesia and health care will increase demand for anaesthesia professionals who are well versed in both the medical and the information sciences.",2015,,https://doi.org/10.1093/bja/aeu552
975,Cheikh {Kacfah Emani} and Nadine Cullot and Christophe Nicolle,Understandable Big Data: A survey,"Big data, Hadoop, Reasoning, Coreference resolution, Entity linking, Information extraction, Ontology alignment","This survey presents the concept of Big Data. Firstly, a definition and the features of Big Data are given. Secondly, the different steps for Big Data data processing and the main problems encountered in big data management are described. Next, a general overview of an architecture for handling it is depicted. Then, the problem of merging Big Data architecture in an already existing information system is discussed. Finally this survey tackles semantics (reasoning, coreference resolution, entity linking, information extraction, consolidation, paraphrase resolution, ontology alignment) in the Big Data context.",2015,,https://doi.org/10.1016/j.cosrev.2015.05.002
976,Krish Krishnan,10 - Building the big data application,"Big data, Business continuity, Research project, Software, Storyboard, User interface","This chapter will discuss the building and delivery of the application. We will look at all aspects of what needs to be done to complete the process from requirements to outcomes, program management, testing, methodology, and all risks and pitfalls. We will discuss KANBAN, budgets and finances, governance, timeline, increase of efficiency, maintenance, support, and application implementation.",2020,,https://doi.org/10.1016/B978-0-12-815746-6.00010-7
977,Jane Englebright and Barbara Caspers,The Role of the Chief Nurse Executive in the Big Data Revolution,,,2016,,https://doi.org/10.1016/j.mnl.2016.01.001
978,Denes V. Agoston,"Chapter 4 - Big Data, Artificial Intelligence, and Machine Learning in Neurotrauma","Big Data, Artificial intelligence and machine learning in neurotrauma","Rapid advances in the collection, storage, and analysis of large volumes of data—Big Data—offer the much-needed help to identify and treat the various pathological conditions triggered by traumatic brain injury (TBI). Big Data (BD) is defined as extremely large, complex, and mostly unstructured data that cannot be analyzed using traditional approaches. BD can be only analyzed by using text mining (TM), artificial intelligence (AI), or machine learning (ML). These approaches can reveal patterns, trends, and associations, critical for understanding the “most complex disease of the most complex organ.” While powerful and successfully tested computational tools are available, using BD approaches in TBI is currently hampered by the limited availability of legacy and/or primary data, by incompatible data formats and standards. This chapter introduces Big Data and Big Data approaches such as text mining, artificial intelligence, and machine learning; outlines the benefits of using BD approaches; and suggests potential solutions that can help using the full potential of BD in TBI. It also identifies necessary changes of how researchers can help ushering in a new era of preclinical and clinical TBI research by recording and storing ALL the data generated and making ALL the data available for BD approaches—text mining, artificial intelligence, and machine learning so new correlations, relationships, and trends can be identified. In turn, these new information will help to develop novel diagnostics, evidence-based treatments, and improve outcomes.",2019,,https://doi.org/10.1016/B978-0-12-809556-0.00004-6
979,Gema Bello-Orgaz and Jason J. Jung and David Camacho,Social big data: Recent achievements and new challenges,"Big data, Data mining, Social media, Social networks, Social-based frameworks and applications","Big data has become an important issue for a large number of research areas such as data mining, machine learning, computational intelligence, information fusion, the semantic Web, and social networks. The rise of different big data frameworks such as Apache Hadoop and, more recently, Spark, for massive data processing based on the MapReduce paradigm has allowed for the efficient utilisation of data mining methods and machine learning algorithms in different domains. A number of libraries such as Mahout and SparkMLib have been designed to develop new efficient applications based on machine learning algorithms. The combination of big data technologies and traditional machine learning algorithms has generated new and interesting challenges in other areas as social media and social networks. These new challenges are focused mainly on problems such as data processing, data storage, data representation, and how data can be used for pattern mining, analysing user behaviours, and visualizing and tracking data, among others. In this paper, we present a revision of the new methodologies that is designed to allow for efficient data mining and information fusion from social media and of the new applications and frameworks that are currently appearing under the “umbrella” of the social networks, social media and big data paradigms.",2016,,https://doi.org/10.1016/j.inffus.2015.08.005
980,Mika Kortesniemi and Virginia Tsapaki and Annalisa Trianni and Paolo Russo and Ad Maas and Hans-Erik Källman and Marco Brambilla and John Damilakis,The European Federation of Organisations for Medical Physics (EFOMP) White Paper: Big data and deep learning in medical imaging and in relation to medical physics profession,,"Big data and deep learning will profoundly change various areas of professions and research in the future. This will also happen in medicine and medical imaging in particular. As medical physicists, we should pursue beyond the concept of technical quality to extend our methodology and competence towards measuring and optimising the diagnostic value in terms of how it is connected to care outcome. Functional implementation of such methodology requires data processing utilities starting from data collection and management and culminating in the data analysis methods. Data quality control and validation are prerequisites for the deep learning application in order to provide reliable further analysis, classification, interpretation, probabilistic and predictive modelling from the vast heterogeneous big data. Challenges in practical data analytics relate to both horizontal and longitudinal analysis aspects. Quantitative aspects of data validation, quality control, physically meaningful measures, parameter connections and system modelling for the future artificial intelligence (AI) methods are positioned firmly in the field of Medical Physics profession. It is our interest to ensure that our professional education, continuous training and competence will follow this significant global development.",2018,,https://doi.org/10.1016/j.ejmp.2018.11.005
981,Kim Hua Tan and YuanZhu Zhan and Guojun Ji and Fei Ye and Chingter Chang,Harvesting big data to enhance supply chain innovation capabilities: An analytic infrastructure based on deduction graph,"Big data, Analytic infrastructure, Competence set, Deduction graph, Supply chain innovation","Today, firms can access to big data (tweets, videos, click streams, and other unstructured sources) to extract new ideas or understanding about their products, customers, and markets. Thus, managers increasingly view data as an important driver of innovation and a significant source of value creation and competitive advantage. To get the most out of the big data (in combination with a firm׳s existing data), a more sophisticated way of handling, managing, analysing and interpreting data is necessary. However, there is a lack of data analytics techniques to assist firms to capture the potential of innovation afforded by data and to gain competitive advantage. This research aims to address this gap by developing and testing an analytic infrastructure based on the deduction graph technique. The proposed approach provides an analytic infrastructure for firms to incorporate their own competence sets with other firms. Case studies results indicate that the proposed data analytic approach enable firms to utilise big data to gain competitive advantage by enhancing their supply chain innovation capabilities.",2015,,https://doi.org/10.1016/j.ijpe.2014.12.034
982,Bilal Jan and Haleem Farman and Murad Khan and Muhammad Imran and Ihtesham Ul Islam and Awais Ahmad and Shaukat Ali and Gwanggil Jeon,Deep learning in big data Analytics: A comparative study,"Big data, Deep learning, Deep belief networks, Convolutional Neural Networks","Deep learning methods are extensively applied to various fields of science and engineering such as speech recognition, image classifications, and learning methods in language processing. Similarly, traditional data processing techniques have several limitations of processing large amount of data. In addition, Big Data analytics requires new and sophisticated algorithms based on machine and deep learning techniques to process data in real-time with high accuracy and efficiency. However, recently, research incorporated various deep learning techniques with hybrid learning and training mechanisms of processing data with high speed. Most of these techniques are specific to scenarios and based on vector space thus, shows poor performance in generic scenarios and learning features in big data. In addition, one of the reason of such failure is high involvement of humans to design sophisticated and optimized algorithms based on machine and deep learning techniques. In this article, we bring forward an approach of comparing various deep learning techniques for processing huge amount of data with different number of neurons and hidden layers. The comparative study shows that deep learning techniques can be built by introducing a number of methods in combination with supervised and unsupervised training techniques.",2019,,https://doi.org/10.1016/j.compeleceng.2017.12.009
983,George A. Heckman and John P. Hirdes and Robert S. McKelvie,The Role of Physicians in the Era of Big Data,,,2020,,https://doi.org/10.1016/j.cjca.2019.09.018
984,Kannan Govindan and T.C.E. Cheng and Nishikant Mishra and Nagesh Shukla,Big data analytics and application for logistics and supply chain management,"Big data analytics, Supply chain management, Logistics","This special issue explores big data analytics and applications for logistics and supply chain management by examining novel methods, practices, and opportunities. The articles present and analyse a variety of opportunities to improve big data analytics and applications for logistics and supply chain management, such as those through exploring technology-driven tracking strategies, financial performance relations with data driven supply chains, and implementation issues and supply chain capability maturity with big data. This editorial note summarizes the discussions on the big data attributes, on effective practices for implementation, and on evaluation and implementation methods.",2018,,https://doi.org/10.1016/j.tre.2018.03.011
985,Alessandro Mantelero,Regulating big data. The guidelines of the Council of Europe in the context of the European data protection framework,"Big data, Data protection, Council of Europe, Risk assessment, Data protection by design, Consent, Data anonymization, Open data, Algorithms","In January 2017 the Consultative Committee of Convention 108 adopted its Guidelines on the Protection of Individuals with Regard to the Processing of Personal Data in a World of Big Data. These are the first guidelines on data protection provided by an international body which specifically address the issues surrounding big data applications. This article examines the main provisions of these Guidelines and highlights the approach adopted by the Consultative Committee, which contextualises the traditional principles of data protection in the big data scenario and also takes into account the challenges of the big data paradigm. The analysis of the different provisions adopted focuses primarily on the core of the Guidelines namely the risk assessment procedure. Moreover, the article discusses the novel solutions provided by the Guidelines with regard to the data subject's informed consent, the by-design approach, anonymization, and the role of the human factor in big data-supported decisions. This critical analysis of the Guidelines introduces a broader reflection on the divergent approaches of the Council of Europe and the European Union to regulating data processing. Where the principle-based model of the Council of Europe differs from the approach adopted by the EU legislator in the detailed Regulation (EU) 2016/679. In the light of this, the provisions of the Guidelines and their attempt to address the major challenges of the new big data paradigm set the stage for concluding remarks about the most suitable regulatory model to deal with the different issues posed by the development of technology.",2017,,https://doi.org/10.1016/j.clsr.2017.05.011
986,J.H. Rekha and R. Parvathi,Survey on Software Project Risks and Big Data Analytics,"software project, big data analytics, anlytics tools.","Software project is collaborative enterprise of making a desired software for the client. Each software is unique and is delivered by following the process. The process includes understanding the requirement, planning, designing the software and implementation. Risk occurs in the software project which need attention by the managers and workers to make the project efficient. Big data analytics is commonly used in all fields. Big data deals with huge data which are unstructured. Using analytics tools, it can be chunked down and analyzed to provide valuable solutions. In this paper, a review of risk in software project and big data analytics are briefed out.",2015,,https://doi.org/10.1016/j.procs.2015.04.045
987,Anna Bernasconi,Data quality-aware genomic data integration,"Data quality, Data integration, Data curation, Genomic datasets, Metadata, Interoperability","Genomic data are growing at unprecedented pace, along with new protocols, update polices, formats and guidelines, terminologies and ontologies, which are made available every day by data providers. In this continuously evolving universe, enforcing quality on data and metadata is increasingly critical. While many aspects of data quality are addressed at each individual source, we focus on the need for a systematic approach when data from several sources are integrated, as such integration is an essential aspect for modern genomic data analysis. Data quality must be assessed from many perspectives, including accessibility, currency, representational consistency, specificity, and reliability. In this article we review relevant literature and, based on the analysis of many datasets and platforms, we report on methods used for guaranteeing data quality while integrating heterogeneous data sources. We explore several real-world cases that are exemplary of more general underlying data quality problems and we illustrate how they can be resolved with a structured method, sensibly applicable also to other biomedical domains. The overviewed methods are implemented in a large framework for the integration of processed genomic data, which is made available to the research community for supporting tertiary data analysis over Next Generation Sequencing datasets, continuously loaded from many open data sources, bringing considerable added value to biological knowledge discovery.",2021,,https://doi.org/10.1016/j.cmpbup.2021.100009
988,Anne Louise Coleman,How Big Data Informs Us About Cataract Surgery: The LXXII Edward Jackson Memorial Lecture,,"Purpose
To characterize the role of Big Data in evaluating quality of care in ophthalmology, to highlight opportunities for studying quality improvement using data available in the American Academy of Ophthalmology Intelligent Research in Sight (IRIS) Registry, and to show how Big Data informs us about rare events such as endophthalmitis after cataract surgery.
Design
Review of published studies, analysis of public-use Medicare claims files from 2010 to 2013, and analysis of IRIS Registry from 2013 to 2014.
Methods
Statistical analysis of observational data.
Results
The overall rate of endophthalmitis after cataract surgery was 0.14% in 216 703 individuals in the Medicare database. In the IRIS Registry the endophthalmitis rate after cataract surgery was 0.08% among 511 182 individuals. Endophthalmitis rates tended to be higher in eyes with combined cataract surgery and anterior vitrectomy (P = .051), although only 0.08% of eyes had this combined procedure. Visual acuity (VA) in the IRIS Registry in eyes with and without postoperative endophthalmitis measured 1–7 days postoperatively were logMAR 0.58 (standard deviation [SD]: 0.84) (approximately Snellen acuity of 20/80) and logMAR 0.31 (SD: 0.34) (approximately Snellen acuity of 20/40), respectively. In 33 547 eyes with postoperative VA after cataract surgery, 18.3% had 1-month-postoperative VA worse than 20/40.
Conclusions
Big Data drawing on Medicare claims and IRIS Registry records can help identify additional areas for quality improvement, such as in the 18.3% of eyes in the IRIS Registry having 1-month-postoperative VA worse than 20/40. The ability to track patient outcomes in Big Data sets provides opportunities for further research on rare complications such as postoperative endophthalmitis and outcomes from uncommon procedures such as cataract surgery combined with anterior vitrectomy. But privacy and data-security concerns associated with Big Data should not be taken lightly.",2015,,https://doi.org/10.1016/j.ajo.2015.09.028
989,Ivana Semanjski and Sidharta Gautama and Rein Ahas and Frank Witlox,Spatial context mining approach for transport mode recognition from mobile sensed big data,"Transport mode recognition, Mobile sensed big data, Spatial awareness, Geographic information systems, Smart city, Support vector machines, Context mining, Urban data","Knowledge about what transport mode people use is important information of any mobility or travel behaviour research. With ubiquitous presence of smartphones, and its sensing possibilities, new opportunities to infer transport mode from movement data are appearing. In this paper we investigate the role of spatial context of human movements in inferring transport mode from mobile sensed data. For this we use data collected from more than 8000 participants over a period of four months, in combination with freely available geographical information. We develop a support vectors machines-based model to infer five transport modes and achieve success rate of 94%. The developed model is applicable across different mobile sensed data, as it is independent on the integration of additional sensors in the device itself. Furthermore, suggested approach is robust, as it strongly relies on pre-processed data, which makes it applicable for big data implementations in (smart) cities and other data-driven mobility platforms.",2017,,https://doi.org/10.1016/j.compenvurbsys.2017.07.004
990,Shahriar Akter and Samuel Fosso Wamba and Angappa Gunasekaran and Rameshwar Dubey and Stephen J. Childe,How to improve firm performance using big data analytics capability and business strategy alignment?,"Capabilities, Entanglement view, Big data analytics, Hierarchical modeling","The recent interest in big data has led many companies to develop big data analytics capability (BDAC) in order to enhance firm performance (FPER). However, BDAC pays off for some companies but not for others. It appears that very few have achieved a big impact through big data. To address this challenge, this study proposes a BDAC model drawing on the resource-based theory (RBT) and the entanglement view of sociomaterialism. The findings show BDAC as a hierarchical model, which consists of three primary dimensions (i.e., management, technology, and talent capability) and 11 subdimensions (i.e., planning, investment, coordination, control, connectivity, compatibility, modularity, technology management knowledge, technical knowledge, business knowledge and relational knowledge). The findings from two Delphi studies and 152 online surveys of business analysts in the U.S. confirm the value of the entanglement conceptualization of the higher-order BDAC model and its impact on FPER. The results also illuminate the significant moderating impact of analytics capability–business strategy alignment on the BDAC–FPER relationship.",2016,,https://doi.org/10.1016/j.ijpe.2016.08.018
991,Qi Liu and Gengzhong Feng and Xi Zhao and Wenlong Wang,Minimizing the data quality problem of information systems: A process-based method,"Data quality, Information system, Petri net, Optimization model, Process model","The low quality of data in information systems poses enormous risks to business operations and decision making. In this paper, a single-period resource allocation problem for controlling the information system's data quality problem is considered. We develop a Data-Quality-Petri net to capture the process through which data quality problem generates, propagates, and accumulates in the information system. The net considers not only the factors leading to the production of the data quality problem by the data operation nodes and the data flow structure, but also the data transfer ratio of the nodes. Then, we propose a nonlinear programming optimization model with control resource constraints. The result of the model provides an optimal strategy to allocate resources for minimizing the expected data quality problem of an information system. Further, we examine the impact of the data flow structure on optimal resource allocation. The result shows that the optimal resource input level for a data operation node is proportional to its potential for downstream propagation. A warehouse management system of an e-commerce company is utilized to illustrate the model. Our study provides a method for data managers to control the information system's data quality problem by employing a process perspective.",2020,,https://doi.org/10.1016/j.dss.2020.113381
992,Ajay Kumar and Ravi Shankar and Lakshman S. Thakur,A big data driven sustainable manufacturing framework for condition-based maintenance prediction,"data driven sustainable enterprise, fuzzy unordered induction algo, big data analytics, condition-based maintenance, machine learning techniques, backward feature elimination","Smart manufacturing refers to a future-state of manufacturing and it can lead to remarkable changes in all aspects of operations through minimizing energy and material usage while simultaneously maximizing sustainability enabling a futuristic more digitalized scenario of manufacturing. This research develops a big data analytics framework that optimizes the maintenance schedule through condition-based maintenance (CBM) optimization and also improves the prediction accuracy to quantify the remaining life prediction uncertainty. Through effective utilization of condition monitoring and prediction information, CBM would enhance equipment reliability leading to reduction in maintenance cost. The proposed framework uses a CBM optimization method that utilizes a new linguistic interval-valued fuzzy reasoning method for predicting the information. The proposed big data analytics framework in our study for estimating the uncertainty based on backward feature elimination and fuzzy unordered rule induction algorithm prediction errors, is an innovative contribution to the remaining life prediction field. Our paper elaborates on the basic underlying structure of CBM system that is defined by transaction matrix and the threshold value of failure probability. We developed this framework for analysing the CBM policy cost more accurately and to find the probabilistic threshold values of covariate that corresponds to the lowest price of predictive maintenance cost. The experimental results are performed on a big dataset which is generated from a sophisticated simulator of a gas turbine propulsion plant. A comparative analysis confirms that the method used in the proposed framework outpaces the classical methods in terms of classification accuracy and other statistical performance evaluation metrics.",2018,,https://doi.org/10.1016/j.jocs.2017.06.006
993,Choiru Za’in and Mahardhika Pratama and Edwin Lughofer and Meftahul Ferdaus and Qing Cai and Mukesh Prasad,Big Data Analytics based on PANFIS MapReduce,"Big data stream analytic, Distributed evolving algorithm, Scalable real-time data mining, Parallel learning, Rule merging strategy","In this paper, a big data analytic framework is introduced for processing high-frequency data stream. This framework architecture is developed by combining an advanced evolving learning algorithm namely Parsimonious Network Fuzzy Inference System (PANFIS) with MapReduce parallel computation, where PANFIS has the capability of processing data stream in large volume. Big datasets are learnt chunk by chunk by processors in MapReduce environment and the results are fused by rule merging method, that reduces the complexity of the rules. The performance measurement has been conducted, and the results are showing that the MapReduce framework along with PANFIS evolving system helps to reduce the processing time around 22 percent in average in comparison with the PANFIS algorithm without reducing performance in accuracy.",2018,,https://doi.org/10.1016/j.procs.2018.10.514
994,Betty Rambur and Therese Fitzpatrick,A plea to nurse educators: Incorporate big data use as a foundational skill for undergraduate and graduate nurses,,,2018,,https://doi.org/10.1016/j.profnurs.2017.10.005
995,Fadoua Khennou and Youness Idrissi Khamlichi and Nour El Houda Chaoui,Improving the Use of Big Data Analytics within Electronic Health Records: A Case Study based OpenEHR,"Electronic Health Records, EHRs, Analytic tools, Big Data, Health Practitioners","Recently there has been an increasing adoption of electronic health records (EHRs) in different countries. Thanks to these systems, multiple health bodies can now store, manage and process their data effectively. However, the existence of such powerful and meticulous entities raise new challenges and issues for health practitioners. In fact, while the main objective of EHRs is to gain actionable big data insights from the health workflow, very few physicians exploit widely analytic tools, this is mainly due to the fact of having to deal with multiple systems and steps, which completely discourage them from engaging more and more. In this paper, we shed light and explore precisely the proper adaptation of analytical tools to EHRs in order to upgrade their use by health practitioners. For that, we present a case study of the implementation process of an EHR based OpenEHR and investigate health analytics adoption in each step of the methodology.",2018,,https://doi.org/10.1016/j.procs.2018.01.098
996,Yongcheng Zhang and Hanbin Luo and Yi He,A System for Tender Price Evaluation of Construction Project Based on Big Data,"System, Bid price evaluation, Construction project, Big data","Tender price evaluation of construction project is one of the most important works for the clients to control project cost in the bidding stage. However,the previously underutilization of project cost data made the tender price evaluation of new projects lack of effective evaluation criterion, which brings challenge to cost control. With the improvement of companies’ information technology application and the advent of big data era, the project cost-related data can be completely and systematically recorded in real time, as well as fully utilized to support decision-making for construction project cost management. In this paper, a system for tender price evaluation of construction project based on big data is presented, aiming to use related technique of big data to analysis project cost data to give a reasonable cost range, which contributes to obtaining the evaluation criterion to support the tender price controls. The paper introduced the data sources, data extraction, data storage and data analysis of the system respectively. A case study is conducted in a metro station project to evaluate the system. The results show that the system based on big data is significant for tender price evaluation in construction project.",2015,,https://doi.org/10.1016/j.proeng.2015.10.114
997,Wei Ji and Lihui Wang,Big data analytics based fault prediction for shop floor scheduling,"Big data analytics, Fault prediction, Shop floor, Scheduling","The current task scheduling mainly concerns the availability of machining resources, rather than the potential errors after scheduling. To minimise such errors in advance, this paper presents a big data analytics based fault prediction approach for shop floor scheduling. Within the context, machining tasks, machining resources, and machining processes are represented by data attributes. Based on the available data on the shop floor, the potential fault/error patterns, referring to machining errors, machine faults and maintenance states, are mined for unsuitable scheduling arrangements before machining as well as upcoming errors during machining. Comparing the data-represented tasks with the mined error patterns, their similarities or differences are calculated. Based on the calculated similarities, the fault probabilities of the scheduled tasks or the current machining tasks can be obtained, and they provide a reference of decision making for scheduling and rescheduling the tasks. By rescheduling high-risk tasks carefully, the potential errors can be avoided. In this paper, the architecture of the approach consisting of three steps in three levels is proposed. Furthermore, big data are considered in three levels, i.e. local data, local network data and cloud data. In order to implement this idea, several key techniques are illustrated in detail, e.g. data attribute, data cleansing, data integration of databases in different levels, and big data analytic algorithms. Finally, a simplified case study is described to show the prediction process of the proposed method.",2017,,https://doi.org/10.1016/j.jmsy.2017.03.008
998,Thomas Lefèvre,Big data in forensic science and medicine,"Forensic science, Big data, Personalized medicine, Predictive medicine, Machine learning, Dimensionality","In less than a decade, big data in medicine has become quite a phenomenon and many biomedical disciplines got their own tribune on the topic. Perspectives and debates are flourishing while there is a lack for a consensual definition for big data. The 3Vs paradigm is frequently evoked to define the big data principles and stands for Volume, Variety and Velocity. Even according to this paradigm, genuine big data studies are still scarce in medicine and may not meet all expectations. On one hand, techniques usually presented as specific to the big data such as machine learning techniques are supposed to support the ambition of personalized, predictive and preventive medicines. These techniques are mostly far from been new and are more than 50 years old for the most ancient. On the other hand, several issues closely related to the properties of big data and inherited from other scientific fields such as artificial intelligence are often underestimated if not ignored. Besides, a few papers temper the almost unanimous big data enthusiasm and are worth attention since they delineate what is at stakes. In this context, forensic science is still awaiting for its position papers as well as for a comprehensive outline of what kind of contribution big data could bring to the field. The present situation calls for definitions and actions to rationally guide research and practice in big data. It is an opportunity for grounding a true interdisciplinary approach in forensic science and medicine that is mainly based on evidence.",2018,,https://doi.org/10.1016/j.jflm.2017.08.001
999,Sadok Ben Yahia and Anne Laurent and Gabriella Pasi,Preface: Special Issue on Big Data,,,2018,,https://doi.org/10.1016/j.fss.2018.05.022
1000,Stefano Nativi and Paolo Mazzetti and Mattia Santoro and Fabrizio Papeschi and Max Craglia and Osamu Ochiai,Big Data challenges in building the Global Earth Observation System of Systems,"GEOSS, Big Data, Multidisciplinary systems, Earth System Science, Research infrastructures, Interoperability, Cloud systems","There are many expectations and concerns about Big Data in the sector of Earth Observation. It is necessary to understand whether Big Data is a radical shift or an incremental change for the existing digital infrastructures. This manuscript explores the impact of Big Data dimensionalities (commonly known as ‘V’ axes: volume, variety, velocity, veracity, visualization) on the Global Earth Observation System of Systems (GEOSS) and particularly its common digital infrastructure (i.e. the GEOSS Common Infrastructure). GEOSS is a global and flexible network of content providers allowing decision makers to access an extraordinary range of data and information. GEOSS is a pioneering framework for global and multidisciplinary data sharing in the EO realm. The manuscript introduces and discusses the general GEOSS strategies to address Big Data challenges, focusing on the cloud-based discovery and access solutions. A final section reports the results of the scalability and flexibility performance tests.",2015,,https://doi.org/10.1016/j.envsoft.2015.01.017
1001,C.J. Puranik and Sreenivasa Rao and S. Chennamaneni,The perils and pitfalls of big data analysis in medicine,,,2019,,https://doi.org/10.1016/j.jtos.2019.07.010
1002,Stuart Maudsley and Viswanath Devanarayan and Bronwen Martin and Hugo Geerts,Intelligent and effective informatic deconvolution of “Big Data” and its future impact on the quantitative nature of neurodegenerative disease therapy,"Big data, Informatics, High-dimensionality, Alzheimer's disease, Aging, Molecular signature, Transcriptomics, Metabolomics, Proteomics, Genomics","Biomedical data sets are becoming increasingly larger and a plethora of high-dimensionality data sets (“Big Data”) are now freely accessible for neurodegenerative diseases, such as Alzheimer's disease. It is thus important that new informatic analysis platforms are developed that allow the organization and interrogation of Big Data resources into a rational and actionable mechanism for advanced therapeutic development. This will entail the generation of systems and tools that allow the cross-platform correlation between data sets of distinct types, for example, transcriptomic, proteomic, and metabolomic. Here, we provide a comprehensive overview of the latest strategies, including latent semantic analytics, topological data investigation, and deep learning techniques that will drive the future development of diagnostic and therapeutic applications for Alzheimer's disease. We contend that diverse informatic “Big Data” platforms should be synergistically designed with more advanced chemical/drug and cellular/tissue-based phenotypic analytical predictive models to assist in either de novo drug design or effective drug repurposing.",2018,,https://doi.org/10.1016/j.jalz.2018.01.014
1003,Antonio {Núñez Reiz} and Fernando {Martínez Sagasti} and Manuel {Álvarez González} and Antonio {Blesa Malpica} and Juan Carlos {Martín Benítez} and Mercedes {Nieto Cabrera} and Ángela {del Pino Ramírez} and José Miguel {Gil Perdomo} and Jesús {Prada Alonso} and Leo Anthony Celi and Miguel Ángel {Armengol de la Hoz} and Rodrigo Deliberato and Kenneth Paik and Tom Pollard and Jesse Raffa and Felipe Torres and Julio Mayol and Joan Chafer and Arturo {González Ferrer} and Ángel Rey and Henar {González Luengo} and Giuseppe Fico and Ivana Lombroni and Liss Hernandez and Laura López and Beatriz Merino and María Fernanda Cabrera and María Teresa Arredondo and María Bodí and Josep Gómez and Alejandro Rodríguez and Miguel {Sánchez García},Big data and machine learning in critical care: Opportunities for collaborative research,"Big data, Machine learning, Artificial intelligence, Clinical databases, MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases de datos clínicos, MIMIC III, Datathon, Trabajo colaborativo","The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.
Resumen
La aparición de los sistemas de información clínica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos clínicos en formato electrónico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas clínicas, para su uso en la gestión de recursos o para sugerir líneas de investigación que luego pueden ser explotadas mediante ensayos clínicos aleatorizados. Sin embargo, los médicos clínicos carecen de la formación necesaria para la explotación de grandes bases de datos, lo que supone un obstáculo para aprovechar esta oportunidad. Además, existen cuestiones de índole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estadísticos, epidemiólogos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigación clínica o análisis de resultados (benchmarking). Se describe la reunión multidisciplinar (Critical Care Datathon) realizada en Madrid los días 1, 2 y 3 de diciembre de 2017. Esta reunión, celebrada bajo los auspicios de la Sociedad Española de Medicina Intensiva, Crítica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovación y el Servicio de Medicina Intensiva del Hospital Clínico San Carlos, así como el grupo de investigación «Life Supporting Technologies» de la Universidad Politécnica de Madrid. Tras unas ponencias de formación sobre big data, seguridad y calidad de los datos, y su aplicación al entorno de la medicina intensiva, un grupo de clínicos, analistas de datos, estadísticos, expertos en seguridad informática de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas clínicas establecidas previamente a la reunión. El trabajo colaborativo permitió establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias líneas de investigación clínica a desarrollar en el futuro. Además, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en España, y se estableció un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparación de resultados entre unidades o la elaboración de nuevas líneas de investigación clínica.",2019,,https://doi.org/10.1016/j.medin.2018.06.002
1004,Xinqing Gao and Fan Yang and Chao Shang and Dexian Huang,A review of control loop monitoring and diagnosis: Prospects of controller maintenance in big data era,"Control loop performance assessment, Industrial alarm system, Process knowledge, Root cause diagnosis, Big data","Owing to wide applications of automatic control systems in the process industries, the impacts of controller performance on industrial processes are becoming increasingly significant. Consequently, controller maintenance is critical to guarantee routine operations of industrial processes. The workflow of controller maintenance generally involves the following steps: monitor operating controller performance and detect performance degradation, diagnose probable root causes of control system malfunctions, and take specific actions to resolve associated problems. In this article, a comprehensive overview of the mainstream of control loop monitoring and diagnosis is provided, and some existing problems are also analyzed and discussed. From the viewpoint of synthesizing abundant information in the context of big data, some prospective ideas and promising methods are outlined to potentially solve problems in industrial applications.",2016,,https://doi.org/10.1016/j.cjche.2016.05.039
1005,Dominique Duncan and Paul Vespa and Asla Pitkänen and Adebayo Braimah and Niina Lapinlampi and Arthur W. Toga,Big data sharing and analysis to advance research in post-traumatic epilepsy,"Biomarkers, EEG, Epilepsy, Epileptogenesis, Informatics, MRI, Neuroimaging, TBI","We describe the infrastructure and functionality for a centralized preclinical and clinical data repository and analytic platform to support importing heterogeneous multi-modal data, automatically and manually linking data across modalities and sites, and searching content. We have developed and applied innovative image and electrophysiology processing methods to identify candidate biomarkers from MRI, EEG, and multi-modal data. Based on heterogeneous biomarkers, we present novel analytic tools designed to study epileptogenesis in animal model and human with the goal of tracking the probability of developing epilepsy over time.",2019,,https://doi.org/10.1016/j.nbd.2018.05.026
1006,Naveen Garg and Sanjay Singla and Surender Jangra,Challenges and Techniques for Testing of Big Data,"Big Data, Testing, Verasity, Hadoop","Big Data, the new buzz word in the industry, is data that exceeds the processing and analytic capacity of conventional database systems within the time necessary to make them useful. With multiple data stores in abundant formats, billions of rows of data with hundreds of millions of data combinations and the urgent need of making best possible decisions, the challenge is big and the solution bigger, Big Data. Comes with it, new advances in computing technology together with its high performance analytics for simpler and faster processing of only relevant data to enable timely and accurate insights using data mining and predictive analytics, text mining, forecasting and optimization on complex data to continuously drive innovation and make the best possible decisions. While Big Data provides solutions to complex business problems like analyzing larger volumes of data than was previously possible to drive more precise answers, analyzing data in motion to capture opportunities that were previously lost, it poses bigger challenges in testing these scenarios. Testing such highly volatile data, which is more often than not unstructured generated from myriad sources such as web logs, radio frequency Id (RFID), sensors embedded in devices, GPS systems etc. and mostly clustered data for its accuracy, high availability, security requires specialization. One of the most challenging things for a tester is to keep pace with changing dynamics of the industry. While on most aspects of testing, the tester need not know the technical details behind the scene however this is where testing Big Data Technology is so different. A tester not only needs to be strong on testing fundamentals but also has to be equally aware of minute details in the architecture of the database designs to analyze several performance bottlenecks and other issues. Like in the example quoted above on In-Memory databases, a tester would need to know how the operating systems allocate and de-allocate memory and understand how much memory is being used at any given time. So, concluding, as the data- analytics Industry evolves further we would see the IT Testing Services getting closely aligned with the Database Engineering and the industry would need more skilled testing professional in this domain to grab the new opportunities.",2016,,https://doi.org/10.1016/j.procs.2016.05.285
1007,Guillaume Taglang and David B. Jackson,Use of “big data” in drug discovery and clinical trials,"Big data, Drug discovery, Clinical trials, Precision medicine, Biomarkers","Oncology is undergoing a data-driven metamorphosis. Armed with new and ever more efficient molecular and information technologies, we have entered an era where data is helping us spearhead the fight against cancer. This technology driven data explosion, often referred to as “big data”, is not only expediting biomedical discovery, but it is also rapidly transforming the practice of oncology into an information science. This evolution is critical, as results to-date have revealed the immense complexity and genetic heterogeneity of patients and their tumors, a sobering reminder of the challenge facing every patient and their oncologist. This can only be addressed through development of clinico-molecular data analytics that provide a deeper understanding of the mechanisms controlling the biological and clinical response to available therapeutic options. Beyond the exciting implications for improved patient care, such advancements in predictive and evidence-based analytics stand to profoundly affect the processes of cancer drug discovery and associated clinical trials.",2016,,https://doi.org/10.1016/j.ygyno.2016.02.022
1008,Jiale Lei and Linghe Kong,2 - Fundamentals of big data in radio astronomy,"Big data, Astronomy, Statistical challenges, Astronomical data analysis, Platforms for big data process","Large digital sky surveys are becoming the dominant source of data in astronomy. There are more than 100 terabytes of data in major archives, and that amount is growing rapidly. A typical sky survey archive has approximately 10 terabytes of image data and a billion detected sources (stars, galaxies, quasars, etc.), with hundreds of measured attributes per source. These surveys span the full range of wavelengths, radio through gamma ray, yet they are just a taste of the much larger datasets to come. Yearly advances in electronics bring new instruments that double the amount of data collected each year and lead to the exponential growth of information in astronomy. Thus, datasets that are orders of magnitude larger, more complex, and more homogeneous than in the past are on the horizon. In comparison, the size of the human genome is about 1 gigabyte and that of the Library of Congress is about 20 terabytes. Truly, astronomy has come to the big data era.",2020,,https://doi.org/10.1016/B978-0-12-819084-5.00010-9
1009,Darshan Lopes and Kevin Palmer and Fiona O'Sullivan,Chapter 10 - Big Data: A Practitioners Perspective,"Pitfalls, Considerations, Implementation, Migration pattern, Practitioner's perspective, Open source, Data warehouse","Big data solutions represent a significant challenge for some organizations. There are a huge variety of software products, deployment patterns and solution options that need to be considered to ensure a successful outcome for an organization trying to implement a big data solution. With that in mind, the chapter “Big Data: a practitioner's perspective” will focus on four key areas associated with big data that require consideration from a practical and implementation perspective: (i) Big Data is a new Paradigm – Differences with Traditional Data Warehouse, Pitfalls and Considerations; (ii) Product considerations for Big Data – Use of Open Source products for Big Data, Pitfalls and Considerations; (iii) Use of Cloud for hosting Big Data – Why use Cloud, Pitfalls and Considerations; and (iv) Big Data Implementation – Architecture definition, processing framework and migration patterns from Data Warehouse to Big Data.",2017,,https://doi.org/10.1016/B978-0-12-805467-3.00010-7
1010,Sunho Kim and Ricardo Pérez-Castillo and Ismael Caballero and Downgwoo Lee,Organizational process maturity model for IoT data quality management,"Data quality, Data quality management, IoT, ISO 8000, Process-centric, Process reference model, Maturity, Process maturity, process attribute","Data quality management (DQM) is one of the most critical aspects to ensure successful applications of the Internet of Things (IoT). So far, most of the approaches for assuring data quality are typically data-centric, i.e., mainly focus on fixing data issues for specific values. However, organizations can also benefit from improving their capabilities of their DQM processes by developing organizational best DQM practices. In this regard, our investigation addresses how well organizations perform their DQM processes in the IoT domain. The main contribution of this study is to establish a framework for IoT DQM maturity. This framework is compliant with ISO 8000-61 (DQM: process reference model) and ISO 8000-62 (DQM: organizational process maturity assessment) and can be used to assess and improve the capabilities of the DQM processes for IoT data. The framework is composed of two elements. First, a process reference model (PRM) for IoT DQM is proposed by extending the PRM for DQM defined in ISO 8000-61, tailoring some existing processes and adding new ones. Second, a maturity model suitable for IoT data is proposed based on the PRM for IoT DQM. The maturity model, named IoT DQM3, is proposed by extending the maturity model defined in ISO 8000-62. However, in order to increase the usability of IoT DQM3, we consider adequate the proposition of a simplification of the IoT DQM3, by introducing a lightweight version to reduce assessment indicators and facilitate its industrial adoption. A simplified method to measure the capability of a process is also suggested considering the relationship of process attributes with the measurement stack defined in ISO 8000-63. The empirical validation of the maturity model is twofold. First, the appropriateness of the two models is surveyed with data quality experts who are currently working in various organizations around the world. Second, in order to demonstrate the feasibility of the proposal, the light-weight version is applied to a manufacturing company as a case study.",2022,,https://doi.org/10.1016/j.jii.2021.100256
1011,John F. Brothers and Matthew Ung and Renan Escalante-Chong and Jermaine Ross and Jenny Zhang and Yoonjeong Cha and Andrew Lysaght and Jason Funt and Rebecca Kusko,"Integrity, standards, and QC-related issues with big data in pre-clinical drug discovery","Big data, Genomics, Transcriptomics, RNA-seq, Microarray, Exome","The tremendous expansion of data analytics and public and private big datasets presents an important opportunity for pre-clinical drug discovery and development. In the field of life sciences, the growth of genetic, genomic, transcriptomic and proteomic data is partly driven by a rapid decline in experimental costs as biotechnology improves throughput, scalability, and speed. Yet far too many researchers tend to underestimate the challenges and consequences involving data integrity and quality standards. Given the effect of data integrity on scientific interpretation, these issues have significant implications during preclinical drug development. We describe standardized approaches for maximizing the utility of publicly available or privately generated biological data and address some of the common pitfalls. We also discuss the increasing interest to integrate and interpret cross-platform data. Principles outlined here should serve as a useful broad guide for existing analytical practices and pipelines and as a tool for developing additional insights into therapeutics using big data.",2018,,https://doi.org/10.1016/j.bcp.2018.03.014
1012,Sandra de F. {Mendes Sampaio} and Chao Dong and Pedro Sampaio,DQ2S – A framework for data quality-aware information management,"Information management, Data quality, Query language extensions, Data profiling, Decision support systems, Big data","This paper describes the design and implementation of the Data Quality Query System (DQ2S), a query processing framework and tool incorporating data quality profiling functionality in the processing of queries involving quality-aware query language extensions. DQ2S supports the combination of performance and quality-oriented query optimizations, and a query processing platform that enables advanced data profiling queries to be formulated based on well established query language constructs, often used to interact with relational database management systems. DQ2S encompasses a declarative query language and a data model that provides users with the capability to express constraints on the quality of query results as well as query quality-related information; a set of algebraic operators for manipulating data quality-related information, and optimization heuristics. The proposed query language and algebra represent seamless extensions to SQL and relational database engines, respectively. The constructs of the proposed data model are implemented at the user’s view level and are internally mapped into relational model constructs. The quality-aware extensions and features are extremely useful when users need to assess the quality of relational data sets and define quality constraints for acceptable data prior to using candidate data sources in decision support systems and conducting big data analytical tasks.",2015,,https://doi.org/10.1016/j.eswa.2015.06.050
1013,Romain Pirracchio and Mitchell J Cohen and Ivana Malenica and Jonathan Cohen and Antoine Chambaz and Maxime Cannesson and Christine Lee and Matthieu Resche-Rigon and Alan Hubbard,Big data and targeted machine learning in action to assist medical decision in the ICU,,"Historically, personalised medicine has been synonymous with pharmacogenomics and oncology. We argue for a new framework for personalised medicine analytics that capitalises on more detailed patient-level data and leverages recent advances in causal inference and machine learning tailored towards decision support applicable to critically ill patients. We discuss how advances in data technology and statistics are providing new opportunities for asking more targeted questions regarding patient treatment, and how this can be applied in the intensive care unit to better predict patient-centred outcomes, help in the discovery of new treatment regimens associated with improved outcomes, and ultimately how these rules can be learned in real-time for the patient.",2019,,https://doi.org/10.1016/j.accpm.2018.09.008
1014,Michael Alles and Glen L. Gray,Incorporating big data in audits: Identifying inhibitors and a research agenda to address those inhibitors,"Big Data, Auditing, Accounting information systems","With corporate investment in Big Data of $34 billion in 2013 growing to $232 billion through 2016 (Gartner 2012), the Big 4 accounting firms are aiming to be at the forefront of Big Data implementations. Notably, they see Big Data as an increasingly essential part of their assurance practice. We argue that while there is a place for Big Data in auditing, its application to auditing is less clear than it is in the other fields, such as marketing and medical research. The objectives of this paper are to: (1) provide a discussion of both the inhibitors of incorporating Big Data into financial statement audits; and (3) present a research agenda to identify approaches to ameliorate those inhibitors.",2016,,https://doi.org/10.1016/j.accinf.2016.07.004
1015,Yi Liu and Jie Xu and Weijie Yi,Massive-scale carbon pollution control and biological fusion under big data context,"Internet-scale network, Dense subgraph mining, Low carbon, Multiple features, Biologically-aware","In the modern society, there are a rich number of low-carbon enterprises that the explicitly/implicitly collaborated. Effectively understanding the mechanism of their complex cooperative relationships is becoming an urgent and significant problem in information processing and management. Traditionally, these cooperation behavior are analyzed in a holistic and non-quantitative way, where the complicated relationships among various enterprises cannot be well represented. In this work, we propose to understand the low-carbon entrepreneurs’ cooperation by leveraging a massive-scale dense subgraph mining technique, based on which an evolutionary graphical model is built to dynamically represent such complex relationships. More specifically, given million-scale low-carbon enterprises, we first extract multiple biologically-aware features (e.g., production value and carbon emission) to represent each of them. Based on this, a massive-scale affinity network is constructed to characterize the relationships among these enterprises. Based on this, an efficient subgraph mining algorithm (called graph shift) is deployed to discover the neighbors for each enterprise. Finally, based on the discovered neighbors of each enterprise, we can build a graphical model to represent the relationships among explicitly/implicitly-connected enterprises. The flows of multiple attributes (benefit exchange and resources swap) can be modeled effectively. To demonstrate the usefulness of our method, we manually label the attributes of 20,000 enterprises, based on which a classification model is trained by encoding the neighboring attributes of each enterprise. Comparative results have clearly demonstrated the competitiveness of our method. Moreover, visualization results can reveal the effectiveness of our method in uncovering the intrinsic distributions/correlations of million-scale enterprises.",2021,,https://doi.org/10.1016/j.future.2021.01.002
1016,B. Saraladevi and N. Pazhaniraja and P. Victer Paul and M.S. Saleem Basha and P. Dhavachelvan,Big Data and Hadoop-a Study in Security Perspective,Big data ;Hadoop ;HDFS ;Security,"Big data is the collection and analysis of large set of data which holds many intelligence and raw information based on user data, Sensor data, Medical and Enterprise data. The Hadoop platform is used to Store, Manage, and Distribute Big data across several server nodes. This paper shows the Big data issues and focused more on security issue arises in Hadoop Architecture base layer called Hadoop Distributed File System (HDFS). The HDFS security is enhanced by using three approaches like Kerberos, Algorithm and Name node.",2015,,https://doi.org/10.1016/j.procs.2015.04.091
1017,Pingjian Yu and Wei Lin,Single-cell Transcriptome Study as Big Data,"Single cell, RNA-seq, Big data, Transcriptional heterogeneity, Signal normalization","The rapid growth of single-cell RNA-seq studies (scRNA-seq) demands efficient data storage, processing, and analysis. Big-data technology provides a framework that facilitates the comprehensive discovery of biological signals from inter-institutional scRNA-seq datasets. The strategies to solve the stochastic and heterogeneous single-cell transcriptome signal are discussed in this article. After extensively reviewing the available big-data applications of next-generation sequencing (NGS)-based studies, we propose a workflow that accounts for the unique characteristics of scRNA-seq data and primary objectives of single-cell studies.",2016,,https://doi.org/10.1016/j.gpb.2016.01.005
1018,Sylvie Koziel and Patrik Hilber and Per Westerlund and Ebrahim Shayesteh,Investments in data quality: Evaluating impacts of faulty data on asset management in power systems,"Asset management, Component replacement, Data quality costs, Electric power distribution, Optimization, Trade-off","Data play an essential role in asset management decisions. The amount of data is increasing through accumulating historical data records, new measuring devices, and communication technology, notably with the evolution toward smart grids. Consequently, the management of data quantity and quality is becoming even more relevant for asset managers to meet efficiency and reliability requirements for power grids. In this work, we propose an innovative data quality management framework enabling asset managers (i) to quantify the impact of poor data quality, and (ii) to determine the conditions under which an investment in data quality improvement is required. To this end, an algorithm is used to determine the optimal year for component replacement based on three scenarios, a Reference scenario, an Imperfect information scenario, and an Investment in higher data quality scenario. Our results indicate that (i) the impact on the optimal year of replacement is the highest for middle-aged components; (ii) the profitability of investments in data quality improvement depends on various factors, including data quality, and the cost of investment in data quality improvement. Finally, we discuss the implementation of the proposed models to control data quality in practice, while taking into account real-world technological and economic limitations.",2021,,https://doi.org/10.1016/j.apenergy.2020.116057
1019,Marcos D. Assunção and Rodrigo N. Calheiros and Silvia Bianchi and Marco A.S. Netto and Rajkumar Buyya,Big Data computing and clouds: Trends and future directions,"Big Data, Cloud computing, Analytics, Data management","This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions.",2015,,https://doi.org/10.1016/j.jpdc.2014.08.003
1020,Sjaak Wolfert and Lan Ge and Cor Verdouw and Marc-Jeroen Bogaardt,Big Data in Smart Farming – A review,"Agriculture, Data, Information and communication technology, Data infrastructure, Governance, Business modelling","Smart Farming is a development that emphasizes the use of information and communication technology in the cyber-physical farm management cycle. New technologies such as the Internet of Things and Cloud Computing are expected to leverage this development and introduce more robots and artificial intelligence in farming. This is encompassed by the phenomenon of Big Data, massive volumes of data with a wide variety that can be captured, analysed and used for decision-making. This review aims to gain insight into the state-of-the-art of Big Data applications in Smart Farming and identify the related socio-economic challenges to be addressed. Following a structured approach, a conceptual framework for analysis was developed that can also be used for future studies on this topic. The review shows that the scope of Big Data applications in Smart Farming goes beyond primary production; it is influencing the entire food supply chain. Big data are being used to provide predictive insights in farming operations, drive real-time operational decisions, and redesign business processes for game-changing business models. Several authors therefore suggest that Big Data will cause major shifts in roles and power relations among different players in current food supply chain networks. The landscape of stakeholders exhibits an interesting game between powerful tech companies, venture capitalists and often small start-ups and new entrants. At the same time there are several public institutions that publish open data, under the condition that the privacy of persons must be guaranteed. The future of Smart Farming may unravel in a continuum of two extreme scenarios: 1) closed, proprietary systems in which the farmer is part of a highly integrated food supply chain or 2) open, collaborative systems in which the farmer and every other stakeholder in the chain network is flexible in choosing business partners as well for the technology as for the food production side. The further development of data and application infrastructures (platforms and standards) and their institutional embedment will play a crucial role in the battle between these scenarios. From a socio-economic perspective, the authors propose to give research priority to organizational issues concerning governance issues and suitable business models for data sharing in different supply chain scenarios.",2017,,https://doi.org/10.1016/j.agsy.2017.01.023
1021,Xiaohong Huang and Kun Xie and Supeng Leng and Tingting Yuan and Maode Ma,Improving Quality of Experience in multimedia Internet of Things leveraging machine learning on big data,"Data fusion, Multimedia Internet of Things, Big data, Quality of Experience, Machine learning, Neural network","With rapid evolution of the Internet of Things (IoT) applications on multimedia, there is an urgent need to enhance the satisfaction level of Multimedia IoT (MIoT) network users. An important and unsolved problem is automatic optimization of Quality of Experience (QoE) through collecting/managing/processing various data from MIoT network. In this paper, we propose an MIoT QoE optimization mechanism leveraging data fusion technology, called QoE optimization via Data Fusion (QoEDF). QoEDF consists of two steps. Firstly, a multimodal data fusion approach is proposed to build a QoE mapping between the uncontrollable user data with the controllable network-related system data. Secondly, an automatic QoE optimization model is built taking fused results, which is different from the traditional way. QoEDF is able to adjust network-related system data automatically so as to achieve optimized user satisfaction. Simulation results show that QoEDF will lead to significant improvements in QoE level as well as be adaptable to dynamic network changes.",2018,,https://doi.org/10.1016/j.future.2018.02.046
1022,Jinlin Zhu and Zhiqiang Ge and Zhihuan Song and Furong Gao,Review and big data perspectives on robust data mining approaches for industrial process modeling with outliers and missing data,"Data mining, Robustness, Process modeling, Statistical process monitoring, Big data analytics","Industrial process data are usually mixed with missing data and outliers which can greatly affect the statistical explanation abilities for traditional data-driven modeling methods. In this sense, more attention should be paid on robust data mining methods so as to investigate those stable and reliable modeling prototypes for decision-making. This paper gives a systematic review of various state-of-the-art data preprocessing tricks as well as robust principal component analysis methods for process understanding and monitoring applications. Afterwards, comprehensive robust techniques have been discussed for various circumstances with diverse process characteristics. Finally, big data perspectives on potential challenges and opportunities have been highlighted for future explorations in the community.",2018,,https://doi.org/10.1016/j.arcontrol.2018.09.003
1023,Ibrahim Abaker Targio Hashem and Victor Chang and Nor Badrul Anuar and Kayode Adewole and Ibrar Yaqoob and Abdullah Gani and Ejaz Ahmed and Haruna Chiroma,The role of big data in smart city,"Smart city, Big data, Internet of things, Smart environments, Cloud computing, Distributed computing","The expansion of big data and the evolution of Internet of Things (IoT) technologies have played an important role in the feasibility of smart city initiatives. Big data offer the potential for cities to obtain valuable insights from a large amount of data collected through various sources, and the IoT allows the integration of sensors, radio-frequency identification, and Bluetooth in the real-world environment using highly networked services. The combination of the IoT and big data is an unexplored research area that has brought new and interesting challenges for achieving the goal of future smart cities. These new challenges focus primarily on problems related to business and technology that enable cities to actualize the vision, principles, and requirements of the applications of smart cities by realizing the main smart environment characteristics. In this paper, we describe the state-of-the-art communication technologies and smart-based applications used within the context of smart cities. The visions of big data analytics to support smart cities are discussed by focusing on how big data can fundamentally change urban populations at different levels. Moreover, a future business model of big data for smart cities is proposed, and the business and technological research challenges are identified. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.",2016,,https://doi.org/10.1016/j.ijinfomgt.2016.05.002
1024,David Gil and Il-Yeol Song and José F. Aldana and Juan Trujillo,Big Data. New approaches of modelling and management,,"Nowadays, there are a huge number of autonomous and diverse information sources providing heterogeneous data. Sensors, social media data, data on the Web, open data, just to name a few, resulting in a major confluence of Big Data. In this survey, we discuss these diverse data sources and detail the way in which data are acquired, stored, processed and analysed. Although some of the opportunities in this new state are mentioned, the main objective of this analysis is to present the challenges for Big Data. To accomplish this goal, we examine the new proposals and approaches presented in this special issue with the aim of establishing new models for improving the management of the volume, velocity, and variety, of Big Data. Some of these schemes establish the use of Ontologies, Semantic Processing, Cloud Computing and Data Management and could be seen as intelligent services integrated as context-aware services.",2017,,https://doi.org/10.1016/j.csi.2017.03.006
1025,Rohit Budhiraja and Robert Thomas and Matthew Kim and Susan Redline,The Role of Big Data in the Management of Sleep-Disordered Breathing,"Sleep-disordered breathing, Big data, Management, Sleep apnea",,2016,,https://doi.org/10.1016/j.jsmc.2016.01.009
1026,Thanos Papadopoulos and Angappa Gunasekaran and Rameshwar Dubey and Nezih Altay and Stephen J. Childe and Samuel Fosso-Wamba,The role of Big Data in explaining disaster resilience in supply chains for sustainability,"Resilience, Big Data, Sustainability, Disaster, Exploratory factor analysis, Confirmatory factor analysis","The purpose of this paper is to propose and test a theoretical framework to explain resilience in supply chain networks for sustainability using unstructured Big Data, based upon 36,422 items gathered in the form of tweets, news, Facebook, WordPress, Instagram, Google+, and YouTube, and structured data, via responses from 205 managers involved in disaster relief activities in the aftermath of Nepal earthquake in 2015. The paper uses Big Data analysis, followed by a survey which was analyzed using content analysis and confirmatory factor analysis (CFA). The results of the analysis suggest that swift trust, information sharing and public–private partnership are critical enablers of resilience in supply chain networks. The current study used cross-sectional data. However the hypotheses of the study can be tested using longitudinal data to attempt to establish causality. The article advances the literature on resilience in disaster supply chain networks for sustainability in that (i) it suggests the use of Big Data analysis to propose and test particular frameworks in the context of resilient supply chains that enable sustainability; (ii) it argues that swift trust, public private partnerships, and quality information sharing link to resilience in supply chain networks; and (iii) it uses the context of Nepal, at the moment of the disaster relief activities to provide contemporaneous perceptions of the phenomenon as it takes place.",2017,,https://doi.org/10.1016/j.jclepro.2016.03.059
1027,Zigeng Fang and Yan Liu and Qiuchen Lu and Michael Pitt and Sean Hanna and Zhichao Tian,BIM-integrated portfolio-based strategic asset data quality management,"Strategic asset management (SAM), Building information modeling (BIM), Portfolio management, Data quality management","A building's strategic asset management (SAM) capability has traditionally been limited by its site-based management. With the emergence of needs from clients about delivering a long-term portfolio-based building asset management plan that minimizes the asset risk and optimizes the value of their asset portfolios, SAM Units have emerged as a new business form to provide various SAM services to their clients. However, the quality of their current data model is still hindered by many issues, such as missing important attributes and the lack of customized information flow guidance. In addition, there is a gap in integrating their existing data collection with various data sources and Building Information Modeling (BIM) to enhance their data quality. By evaluating a SAM Unit's portfolio case study, this paper identifies the factors limiting the quality of SAM Units' data model and develops a guide to integrating various data sources better. We develop a BIM-integrated portfolio-based SAM information flow framework and a detailed hierarchical portfolio-based non-geometric data structure. The proposed framework and data structure will help SAM professionals, building asset owners, and other facilities management professionals embrace the benefits of managing the portfolio-based SAM data.",2022,,https://doi.org/10.1016/j.autcon.2021.104070
1028,Q. Peter He and Jin Wang,Statistical process monitoring as a big data analytics tool for smart manufacturing,"Statistical process monitoring, Big data, Smart manufacturing, Feature extraction, Internet of things","With ever-accelerating advancement of information, communication, sensing and characterization technologies, such as industrial Internet of Things (IoT) and high-throughput instruments, it is expected that the data generated from manufacturing will grow exponentially, generating so called ‘big data’. One of the focuses of smart manufacturing is to create manufacturing intelligence from real-time data to support accurate and timely decision-making. Therefore, big data analytics is expected to contribute significantly to the advancement of smart manufacturing. In this work, a roadmap of statistical process monitoring (SPM) is presented. Most recent developments in SPM are briefly reviewed and summarized. Specific challenges and potential solutions in handling manufacturing big data are discussed. We suggest that process characteristics or feature based SPM, instead of process variable based SPM, is a promising route for next generation SPM and could play a significant role in smart manufacturing. The advantages of feature based SPM are discussed to support the suggestion and future directions in SPM are discussed in the context of smart manufacturing.",2018,,https://doi.org/10.1016/j.jprocont.2017.06.012
1029,Markus Hammer and Ken Somers and Hugo Karre and Christian Ramsauer,Profit Per Hour as a Target Process Control Parameter for Manufacturing Systems Enabled by Big Data Analytics and Industry 4.0 Infrastructure,"Operations Management, Manufacturing Systems 4.0, Profit per Hour, Advanced Process Control, Big Data Analytics, Agile Manufacturing","The rise of Industry 4.0 and in particular Big Data analytics of production parameters offers exciting new ways for optimization. The majority of factories in process industries currently aim for example, either for output maximization, yield increase, or cost reduction. The availability of real-time data and online processing capability with advanced algorithms enables a profit per hour operational management approach. Profit per hour as a target control metric allows running factories at the optimal available operating point taking all revenue and cost drivers into account. This paper describes the suitability of profit per hour as a target process control parameter for production in process industries. The authors explain how this management approach helps to make better operational decisions, trading off yield, energy, throughput, among other factors, and the resulting cumulative benefits. They also lay out how Big Data and advanced algorithms are the key enabler to this new approach, as well as a standardized methodology for implementation. With profit per hour an agile control approach is presented which aims to optimize the performance of industrial manufacturing systems in a world of ever increasing volatility.",2017,,https://doi.org/10.1016/j.procir.2017.03.094
1030,Mandy Chessell and Dan Wolfson and Tim Vincent,Chapter 3 - Architecting to Deliver Value From a Big Data and Hybrid Cloud Architecture,"Enterprise architecture, Self-service data, Systems of insight, Data-driven security, Business-driven governance, Trust and confidence, Hybrid-cloud, Information supply chains","Big data and analytics, particularly when combined with the use of cloud-based deployments, can transform the operation of an organization – increasing innovation, improving time to value and decision-making. However, an organization only derives value from data and analytics when (1) the collection of big data is organized, systematic and automated and (2) the use of data and analytic insight is embedded in the organization's day-to-day operation. Often the ambition of a big data and analytics solution requires data to flow freely across an organization. This can be in direct conflict with the organization's political and process silos that exist to partition the work of the organization into manageable chunks of function and responsibility. Thus the architecture of a big data solution must accommodate the realities within the organization to ensure sufficient value is realized by all of the stakeholders that are needed to enable this data interchange. Through examples of architectures for big data and analytics solutions, we explain how the scope of a big data solution can affect its architecture and the additional components necessary when a big data solution needs to span multiple organization silos.",2017,,https://doi.org/10.1016/B978-0-12-805467-3.00003-X
1031,Manuel Gonçalves-Pinho and Alberto Freitas,Chapter 8 - The use of Big Data in Psychiatry—The role of administrative databases,"Administrative database, Mental Health, Secondary data, Psychiatry, Research design","Administrative databases (AD) are repositories of administrative and clinical data related to patient contact episodes with all sorts of health facilities (primary care, hospitals, pharmacies, etc.). The use of AD data is increasing in Mental Health research as the advantages of using AD surpass some of the difficulties Mental health researchers find when using data from other sources (clinical trials, cohort studies, etc.). The large number of patients/contact episodes available, the systematic and broad register, and the fact that AD provides real-world data are some of the pros in using AD data. There are some methodological aspects that must be addressed when using this type of databases in order to provide solid and valid results. The possibility of clinical and administrative errors in an AD is a reality when using secondary data in Mental Health Research, and diagnostic code validation studies may be performed to estimate clinical and administrative accuracy. This chapter described in detail the pros and cons of using secondary data in mental health research and specifies the methodological steps a researcher must follow in order to find valid conclusions in AD from a clinical point of view.",2021,,https://doi.org/10.1016/B978-0-12-822884-5.00009-X
1032,Quan Xu and Peng Zhang and Wenqin Liu and Qiang Liu and Changxin Liu and Liangyong Wang and Anthony Toprac and S. {Joe Qin},"A Platform for Fault Diagnosis of High-Speed Train based on Big Data⁎⁎Project supported by the National Natural Science Foundation, China(61490704, 61440015) and the National High-Tech. R&D Program, China (No. 2015AA043802).","Fault Diagnosis, High-Speed Train, Big Data, Cloud Computing, Edge Computing","High-speed trains are very fast (e.g. 350km/h) and operate at high traffic density, so once a fault has occurred, the consequences are disastrous. In order to better control the train operational status by timely and rapid detection of faults, we need new methods to handle and analyze the huge volumes of high-speed railway data. In this paper, we propose a novel framework and platform for high-speed train fault diagnosis based on big data technologies. The framework aims to allow researchers to focus on fault detection algorithm development and on-line application, with all the complexities of big data import, storage, management, and realtime use handled transparently by the framework. The framework uses a combination of cloud computing and edge computing and a two-level architecture that handles the massive data of train operations. The platform uses Hadoop as its basic framework and combines HDFS, HBase, Redis and MySQL database as the data storage framework. A lossless data compression method is presented to reduce the data storage space and improve data storage efficiency. In order to support various types of data analysis tasks for fault diagnosis and prognosis, the framework integrates online computation, off-line computation, stream computation, real-time computation and so on. Moreover, the platform provides fault diagnosis and prognosis as services to users and a simple case study is given to further illustrate how the basic functions of the platform are implemented.",2018,,https://doi.org/10.1016/j.ifacol.2018.09.318
1033,Laura Sebastian-Coleman,Chapter 2 - Organizational Data and the Five Challenges of Managing Data Quality,"Data, history of data, data quality, data management, data governance, data stewardship, data and technology, process improvement, technology strategy, culture/organization, data literacy","This chapter describes the five challenges in data quality management (data, process, technology, people, and culture/organization) and proposes that organizations that want to get more value and insight from their data should take a strategic approach to data quality management. This is because quality is not an accident, and it cannot be an afterthought, especially in today’s complex organizations. This chapter provides the context for Section 2 and introduces critical terminology used throughout the book.",2022,,https://doi.org/10.1016/B978-0-12-821737-5.00002-X
1034,Xiaoli Liu and Satu Tamminen and Topi Korhonen and Juha Röning,How Physical Exercise Level Affects Sleep Quality? Analyzing Big Data Collected from Wearables,"Data analytics, wearables, sleep quality, statistical methods","Physical exercise and sleep have independent, yet synergistic, impacts on the health. However, the effects of acute exercise level on sleep quality have not been well investigated. We utilize statistical methods to investigate the differences of exercise level between the good and bad sleep nights. Our results present a complex interrelation between physical exercise and sleep quality with analyzing large personal data sets collected from wearables. As far as we know, this is the first study to investigate insights of interrelation of physical exercise and sleep quality based on a big volume of data collected from wearable devices of real users.",2019,,https://doi.org/10.1016/j.procs.2019.08.035
1035,Antony Clark and Jonathon Q. Ng and Nigel Morlet and James B. Semmens,Big data and ophthalmic research,"data linkage, clinical registry, health services research, ophthalmic epidemiology, big data","Large population-based health administrative databases, clinical registries, and data linkage systems are a rapidly expanding resource for health research. Ophthalmic research has benefited from the use of these databases in expanding the breadth of knowledge in areas such as disease surveillance, disease etiology, health services utilization, and health outcomes. Furthermore, the quantity of data available for research has increased exponentially in recent times, particularly as e-health initiatives come online in health systems across the globe. We review some big data concepts, the databases and data linkage systems used in eye research—including their advantages and limitations, the types of studies previously undertaken, and the future direction for big data in eye research.",2016,,https://doi.org/10.1016/j.survophthal.2016.01.003
1036,Pierluigi Zerbino and Davide Aloini and Riccardo Dulmin and Valeria Mininno,Big Data-enabled Customer Relationship Management: A holistic approach,"Big Data, CRM, Literature review, Critical Success Factors (CSFs), Word tree","This paper aims to figure out the potential impact of Big Data (BD) on Critical Success Factors (CSFs) of Customer Relationship Management (CRM). In fact, while some authors have posited a relationship between BD and CRM, literature lacks works that go into the heart of the matter. Through an extensive up-to-date in-depth literature review about CRM, twenty (20) CSFs were singled out from 104 selected papers, and organized within an ad-hoc classification framework. The consistency of the classification was checked by means of a content analysis. Evidences were discussed and linked to the BD literature, and five propositions about how BD could affect CRM CSFs were formalized. Our results suggest that BD-enabled CRM initiatives could require several changes in the pertinent CSFs. In order to get rid of the hype effect surrounding BD, we suggest to adopt an explorative approach towards them by defining a mandatory business direction through sound business cases and pilot tests. From a general standpoint, BD could be framed as an enabling factor of well-known projects, like CRM initiatives, in order to reap the benefits from the new technologies by addressing the efforts through already acknowledged management paths.",2018,,https://doi.org/10.1016/j.ipm.2017.10.005
1037,Ida Arlene Joiner,Chapter 5 - Information Seeking With Big Data: Not Just the Facts,"Big data, libraries, security, privacy, infrastructure, Hadoop","As experts at searching, retrieving, analyzing, and managing information, librarians are uniquely suited to work with big data. This chapter provides an overview of the popular big data technology. We examine what big data is, challenges and opportunities, and how it is currently being used in many industries and libraries. The chapter concludes with additional resources, some technologies for managing big data, big data terminology, and questions for further discussion.",2018,,https://doi.org/10.1016/B978-0-08-102253-5.00005-8
1038,Truong Nguyen and Li ZHOU and Virginia Spiegler and Petros Ieromonachou and Yong Lin,Big data analytics in supply chain management: A state-of-the-art literature review,"Literature review, Big data, Big data analytics, Supply chain management, Research directions","The rapidly growing interest from both academics and practitioners in the application of big data analytics (BDA) in supply chain management (SCM) has urged the need for review of up-to-date research development in order to develop a new agenda. This review responds to the call by proposing a novel classification framework that provides a full picture of current literature on where and how BDA has been applied within the SCM context. The classification framework is structurally based on the content analysis method of Mayring (2008), addressing four research questions: (1) in what areas of SCM is BDA being applied? (2) At what level of analytics is BDA used in these SCM areas? (3) What types of BDA models are used in SCM? (4) What BDA techniques are employed to develop these models? The discussion tackling these four questions reveals a number of research gaps, which leads to future research directions.",2018,,https://doi.org/10.1016/j.cor.2017.07.004
1039,Qingchen Zhang and Laurence T. Yang and Zhikui Chen and Peng Li,A survey on deep learning for big data,"Deep learning, Big data, Stacked auto-encoders, Deep belief networks, Convolutional neural networks, Recurrent neural networks","Deep learning, as one of the most currently remarkable machine learning techniques, has achieved great success in many applications such as image analysis, speech recognition and text understanding. It uses supervised and unsupervised strategies to learn multi-level representations and features in hierarchical architectures for the tasks of classification and pattern recognition. Recent development in sensor networks and communication technologies has enabled the collection of big data. Although big data provides great opportunities for a broad of areas including e-commerce, industrial control and smart medical, it poses many challenging issues on data mining and information processing due to its characteristics of large volume, large variety, large velocity and large veracity. In the past few years, deep learning has played an important role in big data analytic solutions. In this paper, we review the emerging researches of deep learning models for big data feature learning. Furthermore, we point out the remaining challenges of big data deep learning and discuss the future topics.",2018,,https://doi.org/10.1016/j.inffus.2017.10.006
1040,Hai Wang and Zeshui Xu and Hamido Fujita and Shousheng Liu,Towards felicitous decision making: An overview on challenges and trends of Big Data,"Big Data, Data deluge, Decision making, Data analysis, Data-intensive applications, Computational social science","The era of Big Data has arrived along with large volume, complex and growing data generated by many distinct sources. Nowadays, nearly every aspect of the modern society is impacted by Big Data, involving medical, health care, business, management and government. It has been receiving growing attention of researches from many disciplines including natural sciences, life sciences, engineering and even art & humanities. It also leads to new research paradigms and ways of thinking on the path of development. Lots of developed and under-developing tools improve our ability to make more felicitous decisions than what we have made ever before. This paper presents an overview on Big Data including four issues, namely: (i) concepts, characteristics and processing paradigms of Big Data; (ii) the state-of-the-art techniques for decision making in Big Data; (iii) felicitous decision making applications of Big Data in social science; and (iv) the current challenges of Big Data as well as possible future directions.",2016,,https://doi.org/10.1016/j.ins.2016.07.007
1041,Richard Addo-Tenkorang and Petri T. Helo,Big data applications in operations/supply-chain management: A literature review,"Big data – applications and analysis, Internet of Things (IoT), Cloud computing, Master database management, Operations/supply-chain management","Purpose
Big data is increasingly becoming a major organizational enterprise force to reckon with in this global era for all sizes of industries. It is a trending new enterprise system or platform which seemingly offers more features for acquiring, storing and analysing voluminous generated data from various sources to obtain value-additions. However, current research reveals that there is limited agreement regarding the performance of “big data.” Therefore, this paper attempts to thoroughly investigate “big data,” its application and analysis in operations or supply-chain management, as well as the trends and perspectives in this research area. This paper is organized in the form of a literature review, discussing the main issues of “big data” and its extension into “big data II”/IoT–value-adding perspectives by proposing a value-adding framework.
Methodology/research approach
The research approach employed is a comprehensive literature review. About 100 or more peer-reviewed journal articles/conference proceedings as well as industrial white papers are reviewed. Harzing Publish or Perish software was employed to investigate and critically analyse the trends and perspectives of “big data” applications between 2010 and 2015.
Findings/results
The four main attributes or factors identified with “big data” include – big data development sources (Variety – V1), big data acquisition (Velocity – V2), big data storage (Volume – V3), and finally big data analysis (Veracity – V4). However, the study of “big data” has evolved and expanded a lot based on its application and implementation processes in specific industries in order to create value (Value-adding – V5) – “Big Data cloud computing perspective/Internet of Things (IoT)”. Hence, the four Vs of “big data” is now expanded into five Vs.
Originality/value of research
This paper presents original literature review research discussing “big data” issues, trends and perspectives in operations/supply-chain management in order to propose “Big data II” (IoT – Value-adding) framework. This proposed framework is supposed or assumed to be an extension of “big data” in a value-adding perspective, thus proposing that “big data” be explored thoroughly in order to enable industrial managers and businesses executives to make pre-informed strategic operational and management decisions for increased return-on-investment (ROI). It could also empower organizations with a value-adding stream of information to have a competitive edge over their competitors.",2016,,https://doi.org/10.1016/j.cie.2016.09.023
1042,Steven H. Shaha and Zain Sayeed and Afshin A. Anoushiravani and Mouhanad M. El-Othmani and Khaled J. Saleh,"Big Data, Big Problems: Incorporating Mission, Values, and Culture in Provider Affiliations","Big data, Comparative effectiveness, Orthopedics, Total joint arthroplasty, Administrative database, Clinical database",,2016,,https://doi.org/10.1016/j.ocl.2016.05.009
1043,Patrick Mikalef and Ilias O. Pappas and John Krogstie and Paul A. Pavlou,Big data and business analytics: A research agenda for realizing business value,,,2020,,https://doi.org/10.1016/j.im.2019.103237
1044,Subia Saif and Samar Wazir,Performance Analysis of Big Data and Cloud Computing Techniques: A Survey,"Big Data, Big Data Analytics (BDA), Cloud Computing, Cloud based Big Data Enterprise Solutions, Big Data Storage, Big Data Warehouse, Streaming Data, Amazon Web Services (AWS), Google Cloud Platform (GCP), IBM Cloud, Microsoft Azure","A cloud framework refers to the aggregation of components like development tools, middleware and database services, needed for cloud computing, which aids in developing, deploying and managing cloud based applications strenuously, consequently making it an efficacious paradigm for massive scaling of dynamically allocated resources and their complex computing. Big Data Analytics (BDA) delivers data management solutions in the cloud architecture for storing, analysing and processing a huge volume of data. This paper presents a survey for performance based comparative analysis of cloud-based big data frameworks from leading enterprises like Amazon, Google, IBM, and Microsoft, which will assist researcher, IT analysts, reader and business user in picking the framework best suited for their work ensuring success in terms of favourable outcomes.",2018,,https://doi.org/10.1016/j.procs.2018.05.172
1045,Neha Sharma and Malini M. Patil and Madhavi Shamkuwar,Chapter 8 - Why Big Data and What Is It? Basic to Advanced Big Data Journey for the Medical Industry,"Big data, Medical big data, Healthcare data, Medical big data analytics, Healthcare data analytics, Data analytics, Pharmacology data analytics","The idea of big data is mainly reflected in its dimensions, which are popularly known as the Big Vs, which stands for Volume, Variety, Velocity, and Veracity. However, the concept goes beyond the Big Vs and testing of hypotheses, to focus on data analysis, hypothesis generation, and ascertaining the progressive strength of association. Preliminary study reveals that big data analytics adopts many data mining methods, such as descriptive, diagnostic, predictive, and prescriptive analytics. This evolving technology has tremendous application in healthcare, such as surveillance of safety or disease, predictive modeling, public health, pharma data analytics, clinical data analytics, healthcare analytics, and research. Moreover, the journey of big data in the medical domain is proving to be one of the important research thrusts of recent times. Study reveals that medical data is very specific and heterogeneous due to varied data sources such as scanned images, CT scan reports, doctor prescriptions, electronic health records (EHRs), etc. Medical data analytics faces some bottlenecks due to missing data, high dimensions, bias, and limitations of the study of patients through observation. Therefore, special big data techniques are required to handle them. Besides, many ethical, legal, social, clinical, and utility challenges are also a part of the data-handling process, which makes the role of big data in the medical field very challenging. Nevertheless, big data analytics is a fuel to the healthcare system that will provide a healthier life to patients; the issues and bottlenecks when removed from the system will be a boon for the entire human race. The chapter focuses on understanding the big data characteristics in medical big data, medical big data analytics, and its various applications in the interest of society.",2019,,https://doi.org/10.1016/B978-0-12-817356-5.00010-3
1046,Sagit Valeev and Natalya Kondratyeva,Chapter 6 - Big data analytics and process safety,"Analytics, Machine learning, Prediction, Clustering, Classification, Regression, Time series, Text analysis, Image analysis","The chapter discusses the basics of big data analytics and the features of using analytical models in the field of process safety and risk management. The definition and basic principles of data analytics are necessary to understand the analytical techniques. The requirements for input data and the properties of analytical models are important for effective analytics. The concept, basic components, and varieties of machine learning are discussed. We consider such basic machine learning algorithms as clustering, classification, and regression. As advanced methods of data analytics, time series analysis methods, text analysis, and image analysis are proposed. Examples of the application of data analytics for risk management in the framework of process safety are considered.",2021,,https://doi.org/10.1016/B978-0-12-822066-5.00001-7
1047,Nikos Bikakis and George Papastefanatos and Olga Papaemmanouil,"Big Data Exploration, Visualization and Analytics",,,2019,,https://doi.org/10.1016/j.bdr.2019.100123
1048,Desamparados Blazquez and Josep Domenech,Big Data sources and methods for social and economic analyses,"Big Data architecture, Forecasting, Nowcasting, Data lifecycle, Socio-economic data, Non-traditional data sources, Non-traditional analysis methods","The Data Big Bang that the development of the ICTs has raised is providing us with a stream of fresh and digitized data related to how people, companies and other organizations interact. To turn these data into knowledge about the underlying behavior of the social and economic agents, organizations and researchers must deal with such amount of unstructured and heterogeneous data. Succeeding in this task requires to carefully plan and organize the whole process of data analysis taking into account the particularities of the social and economic analyses, which include the wide variety of heterogeneous sources of information and a strict governance policy. Grounded on the data lifecycle approach, this paper develops a Big Data architecture that properly integrates most of the non-traditional information sources and data analysis methods in order to provide a specifically designed system for forecasting social and economic behaviors, trends and changes.",2018,,https://doi.org/10.1016/j.techfore.2017.07.027
1049,Veda C. Storey and Il-Yeol Song,Big data technologies and Management: What conceptual modeling can do,,"The era of big data has resulted in the development and applications of technologies and methods aimed at effectively using massive amounts of data to support decision-making and knowledge discovery activities. In this paper, the five Vs of big data, volume, velocity, variety, veracity, and value, are reviewed, as well as new technologies, including NoSQL databases that have emerged to accommodate the needs of big data initiatives. The role of conceptual modeling for big data is then analyzed and suggestions made for effective conceptual modeling efforts with respect to big data.",2017,,https://doi.org/10.1016/j.datak.2017.01.001
1050,Domingo Llorente Rivera and Markus R. Scholz and Christoph Bühl and Markus Krauss and Klaus Schilling,Is Big Data About to Retire Expert Knowledge? A Predictive Maintenance Study,"industrial analytics, anomaly detection, predictive maintenance, hydraulic pump, stochastic modeling","In this contribution, a data-driven approach towards the prediction of maintenance for the critical component of an injection molding machine is presented. We present our path from exploring and cleaning the data towards the implementation of a prediction algorithm based on kernel density estimation. We give first analytical evidence of the algorithms potential. Moreover, we compare the approach described here with our previous work where we went a model-based approach and present advantages and disadvantages of the two approaches. We try to contribute to a non-comprehensive guide on the implementation of predictive maintenance systems for industrial mass production facilities.",2019,,https://doi.org/10.1016/j.ifacol.2019.12.364
1051,Bala M. Balachandran and Shivika Prasad,Challenges and Benefits of Deploying Big Data Analytics in the Cloud for Business Intelligence,"Cloud Computing, Big Data Analytics, Cloud Analytics, Security, Privacy, Business Intelligence, MapReduce, AaaS, CLaaS","Cloud computing and big data analytics are, without a doubt, two of the most important technologies to enter the mainstream IT industry in recent years. Surprisingly, the two technologies are coming together to deliver powerful results and benefits for businesses. Cloud computing is already changing the way IT services are provided by so called cloud companies and how businesses and users interact with IT resources. Big Data is a data analysis methodology enables by recent advances in information and communications technology. However, big data analysis requires a huge amount of computing resources making adoption costs of big data technology is not affordable for many small to medium enterprises. In this paper, we outline the the benefits and challenges involved in deploying big data analytics through cloud computing. We argue that cloud computing can support the storage and computing requirements of big data analytics. We discuss how the consolidation of these two dominant technologies can enhance the process of big data mining enabling businesses to improve decision-making processes. We also highlight the issues and risks that should be addressed when using a so called CLaaS, cloud-based service model.",2017,,https://doi.org/10.1016/j.procs.2017.08.138
1052,Wu Qing,Chapter 9 - Global Practice of AI and Big Data in Oil and Gas Industry,"artificial intelligence, big data, digital core, digital rock physics, multiphase flow physics information, oil recovery, molecule advanced planning and scheduling, system (MAPS), crude oil selection, crude oil property prediction, process optimization, CCR unit, FCC unit, ethylene cracking unit, correlation analysis, the anomaly detection, parameters optimization analysis, prediction analysis, equipment preventive maintenance, distributed equipment health monitoring system, time series, early warning system for equipment fault monitoring, residual life prediction for equipment","This chapter introduces typical cases of artificial intelligence and big data application in oil and gas industry. In the upstream field, it introduces how to combine digital rock physics with big data and AI to optimize recovery efficiency. Digital core (also known as digital petrophysics—DRP) technology enables more reliable physical information about pore-scale multiphase flows to determine the cause of low recovery and provides new ways for different injection solutions to improve reservoir performance. Combined with digital rock technology and AI, we can integrate the characteristics of digital rock databases into logging and well data, and use a variety of advanced classification techniques to identify the remaining oil and gas potential. Through the multi-phase flow simulation, the multi-scale model can predict the best injection method for maximum recovery under different conditions and propose possible solutions to optimize crude oil production. In the downstream field, the application of AI and big data analysis in planning and scheduling systems, process unit optimization, preventive maintenance of equipment, and other aspects is introduced. Among them, the molecular-level advanced planning and scheduling system (MAPS) can realize the cost performance measurement under different production schemes for potential types of processable crude oil, which is conducive to more accurate selection of crude oil and prediction of crude oil properties. In addition, the whole process simulation can be used to understand the product quality changes under different crude oil blending schemes and different unit operating conditions, which is conducive to timely adjusting the product blending schemes according to economic benefits or ex-factory demands. The operation conditions of secondary units and even the properties of mixed crude oil can be deduced according to different product quality requirements. In the process of optimization, the Continuous Catalytic Reforming (CCR) unit in the refinery process, for example, introduces the application of large data analysis, including correlation analysis, single index detection, multidimensional data anomaly detection, and the parameters of the single objective optimization, a multi-objective parameter optimization analysis, unstructured data analysis, and forecast analysis based on material properties. Good practices in CCR units have also been extended to other oil refining and chemical units, such as Fluid Catalytic Cracking (FCC) and ethylene cracking. In terms of equipment preventive maintenance, it introduced how to integrated application of Internet of things, deep machine learning, knowledge map and other technology to build real-time and on-line distributed equipment health monitoring and early warning system, for early detection of equipment hidden danger, early warning, early treatment of providing effective means, guarantee equipment run healthy and stable for a long period of time, to reduce unplanned downtime losses. In particular, the establishment of equipment prediction model based on time series and AI can realize effective monitoring and early warning of equipment faults such as shaft displacement, shaft fracture, shell cracking, power overload, and prediction of equipment remaining life.",2021,,https://doi.org/10.1016/B978-0-12-820714-7.00009-1
1053,Wendy Arianne Günther and Mohammad H. {Rezazade Mehrizi} and Marleen Huysman and Frans Feldberg,Debating big data: A literature review on realizing value from big data,"Big data, Analytics, Literature review, Value realization, Portability, Interconnectivity","Big data has been considered to be a breakthrough technological development over recent years. Notwithstanding, we have as yet limited understanding of how organizations translate its potential into actual social and economic value. We conduct an in-depth systematic review of IS literature on the topic and identify six debates central to how organizations realize value from big data, at different levels of analysis. Based on this review, we identify two socio-technical features of big data that influence value realization: portability and interconnectivity. We argue that, in practice, organizations need to continuously realign work practices, organizational models, and stakeholder interests in order to reap the benefits from big data. We synthesize the findings by means of an integrated model.",2017,,https://doi.org/10.1016/j.jsis.2017.07.003
1054,Chi Yang and Xuyun Zhang and Changmin Zhong and Chang Liu and Jian Pei and Kotagiri Ramamohanarao and Jinjun Chen,A spatiotemporal compression based approach for efficient big data processing on Cloud,"Big data, Graph data, Spatiotemporal compression, Cloud computing, Scheduling","It is well known that processing big graph data can be costly on Cloud. Processing big graph data introduces complex and multiple iterations that raise challenges such as parallel memory bottlenecks, deadlocks, and inefficiency. To tackle the challenges, we propose a novel technique for effectively processing big graph data on Cloud. Specifically, the big data will be compressed with its spatiotemporal features on Cloud. By exploring spatial data correlation, we partition a graph data set into clusters. In a cluster, the workload can be shared by the inference based on time series similarity. By exploiting temporal correlation, in each time series or a single graph edge, temporal data compression is conducted. A novel data driven scheduling is also developed for data processing optimisation. The experiment results demonstrate that the spatiotemporal compression and scheduling achieve significant performance gains in terms of data size and data fidelity loss.",2014,,https://doi.org/10.1016/j.jcss.2014.04.022
1055,Sunil Tiwari and H.M. Wee and Yosef Daryanto,Big data analytics in supply chain management between 2010 and 2016: Insights to industries,"Big data analytics, Supply chain management, Big data application","This paper investigates big data analytics research and application in supply chain management between 2010 and 2016 and provides insights to industries. In recent years, the amount of data produced from end-to-end supply chain management practices has increased exponentially. Moreover, in current competitive environment supply chain professionals are struggling in handling the huge data. They are surveying new techniques to investigate how data are produced, captured, organized and analyzed to give valuable insights to industries. Big Data analytics is one of the best techniques which can help them in overcoming their problem. Realizing the promising benefits of big data analytics in the supply chain has motivated us to write a review on the importance/impact of big data analytics and its application in supply chain management. First, we discuss big data analytics individually, and then we discuss the role of big data analytics in supply chain management (supply chain analytics). Current research and application are also explored. Finally, we outline the insights to industries. Observations and insights from this paper could provide the guideline for academia and practitioners in implementing big data analytics in different aspects of supply chain management.",2018,,https://doi.org/10.1016/j.cie.2017.11.017
1056,Yichuan Wang and LeeAnn Kung and Terry Anthony Byrd,Big data analytics: Understanding its capabilities and potential benefits for healthcare organizations,"Big data analytics, Big data analytics architecture, Big data analytics capabilities, Business value of information technology (IT), Health care","To date, health care industry has not fully grasped the potential benefits to be gained from big data analytics. While the constantly growing body of academic research on big data analytics is mostly technology oriented, a better understanding of the strategic implications of big data is urgently needed. To address this lack, this study examines the historical development, architectural design and component functionalities of big data analytics. From content analysis of 26 big data implementation cases in healthcare, we were able to identify five big data analytics capabilities: analytical capability for patterns of care, unstructured data analytical capability, decision support capability, predictive capability, and traceability. We also mapped the benefits driven by big data analytics in terms of information technology (IT) infrastructure, operational, organizational, managerial and strategic areas. In addition, we recommend five strategies for healthcare organizations that are considering to adopt big data analytics technologies. Our findings will help healthcare organizations understand the big data analytics capabilities and potential benefits and support them seeking to formulate more effective data-driven analytics strategies.",2018,,https://doi.org/10.1016/j.techfore.2015.12.019
1057,Manish Shukla and Lana Mattar,Next generation smart sustainable auditing systems using Big Data Analytics: Understanding the interaction of critical barriers,"Big Data Analytics, Sustainable auditing systems, Barriers, RSPO, Interpretive Structural Modelling","In the current scenario, sustainable auditing, for example roundtable of sustainable palm oil (RSPO), requires a huge amount of data to be manually collected and entered into paper forms by farmers. Such systems are inherently inefficient, time-consuming, and, prone to errors. Researchers have proposed Big Data Analytics (BDA) based framework for next-generation smart sustainable auditing systems. Though theoretically feasible, real-life implementation of such frameworks is extremely difficult. Thus, this paper aims to identify the critical barriers that hinder the application of BDA based smart sustainable auditing system. It also aims to explore the dynamic interrelations among the barriers. We applied Interpretive Structural Modelling (ISM) approach to develop the model that extrapolates BDA adoption barriers and their relationships. The proposed model illustrates how barriers are spread over various levels and how specific barriers impact other barriers through direct and/or transitive links. This study provides practitioners with a roadmap to prioritise the interventions to facilitate the adoption of BDA in the sustainable auditing systems. Insights of this study could be used by academics to enhance understanding of the barriers to BDA applications.",2019,,https://doi.org/10.1016/j.cie.2018.04.055
1058,JinXing Zuo and Wei Ji and YuJie Ben and Muhammad Azher Hassan and WenHong Fan and Liam Bates and ZhaoMin Dong,Using big data from air quality monitors to evaluate indoor PM2.5 exposure in buildings: Case study in Beijing,"Indoor PM, Infiltration factor, Indoor/outdoor ratio, Beijing","Due to time- and expense- consuming of conventional indoor PM2.5 (particulate matter with aerodynamic diameter of less than 2.5 μm) sampling, the sample size in previous studies was generally small, which leaded to high heterogeneity in indoor PM2.5 exposure assessment. Based on 4403 indoor air monitors in Beijing, this study evaluated indoor PM2.5 exposure from 15th March 2016 to 14th March 2017. Indoor PM2.5 concentration in Beijing was estimated to be 38.6 ± 18.4 μg/m3. Specifically, the concentration in non-heating season was 34.9 ± 15.8 μg/m3, which was 24% lower than that in heating season (46.1 ± 21.2 μg/m3). A significant correlation between indoor and ambient PM2.5 (p < 0.05) was evident with an infiltration factor of 0.21, and the ambient PM2.5 contributed approximately 52% and 42% to indoor PM2.5 for non-heating and heating seasons, respectively. Meanwhile, the mean indoor/outdoor (I/O) ratio was estimated to be 0.73 ± 0.54. Finally, the adjusted PM2.5 exposure level integrating the indoor and outdoor impact was calculated to be 46.8 ± 27.4 μg/m3, which was approximately 42% lower than estimation only relied on ambient PM2.5 concentration. This study is the first attempt to employ big data from commercial air monitors to evaluate indoor PM2.5 exposure and risk in Beijing, which may be instrumental to indoor PM2.5 pollution control.",2018,,https://doi.org/10.1016/j.envpol.2018.05.030
1059,Ahmed Elragal,ERP and Big Data: The Inept Couple,"ERP, Big Data, Research Agenda","The world is witnessing an unprecedented interest in big data. Big data is data that is big in size (volume), big in variety (structured; semi-structured; unstructured), and big in speed of change (velocity). It was reported that almost 90% of the data worldwide was just created in the past 2 years. Therefore, this paper is an attempt to align ERP systems with big data. The objective is to suggest a future research agenda to bring together big data and ERP. While almost everyone is talking about big data at the product or tool level, relationship with social media, relationship with Internet of things, etc. no one has tried to integrate big data and ERP. A research agenda is discussed and introduced in this paper.",2014,,https://doi.org/10.1016/j.protcy.2014.10.089
1060,Patrick Bonnel and Marcela A. Munizaga,Transport survey methods - in the era of big data facing new and old challenges,"ISCTSC, transport, survey methodology, big data","This document presents an introduction to the ISCTSC Special Issue of Transport Research Procedia. It synthesizes the discussions held at the 11th International Conference on Transport Survey Methods, and describes the contents of the selected contributions. This conference has been held in different countries from all over the world, involving an increasing group of enthusiastic and generous specialists, willing to share their knowledge. This 11th conference was an opportunity to discuss the state of the art on transport survey methods, but also to question the way transport surveys are conducted in the era of big data. We took the opportunity to identify the main challenges, and the most important questions.",2018,,https://doi.org/10.1016/j.trpro.2018.10.001
1061,Aisha Siddiqa and Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Mohsen Marjani and Shahabuddin Shamshirband and Abdullah Gani and Fariza Nasaruddin,A survey of big data management: Taxonomy and state-of-the-art,"Big data management, Storage, Big data, Processing, Security","The rapid growth of emerging applications and the evolution of cloud computing technologies have significantly enhanced the capability to generate vast amounts of data. Thus, it has become a great challenge in this big data era to manage such voluminous amount of data. The recent advancements in big data techniques and technologies have enabled many enterprises to handle big data efficiently. However, these advances in techniques and technologies have not yet been studied in detail and a comprehensive survey of this domain is still lacking. With focus on big data management, this survey aims to investigate feasible techniques of managing big data by emphasizing on storage, pre-processing, processing and security. Moreover, the critical aspects of these techniques are analyzed by devising a taxonomy in order to identify the problems and proposals made to alleviate these problems. Furthermore, big data management techniques are also summarized. Finally, several future research directions are presented.",2016,,https://doi.org/10.1016/j.jnca.2016.04.008
1062,Peng Jia and Hong Xue and Shiyong Liu and Hao Wang and Lijian Yang and Therese Hesketh and Lu Ma and Hongwei Cai and Xin Liu and Yaogang Wang and Youfa Wang,Opportunities and challenges of using big data for global health,,,2019,,https://doi.org/10.1016/j.scib.2019.09.011
1063,Grégoire Rey and Karim Bounebache and Claire Rondet,"Causes of deaths data, linkages and big data perspectives","Causes of death data, Data linkages, Big data","The study of cause-specific mortality data is one of the main sources of information for public health monitoring. In most industrialized countries, when a death occurs, it is a legal requirement that a medical certificate based on the international form recommended by World Health Organization's (WHO) is filled in by a physician. The physician reports the causes of death that directly led or contributed to the death on the death certificate. The death certificate is then forwarded to a coding office, where each cause is coded, and one underlying cause is defined, using the rules of the International Classification of Diseases and Related Health Problems, now in its 10th Revision (ICD-10). Recently, a growing number of countries have adopted, or have decided to adopt, the coding software Iris, developed and maintained by an international consortium1. This whole standardized production process results in a high and constantly increasing international comparability of cause-specific mortality data. While these data could be used for international comparisons and benchmarking of global burden of diseases, quality of care and prevention policies, there are also many other ways and methods to explore their richness, especially when they are linked with other data sources. Some of these methods are potentially referring to the so-called “big data” field. These methods could be applied both to the production of the data, to the statistical processing of the data, and even more to process these data linked to other databases. In the present note, we depict the main domains in which this new field of methods could be applied. We focus specifically on the context of France, a 65 million inhabitants country with a centralized health data system. Finally we will insist on the importance of data quality, and the specific problematics related to death certification in the forensic medicine domain.",2018,,https://doi.org/10.1016/j.jflm.2016.12.004
1064,Dong-Hee Shin,Demystifying big data: Anatomy of big data developmental process,"Big data, Data ecosystem, Normalization, Normalization process theory, Big data user, Big data user experience","This study seeks to understand big data ecology, how it is perceived by different stakeholders, the potential value and challenges, and the implications for the private sector and public organizations, as well as for policy makers. With Normalization Process Theory in place, this study conducts socio-technical evaluation on the big data phenomenon to understand the developmental processes through which new practices of thinking and enacting are implemented, embedded, and integrated in South Korea. It also undertakes empirical analyses of user modeling to explore the factors influencing users׳ adoption of big data by integrating cognitive motivations as well as user values as the primary determining factors. Based on the qualitative and quantitative findings, this study concludes that big data should be developed with user-centered ideas and that users should be the focus of big data design.",2016,,https://doi.org/10.1016/j.telpol.2015.03.007
1065,Awais Ahmad and Anand Paul and M. Mazhar Rathore,An efficient divide-and-conquer approach for big data analytics in machine-to-machine communication,"M2M, Big Data, Divide-and-conquer, Data fusion domain","Machine-to-Machine (M2M) communication relies on the physical objects (e.g., satellites, sensors, and so forth) interconnected with each other, creating mesh of machines producing massive volume of data about large geographical area (e.g., living and non-living environment). Thus, the M2M is an ideal example of Big Data. On the contrary, the M2M platforms that handle Big Data might perform poorly or not according to the goals of their operator (in term of cost, database utilization, data quality, processing and computational efficiency, analysis and feature extraction applications). Therefore, to address the aforementioned needs, we propose a new effective, memory and processing efficient system architecture for Big Data in M2M, which, unlike other previous proposals, does not require whole set of data to be processed (including raw data sets), and to be kept in the main memory. Our designed system architecture exploits divide-and-conquer approach and data block-wise vertical representation of the database follows a particular petitionary strategy, which formalizes the problem of feature extraction applications. The architecture goes from physical objects to the processing servers, where Big Data set is first transformed into a several data blocks that can be quickly processed, then it classifies and reorganizes these data blocks from the same source. In addition, the data blocks are aggregated in a sequential manner based on a machine ID, and equally partitions the data using fusion algorithm. Finally, the results are stored in a server that helps the users in making decision. The feasibility and efficiency of the proposed system architecture are implemented on Hadoop single node setup on UBUNTU 14.04 LTS core™i5 machine with 3.2GHz processor and 4GB memory. The results show that the proposed system architecture efficiently extract various features (such as River) from the massive volume of satellite data.",2016,,https://doi.org/10.1016/j.neucom.2015.04.109
1066,Tao Hong and Pierre Pinson,Energy forecasting in the big data world,,,2019,,https://doi.org/10.1016/j.ijforecast.2019.05.004
1067,Dongxiao Gu and Jingjing Li and Xingguo Li and Changyong Liang,Visualizing the knowledge structure and evolution of big data research in healthcare informatics,"Big data, Healthcare informatics, Bibliometrics, Knowledge structure, Knowledge management","Background
In recent years, the literature associated with healthcare big data has grown rapidly, but few studies have used bibliometrics and a visualization approach to conduct deep mining and reveal a panorama of the healthcare big data field.
Methods
To explore the foundational knowledge and research hotspots of big data research in the field of healthcare informatics, this study conducted a series of bibliometric analyses on the related literature, including papers’ production trends in the field and the trend of each paper’s co-author number, the distribution of core institutions and countries, the core literature distribution, the related information of prolific authors and innovation paths in the field, a keyword co-occurrence analysis, and research hotspots and trends for the future.
Results
By conducting a literature content analysis and structure analysis, we found the following: (a) In the early stage, researchers from the United States, the People’s Republic of China, the United Kingdom, and Germany made the most contributions to the literature associated with healthcare big data research and the innovation path in this field. (b) The innovation path in healthcare big data consists of three stages: the disease early detection, diagnosis, treatment, and prognosis phase, the life and health promotion phase, and the nursing phase. (c) Research hotspots are mainly concentrated in three dimensions: the disease dimension (e.g., epidemiology, breast cancer, obesity, and diabetes), the technical dimension (e.g., data mining and machine learning), and the health service dimension (e.g., customized service and elderly nursing).
Conclusion
This study will provide scholars in the healthcare informatics community with panoramic knowledge of healthcare big data research, as well as research hotspots and future research directions.",2017,,https://doi.org/10.1016/j.ijmedinf.2016.11.006
1068,Jan-Phillip Herrmann and Sven Tackenberg and Elio Padoano and Jörg Hartlief and Jens Rautenstengel and Christine Loeser and Jörg Böhme,An ERP Data Quality Assessment Framework for the Implementation of an APS system using Bayesian Networks,"Data Quality Assessment, Advanced Planning, Scheduling, Bayesian Network, Enterprise Resource Planning","In today’s manufacturing industry, enterprise-resource-planning (ERP) systems reach their limit when planning and scheduling production subject to multiple objectives and constraints. Advanced planning and scheduling (APS) systems provide these capabilities and are an extension for ERP systems. However, when integrating an APS and ERP system, the ERP data frequently lacks quality, hindering the APS system from working as required. This paper introduces a data quality (DQ) assessment framework that employs a Bayesian Network (BN) to perform quick DQ assessments based on expert interviews and DQ measurements with actual ERP data. We explain the BN’s functionality, design, and validation and show how using the perceived DQ of experts and a semi-supervised learning algorithm improves the BN’s predictions over time. We discuss applying our framework in an APS system implementation project involving an APS system provider and a medium-sized manufacturer of hydraulic cylinders. Despite considering the DQ assessment framework in such a specific context, it is not restricted to a particular domain. We close by discussing the framework’s limits, particularly the BN as a DQ assessment methodology and future works to improve its performance.",2022,,https://doi.org/10.1016/j.procs.2022.01.218
1069,Maribel Yasmina Santos and Jorge {Oliveira e Sá} and Carina Andrade and Francisca {Vale Lima} and Eduarda Costa and Carlos Costa and Bruno Martinho and João Galvão,A Big Data system supporting Bosch Braga Industry 4.0 strategy,"Big Data, Industry 4.0, Big Data analytics, Big Data architecture, Bosch","People, devices, infrastructures and sensors can constantly communicate exchanging data and generating new data that trace many of these exchanges. This leads to vast volumes of data collected at ever increasing velocities and of different variety, a phenomenon currently known as Big Data. In particular, recent developments in Information and Communications Technologies are pushing the fourth industrial revolution, Industry 4.0, being data generated by several sources like machine controllers, sensors, manufacturing systems, among others. Joining volume, variety and velocity of data, with Industry 4.0, makes the opportunity to enhance sustainable innovation in the Factories of the Future. In this, the collection, integration, storage, processing and analysis of data is a key challenge, being Big Data systems needed to link all the entities and data needs of the factory. Thereby, this paper addresses this key challenge, proposing and implementing a Big Data Analytics architecture, using a multinational organisation (Bosch Car Multimedia – Braga) as a case study. In this work, all the data lifecycle, from collection to analysis, is handled, taking into consideration the different data processing speeds that can exist in the real environment of a factory (batch or stream).",2017,,https://doi.org/10.1016/j.ijinfomgt.2017.07.012
1070,Kushan {De Silva} and Noel Mathews and Helena Teede and Andrew Forbes and Daniel Jönsson and Ryan T. Demmer and Joanne Enticott,Clinical notes as prognostic markers of mortality associated with diabetes mellitus following critical care: A retrospective cohort analysis using machine learning and unstructured big data,"Critical care, Diabetes, Electronic health records, LASSO, Machine learning, Mortality, Natural language processing, Prognosis, Text mining","Background
Clinical notes are ubiquitous resources offering potential value in optimizing critical care via data mining technologies.
Objective
To determine the predictive value of clinical notes as prognostic markers of 1-year all-cause mortality among people with diabetes following critical care.
Materials and methods
Mortality of diabetes patients were predicted using three cohorts of clinical text in a critical care database, written by physicians (n = 45253), nurses (159027), and both (n = 204280). Natural language processing was used to pre-process text documents and LASSO-regularized logistic regression models were trained and tested. Confusion matrix metrics of each model were calculated and AUROC estimates between models were compared. All predictive words and corresponding coefficients were extracted. Outcome probability associated with each text document was estimated.
Results
Models built on clinical text of physicians, nurses, and the combined cohort predicted mortality with AUROC of 0.996, 0.893, and 0.922, respectively. Predictive performance of the models significantly differed from one another whereas inter-rater reliability ranged from substantial to almost perfect across them. Number of predictive words with non-zero coefficients were 3994, 8159, and 10579, respectively, in the models of physicians, nurses, and the combined cohort. Physicians’ and nursing notes, both individually and when combined, strongly predicted 1-year all-cause mortality among people with diabetes following critical care.
Conclusion
Clinical notes of physicians and nurses are strong and novel prognostic markers of diabetes-associated mortality in critical care, offering potentially generalizable and scalable applications. Clinical text-derived personalized risk estimates of prognostic outcomes such as mortality could be used to optimize patient care.",2021,,https://doi.org/10.1016/j.compbiomed.2021.104305
1071,Ruth E. Appel and Sandra C. Matz,Chapter 6 - Psychological targeting in the age of Big Data,"Psychological targeting, Psychological profiling, Psychologically-informed interventions, Big Data, Digital footprints, Methods, Ethics, Privacy, Contextual integrity","Advances in the collection, storage, and processing of large amounts of user data have given rise to psychological targeting, which we define as the process of extracting individuals’ psychological characteristics from their digital footprints in order to target them with psychologically-informed interventions at scale. In this chapter, we introduce a two-stage framework of psychological targeting consisting of (1) psychological profiling and (2) psychologically-informed interventions. We summarize the most important research findings in relation to the two stages and discuss important methodological opportunities and pitfalls. To help researchers make the most of the opportunities, we also provide practical advice on how to deal with some of the potential pitfalls. Finally, we highlight ethical opportunities and challenges and offer some suggestions for addressing these challenges. If done right, psychological targeting has the potential to advance our scientific understanding of human nature and to enhance the well-being of individuals and society at large.",2021,,https://doi.org/10.1016/B978-0-12-819200-9.00015-6
1072,Ahmad Karim and Aisha Siddiqa and Zanab Safdar and Maham Razzaq and Syeda Anum Gillani and Huma Tahir and Sana Kiran and Ejaz Ahmed and Muhammad Imran,"Big data management in participatory sensing: Issues, trends and future directions","Participatory sensing, Big data, Big data management, Big data analytics, Mobile cloud computing","Participatory sensing has become an emerging technology of this era owing to its low cost in big sensor data collection. Prior to participatory sensing, large-scale deployment complexities were found in wireless sensor networks when collecting data from widespread resources. Participatory sensing systems employ handheld devices as sensors to collect data from communities and transmit to the cloud, where data are further analyzed by expert systems. The processes involved in participatory sensing, such as data collection, transmission, analysis, and visualization, exhibit certain management issues. This study aims to identify big data management issues that must be addressed at the cloud side during data processing and storing and at the participant side during data collection and visualization. It then proposes a framework for big data management in participatory sensing to resolve the contemporary big data management issues on the basis of suggested principles. Moreover, this work presents case studies to elaborate the existence of the highlighted issues. Finally, the limitations, recommendations, and future research directions for academia and industry in the domain of participatory sensing are discussed.",2020,,https://doi.org/10.1016/j.future.2017.10.007
1073,Gabriel E. Sánchez-Martínez and Marcela Munizaga,Workshop 5 report: Harnessing big data,"Big data, Measurement, Implementation challenges, Analysis tools, Transit best practices","A group of researchers, consultants, software developers, and transit agencies convened in Santiago, Chile over 3 days as part of the Thredbo workshop titled “Harnessing Big Data”, to present their recent research and discuss the state of practice, state of the art, and future directions of big data in public transportation. This report documents their discussion. The key conclusion of the workshop is that, although much progress has been made in utilizing big data to improve transportation planning and operations, much remains to be done, both in terms of developing further analysis tools and use cases of big data, and of disseminating best practices so that they are adopted across the industry.",2016,,https://doi.org/10.1016/j.retrec.2016.10.008
1074,Tijs {van den Broek} and Anne Fleur {van Veenstra},Governance of big data collaborations: How to balance regulatory compliance and disruptive innovation,"Disruptive innovation, Data protection regulation, Big data, Governance, Inter-organizational collaboration","Big data is an important driver of disruptive innovation that may increase organizations' competitive advantage. To create innovative data combinations and decrease investments, big data is often shared among organizations, crossing organizational boundaries. However, these big data collaborations need to balance disruptive innovation and compliance to a strict data protection regime in the EU. This paper investigates how inter-organizational big data collaborations arrange and govern their activities in the context of this dilemma. We conceptualize big data as inter-organizational systems and build on IS and Organization Theory literature to develop four archetypical governance arrangements: Market, Hierarchy, Bazaar and Network. Subsequently, these arrangements are investigated in four big data collaboration use cases. The contributions of this study to literature are threefold. First, we conceptualize the organization behind big data collaborations as IOS governance. Second, we show that the choice for an inter-organizational governance arrangement highly depends on the institutional pressure from regulation and the type of data that is shared. In this way, we contribute to the limited body of research on the antecedents of IOS governance. Last, we highlight with four use cases how the principles of big data, specifically data maximization, clash with the principles of EU data protection regulation. Practically, our study provides guidelines for IT and innovation managers how to arrange and govern the sharing of data among multiple organizations.",2018,,https://doi.org/10.1016/j.techfore.2017.09.040
1075,Venketesh Palanisamy and Ramkumar Thirunavukarasu,Implications of big data analytics in developing healthcare frameworks – A review,"Big data, Healthcare, Framework, Infrastructure, Analytics, Patterns, Tools","The domain of healthcare acquired its influence by the impact of big data since the data sources involved in the healthcare organizations are well-known for their volume, heterogeneous complexity and high dynamism. Though the role of big data analytical techniques, platforms, tools are realized among various domains, their impact on healthcare organization for implementing and delivering novel use-cases for potential healthcare applications shows promising research directions. In the context of big data, the success of healthcare applications solely depends on the underlying architecture and utilization of appropriate tools as evidenced in pioneering research attempts. Novel research works have been carried out for deriving application specific healthcare frameworks that offer diversified data analytical capabilities for handling sources of data ranging from electronic health records to medical images. In this paper, we have presented various analytical avenues that exist in the patient-centric healthcare system from the perspective of various stakeholders. We have also reviewed various big data frameworks with respect to underlying data sources, analytical capability and application areas. In addition, the implication of big data tools in developing healthcare eco system is also presented.",2019,,https://doi.org/10.1016/j.jksuci.2017.12.007
1076,Aaron N. Richter and Taghi M. Khoshgoftaar,Efficient learning from big data for cancer risk modeling: A case study with melanoma,"Big data, Cloud computing, Machine learning, Electronic health records, Early detection of cancer","Background
Building cancer risk models from real-world data requires overcoming challenges in data preprocessing, efficient representation, and computational performance. We present a case study of a cloud-based approach to learning from de-identified electronic health record data and demonstrate its effectiveness for melanoma risk prediction.
Methods
We used a hybrid distributed and non-distributed approach to computing in the cloud: distributed processing with Apache Spark for data preprocessing and labeling, and non-distributed processing for machine learning model training with scikit-learn. Moreover, we explored the effects of sampling the training dataset to improve computational performance. Risk factors were evaluated using regression weights as well as tree SHAP values.
Results
Among 4,061,172 patients who did not have melanoma through the 2016 calendar year, 10,129 were diagnosed with melanoma within one year. A gradient-boosted classifier achieved the best predictive performance with cross-validation (AUC = 0.799, Sensitivity = 0.753, Specificity = 0.688). Compared to a model built on the original data, a dataset two orders of magnitude smaller could achieve statistically similar or better performance with less than 1% of the training time and cost.
Conclusions
We produced a model that can effectively predict melanoma risk for a diverse dermatology population in the U.S. by using hybrid computing infrastructure and data sampling. For this de-identified clinical dataset, sampling approaches significantly shortened the time for model building while retaining predictive accuracy, allowing for more rapid machine learning model experimentation on familiar computing machinery. A large number of risk factors (>300) were required to produce the best model.",2019,,https://doi.org/10.1016/j.compbiomed.2019.04.039
1077,Deepak Arunachalam and Niraj Kumar and John Paul Kawalek,"Understanding big data analytics capabilities in supply chain management: Unravelling the issues, challenges and implications for practice","Supply chain management, Big data analytics, Capabilities, Maturity model","In the era of Big Data, many organisations have successfully leveraged Big Data Analytics (BDA) capabilities to improve their performance. However, past literature on BDA have put limited focus on understanding the capabilities required to extract value from big data. In this context, this paper aims to provide a systematic literature review of BDA capabilities in supply chain and develop the capabilities maturity model. The paper presents the bibliometric and thematic analysis of research papers from 2008 to 2016. This paper contributes in theorizing BDA capabilities in context of supply chain, and provides future direction of research in this field.",2018,,https://doi.org/10.1016/j.tre.2017.04.001
1078,Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Abdullah Gani and Salimah Mokhtar and Ejaz Ahmed and Nor Badrul Anuar and Athanasios V. Vasilakos,Big data: From beginning to future,"Big data, Parallel and distributed computing, Cloud computing, Internet of things, Social media, Analytics","Big data is a potential research area receiving considerable attention from academia and IT communities. In the digital world, the amounts of data generated and stored have expanded within a short period of time. Consequently, this fast growing rate of data has created many challenges. In this paper, we use structuralism and functionalism paradigms to analyze the origins of big data applications and its current trends. This paper presents a comprehensive discussion on state-of-the-art big data technologies based on batch and stream data processing. Moreover, strengths and weaknesses of these technologies are analyzed. This study also discusses big data analytics techniques, processing methods, some reported case studies from different vendors, several open research challenges, and the opportunities brought about by big data. The similarities and differences of these techniques and technologies based on important parameters are also investigated. Emerging technologies are recommended as a solution for big data problems.",2016,,https://doi.org/10.1016/j.ijinfomgt.2016.07.009
1079,Sushruta Mishra and Brojo Kishore Mishra and Hrudaya Kumar Tripathy and Arijit Dutta,Chapter 1 - Analysis of the role and scope of big data analytics with IoT in health care domain,"Big data analytics, Cloud, Health care domain, Internet of Things (IoT), Sensors","Data analytics play an active role in medical applications to extract relevant information from heaps of data samples. Internet of things (IoT) technology has slowly captured the market and is entering the health care sector too. With the help of big data analytics, various IoT-based devices can auto-monitor the health conditions of patients and can send the status to concerned physicians and family members. Thus, the integration of big data analytics with IoT technology forms a favorable combination in the health care domain. In this chapter, we discuss the two latest trends that include big data analytics and IoT with respect to its relevance in medical fields. We also analyze a health care monitoring system, which is an IoT-based model integrated with big data analytics. The system integrates patient specific information over the cloud. In the more developed model, the implementation was made to monitor the health status of the patients. The developed model was found to be faster and thus it can be easily implemented into a real time patient health monitoring and status management system. Medical experts can take advantage of this system model, thereby providing appropriate information to appropriate patient and doctors at appropriate time.",2020,,https://doi.org/10.1016/B978-0-12-818318-2.00001-5
1080,Muhammad Bilal and Lukumon O. Oyedele and Junaid Qadir and Kamran Munir and Saheed O. Ajayi and Olugbenga O. Akinade and Hakeem A. Owolabi and Hafiz A. Alaka and Maruf Pasha,"Big Data in the construction industry: A review of present status, opportunities, and future trends","Big Data Engineering, Big Data Analytics, Construction industry, Machine learning","The ability to process large amounts of data and to extract useful insights from data has revolutionised society. This phenomenon—dubbed as Big Data—has applications for a wide assortment of industries, including the construction industry. The construction industry already deals with large volumes of heterogeneous data; which is expected to increase exponentially as technologies such as sensor networks and the Internet of Things are commoditised. In this paper, we present a detailed survey of the literature, investigating the application of Big Data techniques in the construction industry. We reviewed related works published in the databases of American Association of Civil Engineers (ASCE), Institute of Electrical and Electronics Engineers (IEEE), Association of Computing Machinery (ACM), and Elsevier Science Direct Digital Library. While the application of data analytics in the construction industry is not new, the adoption of Big Data technologies in this industry remains at a nascent stage and lags the broad uptake of these technologies in other fields. To the best of our knowledge, there is currently no comprehensive survey of Big Data techniques in the context of the construction industry. This paper fills the void and presents a wide-ranging interdisciplinary review of literature of fields such as statistics, data mining and warehousing, machine learning, and Big Data Analytics in the context of the construction industry. We discuss the current state of adoption of Big Data in the construction industry and discuss the future potential of such technologies across the multiple domain-specific sub-areas of the construction industry. We also propose open issues and directions for future work along with potential pitfalls associated with Big Data adoption in the industry.",2016,,https://doi.org/10.1016/j.aei.2016.07.001
1081,Ryan C. LaBrie and Gerhard H. Steinke and Xiangmin Li and Joseph A. Cazier,Big data analytics sentiment: US-China reaction to data collection by business and government,"Big data ethics, Business data usage, Corporate data collection, Government data usage, Technology ethics, US-China similarities, US-China differences","As society continues its rapid change to a digitized individual, corporate, and government environment it is prudent for researchers to investigate the zeitgeist of the global citizenry. The technological changes brought about by big data analytics are changing the way we gather and view data. This big data analytics sentiment research examines how Chinese and American respondents may view big data collection and analytics differently. The paper follows with an analysis of reported attitudes toward possible viewpoints from each country on various big data analytics topics ranging from individual to business and governmental foci. Hofstede's cultural dimensions are used to inform and frame our research hypotheses. Findings suggest that Chinese and American perspectives differ on individual data values, with the Chinese being more open to data collection and analytic techniques targeted toward individuals. Furthermore, support is found that US respondents have a more favorable view of businesses' use of data analytics. Finally, there is a strong difference in the attitudes toward governmental use of data, where US respondents do not favor governmental big data analytics usage and the Chinese respondents indicated a greater acceptance of governmental data usage. These findings are helpful in better understanding appropriate technological change and adoption from a societal perspective. Specifically, this research provides insights for corporate business and government entities suggesting how they might adjust their approach to big data collection and management in order to better support and sustain their organization's services and products.",2018,,https://doi.org/10.1016/j.techfore.2017.06.029
1082,Babak Sohrabi and Ahmad Khalilijafarabad,Systematic method for finding emergence research areas as data quality,"Data quality, Text mining, Science mapping, Data mining, Trend analysis","The analysis of the transformation and changes in scientific disciplines has always been a critical path for policymakers and researchers. The current study examines the changes in the research areas of data and information quality (DIQ). The aim of this study was to detect different types of changes occurring in the scientific areas including birth, death, growth, decline, merge, and splitting. A model has been developed for this data mining. To test the model, all DIQ articles published in online scientific citation indexing service or Web of Science (WOS) between 1970 and 2016 were extracted and analyzed using the given model. The study is related to the Big Data as well as the integration methods in Big Data which is the most important area in DIQ. It is demonstrated that the first and second emerging research areas are sub-disciplines of entity resolution and record linkage. Accordingly, linkage and privacy are the first emerging research area and the entity resolution using ontology is the second in DIQ. This is followed by the social media issues and genetic related DIQ issues.",2018,,https://doi.org/10.1016/j.techfore.2018.08.003
1083,Natalia Norori and Qiyang Hu and Florence Marcelle Aellen and Francesca Dalia Faraci and Athina Tzovara,Addressing bias in big data and AI for health care: A call for open science,"artificial intelligence, deep learning, health care, bias, open science, participatory science, data standards","Summary
Artificial intelligence (AI) has an astonishing potential in assisting clinical decision making and revolutionizing the field of health care. A major open challenge that AI will need to address before its integration in the clinical routine is that of algorithmic bias. Most AI algorithms need big datasets to learn from, but several groups of the human population have a long history of being absent or misrepresented in existing biomedical datasets. If the training data is misrepresentative of the population variability, AI is prone to reinforcing bias, which can lead to fatal outcomes, misdiagnoses, and lack of generalization. Here, we describe the challenges in rendering AI algorithms fairer, and we propose concrete steps for addressing bias using tools from the field of open science.",2021,,https://doi.org/10.1016/j.patter.2021.100347
1084,Ruifang Zhu and Shifan Han and Yanbing Su and Chichen Zhang and Qi Yu and Zhiguang Duan,The application of big data and the development of nursing science: A discussion paper,"Artificial intelligence, Data mining, Knowledge bases, Nursing","Based on the concept and research status of big data, we analyze and examine the importance of constructing the knowledge system of nursing science for the development of the nursing discipline in the context of big data and propose that it is necessary to establish big data centers for nursing science to share resources, unify language standards, improve professional nursing databases, and establish a knowledge system structure.",2019,,https://doi.org/10.1016/j.ijnss.2019.03.001
1085,Frederick J. Riggins and Bonnie K. Klamm,Data governance case at KrauseMcMahon LLP in an era of self-service BI and Big Data,"Big Data, Data governance, Self-service business intelligence","This case increases your understanding of data governance in an era of sophisticated analytics and Big Data where corporate data integrity and data quality may be at risk. KrauseMcMahon, a large certified public accounting and business consulting firm, faces a tradeoff of increasing control of the company’s data assets versus unleashing end user innovation due to the proliferation of self-service business intelligence tools. You are required to analyze the issues in the case from organizational, financial, and technical perspectives to propose alternatives the organization should consider and make specific recommendations on how the company should proceed. By completing this case, you will demonstrate cross-disciplinary abilities related to foundational business, accounting, and broad management competencies. By addressing such competencies, the case requires your use of accounting, MIS, and upper-level business skills.",2017,,https://doi.org/10.1016/j.jaccedu.2016.12.002
1086,Laura Sebastian-Coleman,Chapter 10 - Dimensions of Data Quality,"Data quality dimensions, data completeness, data integrity, validity, data currency, metadata management, reference data management, data modeling","This chapter provides an in-depth discussion about a core concept in data quality management: data quality dimensions. Dimensions provide a framework through which we can understand the core capabilities. As the foundation for data quality rules and requirements, they play a critical role in helping answer the fundamental questions about data quality: “What do we mean by high-quality data?” “How do we detect low-quality data?” and “What action will we take when data does not meet quality standards?” This chapter will review a comprehensive set of dimensions (i.e., completeness, correctness, uniqueness, consistency, currency, validity, integrity, reasonability, precision, clarity, accessibility, timeliness, relevance, usability, trustworthiness) in the context of challenges associated with data structure and meaning, the processes for creating data, the influence of technology on quality, and the perceptions of data consumers.",2022,,https://doi.org/10.1016/B978-0-12-821737-5.00010-9
1087,A. Vidhyalakshmi and C. Priya,Chapter 1 - Medical big data mining and processing in e-health care,"Big data, IoT, health care, telemedicine, WHO, image processing","Health care is thought to be one of the business fields with the largest big data potential. Based on the prevailing definition, big data has a large amount of data which can be processed easily and can be modified or updated easily. These data can be quickly stored, processed, and transformed into valuable information using older technologies. At present, many new trends regarding new data resources and innovative data analysis are followed in medicine and health care. In practice, electronic health-care records, free open-source data, and the “quantified self” provide new approaches for analyzing data. Some of these advancements have been made in information extraction from the text data based on analytics, which is useful in data unlocking for analytics purposes from clinical documentation. Choosing big data approaches in the medicine and health-care fields has been lagging. This has led to the rise specific problems regarding data complexity and organizational, legal, and ethical challenges. With the growth of the uptake of big data in general, and medicine and health care in specific, innovative ideas and solutions are expected. Telemedicine is a new opportunity for the Internet of Things (IoT). This enables the specialist to consult a patient despite them being in different places. Medical image segmentation is needed for the analysis, storage, and protection of medical images in telemedicine. Telemedicine is defined by the World Health Organization (WHO) as “the practice of medical care using interactive audiovisual and data communications. This includes the delivery of medical care services, diagnosis, consultation, treatment, as well as health education and the transfer of medical data.” IoT-based applications mainly include remote patient monitoring and clinical monitoring. In addition, preventive measures-based applications are also part of smart health care. These applications require image processing-based technologies which could be integrated into medical health-care systems. Various types of input taken from cameras and processing of CT and MRI images could be integrated into IoT-based medical applications.",2020,,https://doi.org/10.1016/B978-0-12-821326-1.00001-2
1088,Earl McKinney and Charles J. Yoos and Ken Snead,The need for ‘skeptical’ accountants in the era of Big Data,"Big Data, Analysis, Informed skepticism, Questioning","Big Data is now readily available for analysis, and analysts trained to conduct effective analysis in this area are in high demand. However, there is a dearth of discussion in the literature related to identifying the important cognitive skills required for accountants to conduct effective Big Data analysis. Here we argue that accountants need to approach Big Data analysis as informed skeptics, being ever ready to challenge the analysis by asking good questions in appropriate topical areas. These areas include understanding the limits of measurement and representation, the subjectiveness of insight, the challenges of statistics and integrating data sets, and the effects of underdetermination and inductive reasoning. Accordingly, we develop a framework and an illustrative example to facilitate the training of accounting students to become informed skeptics in the era of Big Data by explaining the conceptual relevance of each of the topical areas to Big Data analysis. In addition, example questions are identified that accountants conducting Big Data analysis should be asking regarding each topic. Further, for each topic, references to additional resources are provided that students can access to learn more about effectively conducting Big Data analysis.",2017,,https://doi.org/10.1016/j.jaccedu.2016.12.007
1089,Yichuan Wang and Nick Hajli,Exploring the path to big data analytics success in healthcare,"Big data analytics, Business value, Capability building view, Resource-based theory, Information technology source management, Health care industries","Although big data analytics have tremendous benefits for healthcare organizations, extant research has paid insufficient attention to the exploration of its business value. In order to bridge this knowledge gap, this study proposes a big data analytics-enabled business value model in which we use the resource-based theory (RBT) and capability building view to explain how big data analytics capabilities can be developed and what potential benefits can be obtained by these capabilities in the health care industries. Using this model, we investigate 109 case descriptions, covering 63 healthcare organizations to explore the causal relationships between the big data analytics capabilities and business value and the path-to-value chains for big data analytics success. Our findings provide new insights to healthcare practitioners on how to constitute big data analytics capabilities for business transformation and offer an empirical basis that can stimulate a more detailed investigation of big data analytics implementation.",2017,,https://doi.org/10.1016/j.jbusres.2016.08.002
1090,Dennis Broeders and Erik Schrijvers and Bart {van der Sloot} and Rosamunde {van Brakel} and Josta {de Hoog} and Ernst {Hirsch Ballin},Big Data and security policies: Towards a framework for regulating the phases of analytics and use of Big Data,"Big Data, Security, Data protection, Privacy, Regulation, Fraud, Policing, Surveillance, Algorithmic accountability, the Netherlands","Big Data analytics in national security, law enforcement and the fight against fraud have the potential to reap great benefits for states, citizens and society but require extra safeguards to protect citizens' fundamental rights. This involves a crucial shift in emphasis from regulating Big Data collection to regulating the phases of analysis and use. In order to benefit from the use of Big Data analytics in the field of security, a framework has to be developed that adds new layers of protection for fundamental rights and safeguards against erroneous and malicious use. Additional regulation is needed at the levels of analysis and use, and the oversight regime is in need of strengthening. At the level of analysis – the algorithmic heart of Big Data processes – a duty of care should be introduced that is part of an internal audit and external review procedure. Big Data projects should also be subject to a sunset clause. At the level of use, profiles and (semi-) automated decision-making should be regulated more tightly. Moreover, the responsibility of the data processing party for accuracy of analysis – and decisions taken on its basis – should be anchored in legislation. The general and security-specific oversight functions should be strengthened in terms of technological expertise, access and resources. The possibilities for judicial review should be expanded to stimulate the development of case law.",2017,,https://doi.org/10.1016/j.clsr.2017.03.002
1091,Ekaterina Smirnova and Andrada Ivanescu and Jiawei Bai and Ciprian M. Crainiceanu,A practical guide to big data,"Big data, Wearable and implantable computing, Accelerometer",Big Data is increasingly prevalent in science and data analysis. We provide a short tutorial for adapting to these changes and making the necessary adjustments to the academic culture to keep Biostatistics truly impactful in scientific research.,2018,,https://doi.org/10.1016/j.spl.2018.02.014
1092,Bonnie L. Westra and Jessica J. Peterson,Big Data and Perioperative Nursing,"big data, perioperative nursing, quality care, nursing knowledge, nursing informatics","Big data are large volumes of digital data that can be collected from disparate sources and are challenging to analyze. These data are often described with the five “Vs”: volume, velocity, variety, veracity, and value. Perioperative nurses contribute to big data through documentation in the electronic health record during routine surgical care, and these data have implications for clinical decision making, administrative decisions, quality improvement, and big data science. This article explores methods to improve the quality of perioperative nursing data and provides examples of how these data can be combined with broader nursing data for quality improvement. We also discuss a national action plan for nursing knowledge and big data science and how perioperative nurses can engage in collaborative actions to transform health care. Standardized perioperative nursing data has the potential to affect care far beyond the original patient.",2016,,https://doi.org/10.1016/j.aorn.2016.07.009
1093,Kaile Zhou and Chao Fu and Shanlin Yang,Big data driven smart energy management: From big data to big insights,"Energy big data, Smart energy management, Big data analytics, Smart grid, Demand side management (DSM)","Large amounts of data are increasingly accumulated in the energy sector with the continuous application of sensors, wireless transmission, network communication, and cloud computing technologies. To fulfill the potential of energy big data and obtain insights to achieve smart energy management, we present a comprehensive study of big data driven smart energy management. We first discuss the sources and characteristics of energy big data. Also, a process model of big data driven smart energy management is proposed. Then taking smart grid as the research background, we provide a systematic review of big data analytics for smart energy management. It is discussed from four major aspects, namely power generation side management, microgrid and renewable energy management, asset management and collaborative operation, as well as demand side management (DSM). Afterwards, the industrial development of big data-driven smart energy management is analyzed and discussed. Finally, we point out the challenges of big data-driven smart energy management in IT infrastructure, data collection and governance, data integration and sharing, processing and analysis, security and privacy, and professionals.",2016,,https://doi.org/10.1016/j.rser.2015.11.050
1094,Fabien Lareyre and Cédric Adam and Marion Carrier and Juliette Raffort,Artificial Intelligence in Vascular Surgery: Moving from Big Data to Smart Data,,,2020,,https://doi.org/10.1016/j.avsg.2020.04.022
1095,Reza Saneei Moghadam and Ricardo Colomo-Palacios,Information security governance in big data environments: A systematic mapping,"information security governance, big data, framework, systematic mapping","Information security governance is an important aspects for all organizations. Given the crucial importance of IT systems and the increasing range of threats these systems are facing, there is an increasing interest on the topic. On the other hand, Big Data environments are also beginning to be more pervasive as IT is increasing its importance for organizations worldwide. In order to better know which aspects are the most important for the intersection of Big Data and information security governance, authors present in this paper a systematic mapping on this topic. Authors illustrate challenges and gaps concerning the topic and clarify these challenges by means of a classification of the environments they take place, the security risk spectrums they concern, and the security governance measures they take to mitigate them; by providing solutions as in a framework, model, software or tool, wherever possible. Results are expected to be useful for IT security professionals and information systems practitioners as a whole.",2018,,https://doi.org/10.1016/j.procs.2018.10.057
1096,A. {Núñez Reiz},Big data and machine learning in critical care: Opportunities for collaborative research,"Big data, Machine learning, Artificial intelligence, Clinical databases, MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases de datos clínicos, MIMIC III, Datathon, Trabajo colaborativo","The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.
Resumen
La aparición de los sistemas de información clínica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos clínicos en formato electrónico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas clínicas, para su uso en la gestión de recursos o para sugerir líneas de investigación que luego pueden ser explotadas mediante ensayos clínicos aleatorizados. Sin embargo, los médicos clínicos carecen de la formación necesaria para la explotación de grandes bases de datos, lo que supone un obstáculo para aprovechar esta oportunidad. Además, existen cuestiones de índole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estadísticos, epidemiólogos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigación clínica o análisis de resultados (benchmarking). Se describe la reunión multidisciplinar (Critical Care Datathon) realizada en Madrid los días 1, 2 y 3 de diciembre de 2017. Esta reunión, celebrada bajo los auspicios de la Sociedad Española de Medicina Intensiva, Crítica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovación y el Servicio de Medicina Intensiva del Hospital Clínico San Carlos, así como el grupo de investigación «Life Supporting Technologies» de la Universidad Politécnica de Madrid. Tras unas ponencias de formación sobre big data, seguridad y calidad de los datos, y su aplicación al entorno de la medicina intensiva, un grupo de clínicos, analistas de datos, estadísticos, expertos en seguridad informática de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas clínicas establecidas previamente a la reunión. El trabajo colaborativo permitió establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias líneas de investigación clínica a desarrollar en el futuro. Además, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en España, y se estableció un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparación de resultados entre unidades o la elaboración de nuevas líneas de investigación clínica.",2019,,https://doi.org/10.1016/j.medine.2018.06.006
1097,Zakariae {El Ouazzani} and Hanan {El Bakkali},A new technique ensuring privacy in big data: K-anonymity without prior value of the threshold k,"k-anonymity, quasi identifier attributes, big data, anonymization, privacy","Big data has become omnipresent and crucial for many application domains. Big data makes reference to the explosive quantity of data generated in today’s society that might contain personally identifiable information (PII). That’s why the challenge from the point of view of data privacy is one of the major hurdles for the application of big data. In that situation, several techniques were exposed in order to ensure privacy in big data including generalization, randomization and cryptographic techniques as well. It is well known that there exist two main types of attributes in the literature, quasi identifier and sensitive attributes. In this paper, we are going to focus on quasi identifier attributes. Over the years, k-anonymity has been treated with great interest as an anonymization technique ensuring privacy in big data when we are dealing with quasi identifier attributes. Despite the fact that many algorithms of k-anonymity have been proposed, most of them admit that the threshold k of k-anonymity has to be known before anonymizing the data set. Here, a novel way in applying k-anonymity for quasi identifier attributes is presented. It’s a new algorithm called “k-anonymity without prior value of the threshold k”. Our proposed algorithm was experimentally evaluated using a test table of quasi identifier attributes. Furthermore, we highlight all the steps of our proposed algorithm with detailed comments.",2018,,https://doi.org/10.1016/j.procs.2018.01.097
1098,YuJie Ben and FuJun Ma and Hao Wang and Muhammad Azher Hassan and Romanenko Yevheniia and WenHong Fan and Yubiao Li and ZhaoMin Dong,A spatio-temporally weighted hybrid model to improve estimates of personal PM2.5 exposure: Incorporating big data from multiple data sources,"Exposure assessment, Indoor PM, Ambient PM, In-home monitors, Shanghai","An accurate estimation of population exposure to particulate matter with an aerodynamic diameter <2.5 μm (PM2.5) is crucial to hazard assessment and epidemiology. This study integrated annual data from 1146 in-home air monitors, air quality monitoring network, public applications, and traffic smart cards to determine the pattern of PM2.5 concentrations and activities in different microenvironments (including outdoors, indoors, subways, buses, and cars). By combining massive amounts of signaling data from cell phones, this study applied a spatio-temporally weighted model to improve the estimation of PM2.5 exposure. Using Shanghai as a case study, the annual average indoor PM2.5 concentration was estimated to be 29.3 ± 27.1 μg/m3 (n = 365), with an average infiltration factor of 0.63. The spatio-temporally weighted PM2.5 exposure was estimated to be 32.1 ± 13.9 μg/m3 (n = 365), with indoor PM2.5 contributing the most (85.1%), followed by outdoor (7.6%), bus (3.7%), subway (3.1%), and car (0.5%). However, considering that outdoor PM2.5 makes a significant contribution to indoor PM2.5, outdoor PM2.5 was responsible for most of the exposure in Shanghai. A heatmap of PM2.5 exposure indicated that the inner-city exposure index was significantly higher than that of the outskirts city, which demonstrated that the importance of spatial differences in population exposure estimation.",2019,,https://doi.org/10.1016/j.envpol.2019.07.034
1099,Abdulsalam Yassine and Shailendra Singh and M. Shamim Hossain and Ghulam Muhammad,IoT big data analytics for smart homes with fog and cloud computing,"Internet of Things (IoT), Cloud computing, Fog computing, Big data analytics, Energy management, Smart homes","Internet of Things (IoT) analytics is an essential mean to derive knowledge and support applications for smart homes. Connected appliances and devices inside the smart home produce a significant amount of data about consumers and how they go about their daily activities. IoT analytics can aid in personalizing applications that benefit both homeowners and the ever growing industries that need to tap into consumers profiles. This article presents a new platform that enables innovative analytics on IoT captured data from smart homes. We propose the use of fog nodes and cloud system to allow data-driven services and address the challenges of complexities and resource demands for online and offline data processing, storage, and classification analysis. We discuss in this paper the requirements and the design components of the system. To validate the platform and present meaningful results, we present a case study using a dataset acquired from real smart home in Vancouver, Canada. The results of the experiments show clearly the benefit and practicality of the proposed platform.",2019,,https://doi.org/10.1016/j.future.2018.08.040
1100,Álvaro Valencia-Parra and Luisa Parody and Ángel Jesús Varela-Vaca and Ismael Caballero and María Teresa Gómez-López,DMN4DQ: When data quality meets DMN,"Data usability, Data quality, Decision model and notation, Data quality rule, Data quality assessment, Data quality measurement","To succeed in their business processes, organizations need data that not only attains suitable levels of quality for the task at hand, but that can also be considered as usable for the business. However, many researchers ground the potential usability of the data on its quality. Organizations would benefit from receiving recommendations on the usability of the data before its use. We propose that the recommendation on the usability of the data be supported by a decision process, which includes a context-dependent data-quality assessment based on business rules. Ideally, this recommendation would be generated automatically. Decision Model and Notation (DMN) enables the assessment of data quality based on the evaluation of business rules, and also, provides stakeholders (e.g., data stewards) with sound support for the automation of the whole process of generation of a recommendation regarding usability based on data quality. The main contribution of the proposal involves designing and enabling both DMN-driven mechanisms and a guiding methodology (DMN4DQ) to support the automatic generation of a decision-based recommendation on the potential usability of a data record in terms of its level of data quality. Furthermore, the validation of the proposal is performed through the application of a real dataset.",2021,,https://doi.org/10.1016/j.dss.2020.113450
1101,In Lee,"Big data: Dimensions, evolution, impacts, and challenges","Big data, Internet of things, Data analytics, Sentiment analysis, Social network analysis, Web analytics","Big data represents a new technology paradigm for data that are generated at high velocity and high volume, and with high variety. Big data is envisioned as a game changer capable of revolutionizing the way businesses operate in many industries. This article introduces an integrated view of big data, traces the evolution of big data over the past 20 years, and discusses data analytics essential for processing various structured and unstructured data. This article illustrates the application of data analytics using merchant review data. The impacts of big data on key business performances are then evaluated. Finally, six technical and managerial challenges are discussed.",2017,,https://doi.org/10.1016/j.bushor.2017.01.004
1102,Sunil Erevelles and Nobuyuki Fukawa and Linda Swayne,Big Data consumer analytics and the transformation of marketing,"Big Data, Consumer analytics, Consumer insights, Resource-based theory, Induction, Ignorance","Consumer analytics is at the epicenter of a Big Data revolution. Technology helps capture rich and plentiful data on consumer phenomena in real time. Thus, unprecedented volume, velocity, and variety of primary data, Big Data, are available from individual consumers. To better understand the impact of Big Data on various marketing activities, enabling firms to better exploit its benefits, a conceptual framework that builds on resource-based theory is proposed. Three resources—physical, human, and organizational capital—moderate the following: (1) the process of collecting and storing evidence of consumer activity as Big Data, (2) the process of extracting consumer insight from Big Data, and (3) the process of utilizing consumer insight to enhance dynamic/adaptive capabilities. Furthermore, unique resource requirements for firms to benefit from Big Data are discussed.",2016,,https://doi.org/10.1016/j.jbusres.2015.07.001
1103,Yichuan Wang and LeeAnn Kung and William Yu Chung Wang and Casey G. Cegielski,An integrated big data analytics-enabled transformation model: Application to health care,"Big data analytics, IT-enabled transformation, Practice-based view, Business value of IT, Healthcare, Content analysis","A big data analytics-enabled transformation model based on practice-based view is developed, which reveals the causal relationships among big data analytics capabilities, IT-enabled transformation practices, benefit dimensions, and business values. This model was then tested in healthcare setting. By analyzing big data implementation cases, we sought to understand how big data analytics capabilities transform organizational practices, thereby generating potential benefits. In addition to conceptually defining four big data analytics capabilities, the model offers a strategic view of big data analytics. Three significant path-to-value chains were identified for healthcare organizations by applying the model, which provides practical insights for managers.",2018,,https://doi.org/10.1016/j.im.2017.04.001
1104,Sergi Nadal and Victor Herrero and Oscar Romero and Alberto Abelló and Xavier Franch and Stijn Vansummeren and Danilo Valerio,A software reference architecture for semantic-aware Big Data systems,"Big Data, Software reference architecture, Semantic-aware, Data management, Data analysis","Context: Big Data systems are a class of software systems that ingest, store, process and serve massive amounts of heterogeneous data, from multiple sources. Despite their undisputed impact in current society, their engineering is still in its infancy and companies find it difficult to adopt them due to their inherent complexity. Existing attempts to provide architectural guidelines for their engineering fail to take into account important Big Data characteristics, such as the management, evolution and quality of the data. Objective: In this paper, we follow software engineering principles to refine the λ-architecture, a reference model for Big Data systems, and use it as seed to create Bolster, a software reference architecture (SRA) for semantic-aware Big Data systems. Method: By including a new layer into the λ-architecture, the Semantic Layer, Bolster is capable of handling the most representative Big Data characteristics (i.e., Volume, Velocity, Variety, Variability and Veracity). Results: We present the successful implementation of Bolster in three industrial projects, involving five organizations. The validation results show high level of agreement among practitioners from all organizations with respect to standard quality factors. Conclusion: As an SRA, Bolster allows organizations to design concrete architectures tailored to their specific needs. A distinguishing feature is that it provides semantic-awareness in Big Data Systems. These are Big Data system implementations that have components to simplify data definition and exploitation. In particular, they leverage metadata (i.e., data describing data) to enable (partial) automation of data exploitation and to aid the user in their decision making processes. This simplification supports the differentiation of responsibilities into cohesive roles enhancing data governance.",2017,,https://doi.org/10.1016/j.infsof.2017.06.001
1105,Ejaz Ahmed and Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Imran Khan and Abdelmuttlib Ibrahim Abdalla Ahmed and Muhammad Imran and Athanasios V. Vasilakos,The role of big data analytics in Internet of Things,"Internet of Things, Big data, Analytics, Distributed computing, Smart city","The explosive growth in the number of devices connected to the Internet of Things (IoT) and the exponential increase in data consumption only reflect how the growth of big data perfectly overlaps with that of IoT. The management of big data in a continuously expanding network gives rise to non-trivial concerns regarding data collection efficiency, data processing, analytics, and security. To address these concerns, researchers have examined the challenges associated with the successful deployment of IoT. Despite the large number of studies on big data, analytics, and IoT, the convergence of these areas creates several opportunities for flourishing big data and analytics for IoT systems. In this paper, we explore the recent advances in big data analytics for IoT systems as well as the key requirements for managing big data and for enabling analytics in an IoT environment. We taxonomized the literature based on important parameters. We identify the opportunities resulting from the convergence of big data, analytics, and IoT as well as discuss the role of big data analytics in IoT applications. Finally, several open challenges are presented as future research directions.",2017,,https://doi.org/10.1016/j.comnet.2017.06.013
1106,Samiya Khan and Xiufeng Liu and Kashish A. Shakil and Mansaf Alam,A survey on scholarly data: From big data perspective,"Scholarly data, Big data, Cloud-based big data analytics, Big scholarly data, Big data platform, Scholarly applications","Recently, there has been a shifting focus of organizations and governments towards digitization of academic and technical documents, adding a new facet to the concept of digital libraries. The volume, variety and velocity of this generated data, satisfies the big data definition, as a result of which, this scholarly reserve is popularly referred to as big scholarly data. In order to facilitate data analytics for big scholarly data, architectures and services for the same need to be developed. The evolving nature of research problems has made them essentially interdisciplinary. As a result, there is a growing demand for scholarly applications like collaborator discovery, expert finding and research recommendation systems, in addition to several others. This research paper investigates the current trends and identifies the existing challenges in development of a big scholarly data platform, with specific focus on directions for future research and maps them to the different phases of the big data lifecycle.",2017,,https://doi.org/10.1016/j.ipm.2017.03.006
1107,Nusrat J. Shoumy and Li-Minn Ang and Kah Phooi Seng and D.M.Motiur Rahaman and Tanveer Zia,"Multimodal big data affective analytics: A comprehensive survey using text, audio, visual and physiological signals","Affective computing, Multimodal fusion, Sentiment databases, Sentiment analysis, Affective applications","Affective computing is an emerging multidisciplinary research field that is increasingly drawing the attention of researchers and practitioners in various fields, including artificial intelligence, natural language processing, cognitive and social sciences. Research in affective computing includes areas such as sentiment, emotion, and opinion modelling. The internet is an excellent source of data required for sentiment analysis, such as customer reviews of products, social media, forums, blogs, etc. Most of these data, called big data, are unstructured and unorganized. Hence there is a strong demand for developing suitable data processing techniques to process these rich and valuable data to produce useful information. Early surveys on sentiment and emotion recognition in the literature have been limited to discussions using text, audio, and visual modalities. So far, to the author's knowledge, a comprehensive survey combining physiological modalities with these other modalities for affective computing has yet to be reported. The objective of this paper is to fill the gap in this surveyed area. The usage of physiological modalities for affective computing brings several benefits in that the signals can be used in different environmental conditions, more robust systems can be constructed in combination with other modalities, and it has increased anti-spoofing characteristics. The paper includes extensive reviews on different frameworks and categories for state-of-the-art techniques, critical analysis of their performances, and discussions of their applications, trends and future directions to serve as guidelines for readers towards this emerging research area.",2020,,https://doi.org/10.1016/j.jnca.2019.102447
1108,Liyakathunisa Syed and Saima Jabeen and Manimala S. and Abdullah Alsaeedi,Smart healthcare framework for ambient assisted living using IoMT and big data analytics techniques,"Ambient Assisted Living (AAL), Big data analytics, Internet of Medical Things (IoMT), Machine learning techniques, Physical activities, Wearable sensors","In the era of pervasive computing, human living has become smarter by the latest advancements in IoMT (Internet of Medical Things), wearable sensors and telecommunication technologies in order to deliver smart healthcare services. IoMT has the potential to revolutionize the healthcare industry. IoMT interconnects wearable sensors, patients, healthcare providers and caregivers via software and ICT (Information and Communication Technology). AAL (Ambient Assisted Living) enables integration of new technologies to be part of our daily life activities. In this paper, we have provided a novel smart healthcare framework for AAL to monitor the physical activities of elderly people using IoMT and intelligent machine learning algorithms for faster analysis, decision making and better treatment recommendations. Data is collected from multiple wearable sensors placed on subject’s left ankle, right arm, and chest, is transmitted through IoMT devices to the integrated cloud and data analytics layer. To process huge amounts of data in parallel, Hadoop MapReduce techniques are used. Multinomial Naïve Bayes classifier, which fits into the MapReduce paradigm, is utilized to recognize the motion experienced by different body parts and provides higher scalability and better performance with parallel processing when compared to serial processor. Our proposed framework predicts 12 physical activities with an overall accuracy of 97.1%. This can be considered as an optimal solution for recognizing physical activities to remotely monitor health conditions of elderly people.",2019,,https://doi.org/10.1016/j.future.2019.06.004
1109,Songnian Li and Suzana Dragicevic and Francesc Antón Castro and Monika Sester and Stephan Winter and Arzu Coltekin and Christopher Pettit and Bin Jiang and James Haworth and Alfred Stein and Tao Cheng,Geospatial big data handling theory and methods: A review and research challenges,"Big data, Geospatial, Data handling, Analytics, Spatial modeling, Review","Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future.",2016,,https://doi.org/10.1016/j.isprsjprs.2015.10.012
1110,Ersan Kabalci and Yasin Kabalci,"Chapter 8 - Big data, privacy and security in smart grids","Internet of things (IoT), Machine-to-machine communication (M2M), Human to machine (H2M), Machine learning, Smart grid security, Hadoop, Data mining, Security, Privacy","The smart grid applications are related with monitoring and control operations of conventional power grid. The integration of information and communication technologies (ICT) to existing power network has leveraged interaction of different generators, controllers, monitoring and measurement devices, and intelligent loads. The two-way communication infrastructure is comprised by numerous sensor networks that increased deployment of massive data from measurement nodes to monitoring centers. Big data is a widespread concept which become a trend for massive data streams that are transferred and processed in an ecosystem. The enormous amount of data are generated, transferred and stored to improve operating and management quality of smart grid. The big data analytics are performed to improve quality of service in terms of grid operators and consumers. The big data acquisition, processing, storing, and clustering stages are widely researched by a wide variety of specialist. In this chapter, the all these stages, big data acquisition technologies, machine learning methods used in big data analytics, privacy and security of big data infrastructure are introduced in detail. The privacy preserving methods, big data processing technologies and firmware infrastructures are presented in the context of this chapter.",2019,,https://doi.org/10.1016/B978-0-12-819710-3.00008-9
1111,Natalija Koseleva and Guoda Ropaite,Big Data in Building Energy Efficiency: Understanding of Big Data and Main Challenges,"Energy Efficiency, Big Data, Characteristics of Big Data, Big Data analysis, Construction, Web of Science","Data generation has increased drastically over the past few years. Data management has also grown in importance because extracting the significant value out of a huge pile of raw data is of prime important thing to make different decisions. One of the important sectors nowadays is construction sector, especially building energy efficiency field. Collecting big amount of data, using different kinds of big data analysis can help to improve construction process from the energy efficiency perspective. This article reviews the understanding of Big Data, methods used for Big Data analysis and the main problems with Big Data in the field of energy.",2017,,https://doi.org/10.1016/j.proeng.2017.02.064
1112,Fernando Gualo and Moisés Rodriguez and Javier Verdugo and Ismael Caballero and Mario Piattini,Data quality certification using ISO/IEC 25012: Industrial experiences,"Data quality evaluation process, Data quality certification, Data quality management, ISO/IEC 25012, ISO/IEC 25024, ISO/IEC 25040","The most successful organizations in the world are data-driven businesses. Data is at the core of the business of many organizations as one of the most important assets, since the decisions they make cannot be better than the data on which they are based. Due to this reason, organizations need to be able to trust their data. One important activity that helps to achieve data reliability is the evaluation and certification of the quality level of organizational data repositories. This paper describes the results of the application of a data quality evaluation and certification process to the repositories of three European organizations belonging to different sectors. We present findings from the point of view of both the data quality evaluation team and the organizations that underwent the evaluation process. In this respect, several benefits have been explicitly recognized by the involved organizations after achieving the data quality certification for their repositories (e.g., long-term organizational sustainability better internal knowledge of data, and a more efficient management of data quality). As a result of this experience, we have also identified a set of best practices aimed to enhance the data quality evaluation process.",2021,,https://doi.org/10.1016/j.jss.2021.110938
1113,Karthik Kambatla and Giorgos Kollias and Vipin Kumar and Ananth Grama,Trends in big data analytics,"Big-data, Analytics, Data centers, Distributed systems","One of the major applications of future generation parallel and distributed systems is in big-data analytics. Data repositories for such applications currently exceed exabytes and are rapidly increasing in size. Beyond their sheer magnitude, these datasets and associated applications’ considerations pose significant challenges for method and software development. Datasets are often distributed and their size and privacy considerations warrant distributed techniques. Data often resides on platforms with widely varying computational and network capabilities. Considerations of fault-tolerance, security, and access control are critical in many applications (Dean and Ghemawat, 2004; Apache hadoop). Analysis tasks often have hard deadlines, and data quality is a major concern in yet other applications. For most emerging applications, data-driven models and methods, capable of operating at scale, are as-yet unknown. Even when known methods can be scaled, validation of results is a major issue. Characteristics of hardware platforms and the software stack fundamentally impact data analytics. In this article, we provide an overview of the state-of-the-art and focus on emerging trends to highlight the hardware, software, and application landscape of big-data analytics.",2014,,https://doi.org/10.1016/j.jpdc.2014.01.003
1114,Mauro Andreolini and Michele Colajanni and Marcello Pietri and Stefania Tosi,"Adaptive, scalable and reliable monitoring of big data on clouds","Adaptivity, Monitoring, Cloud computing, Big data, Scalability","Real-time monitoring of cloud resources is crucial for a variety of tasks such as performance analysis, workload management, capacity planning and fault detection. Applications producing big data make the monitoring task very difficult at high sampling frequencies because of high computational and communication overheads in collecting, storing, and managing information. We present an adaptive algorithm for monitoring big data applications that adapts the intervals of sampling and frequency of updates to data characteristics and administrator needs. Adaptivity allows us to limit computational and communication costs and to guarantee high reliability in capturing relevant load changes. Experimental evaluations performed on a large testbed show the ability of the proposed adaptive algorithm to reduce resource utilization and communication overhead of big data monitoring without penalizing the quality of data, and demonstrate our improvements to the state of the art.",2015,,https://doi.org/10.1016/j.jpdc.2014.08.007
1115,Shastri L Nimmagadda and Torsten Reiners and Lincoln C Wood,On Modelling Big Data Guided Supply Chains in Knowledge-Base Geographic Information Systems,"Supply Chain Management, Project Management, Laws of Geography, Domain Ontologies, Data Mining","We examine the existing goals of business- and geographic - information systems and their influence on logistics and supply chain management systems. Modelling supply chain management systems is held back because of lack of consistent and poorly aligned data with supply chain elements and processes. The issues constraining the decision-making process limit the connectivity between supply chains and geographically controlled database systems. The heterogeneous and unstructured data are added challenges to connectivity and integration processes. The research focus is on analysing the data heterogeneity and multidimensionality relevant to supply chain systems and geographically controlled databases. In pursuance of the challenges, a unified methodological framework is designed with data structuring, data warehousing and mining, visualization and interpretation artefacts to support connectivity and integration process. Multidimensional ontologies, ecosystem conceptualization and Big Data novelty are added motivations, facilitating the relationships between events of supply chain operations. The models construed for optimizing the resources are analysed in terms of effectiveness of the integrated framework articulations in global supply chains that obey laws of geography. The integrated articulations analysed with laws of geography can affect the operational costs, sure for better with reduced lead times and enhanced stock management.",2019,,https://doi.org/10.1016/j.procs.2019.09.284
1116,Linda D. Sharples,The role of statistics in the era of big data: Electronic health records for healthcare research,Electronic healthcare records,"The transferring of medical records into huge electronic databases has opened up opportunities for research but requires attention to data quality, study design and issues of bias and confounding.",2018,,https://doi.org/10.1016/j.spl.2018.02.044
1117,Laura Sebastian-Coleman,Chapter 9 - Core Data Quality Management Capabilities,"Data quality standards, assessing data quality, data quality monitoring, data quality reporting, data issue management, quality improvement methodology","This chapter describes the functions required to build organizational capacity to manage data for quality over time. They include: data quality standards, data quality assessment, data quality monitoring, data quality reporting, data quality issue management, and data quality improvement. These activities are likely to be executed more consistently and with greater impact if there is a data quality team specifically responsible for defining them and facilitating their adoption within the organization.",2022,,https://doi.org/10.1016/B978-0-12-821737-5.00009-2
1118,Surabhi Verma and Som Sekhar Bhattacharyya and Saurav Kumar,An extension of the technology acceptance model in the big data analytics system implementation environment,"Technology acceptance model, Big data analytics system, System quality, Information quality","Research on the adoption of systems for big data analytics has drawn enormous attention in Information Systems research. This study extends big data analytics adoption research by examining the effects of system characteristics on the attitude of managers towards the usage of big data analytics systems. A research model has been proposed in this study based on an extensive review of literature pertaining to the Technology Acceptance Model, with further validation by a survey of 150 big data analytics users. Results of this survey confirm that characteristics of the big data analytics system have significant direct and indirect effects on belief in the benefits of big data analytics systems and perceived usefulness, attitude and adoption. Moreover, there are mediation effects that exist among the system characteristics, benefits of big data analytics systems, perceived usefulness and the attitude towards using big data analytics system. This study expands the existing body of knowledge on the adoption of big data analytics systems, and benefits big data analytics providers and vendors while helping in the formulation of their business models.",2018,,https://doi.org/10.1016/j.ipm.2018.01.004
1119,Prableen Kaur and Manik Sharma and Mamta Mittal,Big Data and Machine Learning Based Secure Healthcare Framework,"Big Data, Healthcare, Big data analytics, disease diagnosis, predictive analysis, security, privacy","The paper presents a brief introduction to big data and its role in healthcare applications. It is observed that the use of big data architecture and techniques are continuously assisting in managing the expeditious data growth in healthcare industry. Here, initially an empirical study is performed to analyze the role of big data in healthcare industry. It has been observed that significant work has been done using big data in healthcare sector. Nowadays, it is intricate to envision the way the machine learning and big data can influence the healthcare industries. It has been observed that most of the authors who implemented the use of machine learning and big data analytics in disease diagnosis have not given significant weightage to the privacy and security of the data. Here, a novel design of smart and secure healthcare information system using machine learning and advanced security mechanism has been proposed to handle big data of medical industry. The innovation lies in the incorporation of optimal storage and data security layer used to maintain security and privacy. Different techniques like masking encryption, activity monitoring, granular access control, dynamic data encryption and end point validation have been incorporated. The proposed hybrid four layer healthcare model seems to be more effective disease diagnostic big data system.",2018,,https://doi.org/10.1016/j.procs.2018.05.020
1120,Qiumei Ouyang and Chao Wu and Lang Huang,"Methodologies, principles and prospects of applying big data in safety science research","Safety big data (SBD), Safety small data (SSD), Safety science, Big data application, Method, Principle, Prospect and challenge","It is clear that big data has numerous potential impacts in many fields. However, few papers discussed its applications in the field of safety science research. Additionally, there exist many problems that cannot be ignored when big data is applied to safety science, most outstanding of which is lack of universal supporting theory that guides how to apply big data to safety science research like methods, principles and approaches, etc. In other terms, it is not enough for big data to be viewed asa strong enabler for safety science applications mainly due to lack of universal and basic theory from the perspective of safety science. Considering the above analyzes, the two key objectives of this paper are: (1) to propose the connotation of safety big data (SBD) and its applying rules, methods and principles, and (2) to put forward some application prospects and challenges of big data to safety science research seen from theoretical research. First, by comparing SBD and traditional safety small data (SSD) from four aspects including theoretical research, typical research method, specific analysis method and processing mode, this paper puts forward the definition and connotation of SBD. Subsequently this paper further summarizes and extracts the application rules and methods of SBD. And then nine principles of SBD are explored and their relationship and application are addressed from the view of theory architecture and working framework in data processing flow. At last, this paper also discusses the potential applications and some hot issues of SBD. Overall, this paper will play an essential role in supporting the application of SBD. In addition, it will fill in the theory gaps in the field of SBD beyond traditional safety statistics, and further enriches the contents of safety science.",2018,,https://doi.org/10.1016/j.ssci.2017.08.012
1121,Akemi Takeoka Chatfield and Christopher G. Reddick,Customer agility and responsiveness through big data analytics for public value creation: A case study of Houston 311 on-demand services,"On-demand services, Customer agility, Systemic use, Big data, Big data analytics, IT assimilation, Process-level strategic alignment, Digital infrastructures, 311 services, Government","A theoretical framework for big data analytics-enabled customer agility and responsiveness was developed from extant IS research. In on-demand service environments, customer agility involves dynamic capabilities in sensing and responding to citizens. Using this framework, a case study examined a large city government's 311 on-demand services which had leveraged big data analytics. While we found the localized big data analytics use by some of the 22 departments for enhanced customer agility and on-demand 311 services, city-wide systemic change in on-demand service delivery through big data analytics use was not evident. From the case study we identified key institutional mechanisms for linking customer agility to public value creation through 311 services. We posit how systemic use of big data analytics embedded into critical processes enables the government to co-create public values with citizens through 311 on-demand services, indicating the importance of creating a culture of analytics driven by strong political leadership.",2018,,https://doi.org/10.1016/j.giq.2017.11.002
1122,Ahmed Oussous and Fatima-Zahra Benjelloun and Ayoub {Ait Lahcen} and Samir Belfkih,Big Data technologies: A survey,"Big Data, Hadoop, Big Data distributions, Big Data analytics, NoSQL, Machine learning","Developing Big Data applications has become increasingly important in the last few years. In fact, several organizations from different sectors depend increasingly on knowledge extracted from huge volumes of data. However, in Big Data context, traditional data techniques and platforms are less efficient. They show a slow responsiveness and lack of scalability, performance and accuracy. To face the complex Big Data challenges, much work has been carried out. As a result, various types of distributions and technologies have been developed. This paper is a review that survey recent technologies developed for Big Data. It aims to help to select and adopt the right combination of different Big Data technologies according to their technological needs and specific applications’ requirements. It provides not only a global view of main Big Data technologies but also comparisons according to different system layers such as Data Storage Layer, Data Processing Layer, Data Querying Layer, Data Access Layer and Management Layer. It categorizes and discusses main technologies features, advantages, limits and usages.",2018,,https://doi.org/10.1016/j.jksuci.2017.06.001
1123,Lang Huang and Chao Wu and Bing Wang and Qiumei Ouyang,Big-data-driven safety decision-making: A conceptual framework and its influencing factors,"Big data (BD), Safety big data (SBD), Safety decision-making (SDM), Safety insight (SI), Data-driven","Safety data and information are the most valuable assets for organizations’ safety decision-making (SDM), especially in the era of big data (BD). In this study, a conceptual framework for SDM based on BD, known as BD-driven SDM, was developed and its detailed structure and elements as well as strategies were presented. Other theoretical and practical contributions include: (a) the description of the meta-process and interdisciplinary research area of BD-driven SDM, (b) the design of six types of general analytics and five types of special analytics for SBD mining according to different requirements of safety management applications, (c) the analysis of influencing factors of BD-driven SDM, and (d) the discussion of advantages and limitations in this research as well as suggestions for future research. The results obtained from this study are of important implications for research and practice on BD-driven SDM.",2018,,https://doi.org/10.1016/j.ssci.2018.05.012
1124,Bonnie L. Westra and Martha Sylvia and Elizabeth F. Weinfurter and Lisiane Pruinelli and Jung In Park and Dianna Dodd and Gail M. Keenan and Patricia Senk and Rachel L. Richesson and Vicki Baukner and Christopher Cruz and Grace Gao and Luann Whittenburg and Connie W. Delaney,Big data science: A literature review of nursing research exemplars,"Big data, Data science, Nursing informatics, Nursing research, Nurse scientist","Background
Big data and cutting-edge analytic methods in nursing research challenge nurse scientists to extend the data sources and analytic methods used for discovering and translating knowledge.
Purpose
The purpose of this study was to identify, analyze, and synthesize exemplars of big data nursing research applied to practice and disseminated in key nursing informatics, general biomedical informatics, and nursing research journals.
Methods
A literature review of studies published between 2009 and 2015. There were 650 journal articles identified in 17 key nursing informatics, general biomedical informatics, and nursing research journals in the Web of Science database. After screening for inclusion and exclusion criteria, 17 studies published in 18 articles were identified as big data nursing research applied to practice.
Discussion
Nurses clearly are beginning to conduct big data research applied to practice. These studies represent multiple data sources and settings. Although numerous analytic methods were used, the fundamental issue remains to define the types of analyses consistent with big data analytic methods.
Conclusion
There are needs to increase the visibility of big data and data science research conducted by nurse scientists, further examine the use of state of the science in data analytics, and continue to expand the availability and use of a variety of scientific, governmental, and industry data resources. A major implication of this literature review is whether nursing faculty and preparation of future scientists (PhD programs) are prepared for big data and data science.",2017,,https://doi.org/10.1016/j.outlook.2016.11.021
1125,Shuangqi Li and Jianwei Li and Hongwen He and Hanxiao Wang,Lithium-ion battery modeling based on Big Data,"electric vehicle, lithium-ion power battery, modeling, battery management, bigdata, deeplearning","Battery is the bottleneck technology of electric vehicles. The complex chemical reactions inside the battery are difficult to monitor directly. The establishment of a precise mathematical model for the battery is of great significance in ensuring the secure and stable operation of the battery management system. First of all, a data cleaning method based on machine learning is put forward, which is applicable to the characteristics of big data from batteries in electric vehicles. Secondly, this paper establishes a lithium-ion battery model based on deep learning algorithm and the error of model based on different algorithms is compared. The data of electric buses are used for validating the effectiveness of the model. The result shows that the data cleaning method achieves good results, in the case of the terminal voltage missing, the mean absolute percentage error of filling is within 4%, and the battery modeling method in this paper is able to simulate the battery characteristics accurately, and the mean absolute percentage error of the terminal voltage estimation is within 2.5%.",2019,,https://doi.org/10.1016/j.egypro.2018.12.046
1126,Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang,A multidisciplinary perspective of big data in management research,"Big data, Management research, Literature review","In recent years, big data has emerged as one of the prominent buzzwords in business and management. In spite of the mounting body of research on big data across the social science disciplines, scholars have offered little synthesis on the current state of knowledge. To take stock of academic research that contributes to the big data revolution, this paper tracks scholarly work's perspectives on big data in the management domain over the past decade. We identify key themes emerging in management studies and develop an integrated framework to link the multiple streams of research in fields of organisation, operations, marketing, information management and other relevant areas. Our analysis uncovers a growing awareness of big data's business values and managerial changes led by data-driven approach. Stemming from the review is the suggestion for research that both structured and unstructured big data should be harnessed to advance understanding of big data value in informing organisational decisions and enhancing firm competitiveness. To discover the full value, firms need to formulate and implement a data-driven strategy. In light of these, the study identifies and outlines the implications and directions for future research.",2017,,https://doi.org/10.1016/j.ijpe.2017.06.006
1127,Jacky Akoka and Isabelle Comyn-Wattiau and Nabil Laoufi,Research on Big Data – A systematic mapping study,"Big Data, Systematic mapping study, Framework, Artefact, Usage, Analytics","Big Data has emerged as a significant area of study for both practitioners and researchers. Big Data is a term for massive data sets with large structure. In 2012, Big Data passed the top of the Gartner Hype Cycle, attesting the maturity level of this technology and its applications. The aim of this paper is to examine how do researchers grasp the big data concept? We will answer the following questions: How many research papers are produced? What is the annual trend of publications? What are the hot topics in big data research? What are the most investigated big data topics? Why the research is performed? What are the most frequently obtained research artefacts? What does big data research produces? Who are the active authors? Which journals include papers on Big Data? What are the active disciplines? For this purpose, we provide a framework identifying existing and emerging research areas of Big Data. This framework is based on eight dimensions, including the SMACIT (Social Mobile Analytics Cloud Internet of Things) perspective. Current and past research in Big Data are analyzed using a systematic mapping study of publications based on more than a decade of related academic publications. The results have shown that significant contributions have been made by the research community, attested by a continuous increase in the number of scientific publications that address Big Data. We found that researchers are increasingly involved in research combining Big Data and Analytics, Cloud, Internet of things, mobility or social media. As for quality objectives, besides an interest in performance, other topics as scalability is emerging. Moreover, security and quality aspects become important. Researchers on Big Data provide more algorithms, frameworks, and architectures than other artifacts. Finally, application domains such as earth, energy, medicine, ecology, marketing, and health attract more attention from researchers on big data. A complementary content analysis on a subset of papers sheds some light on the evolving field of big data research.",2017,,https://doi.org/10.1016/j.csi.2017.01.004
1128,Xiaowen Ruan and Yue Li and Xiaohui Jin and Pan Deng and Jiaying Xu and Na Li and Xian Li and Yuqi Liu and Yiyi Hu and Jingwen Xie and Yingnan Wu and Dongyan Long and Wen He and Dongsheng Yuan and Yifei Guo and Heng Li and He Huang and Shan Yang and Mei Han and Bojin Zhuang and Jiang Qian and Zhenjie Cao and Xuying Zhang and Jing Xiao and Liang Xu,"Health-adjusted life expectancy (HALE) in Chongqing, China, 2017: An artificial intelligence and big data method estimating the burden of disease at city level",,"Background
A universally applicable approach that provides standard HALE measurements for different regions has yet to be developed because of the difficulties of health information collection. In this study, we developed a natural language processing (NLP) based HALE estimation approach by using individual-level electronic medical records (EMRs), which made it possible to calculate HALE timely in different temporal or spatial granularities.
Methods
We performed diagnostic concept extraction and normalisation on 13•99 million EMRs with NLP to estimate the prevalence of 254 diseases in WHO Global Burden of Disease Study (GBD). Then, we calculated HALE in Chongqing, 2017, by using the life table technique and Sullivan's method, and analysed the contribution of diseases to the expected years “lost” due to disability (DLE).
Findings
Our method identified a life expectancy at birth (LE0) of 77•9 years and health-adjusted life expectancy at birth (HALE0) of 71•7 years for the general Chongqing population of 2017. In particular, the male LE0 and HALE0 were 76•3 years and 68•9 years, respectively, while the female LE0 and HALE0 were 80•0 years and 74•4 years, respectively. Cerebrovascular diseases, cancers, and injuries were the top three deterioration factors, which reduced HALE by 2•67, 2•15, and 1•19 years, respectively.
Interpretation
The results demonstrated the feasibility and effectiveness of EMRs-based HALE estimation. Moreover, the method allowed for a potentially transferable framework that facilitated a more convenient comparison of cross-sectional and longitudinal studies on HALE between regions. In summary, this study provided insightful solutions to the global ageing and health problems that the world is facing.
Funding
National Key R and D Program of China (2018YFC2000400).",2021,,https://doi.org/10.1016/j.lanwpc.2021.100110
1129,Robert Whelan and Zhipeng Cao and Laura O'Halloran and Brian Pennie,"Chapter 27 - Genetics, imaging, and cognition: big data approaches to addiction research","Addiction, Big Data, Cognition, Genetics, Machine learning, Neuroimaging, Online methods","The etiology and trajectory of addictions is complex, caused and moderated by individual differences in cognition that are themselves a function of genetics and of environment. In this chapter, we discuss how “Big Data” can shed light on the cognitive correlates of addiction. Big Data is primarily data-driven, using algorithms that search for patterns in data, with accurate prediction on previously unseen data as the metric of success. In this chapter, we introduce and provide practical advice on Big Data approaches for addiction. In the first part of this chapter, we describe how online methods of data collection facilitate the collection of large datasets. In the second section, we outline some recent advances in neuroimaging, with a focus on prediction of substance use using machine learning methods. In the final section, we present advances in genetics—meta- and megaanalyses—which may provide breakthroughs in our understanding of the genetics of addiction.",2020,,https://doi.org/10.1016/B978-0-12-815298-0.00027-7
1130,Aurelle {Tchagna Kouanou} and Daniel Tchiotsop and Romanic Kengne and Djoufack Tansaa Zephirin and Ngo Mouelas {Adele Armele} and René Tchinda,An optimal big data workflow for biomedical image analysis,"Biomedical images, Big data, Artificial intelligence, Machine learning, Hadoop/spark","Background and objective
In the medical field, data volume is increasingly growing, and traditional methods cannot manage it efficiently. In biomedical computation, the continuous challenges are: management, analysis, and storage of the biomedical data. Nowadays, big data technology plays a significant role in the management, organization, and analysis of data, using machine learning and artificial intelligence techniques. It also allows a quick access to data using the NoSQL database. Thus, big data technologies include new frameworks to process medical data in a manner similar to biomedical images. It becomes very important to develop methods and/or architectures based on big data technologies, for a complete processing of biomedical image data.
Method
This paper describes big data analytics for biomedical images, shows examples reported in the literature, briefly discusses new methods used in processing, and offers conclusions. We argue for adapting and extending related work methods in the field of big data software, using Hadoop and Spark frameworks. These provide an optimal and efficient architecture for biomedical image analysis. This paper thus gives a broad overview of big data analytics to automate biomedical image diagnosis. A workflow with optimal methods and algorithm for each step is proposed.
Results
Two architectures for image classification are suggested. We use the Hadoop framework to design the first, and the Spark framework for the second. The proposed Spark architecture allows us to develop appropriate and efficient methods to leverage a large number of images for classification, which can be customized with respect to each other.
Conclusions
The proposed architectures are more complete, easier, and are adaptable in all of the steps from conception. The obtained Spark architecture is the most complete, because it facilitates the implementation of algorithms with its embedded libraries.",2018,,https://doi.org/10.1016/j.imu.2018.05.001
1131,Leo Willyanto Santoso and  Yulia,Data Warehouse with Big Data Technology for Higher Education,"Data Warehouse, Big Data, Academic, Hadoop, Higher Education, Analysis","Nowadays, data warehouse tools and technologies cannot handle the load and analytic process of data into meaningful information for top management. Big data technology should be implemented to extend the existing data warehouse solutions. Universities already collect vast amounts of data so the academic data of university has been growing significantly and become a big academic data. These datasets are rich and growing. University’s top-level management needs tools to produce information from the records. The generated information is expected to support the decision-making process of top-level management. This paper explores how big data technology could be implemented with data warehouse to support decision making process. In this framework, we propose Hadoop as big data analytic tools to be implemented for data ingestion/staging. The paper concludes by outlining future directions relating to the development and implementation of an institutional project on Big Data.",2017,,https://doi.org/10.1016/j.procs.2017.12.134
1132,Walter Verbrugghe and Kirsten Colpaert,"The electronic medical record: Big data, little information?",,,2019,,https://doi.org/10.1016/j.jcrc.2019.09.005
1133,Xinping Yan and Kai Wang and Yupeng Yuan and Xiaoli Jiang and Rudy R. Negenborn,Energy-efficient shipping: An application of big data analysis for optimizing engine speed of inland ships considering multiple environmental factors,"Ship energy efficiency, Speed optimization, Big data analysis, Parallel k-means algorithm, Hadoop","Energy efficiency of inland ships is significantly influenced by navigational environment, including wind speed and direction as well as water depth and speed. The complexity of the inland navigational environment makes it rather difficult to determine the optimal speeds under different environmental conditions to achieve the best energy efficiency. Route division according to the characteristics of these environmental factors could provide a good solution for the optimization of ship engine speed under different navigational environments. In this paper, the distributed parallel k-means clustering algorithm is adopted to achieve an elaborate route division by analyzing the corresponding environmental factors based on a self-developed big data analytics platform. Subsequently, a ship energy efficiency optimization model considering multiple environmental factors is established through analyzing the energy transfer among hull, propeller and main engine. Then, decisions are made concerning the optimal engine speeds in different segments along the path. Finally, a case study on the Yangtze River is performed to validate the present optimization method. The results show that the proposed method can effectively reduce energy consumption and CO2 emissions of ships.",2018,,https://doi.org/10.1016/j.oceaneng.2018.08.050
1134,E. Marian Scott,"The role of Statistics in the era of big data: Crucial, critical and under-valued","Data, Sampling, Variability, Inference, Uncertainty","What is the role of Statistics in the era of big data, or is Statistics still relevant? I will start this rather personal view with my answer. Statistics remains highly relevant irrespective of ‘bigness’ of data, its role remains what is has always been, but is even more important now. As a community, we need to improve our explanations and presentations to make more visible our relevance.",2018,,https://doi.org/10.1016/j.spl.2018.02.050
1135,Lotus A. Lofgren and Jason E. Stajich,"Fungal biodiversity and conservation mycology in light of new technology, big data, and changing attitudes",,"Summary
Fungi have successfully established themselves across seemingly every possible niche, substrate, and biome. They are fundamental to biogeochemical cycling, interspecies interactions, food production, and drug bioprocessing, as well as playing less heroic roles as difficult to treat human infections and devastating plant pathogens. Despite community efforts to estimate and catalog fungal diversity, we have only named and described a minute fraction of the fungal world. The identification, characterization, and conservation of fungal diversity is paramount to preserving fungal bioresources, and to understanding and predicting ecosystem cycling and the evolution and epidemiology of fungal disease. Although species and ecosystem conservation are necessarily the foundation of preserving this diversity, there is value in expanding our definition of conservation to include the protection of biological collections, ecological metadata, genetic and genomic data, and the methods and code used for our analyses. These definitions of conservation are interdependent. For example, we need metadata on host specificity and biogeography to understand rarity and set priorities for conservation. To aid in these efforts, we need to draw expertise from diverse fields to tie traditional taxonomic knowledge to data obtained from modern -omics-based approaches, and support the advancement of diverse research perspectives. We also need new tools, including an updated framework for describing and tracking species known only from DNA, and the continued integration of functional predictions to link genetic diversity to functional and ecological diversity. Here, we review the state of fungal diversity research as shaped by recent technological advancements, and how changing viewpoints in taxonomy, -omics, and systematics can be integrated to advance mycological research and preserve fungal biodiversity.",2021,,https://doi.org/10.1016/j.cub.2021.06.083
1136,Xiaolong Jin and Benjamin W. Wah and Xueqi Cheng and Yuanzhuo Wang,Significance and Challenges of Big Data Research,"Big data, Data complexity, Computational complexity, System complexity","In recent years, the rapid development of Internet, Internet of Things, and Cloud Computing have led to the explosive growth of data in almost every industry and business area. Big data has rapidly developed into a hot topic that attracts extensive attention from academia, industry, and governments around the world. In this position paper, we first briefly introduce the concept of big data, including its definition, features, and value. We then identify from different perspectives the significance and opportunities that big data brings to us. Next, we present representative big data initiatives all over the world. We describe the grand challenges (namely, data complexity, computational complexity, and system complexity), as well as possible solutions to address these challenges. Finally, we conclude the paper by presenting several suggestions on carrying out big data projects.",2015,,https://doi.org/10.1016/j.bdr.2015.01.006
1137,Ari Wibisono and Devvi Sarwinda,Average Restrain Divider of Evaluation Value (ARDEV) in data stream algorithm for big data prediction,"ARDEV, Big data prediction, FIMT-DD, Tree regression","Today, big data processing has become a challenging task due to the amount of data collected using various sensors increasingly significantly. To build knowledge and predict the data, traditional data mining methods calculate all numerical attributes into the memory simultaneously. The data stream method is a solution for processing and calculating data. The method streams incrementally in batch form; therefore, infrastructure memory is sufficient to develop knowledge. The existing method for data stream prediction is FIMT-DD (Fast Incremental Model Tree-Drift Detection). Using this method, knowledge is developed in tree form for every instance. In this paper, enhanced FIMT-DD is proposed using ARDEV (Average Restrain Divider of Evaluation Value). ARDEV utilizes the Chernoff bound approach with error evaluation, improvement in learning rate, modification of perceptron rule calculation, and utilization of activation function. Standard FIMT-DD separates the tree formation process and perceptron prediction. The proposed method evaluates and connects the development of the tree for knowledge formation and the perceptron rule for prediction. The prediction accuracy of the proposed method is measured using MAE, RMSE and MAPE. From the experiment performed, the utilization of ARDEV enhancement shows significant improvement in terms of accuracy prediction. Statistically, the overall accuracy prediction improvement is approximately 6.99 % compared to standard FIMT-DD with a traffic dataset.",2019,,https://doi.org/10.1016/j.knosys.2019.03.019
1138,Hossein Akhavan-Hejazi and Hamed Mohsenian-Rad,Power systems big data analytics: An assessment of paradigm shift barriers and prospects,"Energy, Big data analytics, Internet of energy, Smart grid","Electric power systems are taking drastic advances in deployment of information and communication technologies; numerous new measurement devices are installed in forms of advanced metering infrastructure, distributed energy resources (DER) monitoring systems, high frequency synchronized wide-area awareness systems that with great speed are generating immense volume of energy data. However, it is still questioned that whether the today’s power system data, the structures and the tools being developed are indeed aligned with the pillars of the big data science. Further, several requirements and especial features of power systems and energy big data call for customized methods and platforms. This paper provides an assessment of the distinguished aspects in big data analytics developments in the domain of power systems. We perform several taxonomy of the existing and the missing elements in the structures and methods associated with big data analytics in power systems. We also provide a holistic outline, classifications, and concise discussions on the technical approaches, research opportunities, and application areas for energy big data analytics.",2018,,https://doi.org/10.1016/j.egyr.2017.11.002
1139,Maryam Ghasemaghaei and Goran Calic,Does big data enhance firm innovation competency? The mediating role of data-driven insights,"Big data characteristics, Descriptive insight, Predictive insight, Prescriptive insight, Innovation competency","Grounded in gestalt insight learning theory and organizational learning theory, we collected data from 280 middle and top-level managers to investigate the impact of each big data characteristic (i.e., data volume, data velocity, data variety, and data veracity) on firm innovation competency (i.e., exploitation competency and exploration competency), mediated through data-driven insight generation (i.e., descriptive insight, predictive insight, and prescriptive insight). Findings show that while data velocity, variety, and veracity enhance data-driven insight generation, data volume does not impact it. Additionally, results of the post hoc analysis indicate that while descriptive and predictive insights improve innovation competency, prescriptive insight does not affect it. These results provide interesting and unique theoretical and practical insights.",2019,,https://doi.org/10.1016/j.jbusres.2019.07.006
1140,Shivam Gupta and Arpan Kumar Kar and Abdullah Baabdullah and Wassan A.A. Al-Khowaiter,Big data with cognitive computing: A review for the future,"Big data, Cognitive computing, Literature review, Resource based View, Institutional theory","Analysis of data by humans can be a time-consuming activity and thus use of sophisticated cognitive systems can be utilized to crunch this enormous amount of data. Cognitive computing can be utilized to reduce the shortcomings of the concerns faced during big data analytics. The aim of the study is to provide readers a complete understanding of past, present and future directions in the domain big data and cognitive computing. A systematic literature review has been adopted for this study by using the Scopus, DBLP and Web of Science databases. The work done in the field of big data and cognitive computing is currently at the nascent stage and this is evident from the publication record. The characteristics of cognitive computing, namely observation, interpretation, evaluation and decision were mapped to the five V’s of big data namely volume, variety, veracity, velocity and value. Perspectives which touch all these parameters are yet to be widely explored in existing literature.",2018,,https://doi.org/10.1016/j.ijinfomgt.2018.06.005
1141,Ibrahim Abaker Targio Hashem and Ibrar Yaqoob and Nor Badrul Anuar and Salimah Mokhtar and Abdullah Gani and Samee {Ullah Khan},The rise of “big data” on cloud computing: Review and open research issues,"Big data, Cloud computing, Hadoop","Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized.",2015,,https://doi.org/10.1016/j.is.2014.07.006
1142,D.R. Cox and Christiana Kartsonaki and Ruth H. Keogh,Big data: Some statistical issues,"Big data, Electronic health records, Epidemiology, Metrology, Precision",A broad review is given of the impact of big data on various aspects of investigation. There is some but not total emphasis on issues in epidemiological research.,2018,,https://doi.org/10.1016/j.spl.2018.02.015
1143,Nanfei Sun and Bingjun Sun and Jian (Denny) Lin and Michael Yu-Chi Wu,Lossless Pruned Naive Bayes for Big Data Classifications,"Big data, Classification, Naive Bayes, Large-scale taxonomy, Lossless, Pruned","In a fast growing big data era, volume and varieties of data processed in Internet applications drastically increase. Real-world search engines commonly use text classifiers with thousands of classes to improve relevance or data quality. These large scale classification problems lead to severe runtime performance challenges, so practitioners often resort to fast approximation techniques. However, the increase in classification speed comes at a cost, as approximations are lossy, mis-assigning classes relative to the original reference classification algorithm. To address this problem, we introduce a Lossless Pruned Naive Bayes (LPNB) classification algorithm tailored to real-world, big data applications with thousands of classes. LPNB achieves significant speed-ups by drawing on Information Retrieval (IR) techniques for efficient posting list traversal and pruning. We show empirically that LPNB can classify text up to eleven times faster than standard Naive Bayes on a real-world data set with 7205 classes, with larger gains extrapolated for larger taxonomies. In practice, the achieved acceleration is significant as it can greatly cut required computation time. In addition, it is lossless: the output is identical to standard Naive Bayes, in contrast to extant techniques such as hierarchical classification. The acceleration does not rely on the taxonomy structure, and it can be used for both hierarchical and flat taxonomies.",2018,,https://doi.org/10.1016/j.bdr.2018.05.007
1144,Andy M.Y. Tai and Alcides Albuquerque and Nicole E. Carmona and Mehala Subramanieapillai and Danielle S. Cha and Margarita Sheko and Yena Lee and Rodrigo Mansur and Roger S. McIntyre,Machine learning and big data: Implications for disease modeling and therapeutic discovery in psychiatry,"Big data, Machine learning, Precision medicine, AI, Mental health, Mental disease, Psychiatry, Data mining, RDoC, Research domain criteria, DSM-5. Schizophrenia, ADHD, Alzheimer, Depression, fMRI, MRI, Algorithms, IBM Watson, Neuro networking, Random forests, Decision trees, Support vector machines","Introduction
Machine learning capability holds promise to inform disease models, the discovery and development of novel disease modifying therapeutics and prevention strategies in psychiatry. Herein, we provide an introduction on how machine learning/Artificial Intelligence (AI) may instantiate such capabilities, as well as provide rationale for its application to psychiatry in both research and clinical ecosystems.
Methods
Databases PubMed and PsycINFO were searched from 1966 to June 2016 for keywords:Big Data, Machine Learning, Precision Medicine, Artificial Intelligence, Mental Health, Mental Disease, Psychiatry, Data Mining, RDoC, and Research Domain Criteria. Articles selected for review were those that were determined to be aligned with the objective of this particular paper.
Results
Results indicate that AI is a viable option to build useful predictors of outcome while offering objective and comparable accuracy metrics, a unique opportunity, particularly in mental health research. The approach has also consistently brought notable insight into disease models through processing the vast amount of already available multi-domain, semi-structured medical data. The opportunity for AI in psychiatry, in addition to disease-model refinement, is in characterizing those at risk, and it is likely also relevant to personalizing and discovering therapeutics.
Conclusions
Machine learning currently provides an opportunity to parse disease models in complex, multi-factorial disease states (e.g. mental disorders) and could possibly inform treatment selection with existing therapies and provide bases for domain-based therapeutic discovery.",2019,,https://doi.org/10.1016/j.artmed.2019.101704
1145,Mandeep Kaur Saggi and Sushma Jain,A survey towards an integration of big data analytics to big insights for value-creation,"Big data, Data analytics, Machine learning, Big data visualization, Decision-making, Smart agriculture, Smart city application, Value-creation, Value-discover, Value-realization","Big Data Analytics (BDA) is increasingly becoming a trending practice that generates an enormous amount of data and provides a new opportunity that is helpful in relevant decision-making. The developments in Big Data Analytics provide a new paradigm and solutions for big data sources, storage, and advanced analytics. The BDA provide a nuanced view of big data development, and insights on how it can truly create value for firm and customer. This article presents a comprehensive, well-informed examination, and realistic analysis of deploying big data analytics successfully in companies. It provides an overview of the architecture of BDA including six components, namely: (i) data generation, (ii) data acquisition, (iii) data storage, (iv) advanced data analytics, (v) data visualization, and (vi) decision-making for value-creation. In this paper, seven V's characteristics of BDA namely Volume, Velocity, Variety, Valence, Veracity, Variability, and Value are explored. The various big data analytics tools, techniques and technologies have been described. Furthermore, it presents a methodical analysis for the usage of Big Data Analytics in various applications such as agriculture, healthcare, cyber security, and smart city. This paper also highlights the previous research, challenges, current status, and future directions of big data analytics for various application platforms. This overview highlights three issues, namely (i) concepts, characteristics and processing paradigms of Big Data Analytics; (ii) the state-of-the-art framework for decision-making in BDA for companies to insight value-creation; and (iii) the current challenges of Big Data Analytics as well as possible future directions.",2018,,https://doi.org/10.1016/j.ipm.2018.01.010
1146,Gabriel {de Freitas Viscondi} and Solange N. Alves-Souza,A Systematic Literature Review on big data for solar photovoltaic electricity generation forecasting,"Systematic Literature Review, Solar energy forecasting, Machine learning, Data mining","Solar power is expected to play a substantial role globally, due to it being one of the leading renewable electricity sources for future use. Even though the use of solar irradiation to generate electricity is currently at a fast deployment pace and technological evolution, its natural variability still presents an important barrier to overcome. Machine learning and data mining techniques arise as alternatives to aid solar electricity generation forecast reducing the impacts of its natural inconstant power supply. This paper presents a literature review on big data models for solar photovoltaic electricity generation forecasts, aiming to evaluate the most applicable and accurate state-of-art techniques to the problem, including the motivation behind each project proposal, the characteristics and quality of data used to address the problem, among other issues. A Systematic Literature Review (SLR) method was used, in which research questions were defined and translated into search strings. The search returned 38 papers for final evaluation, affirming that the use of these models to predict solar electricity generation is currently an ongoing academic research question. Machine learning is widely used, and neural networks is considered the most accurate algorithm. Extreme learning machine learning has reduced time and raised precision.",2019,,https://doi.org/10.1016/j.seta.2018.11.008
1147,Adrienne N. Cobb and Andrew J. Benjamin and Erich S. Huang and Paul C. Kuo,Big data: More than big data sets,,"The term big data has been popularized over the past decade and is often used to refer to data sets that are too large or complex to be analyzed by traditional means. Although the term has been utilized for some time in business and engineering, the concept of big data is relatively new to medicine. The reception from the medical community has been mixed; however, the widespread utilization of electronic health records in the United States, the creation of large clinical data sets and national registries that capture information on numerous vectors affecting healthcare delivery and patient outcomes, and the sequencing of the human genome are all opportunities to leverage big data. This review was inspired by a lively panel discussion on big data that took place at the 75th Central Surgical Association Annual Meeting. The authors’ aim was to describe big data, the methodologies used to analyze big data, and their practical clinical application.",2018,,https://doi.org/10.1016/j.surg.2018.06.022
1148,Lynn E. Bayne,"Big Data in Neonatal Health Care: Big Reach, Big Reward?","Big data, Electronic health record (EHR), Neonatology, Cost-savings, Clinical decision making, Healthcare analytics",,2018,,https://doi.org/10.1016/j.cnc.2018.07.005
1149,Hicham Moad Safhi and Bouchra Frikh and Brahim Ouhbi,Assessing reliability of Big Data Knowledge Discovery process,"Knowledge discovery, Reliability, Trustworthiness, Quality, Big Data mining","Extracting knowledge from Big Data is the process of transforming this data into actionable information. The exponential growth of data has initiated a myriad of new opportunities, and made data become the most valuable raw material of production for many organizations. Mining Big Data is coupled with some challenges, known as the 3V’s of Big Data: Volume, Variety and Velocity. However, a major challenge that needs to be addressed, and often is ignored in the literature, concerns reliability. Actually, data is agglomerated from multiple disparate sources, and each of Knowledge Discovery (KDD) process steps may be carried out by different organizations. These considerations lead us to ask a critical question that is weather the information we have at each step is reliable enough to proceed to the next one? This paper therefore aims to provide a framework that automatically assesses reliability of the knowledge discovery process. We focus on Linked Open Data (LOD) as a source of data, as it constitutes a relevant data provider in many Big Data applications. However, our framework can also be adapted for unstructured data. This framework will assist scientists to automatically and efficiently measure the reliability of each KDD process stage as well as detect unreliable steps that should be revised. Following this methodology, KDD process will be optimized and therefore produce knowledge with higher quality.",2019,,https://doi.org/10.1016/j.procs.2019.01.005
1150,Md. Abdul Moktadir and Syed Mithun Ali and Sanjoy Kumar Paul and Nagesh Shukla,Barriers to big data analytics in manufacturing supply chains: A case study from Bangladesh,"AHP, Big data analytics, Barriers to BDA, Delphi, Information and communication technology (ICT), Manufacturing supply chains","Recently, big data (BD) has attracted researchers and practitioners due to its potential usefulness in decision-making processes. Big data analytics (BDA) is becoming increasingly popular among manufacturing companies as it helps gain insights and make decisions based on BD. However, there many barriers to the adoption of BDA in manufacturing supply chains. It is therefore necessary for manufacturing companies to identify and examine the nature of each barrier. Previous studies have mostly built conceptual frameworks for BDA in a given situation and have ignored examining the nature of the barriers to BDA. Due to the significance of both BD and BDA, this research aims to identify and examine the critical barriers to the adoption of BDA in manufacturing supply chains in the context of Bangladesh. This research explores the existing body of knowledge by examining these barriers using a Delphi-based analytic hierarchy process (AHP). Data were obtained from five Bangladeshi manufacturing companies. The findings of this research are as follows: (i) data-related barriers are most important, (ii) technology-related barriers are second, and (iii) the five most important components of these barriers are (a) lack of infrastructure, (b) complexity of data integration, (c) data privacy, (d) lack of availability of BDA tools and (e) high cost of investment. The findings can assist industrial managers to understand the actual nature of the barriers and potential benefits of using BDA and to make policy regarding BDA adoption in manufacturing supply chains. A sensitivity analysis was carried out to justify the robustness of the barrier rankings.",2019,,https://doi.org/10.1016/j.cie.2018.04.013
1151,Jizhe Xia and Chaowei Yang and Qingquan Li,"Using spatiotemporal patterns to optimize Earth Observation Big Data access: Novel approaches of indexing, service modeling and cloud computing","Spatiotemporal optimization, Big Data, Cloud computing, Global operation, GEOSS","Based on our GEOSS Clearinghouse operating experience, we summarized the three Earth Observation (EO) Big Data access challenges, namely, fast access, accurate service estimation and global access, and two essential research questions: are there any spatiotemporal patterns when end users access EO data, and how can these spatiotemporal patterns be utilized to better facilitate EO Big Data access? To tackle these two research questions, we conducted a two-year pattern analysis with 2+ million user access records. The spatial pattern, temporal pattern and spatiotemporal pattern of user-data interactions were explored. For the second research question, we developed three spatiotemporal optimization strategies to respond to the three access challenges: a) spatiotemporal indexing to accelerate data access, b) spatiotemporal service modeling to improve data access accuracy and c) spatiotemporal cloud computing to enhance global access. This research is a pioneering framework for spatiotemporal optimization of EO Big Data access and valuable for other multidisciplinary geographic data and information research.",2018,,https://doi.org/10.1016/j.compenvurbsys.2018.06.010
1152,Liang Hong and Mengqi Luo and Ruixue Wang and Peixin Lu and Wei Lu and Long Lu,Big Data in Health Care: Applications and Challenges,"Big Data, public health, cloud computing, medical applications","The concept of Big Data is popular in a variety of domains. The purpose of this review was to summarize the features, applications, analysis approaches, and challenges of Big Data in health care. Big Data in health care has its own features, such as heterogeneity, incompleteness, timeliness and longevity, privacy, and ownership. These features bring a series of challenges for data storage, mining, and sharing to promote health-related research. To deal with these challenges, analysis approaches focusing on Big Data in health care need to be developed and laws and regulations for making use of Big Data in health care need to be enacted. From a patient perspective, application of Big Data analysis could bring about improved treatment and lower costs. In addition to patients, government, hospitals, and research institutions could also benefit from the Big Data in health care.",2018,,https://doi.org/10.2478/dim-2018-0014
1153,Shivam Gupta and Haozhe Chen and Benjamin T. Hazen and Sarabjot Kaur and Ernesto D.R. {Santibañez Gonzalez},Circular economy and big data analytics: A stakeholder perspective,"Circular economy, Big data, Stakeholder theory, Relational view, Supply chain management, Sustainability","The business concept of the circular economy (CE) has gained significant momentum among practitioners and researchers alike. However, successful adoption and implementation of this paradigm of managing business remains a challenge. In this article, we build a case for utilizing big data analytics (BDA) as a fundamental basis for informed and data driven decision making in supply chain networks supporting CE. We view this from a stakeholder perspective and argue that a collaborative association among all supply chain members can positively affect CE implementation. We propose a model highlighting the facilitating role of big data analytics for achieving shared sustainability goals. The model is based on integrating thematic categories coming out of 10 semi-structured interviews with key position holders in industry. We argue that mutual support and coordination driven by a stakeholder perspective coupled with holistic information processing and sharing along the entire supply chain network can effectively create a basis for achieving the triple bottom line of economic, ecological and social benefits. The proposed model is useful for managers in that it provides a reference point for aligning activities with the circular economy paradigm. The conceptual model provides a theoretical basis for future empirical research in this domain.",2019,,https://doi.org/10.1016/j.techfore.2018.06.030
1154,Marijn Janssen and Haiko {van der Voort} and Agung Wahyudi,Factors influencing big data decision-making quality,"Big data, Big data analytics, Big data chain, E-government, Governance, Decision-making, Decision-making quality","Organizations are looking for ways to harness the power of big data (BD) to improve their decision making. Despite its significance the effects of BD on decision-making quality has been given scant attention in the literature. In this paper factors influencing decision-making based on BD are identified using a case study. BD is collected from different sources that have various data qualities and are processed by various organizational entities resulting in the creation of a big data chain. The veracity (manipulation, noise), variety (heterogeneity of data) and velocity (constantly changing data sources) amplified by the size of big data calls for relational and contractual governance mechanisms to ensure BD quality and being able to contextualize data. The case study reveals that taking advantage of big data is an evolutionary process in which the gradually understanding of the potential of big data and the routinization of processes plays a crucial role.",2017,,https://doi.org/10.1016/j.jbusres.2016.08.007
1155,Shuangqi Li and Hongwen He and Jianwei Li,Big data driven lithium-ion battery modeling method based on SDAE-ELM algorithm and data pre-processing technology,"Electric vehicles, Battery energy storage, Temperature-dependent model, Battery management system, Big data, Deep learning","As one of the bottleneck technologies of electric vehicles (EVs), the battery hosts complex and hardly observable internal chemical reactions. Therefore, a precise mathematical model is crucial for the battery management system (BMS) to ensure the secure and stable operation of the battery in a multi-variable environment. First, a Cloud-based BMS (C-BMS) is established based on a database containing complete battery status information. Next, a data cleaning method based on machine learning is applied to the big data of batteries. Meanwhile, to improve the model stability under dynamic conditions, an F-divergence-based data distribution quality assessment method and a sampling-based data preprocess method is designed. Then, a lithium-ion battery temperature-dependent model is built based on Stacked Denoising Autoencoders- Extreme Learning Machine (SDAE-ELM) algorithm, and a new training method combined with data preprocessing is also proposed to improve the model accuracy. Finally, to improve reliability, a conjunction working mode between the C-BMS and the BMS in vehicles (V-BMS) is also proposed, providing as an applied case of the model. Using the battery data extracted from electric buses, the effectiveness and accuracy of the model are validated. The error of the estimated battery terminal voltage is within 2%, and the error of the estimated State of Charge (SoC) is within 3%.",2019,,https://doi.org/10.1016/j.apenergy.2019.03.154
1156,Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira and Aleš Popovič,Unlocking the drivers of big data analytics value in firms,"IT business value, Big data analytics (BDA), Delphi method, Mixed methodology, Competitive advantage","Although big data analytics (BDA) is considered the next “frontier” in data science by creating potential business opportunities, the way to extract those opportunities is unclear. This paper aims to understand the antecedents of BDA value at a firm level. The authors performed a study using a mixed methodology approach. First, by carrying out a Delphi study to explore and rank the antecedents affecting the creation of BDA value. Based on the Delphi results, we propose an empirically validated model supported by a survey conducted on 175 European firms to explain the antecedents of BDA sustained value. The results show that the proposed model explains 62% of BDA sustained value at the firm level, where the most critical contributor is BDA use. We provide directions for managers to support their decisions on BDA strategy definition and refinement. For academics, we extend BDA value literature and outline some potential research opportunities.",2019,,https://doi.org/10.1016/j.jbusres.2018.12.072
1157,Deepak Kumar Sharma and Bhanu Tokas and Leo Adlakha,Chapter 2 - Deep learning in big data and data mining,"Aspect extraction, Big data, CRISP-DM, Customer relations, Data mining, Data visualization, Deep learning, Distributed computing, LSTM, Machine learning","The growth of the digital age has led to a colossal leap in data generated by the average user. This growing data has several applications: businesses can use it to give a more personalized touch to their services, governments can use it to better allocate their funds, and companies can utilize it to select the best candidates for a job. While these applications may seem extremely enticing, there are a couple of problems that must be solved first, namely, data collection and extraction of useful patterns from the data. The disciplines of data mining and big data deal with these problems, respectively. But, as we have already discussed, the amount of data is so vast that any manual approach is extremely time intensive and costly. Thus this limits the potential outcomes from this data. This problem has been solved by the application of deep learning. Deep learning has allowed us to automate processes that were not only time intensive but also mentally arduous. It has achieved better than human accuracy in several types of discriminative and recognition tasks making it a viable alternative to inefficient human labor. Deep learning plays a vital role in this analysis and has enabled several businesses to comprehend customer needs and accordingly improve their own services, thus giving them the opportunity to outdo their competitors. Similarly, deep learning has also been instrumental in analyzing the trends and associations of securities in the financial market. It has even helped to create fraud detection and loan underwriting applications, which have contributed to making financial institutions more transparent and efficient. Apart from directly improving the efficiency in these fields, deep learning has also been instrumental in improving the fields of data mining and big data. Machine learning algorithms can actually utilize the existing data to predict the unknowns, including future trends in data. Due to its potential applications the field of machine learning is deeply interconnected with data mining. Nevertheless, machine learning algorithms are often heavily dependent on the availability of huge datasets to ensure useful accuracy. Deep learning algorithms have allowed the different components of data (i.e., multimedia data) in the data mining process itself to be identified. Similarly, semantic indexing and tagging algorithms have allowed the processes of big data to speed up. In this chapter, we will discuss the applications of deep learning in these fields and give a brief overview of the concepts involved.",2021,,https://doi.org/10.1016/B978-0-12-822226-3.00002-7
1158,Vasileios C. Pezoulas and Konstantina D. Kourou and Fanis Kalatzis and Themis P. Exarchos and Aliki Venetsanopoulou and Evi Zampeli and Saviana Gandolfo and Fotini Skopouli and Salvatore {De Vita} and Athanasios G. Tzioufas and Dimitrios I. Fotiadis,Medical data quality assessment: On the development of an automated framework for medical data curation,"Big data, Data quality, Data quality assessment, Data curation, Data standardization","Data quality assessment has gained attention in the recent years since more and more companies and medical centers are highlighting the importance of an automated framework to effectively manage the quality of their big data. Data cleaning, also known as data curation, lies in the heart of the data quality assessment and is a key aspect prior to the development of any data analytics services. In this work, we present the objectives, functionalities and methodological advances of an automated framework for data curation from a medical perspective. The steps towards the development of a system for data quality assessment are first described along with multidisciplinary data quality measures. A three-layer architecture which realizes these steps is then presented. Emphasis is given on the detection and tracking of inconsistencies, missing values, outliers, and similarities, as well as, on data standardization to finally enable data harmonization. A case study is conducted in order to demonstrate the applicability and reliability of the proposed framework on two well-established cohorts with clinical data related to the primary Sjögren's Syndrome (pSS). Our results confirm the validity of the proposed framework towards the automated and fast identification of outliers, inconsistencies, and highly-correlated and duplicated terms, as well as, the successful matching of more than 85% of the pSS-related medical terms in both cohorts, yielding more accurate, relevant, and consistent clinical data.",2019,,https://doi.org/10.1016/j.compbiomed.2019.03.001
1159,Shan Ren and Yingfeng Zhang and Yang Liu and Tomohiko Sakao and Donald Huisingh and Cecilia M.V.B. Almeida,"A comprehensive review of big data analytics throughout product lifecycle to support sustainable smart manufacturing: A framework, challenges and future research directions","Big data analytics, Smart manufacturing, Servitization, Sustainable production, Conceptual framework, Product lifecycle","Smart manufacturing has received increased attention from academia and industry in recent years, as it provides competitive advantage for manufacturing companies making industry more efficient and sustainable. As one of the most important technologies for smart manufacturing, big data analytics can uncover hidden knowledge and other useful information like relations between lifecycle decisions and process parameters helping industrial leaders to make more-informed business decisions in complex management environments. However, according to the literature, big data analytics and smart manufacturing were individually researched in academia and industry. To provide theoretical foundations for the research community to further develop scientific insights in applying big data analytics to smart manufacturing, it is necessary to summarize the existing research progress and weakness. In this paper, through combining the key technologies of smart manufacturing and the idea of ubiquitous servitization in the whole lifecycle, the term of sustainable smart manufacturing was coined. A comprehensive overview of big data in smart manufacturing was conducted, and a conceptual framework was proposed from the perspective of product lifecycle. The proposed framework allows analyzing potential applications and key advantages, and the discussion of current challenges and future research directions provides valuable insights for academia and industry.",2019,,https://doi.org/10.1016/j.jclepro.2018.11.025
1160,Amanda R. Siems-Anderson and Curtis L. Walker and Gerry Wiener and William P. Mahoney and Sue Ellen Haupt,An adaptive big data weather system for surface transportation,"Big data, Pikalert, Road weather, Surface transportation, Pavement condition, Weather forecasts","Operating modern multi-modal surface transportation systems are becoming increasingly automated and driven by decision support systems. One aspect necessary for successful, safe, reliable, and efficient operation of any transportation network is real-time and forecasted weather and pavement condition information. Providing such information requires an adaptive system capable of blending large amounts of observational and model data that arrives quickly, in disparate formats and times, and blends and optimizes their use via expert systems and machine-learning algorithms. Quality control of the data is also essential, and historical data is required to both develop expert-based empirical algorithms and train machine learning models. This paper reports on the open-source Pikalert® system that brings together weather information and real-time data from connected vehicles to provide crucial information to enhance the safety and efficiency of surface transportation systems. This robust framework can be applied to a diverse array of user community specifications and is designed to rapidly ingest more, unique data sets as they become available. Ultimately, the developmental framework of this system will provide critical environmental information necessary to promote the development, growth, refinement, and expanded adoption of automated and connected multi-modal vehicular systems globally.",2019,,https://doi.org/10.1016/j.trip.2019.100071
1161,Ali Al-Badi and Ali Tarhini and Asharul Islam Khan,Exploring Big Data Governance Frameworks,"Big Data, Big Data model, Big Data governance, Data management, Big Data governance framework, Big Data analytic","The recent explosion in ICT and digital data has led organizations, both private and public, to efficient decision-making. Nowadays organizations can store huge amounts of data, which can be accessible at any time. Big Data governance refers to the management of huge volumes of an organization’s data, exploiting it in the organization’s decision-making using different analytical tools. Big Data emergence provides great convenience, but it also brings challenges. Nevertheless, for Big Data governance, data has to be prepared in a timely manner, keeping in view the consistency and reliability of the data, and being able to trust its source and the meaningfulness of the result. Hence, a framework for Big Data governance would have many advantages. There are Big Data governance frameworks, which guide the management of Big Data. However, there are also limitations associated with these frameworks. Therefore, this study aims to explore the existing Big Data governance frameworks and their shortcomings, and propose a new framework. The proposed framework consists of eight components. As a framework validation, the proposed framework has been compared with the ISO 8000 data governance framework.",2018,,https://doi.org/10.1016/j.procs.2018.10.181
1162,Jesus Silva and Jack Zilberman and Ligia Romero and Omar Bonerge Pineda and Yaneth Herazo-Beltran,Identification of Patterns of Fatal Injuries in Humans through Big Data,"Recognition of automated standards, mining, decision trees","External cause injuries are defined as intentionally or unintentionally harm or injury to a person, which may be caused by trauma, poisoning, assault, accidents, etc., being fatal (fatal injury) or not leading to death (non-fatal injury). External injuries have been considered a global health problem for two decades. This work aims to determine criminal patterns using data mining techniques to a sample of patients from Mumbai city in India.",2020,,https://doi.org/10.1016/j.procs.2020.03.114
1163,Shannon Haymond and Randall K. Julian and Emily L. Gill and Stephen R. Master,Chapter 3 - Machine learning and big data in pediatric laboratory medicine,"Artificial intelligence, Big data, Laboratory medicine, Machine learning, Pediatrics, Regulation","Clinical laboratories generate a large number of test results, creating opportunities for improved data management and the use of analytics. Aggregate analyses of these data have potential diagnostic value but require labs to utilize computational tools for the analysis of high-dimensional data. Machine learning can be used to aid decision-making, whether for clinical or operational purposes, using a variety of algorithms to analyze complex data sets and make reliable predictions. This chapter discusses key concepts related to big data and its application to pediatric laboratory medicine. Machine learning workflows, concepts, common algorithms, and related infrastructure requirements are also covered.",2021,,https://doi.org/10.1016/B978-0-12-817962-8.00018-4
1164,Yiming Lin and Hongzhi Wang and Jianzhong Li and Hong Gao,Data source selection for information integration in big data era,"Source selection, Data integration, Data cleaning","In big data era, information integration often requires abundant data extracted from massive data sources. Due to a large number of data sources, data source selection plays a crucial role in information integration, since it is costly and even impossible to access all data sources. Data Source selection should consider both efficiency and effectiveness issues. For efficiency, the approach should scale to large data source amount. From effectiveness aspect, data quality and overlapping of sources are to be considered. In this paper, we study source selection problem in Big Data and propose methods which can scale to datasets with up to millions of data sources and guarantee the quality of results. Motivated by this, we propose a new metric taking the expected number of true values a source can provide as a criteria to evaluate the contribution of a data source. Based on our proposed index, we present a scalable algorithm and two pruning strategies to improve the efficiency without sacrificing precision. Experimental results on both real world and synthetic data sets show that our methods can select sources providing a large proportion of true values efficiently and can scale to massive data sources.",2019,,https://doi.org/10.1016/j.ins.2018.11.029
1165,Ali Zarrinpar and Ting-Yuan {David Cheng} and Zhiguang Huo,What Can We Learn About Drug Safety and Other Effects in the Era of Electronic Health Records and Big Data That We Would Not Be Able to Learn From Classic Epidemiology?,"Electronic health record, Big data, Drug safety, Health care database, Cancer risk","As more and more health systems have converted to the use of electronic health records, the amount of searchable and analyzable data is exploding. This includes not just provider or laboratory created data but also data collected by instruments, personal devices, and patients themselves, among others. This has led to more attention being paid to the analysis of these data to answer previously unaddressed questions. This is especially important given the number of therapies previously found to be beneficial in clinical trials that are currently being re-scrutinized. Because there are orders of magnitude more information contained in these data sets, a fundamentally different approach needs to be taken to their processing and analysis and the generation of knowledge. Health care and medicine are drivers of this phenomenon and will ultimately be the main beneficiaries. Concurrently, many different types of questions can now be asked using these data sets. Research groups have become increasingly active in mining large data sets, including nationwide health care databases, to learn about associations of medication use and various unrelated diseases such as cancer. Given the recent increase in research activity in this area, its promise to radically change clinical research, and the relative lack of widespread knowledge about its potential and advances, we surveyed the available literature to understand the strengths and limitations of these new tools. We also outline new databases and techniques that are available to researchers worldwide, with special focus on work pertaining to the broad and rapid monitoring of drug safety and secondary effects.",2020,,https://doi.org/10.1016/j.jss.2019.09.053
1166,Madhu Mahesh Nashipudimath and Subhash K. Shinde and Jayshree Jain,An efficient integration and indexing method based on feature patterns and semantic analysis for big data,"Big data, Integration, Feature patterns, Indexing, Semantic analysis","Big Data has received much attention in the multi-domain industry. In the digital and computing world, information is generated and collected at a rate that quickly exceeds the boundaries. The traditional data integration system interconnects the limited number of resources and is built with relatively stable and generally complex and time-consuming design activities. However, the rapid growth of these large data sets creates difficulties in learning heterogeneous data structures for integration and indexing. It also creates difficulty in information retrieval for the various data analysis requirements. In this paper, a probabilistic feature Patterns (PFP) approach using feature transformation and selection method is proposed for efficient data integration and utilizing the features latent semantic analysis (F-LSA) method for indexing the unsupervised multiple heterogeneous integrated cluster data sources. The PFP approach takes the advantage of the features transformation and selection mechanism to map and cluster the data for the integration, and an analysis of the data features context relation using LSA to provide the appropriate index for fast and accurate data extraction. A huge volume of BibText dataset from different publication sources are processed to evaluated to understand the effectiveness of the proposal. The analytical study and the outcome results show the improvisation in integration and indexing of the work.",2020,,https://doi.org/10.1016/j.array.2020.100033
1167,Jingjing Li and Lizhi Xu and Ling Tang and Shouyang Wang and Ling Li,Big data in tourism research: A literature review,"Tourism research, Big data, Literature review, Tourism management, Tourist behavior","Even at an early stage, diverse big data have been applied to tourism research and made an amazing improvement. This paper might be the first attempt to present a comprehensive literature review on different types of big data in tourism research. By data sources, the tourism-related big data fall into three primary categories: UGC data (generated by users), including online textual data and online photo data; device data (by devices), including GPS data, mobile roaming data, Bluetooth data, etc.; transaction data (by operations), including web search data, webpage visiting data, online booking data, etc. Carrying different information, different data types address different tourism issues. For each type, a systematical analysis is conducted from the perspectives of research focuses, data characteristics, analytic techniques, major challenges and further directions. This survey facilitates a thorough understanding of this sunrise research and offers valuable insights into its future prospects.",2018,,https://doi.org/10.1016/j.tourman.2018.03.009
1168,H.G. {van der Voort} and A.J. Klievink and M. Arnaboldi and A.J. Meijer,Rationality and politics of algorithms. Will the promise of big data survive the dynamics of public decision making?,,"Big data promises to transform public decision-making for the better by making it more responsive to actual needs and policy effects. However, much recent work on big data in public decision-making assumes a rational view of decision-making, which has been much criticized in the public administration debate. In this paper, we apply this view, and a more political one, to the context of big data and offer a qualitative study. We question the impact of big data on decision-making, realizing that big data – including its new methods and functions – must inevitably encounter existing political and managerial institutions. By studying two illustrative cases of big data use processes, we explore how these two worlds meet. Specifically, we look at the interaction between data analysts and decision makers. In this we distinguish between a rational view and a political view, and between an information logic and a decision logic. We find that big data provides ample opportunities for both analysts and decision makers to do a better job, but this doesn't necessarily imply better decision-making, because big data also provides opportunities for actors to pursue their own interests. Big data enables both data analysts and decision makers to act as autonomous agents rather than as links in a functional chain. Therefore, big data's impact cannot be interpreted only in terms of its functional promise; it must also be acknowledged as a phenomenon set to impact our policymaking institutions, including their legitimacy.",2019,,https://doi.org/10.1016/j.giq.2018.10.011
1169,Patrick Mikalef and John Krogstie and Ilias O. Pappas and Paul Pavlou,Exploring the relationship between big data analytics capability and competitive performance: The mediating roles of dynamic and operational capabilities,"Big data analytics, Dynamic capabilities, Operational capabilities, Business value, Resource-based view","A central question for information systems (IS) researchers and practitioners is if, and how, big data can help attain a competitive advantage. To address this question, this study draws on the resource-based view, dynamic capabilities view, and on recent literature on big data analytics, and examines the indirect relationship between a firm’s big data analytics capability (BDAC) and competitive performance. The study extends existing research by proposing that BDACs enable firms to generate insight that can help strengthen their dynamic capabilities, which, in turn, positively impact marketing and technological capabilities. To test our proposed research model, we used survey data from 202 chief information officers and IT managers working in Norwegian firms. By means of partial least squares structural equation modeling, results show that a strong BDAC can help firms build a competitive advantage. This effect is not direct but fully mediated by dynamic capabilities, which exerts a positive and significant effect on two types of operational capabilities: marketing and technological capabilities. The findings suggest that IS researchers should look beyond direct effects of big data investments and shift their attention on how a BDAC can be leveraged to enable and support organizational capabilities.",2020,,https://doi.org/10.1016/j.im.2019.05.004
1170,Obinna C.D. Anejionu and Piyushimita (Vonu) Thakuriah and Andrew McHugh and Yeran Sun and David McArthur and Phil Mason and Rod Walpole,Spatial urban data system: A cloud-enabled big data infrastructure for social and economic urban analytics,"Urban big data infrastructure, Urban analytics, Spatial urban indicators, Small area assessment, Spatial big data","The Spatial Urban Data System (SUDS) is a spatial big data infrastructure to support UK-wide analytics of the social and economic aspects of cities and city-regions. It utilises data generated from traditional as well as new and emerging sources of urban data. The SUDS deploys geospatial technology, synthetic small area urban metrics, and cloud computing to enable urban analytics, and geovisualization with the goal of deriving actionable knowledge for better urban management and data-driven urban decision making. At the core of the system is a programme of urban indicators generated by using novel forms of data and urban modelling and simulation programme. SUDS differs from other similar systems by its emphasis on the generation and use of regularly updated spatially-activated urban area metrics from real or near-real time data sources, to enhance understanding of intra-city interactions and dynamics. By deploying public transport, labour market accessibility and housing advertisement data in the system, we were able to identify spatial variations of key urban services at intra-city levels as well as social and economically-marginalised output areas in major cities across the UK. This paper discusses the design and implementation of SUDS, the challenges and limitations encountered, and considerations made during its development. The innovative approach adopted in the design of SUDS will enable it to support research and analysis of urban areas, policy and city administration, business decision-making, private sector innovation, and public engagement. Having been tested with housing, transport and employment metrics, efforts are ongoing to integrate information from other sources such as IoT, and User Generated Content into the system to enable urban predictive analytics.",2019,,https://doi.org/10.1016/j.future.2019.03.052
1171,Lila Rajabion and Abdusalam Abdulla Shaltooki and Masoud Taghikhah and Amirhossein Ghasemi and Arshad Badfar,Healthcare big data processing mechanisms: The role of cloud computing,"Cloud computing, Processing, Healthcare, Big data, Review","Recently, patient safety and healthcare have gained high attention in professional and health policy-makers. This rapid growth causes generating a high amount of data, which is known as big data. Therefore, handling and processing of this data are attracted great attention. Cloud computing is one of the main choices for handling and processing of this type of data. But, as far as we know, the detailed review and deep discussion in this filed are very rare. Therefore, this paper reviews and discusses the recently introduced mechanisms in this field as well as providing a deep analysis of their applied mechanisms. Moreover, the drawbacks and benefits of the reviewed mechanisms have been discussed and the main challenges of these mechanisms are highlighted for developing more efficient healthcare big data processing techniques over cloud computing in the future.",2019,,https://doi.org/10.1016/j.ijinfomgt.2019.05.017
1172,Yoshiaki Ueda and Shuichi Yanagisawa,Delineation of Nitrogen Signaling Networks: Computational Approaches in the Big Data Era,,,2019,,https://doi.org/10.1016/j.molp.2019.01.008
1173,Yunji Liang and Xiaolong Zheng and Daniel D. Zeng,A survey on big data-driven digital phenotyping of mental health,"Digital phenotyping, Big data, Mental health, Data mining, Information fusion","The landscape of mental health has undergone tremendous changes within the last two decades, but the research on mental health is still at the initial stage with substantial knowledge gaps and the lack of precise diagnosis. Nowadays, big data and artificial intelligence offer new opportunities for the screening and prediction of mental problems. In this review paper, we outline the vision of digital phenotyping of mental health (DPMH) by fusing the enriched data from ubiquitous sensors, social media and healthcare systems, and present a broad overview of DPMH from sensing and computing perspectives. We first conduct a systematical literature review and propose the research framework, which highlights the key aspects related with mental health, and discuss the challenges elicited by the enriched data for digital phenotyping. Next, five key research strands including affect recognition, cognitive analytics, behavioral anomaly detection, social analytics, and biomarker analytics are unfolded in the psychiatric context. Finally, we discuss various open issues and the corresponding solutions to underpin the digital phenotyping of mental health.",2019,,https://doi.org/10.1016/j.inffus.2019.04.001
1174,Eric T. Bradlow and Manish Gangwar and Praveen Kopalle and Sudhir Voleti,The Role of Big Data and Predictive Analytics in Retailing,"Big data, Predictive analytics, Retailing, Pricing","The paper examines the opportunities in and possibilities arising from big data in retailing, particularly along five major data dimensions—data pertaining to customers, products, time, (geo-spatial) location and channel. Much of the increase in data quality and application possibilities comes from a mix of new data sources, a smart application of statistical tools and domain knowledge combined with theoretical insights. The importance of theory in guiding any systematic search for answers to retailing questions, as well as for streamlining analysis remains undiminished, even as the role of big data and predictive analytics in retailing is set to rise in importance, aided by newer sources of data and large-scale correlational techniques. The Statistical issues discussed include a particular focus on the relevance and uses of Bayesian analysis techniques (data borrowing, updating, augmentation and hierarchical modeling), predictive analytics using big data and a field experiment, all in a retailing context. Finally, the ethical and privacy issues that may arise from the use of big data in retailing are also highlighted.",2017,,https://doi.org/10.1016/j.jretai.2016.12.004
1175,Martí Cuquet and Anna Fensel,The societal impact of big data: A research roadmap for Europe,"Big data, Research roadmap, Societal externalities, Skills development, Standardisation","With its rapid growth and increasing adoption, big data is producing a substantial impact in society. Its usage is opening both opportunities such as new business models and economic gains and risks such as privacy violations and discrimination. Europe is in need of a comprehensive strategy to optimise the use of data for a societal benefit and increase the innovation and competitiveness of its productive activities. In this paper, we contribute to the definition of this strategy with a research roadmap to capture the economic, social and ethical, legal and political benefits associated with the use of big data in Europe. The present roadmap considers the positive and negative externalities associated with big data, maps research and innovation topics in the areas of data management, processing, analytics, protection, visualisation, as well as non-technical topics, to the externalities they can tackle, and provides a time frame to address these topics in order to deliver social impact, skills development and standardisation. Finally, it also identifies what sectors will be most benefited by each of the research efforts. The goal of the roadmap is to guide European research efforts to develop a socially responsible big data economy, and to allow stakeholders to identify and meet big data challenges and proceed with a shared understanding of the societal impact, positive and negative externalities and concrete problems worth investigating in future programmes.",2018,,https://doi.org/10.1016/j.techsoc.2018.03.005
1176,Huang Huanchun and Yang Hailin and Deng Xin and Hao Cui and Liu Zhifeng and Liu Wei and Zeng Peng,Analyzing the Influencing Factors of Urban Thermal Field Intensity Using Big-Data-Based GIS,"land-surface temperature, thermal field pattern, POI data, GIS, air temperature","The effects of human activities and land cover changes on urban thermal field patterns are closely related to the land surface temperature (LST) and air temperature. At present, the number of studies on the quantitative relationship between these two indexes and the effect of the observational scale on their influence is insufficient. In this study, spatial analysis methods such as geographic modeling were combined with remote sensing images, meteorological data, and points of insert and used to investigate the composition and scale of the factors influencing the temperature field in Beijing. The results showed that there are differences in the positive and negative correlations between LST and air temperature and various influencing factors. At a spatial resolution of 90 m, LST had a strong linear relationship with the average air temperature. Indicators reflecting elements of human activity, such as buildings, roads, and entertainment, were easily measured by meteorological stations at a small scale, and the natural green space ratio could also be easily captured by satellite thermal sensors at small scales. These results have substantial implications for environmental impact assessments in areas experiencing an increasing urban heat island effect due to rapid urbanization.",2020,,https://doi.org/10.1016/j.scs.2020.102024
1177,P. Galetsi and K. Katsaliaki and S. Kumar,"Values, challenges and future directions of big data analytics in healthcare: A systematic review","Systematic review, Big data analytics, Health-medicine, Decision-making, Organizational and societal values, Preferred reporting items for systematic reviews and meta-analyses","The emergence of powerful software has created conditions and approaches for large datasets to be collected and analyzed which has led to informed decision-making towards tackling health issues. The objective of this study is to systematically review 804 scholarly publications related to big data analytics in health in order to identify the organizational and social values along with associated challenges. Key principles of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology were followed for conducting systematic reviews. Following a research path, we present the values, challenges and future directions of the scientific area using indicative examples from relevant published articles. The study reveals that one of the main values created is the development of analytical techniques which provides personalized health services to users and supports human decision-making using automated algorithms, challenging the power issues in the doctor-patient relationship and creating new working conditions. A main challenge to data analytics is data management and security when processing large volumes of sensitive, personal health data. Future research is directed towards the development of systems that will standardize and secure the process of extracting private healthcare datasets from relevant organizations. Our systematic literature review aims to provide to governments and health policy-makers a better understanding of how the development of a data driven strategy can improve public health and the functioning of healthcare organizations but also how can create challenges that need to be addressed in the near future to avoid societal malfunctions.",2019,,https://doi.org/10.1016/j.socscimed.2019.112533
1178,Miltiadis D. Lytras and Anna Visvizi and Akila Sarirete and Kwok Tai Chui,Preface: artificial intelligence and big data analytics for smart healthcare: a digital transformation of healthcare primer,,,2021,,https://doi.org/10.1016/B978-0-12-822060-3.00018-8
1179,Muhammad Habib {ur Rehman} and Ibrar Yaqoob and Khaled Salah and Muhammad Imran and Prem Prakash Jayaraman and Charith Perera,The role of big data analytics in industrial Internet of Things,"Internet of Things, Cyber-physical systems, Cloud computing, Analytics, Big data","Big data production in industrial Internet of Things (IIoT) is evident due to the massive deployment of sensors and Internet of Things (IoT) devices. However, big data processing is challenging due to limited computational, networking and storage resources at IoT device-end. Big data analytics (BDA) is expected to provide operational- and customer-level intelligence in IIoT systems. Although numerous studies on IIoT and BDA exist, only a few studies have explored the convergence of the two paradigms. In this study, we investigate the recent BDA technologies, algorithms and techniques that can lead to the development of intelligent IIoT systems. We devise a taxonomy by classifying and categorising the literature on the basis of important parameters (e.g. data sources, analytics tools, analytics techniques, requirements, industrial analytics applications and analytics types). We present the frameworks and case studies of the various enterprises that have benefited from BDA. We also enumerate the considerable opportunities introduced by BDA in IIoT. We identify and discuss the indispensable challenges that remain to be addressed, serving as future research directions.",2019,,https://doi.org/10.1016/j.future.2019.04.020
1180,Shastri L. Nimmagadda and Torsten Reiners and Lincoln C. Wood,On big data-guided upstream business research and its knowledge management,"Upstream business, Heterogeneous and multidimensional data, Data warehousing and mining, Big Data paradigm, Spatial-temporal dimensions","The emerging Big Data integration imposes diverse challenges, compromising the sustainable business research practice. Heterogeneity, multi-dimensionality, velocity, and massive volumes that challenge Big Data paradigm may preclude the effective data and system integration processes. Business alignments get affected within and across joint ventures as enterprises attempt to adapt to changes in industrial environments rapidly. In the context of the Oil and Gas industry, we design integrated artefacts for a resilient multidimensional warehouse repository. With access to several decades of resource data in upstream companies, we incorporate knowledge-based data models with spatial-temporal dimensions in data schemas to minimize ambiguity in warehouse repository implementation. The design considerations ensure uniqueness and monotonic properties of dimensions, maintaining the connectivity between artefacts and achieving the business alignments. The multidimensional attributes envisage Big Data analysts a scope of business research with valuable new knowledge for decision support systems and adding further business values in geographic scales.",2018,,https://doi.org/10.1016/j.jbusres.2018.04.029
1181,Debashis Das and Chinmay Chakraborty and Sourav Banerjee,Chapter 7 - A Framework Development on Big Data Analytics for Terahertz Healthcare,"3D imaging, Big data, Genomic expression, Healthcare technology, Medical sensors, Personal medical information, Radiology images","This chapter is mainly focused on the development of big data analytics in terahertz healthcare technology. In today's world, “big data” is a very familiar term, but the way it is interpreted is modified day by day. Healthcare is one aspect in which big data can be utilized to improve the overall system of healthcare, as, in the context of healthcare, the three primary “V's” of big data definition, volume, variety, and velocity, are very well suited. According to big data analysts, error-free analysis and outcome are ensured from big data, but this is really difficult for medical data due to the issues regarding the data quality. The final V related to big data is value, i.e., how much leverage the data can provide. Thus we can conclude that healthcare is a much preferred area for expert big data analysis. But there are several challenges to be faced in every aspect, starting from data collection to storage, analysis, prediction, etc. The main challenge is the unstructured nature of the data and the organizations from where the data are collected, following no standardized rules, which forms a big gap in the processing of information. Also, a huge investment is required for resources such as high-level expertise, knowledge, technologies used for data analytics, common data warehouses (for obtaining homogeneous data), etc. Despite having so many obstructions, big data analytics has already started growing rapidly in the healthcare sector.",2020,,https://doi.org/10.1016/B978-0-12-818556-8.00007-0
1182,Maria Holmlund and Yves {Van Vaerenbergh} and Robert Ciuchita and Annika Ravald and Panagiotis Sarantopoulos and Francisco Villarroel Ordenes and Mohamed Zaki,Customer experience management in the age of big data analytics: A strategic framework,"Customer experience, Customer experience management, Customer experience insight, Big data analytics","Customer experience (CX) has emerged as a sustainable source of competitive differentiation. Recent developments in big data analytics (BDA) have exposed possibilities to unlock customer insights for customer experience management (CXM). Research at the intersection of these two fields is scarce and there is a need for conceptual work that (1) provides an overview of opportunities to use BDA for CXM and (2) guides management practice and future research. The purpose of this paper is therefore to develop a strategic framework for CXM based on CX insights resulting from BDA. Our conceptualisation is comprehensive and is particularly relevant for researchers and practitioners who are less familiar with the potential of BDA for CXM. For managers, we provide a step-by-step guide on how to kick-start or implement our strategic framework. For researchers, we propose some opportunities for future studies in this promising research area.",2020,,https://doi.org/10.1016/j.jbusres.2020.01.022
1183,Katarzyna Biesialska and Xavier Franch and Victor Muntés-Mulero,Big Data analytics in Agile software development: A systematic mapping study,"Agile software development, Software analytics, Data analytics, Machine learning, Artificial intelligence, Literature review","Context:
Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity.
Objective:
Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA).
Method:
As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019.
Results:
In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics.
Conclusions:
As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.",2021,,https://doi.org/10.1016/j.infsof.2020.106448
1184,Ofir Ben-Assuli and Tsipi Heart and Nir Shlomo and Robert Klempfner,Bringing big data analytics closer to practice: A methodological explanation and demonstration of classification algorithms,"Congestive heart failure, Machine learning, Logistic regression, Boosted decision tree, Support vector machine, Neural network","Background
Big data analytics are becoming more prevalent due to the recent availability of health data. Yet in spite of evidence supporting the potential contribution of big data analytics to health policy makers and care providers, these tools are still too complex to be routinely used. Further, access to comprehensive datasets required for more accurate results is complex and costly. Consequently, big data analytics are mostly used by researchers and experts who are far removed from actual clinical practice. Hence, policy makers should allocate resources to encourage studies that clarify and simplify big data analytics so it can be used by non-experts (e.g., clinicians, practitioners and decision-makers who may not have advanced computer skills). It is also important to fund data collection and integration from various health IT, a pre-condition for any big data analytics project.
Objectives
To methodologically clarify the rationale and logic behind several analytics algorithms to help non-expert users employ big data analytics by understanding how to implement relatively easy to use platforms as Azure ML.
Methods
We demonstrate the predictive power of four known algorithms and compare their accuracy in predicting early mortality of Congestive Heart Failure (CHF) patients.
Results
The results of our models outperform those reported in the literature, attesting to the strength of some of the models, and the utility of comprehensive data.
Conclusions
The results support our call to policy makers to allocate resources to establishing comprehensive, integrated health IT systems, and to projects aimed at simplifying ML analytics.",2019,,https://doi.org/10.1016/j.hlpt.2018.12.003
1185,Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Bhaskar B. Gardas and Pragati Priyadarshinee and Balkrishna E. Narkhede,Linking big data analytics and operational sustainability practices for sustainable business management,"Big-data analytics, Ecological-economic-social sustainability, Green practices, SustainableOperations management, Structural equation modelling-artificial neural network, Emerging economies","Big data analytics is becoming very popular concept in academia as well as in industry. It has come up with new decision tools to design data-driven supply chains. The manufacturing industry is under huge pressure to integrate sustainable practices into their overall business for sustainbale operations management. The purpose of this study is to analyse the predictors of sustainable business performance through big data analytics in the context of developing countries. Data was collected from manufacturing firms those have adopted sustainable practices. A hybrid Structural Equation Modelling - Artificial Neural Network model is used to analyse 316 responses of Indian professional experts. Factor analysis results shows that management and leadership style, state and central-government policy, supplier integration, internal business process, and customer integration have a significant influence on big data analytics and sustainability practices. Furthermore, the results obtained from structural equation modelling were feed as input to the artificial neural network model. The study findings shows that management and leadership style, state and central-government policy as the two most important predictors of big data analytics and sustainability practices. The results provide unique insights into manufacturing firms to improve their sustainable business performance from an operations management viewpoint. The study provides theoretical and practical insights into big data implementation issues in accomplishing sustainability practices in business organisations of emerging economies.",2019,,https://doi.org/10.1016/j.jclepro.2019.03.181
1186,Norjihan Abdul Ghani and Suraya Hamid and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed,Social media big data analytics: A survey,"Big data, Social media, Machine learning, Analytics","Big data analytics has recently emerged as an important research area due to the popularity of the Internet and the advent of the Web 2.0 technologies. Moreover, the proliferation and adoption of social media applications have provided extensive opportunities and challenges for researchers and practitioners. The massive amount of data generated by users using social media platforms is the result of the integration of their background details and daily activities. This enormous volume of generated data known as “big data” has been intensively researched recently. A review of the recent works is presented to obtain a broad perspective of the social media big data analytics research topic. We classify the literature based on important aspects. This study also compares possible big data analytics techniques and their quality attributes. Moreover, we provide a discussion on the applications of social media big data analytics by highlighting the state-of-the-art techniques, methods, and the quality attributes of various studies. Open research challenges in big data analytics are described as well.",2019,,https://doi.org/10.1016/j.chb.2018.08.039
1187,David Loshin,Chapter 5 - Data Governance for Big Data Analytics: Considerations for Data Policies and Processes,"Data governance, accuracy, completeness, consistency, currency, data requirements, consumer expectations, data quality dimensions, metadata, reference data, repurposing, enrichment, enhancement","In this chapter we look at the need for oversight and governance for the data, especially when those developing big data applications often bypass traditional IT and data management channels. Some of the key issues involve the fact that for big data applications that consume massive amounts of data streamed from external sources, there is little or no control that can be exerted to ensure data quality and usability. We consider five key concepts, namely managing data consumer expectations, defining critical data quality dimensions, monitoring consistency of metadata, data repurposing and reinterpretation, and data enrichment when possible.",2013,,https://doi.org/10.1016/B978-0-12-417319-4.00005-3
1188,Valeriia Boldosova,Telling stories that sell: The role of storytelling and big data analytics in smart service sales,"Storytelling, Big data analytics, Smart service, Customer reference, Customer-supplier relationships","The emergence of digitally connected products and big data analytics (BDA) in industrial marketing has attracted academic and managerial interest in smart services. However, suppliers' provision of smart services and customers' adoption of these services have received scarce attention in the literature, demonstrating the need to address the changing nature of customer-supplier interactions in the digital era. Responding to prior research calls, this study utilizes ethnographic research and a storytelling lens to advance our knowledge of how stories and BDA can enhance customers' attitudes toward suppliers' smart services, their behavioral intentions and their actual adoption of smart services. The study's findings demonstrate that storytelling is a collective sensemaking and sensegiving process that occurs in interactions between customers and suppliers in which both parties contribute to the story development. The use of BDA in storytelling enhances customer sensemaking of smart services by highlighting the business value extracted from the digitized data of a reference customer. By synthesizing insights from servitization, storytelling, BDA and the customer reference literature, this study offers managers practical guidance regarding how to increase smart service sales. An example of a story used to facilitate customer adoption of a supplier's smart services in the manufacturing sector is provided.",2020,,https://doi.org/10.1016/j.indmarman.2019.12.004
1189,Shangxin Feng and Zuyu Chen and Hua Luo and Shanyong Wang and Yufei Zhao and Lipeng Liu and Daosheng Ling and Liujie Jing,Tunnel boring machines (TBM) performance prediction: A case study using big data and deep learning,"TBM performance prediction, Deep belief network (DBN), Yingsong Water Diversion Project, Field penetration index prediction","This work explores the potential for predicting TBM performance using deep learning. It focuses on a 17.5-km-long tunnel excavated for the Yingsong Water Diversion Project in Northeastern China with its 728 days’ continuous monitoring of mechanical data. The prediction uses the deep belief network (DBN) proposed by Hinton et al. (2006),on the penetration rate, cutter rotation speed, torque, and thrust force. Field Penetration Index (FPI) is introduced to quantify TBM performance in the field. The DBN algorithm trains on nth number of preceding elements and predicts the performance of the n + 1th element. Prior to the implementation of the DBN, a pilot test was performed to find the optimal values for the network structural parameters (number of input nodes, number of hidden layers, number of nodes in the hidden layers, and learning rate). Predictions on FPIs in all the three rock types were then proceeded with good agreement with the field measured data. The mean relative errors for the predicted measured FPIs are generally less than 0.15 and the correlation coefficients (R) can be higher than 0.78. The predicted and measured FPI values along the length of the tunnel graphically follow the same trends. These results confirm the usefulness of big data and the deep learning in predicting TBM performance.",2021,,https://doi.org/10.1016/j.tust.2020.103636
1190,Zakarya Elaggoune and Ramdane Maamri and Imane Boussebough,A fuzzy agent approach for smart data extraction in big data environments,"Big data, Multi-agent systems, Wireless sensor network, Fuzzy logic, Smart data","The era of big data has brought new challenges in data processing ad management. Existing analytical tools are now close to facing ongoing challenges thus providing satisfactory results at a reasonable cost. However, the velocity at which new data are flooded and the noise generated from such a large volume leads to various new challenges. The present research combines two artificial intelligence fields the represented by multi-agent technologies and fuzzy logic inference systems in order to extract the needed smart data from big noisy ones. A multi-fuzzy agent-based large-scale wireless sensor network has been used to demonstrate the effectiveness of the proposed approach. It handles sensors as autonomous fuzzy agents to measure the relevance of the collected data and eliminate the irrelevant ones. The results of the simulation exhibit a high quality of the data with a decrease in the sensors energy consumption, leading to a longer lifetime of the network.",2020,,https://doi.org/10.1016/j.jksuci.2019.05.009
1191,Tim Hulsen,Chapter 4 - Challenges and solutions for big data in personalized healthcare,"Big data, Precision medicine, Personalized healthcare, Data science, Big data analytics","“Big data” is a term that has been used often in the past decade to describe datasets that are extremely large and complex so that traditional software is unable to store and analyze them in an accurate way. It can refer to “long data,” “wide data,” and both. Big data is of increasing importance in healthcare as well: new methods dedicated to improving data collection, storage, cleaning, processing, and interpretation for medical research continue to be developed. Exploiting new tools and methods to extract meaning from large volume information has the potential to drive real change in clinical practice, and combining this novel data-driven research with the classical hypothesis-driven research will have a large impact on personalized healthcare. However, significant challenges remain. Here we discuss the challenges (and possible solutions) posed to biomedical research by our increasing ability to collect, store, and analyze large datasets. Important challenges include: (1) the need for standardization of data content, format, and clinical definitions, adhering to the FAIR guiding principles; (2) the need for collaborative networks with sharing of both data and expertise, for example through a federated approach; (3) stricter privacy and ethics regulations, in particular the GDPR in the European Union; and (4) a need to reconsider how and when analytic methodology (data science) is taught to medical researchers. Overcoming these challenges will help to make a success of the use of big data in medical and translational research.",2021,,https://doi.org/10.1016/B978-0-12-822884-5.00016-7
1192,Dave Milne and David Watling,Big data and understanding change in the context of planning transport systems,,"This paper considers the implications of so-called ‘big data’ for the analysis, modelling and planning of transport systems. The primary conceptual focus is on the needs of the practical context of medium-term planning and decision-making, from which perspective the paper seeks to achieve three goals: (i) to try to identify what is truly ‘special’ about big data; (ii) to provoke debate on the future relationship between transport planning and big data; and (iii) to try to identify promising themes for research and application. Differences in the information that can be derived from the data compared to more traditional surveys are discussed, and the respects in which they may impact on the role of models in supporting transport planning and decision-making are identified. It is argued that, over time, changes to the nature of data may lead to significant differences in both modelling approaches and in the expectations placed upon them. Furthermore, it is suggested that the potential widespread availability of data to commercial actors and travellers will affect the performance of the transport systems themselves, which might be expected to have knock-on effects for planning functions. We conclude by proposing a series of research challenges that we believe need to be addressed and warn against adaptations based on minimising change from the status quo.",2019,,https://doi.org/10.1016/j.jtrangeo.2017.11.004
1193,António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira,Are Simulation Tools Ready For Big Data? Computational Experiments with Supply Chain Models Developed in Simio,"Simulation, Supply Chain, Big Data, Industry 4.0","The need and potential benefits for the combined use of Simulation and Big Data in Supply Chains (SCs) has been widely recognized. Having worked on such project, some simulation experiments of the modelled SC system were conducted in SIMIO. Different circumstances were tested, including running the model based on the stored data, on statistical distributions and considering risk situations. Thus, this paper aimed to evaluate such experiments, to evaluate the performance of simulations in these contexts. After analyzing the obtained results, it was found that whilst running the model based on the real data required considerable amounts of computer memory, running the model based on statistical distributions reduced such values, albeit required considerable higher time to run a single replication. In all the tested experiments, the simulation took considerable time to run and was not smooth, which can reduce the stakeholders’ interest in the developed tool, despite its benefits for the decision-making process. For future researches, it would be beneficial to test other simulation tools and other strategies and compare those results to the ones provided in this paper.",2020,,https://doi.org/10.1016/j.promfg.2020.02.093
1194,Abdellah Daissaoui and Azedine Boulmakoul and Lamia Karim and Ahmed Lbath,IoT and Big Data Analytics for Smart Buildings: A Survey,"Smart buildings, IoT, Big data analytics, Reactif systems, Complex event processing","The processes of digital transformation have involved a variety of socio-technical activities, with the objective of increasing productivity, safety and quality of execution, sustainable development, collaborative working and solutions for the sustainable smart city. The major digital trends, changing the building sector and revealing new trends of understanding information technologies to integrate in this sector. Current smart building management systems incorporate a variety of sensors, actuators and dedicated networks. Their objectives are to observe the condition of specific areas and apply appropriate rules to preserve or improve comfort while saving energy. In this paper, we propose a review of related works to IoT, Big Data Analytics in smart buildings.",2020,,https://doi.org/10.1016/j.procs.2020.03.021
1195,Amine Belhadi and Karim Zkik and Anass Cherrafi and Sha'ri M. Yusof and Said {El fezazi},Understanding Big Data Analytics for Manufacturing Processes: Insights from Literature Review and Multiple Case Studies,"Big Data Analytics, Manufacturing process, Big Data Analytics capabilities, Business intelligence, Literature review, Multiple case study","Today, we are undoubtedly in the era of data. Big Data Analytics (BDA) is no longer a perspective for all level of the organization. This is of special interest in the manufacturing process with their high capital intensity, time constraints and given the huge amount of data already captured. However, there is a paucity in past literature on BDA to develop better understanding of the capabilities and strategic implications to extract value from BDA. In that vein, the central aim of this paper is to develop a novel model that summarizes the main capabilities of BDA in the context of manufacturing process. This is carried out by relying on the findings of a review of the ongoing research along with a multiple case studies within a leading phosphate derivatives manufacturer to point out the capabilities of BDA in manufacturing processes and outline recommendations to advance research in the field. The findings will help companies to understand the big data analytics capabilities and its potential implications for their manufacturing processes and support them seeking to design more effective BDA-enabler infrastructure.",2019,,https://doi.org/10.1016/j.cie.2019.106099
1196,Esa Hämäläinen and Tommi Inkinen,Industrial applications of big data in disruptive innovations supporting environmental reporting,"Big data, Disruption, Responsible, Process industry, Economic efficiency, Economic geography","Disruptive innovations are usually identified as ideas that are created ‘outside the box’. They are expected to fundamentally change existing business models and processes founded on technological applications. Disruptive innovations can be challenging to define. Information technology (IT) solutions focus on collecting, processing, and reporting different types of data. Commonly, is the solutions are expected (in cybernetics or self-regulating processes) to provide feedback to original processes and to steer them based on the data. To achieve continuous improvement with regard to environmental responsibility and profitability, new thinking and, in particular, accurate and reliable data are needed for decision-making. Very large data storages, known as big data, contain an increasing mass of different types of homogenous and non-homogenous information, as well as extensive time-series. New, innovative algorithms are required to reveal relevant information and opportunities hidden in these data storages. Global environmental challenges and zero-emission responsible production issues can only be solved using relevant and reliable continuous data as the basis. The final goal should be the creation of scalable environmental solutions based on disruptive innovations and accurate data. The aim of this paper is to determine the explicit steps for replacing silo-based reporting with company-wide, refined information, which enables decision-makers in all industries the chance to make responsible choices.",2019,,https://doi.org/10.1016/j.jii.2019.100105
1197,Yongheng Zhang and Rui Zhang and Yizhong Wang and Hongfei Guo and Ray Y Zhong and Ting Qu and Zhiwu Li,Big data driven decision-making for batch-based production systems,"Big data, Smart Product-Service System, Sales predict, Economic batch quantity, production plan","The era of big data has brought new challenges to chemical enterprises. In order to maximize the benefits, enterprises are considering to implement intelligent service technology into traditional production systems to improve the level of intelligence in business. This paper proposes a service framework based on big data driven prediction, which includes information perception layer, information application layer and big data service layer. In this paper, the composition of big data service layer is described in detail, and a sales predicting method based on neural network is introduced. The salability of products is divided, and the qualitative economic production volume mechanism is finally given. Based on the framework, an intelligent service system for enterprises with the characteristics of mass production is implemented. Experimental results show that the big data service framework can support chemical enterprises to make decisions to reduce costs, and provides an effective method for Smart Product Service System (PSS).",2019,,https://doi.org/10.1016/j.procir.2019.05.023
1198,Wei Tu and Tingting Zhu and Jizhe Xia and Yulun Zhou and Yani Lai and Jincheng Jiang and Qingquan Li,Portraying the spatial dynamics of urban vibrancy using multisource urban big data,"Urban vibrancy, Geographically weighted regression, mobile phone data, Social media, Points-of-interest, Big data","Understanding urban vibrancy aids policy-making to foster urban space and therefore has long been a goal of urban studies. Recently, the emerging urban big data and urban analytic methods have enabled us to portray citywide vibrancy. From the social sensing perspective, this study presents a comprehensive and comparative framework to cross-validate urban vibrancy and uncover associated spatial effects. Spatial patterns of urban vibrancy indicated by multisource urban sensing data (points-of-interest, social media check-ins, and mobile phone records) were investigated. A comprehensive urban vibrancy metric was formed by adaptively weighting these metrics. The association between urban vibrancy and demographic, economic, and built environmental factors was revealed with global regression models and local regression models. An empirical experiment was conducted in Shenzhen. The results demonstrate that four urban vibrancy metrics are all higher in the special economic zone (SEZ) and lower in non-SEZs but with different degrees of spatial aggregation. The influences of employment and road density on all vibrancy metrics are significant and positive. However, the effects of metro stations, land use mix, building footprints, and distance to district center depend on the vibrancy indicator and location. These findings unravel the commonalities and differences in urban vibrancy metrics derived from multisource urban big data and the hidden spatial dynamics of the influences of associated factors. They further suggest that urban policies should be proposed to foster vibrancy in Shenzhen therefore benefit social wellbeing and urban development in the long term. They also provide valuable insights into the reliability of urban big data-driven urban studies.",2020,,https://doi.org/10.1016/j.compenvurbsys.2019.101428
1199,Danette McGilvray,Chapter 1 - Data Quality and the Data-Dependent World,"Data-dependent world, data-driven, assets, technology, legal and regulatory tsunami, Internet of Things (IoT), big data, artificial intelligence (AI), machine learning (ML), The Leader’s Data Manifesto, data literacy, change, COVID-19","Chapter 1 addresses topics in our world today, shows how they are dependent on data and information, why data quality is more relevant and critical now than ever before, and how the Ten Steps methodology will help. Topics include COVID-19, the legal and regulatory tsunami, big data, Internet of Things (IoT), 5G, artificial intelligence (AI) and machine learning (ML). Our data-dependent world is broader than, yet encompasses, being data-driven. Data and information are assets to be managed and are compared to how human and financial resources are managed. The Leader’s Data Manifesto is introduced as a starting point for conversations about the importance of managing data and information assets. While the Ten Steps methodology is meant for specific audiences, three recommendations were provided that anyone, in any organization, can do to help raise awareness of the importance of data quality: add data and information to the conversation, increase data literacy in the workplace, and include data (quality) management in learning institutions at all levels.",2021,,https://doi.org/10.1016/B978-0-12-818015-0.00021-9
1200,András Urbán and Axel Groniewsky and Milan Malý and Viktor Józsa and Jan Jedelský,Application of big data analysis technique on high-velocity airblast atomization: Searching for optimum probability density function,"Big data, Airblast, Rapeseed oil, PDA, Probability density function, Likelihood","In this paper, the droplet size distributions of high-velocity airblast atomization were analyzed. The spray measurement was performed by a Phase-Doppler anemometer at several points and different diameters across the spray for diesel oil, light heating oil, crude rapeseed oil, and water. The atomizing gauge pressure and the liquid preheating temperature varied from 0.3 to 2.4 bar and 25 to 100 °C, respectively. Approximately 400 million individual droplets were recorded; therefore, a big data evaluation technique was applied. 18 of the most commonly used probability density functions (PDF) were fitted to the histogram of each measuring point and evaluated by their relative log-likelihood. Among the three-parameter PDFs, Generalized Extreme Value and Burr PDFs provided the most desirable result to describe a complete drop size distribution. With restriction to two-parameter PDFs, the Nakagami PDF unexpectedly outperformed all the others, including Weibull (Rosin-Rammler) PDF, which is commonly used in atomization. However, if the spray is characterized by a single value, such as the Sauter Mean Diameter, i.e. an expected value-like parameter is of primary importance over the distribution, Gamma PDF is the best option, used in several papers of the atomization literature.",2020,,https://doi.org/10.1016/j.fuel.2020.117792
1201,Konstantinos Perakis and Fenareti Lampathaki and Konstantinos Nikas and Yiannis Georgiou and Oskar Marko and Jarissa Maselyne,CYBELE – Fostering precision agriculture & livestock farming through secure access to large-scale HPC enabled virtual industrial experimentation environments fostering scalable big data analytics,"Precision agriculture, Precision livestock farming, High performance computing, Big data analytics","According to McKinsey & Company, about a third of food produced is lost or wasted every year, amounting to a $940 billion economic hit. Inefficiencies in planting, harvesting, water use, reduced animal contributions, as well as uncertainty about weather, pests, consumer demand and other intangibles contribute to the loss. Precision Agriculture (PA) and Precision Livestock Farming (PLF) come to assist in optimizing agricultural and livestock production and minimizing the wastes and costs aforementioned. PA is a technology-enabled, data-driven approach to farming management that observes, measures, and analyzes the needs of individual fields and crops. PLF is also a technology-enabled, data-driven approach to livestock production management, which exploits technology to quantitatively measure the behavior, health and performance of animals. Big data delivered by a plethora of data sources related to these domains, has a multitude of payoffs including precision monitoring of fertilizer and fungicide levels to optimize crop yields, risk mitigation that results from monitoring when temperature and humidity levels reach dangerous levels for crops, increasing livestock production while minimizing the environmental footprint of livestock farming, ensuring high levels of welfare and health for animals, and more. By adding analytics to these sensor and image data, opportunities also exist to further optimize PA and PLF by having continuous data on how a field or the livestock is responding to a protocol. For these domains, two main challenges exist: 1) to exploit this multitude of data facilitating dedicated improvements in performance, and 2) to make available advanced infrastructure so as to harness the power of this information in order to benefit from the new insights, practices and products, efficiently time-wise, lowering responsiveness down to seconds so as to cater for time-critical decisions. The current paper aims to introduce CYBELE, a platform aspiring to safeguard that the stakeholders involved in the agri-food value chain (research community, SMEs, entrepreneurs, etc.) have integrated, unmediated access to a vast amount of very large scale datasets of diverse types and coming from a variety of sources, and that they are capable of actually generating value and extracting insights out of these data, by providing secure and unmediated access to large-scale High Performance Computing (HPC) infrastructures supporting advanced data discovery, processing, combination and visualization services, solving computationally-intensive challenges modelled as mathematical algorithms requiring very high computing power and capability.",2020,,https://doi.org/10.1016/j.comnet.2019.107035
1202,Ahmed Abdulhasan Alwan and Mihaela Anca Ciupala and Allan J. Brimicombe and Seyed Ali Ghorashi and Andres Baravalle and Paolo Falcarin,Data quality challenges in large-scale cyber-physical systems: A systematic review,"Cyber-physical systems (CPS), Wireless Sensor Networks(WSN), Data quality management, Data quality dimensions, Smart cities, Quality of observations","Cyber-physical systems (CPSs) are integrated systems engineered to combine computational control algorithms and physical components such as sensors and actuators, effectively using an embedded communication core. Smart cities can be viewed as large-scale, heterogeneous CPSs that utilise technologies like the Internet of Things (IoT), surveillance, social media, and others to make informed decisions and drive the innovations of automation in urban areas. Such systems incorporate multiple layers and complex structure of hardware, software, analytical algorithms, business knowledge and communication networks, and operate under noisy and dynamic conditions. Thus, large-scale CPSs are vulnerable to enormous technical and operational challenges that may compromise the quality of data of their applications and accordingly reduce the quality of their services. This paper presents a systematic literature review to investigate data quality challenges in smart-cities large-scale CPSs and to identify the most common techniques used to address these challenges. This systematic literature review showed that significant work had been conducted to address data quality management challenges in smart cities, large-scale CPS applications. However, still, more is required to provide a practical, comprehensive data quality management solution to detect errors in sensor nodes’ measurements associated with the main data quality dimensions of accuracy, timeliness, completeness, and consistency. No systematic or generic approach was demonstrated for detecting sensor nodes and sensor node networks failures in large-scale CPS applications. Moreover, further research is required to address the challenges of ensuring the quality of the spatial and temporal contextual attributes of sensor nodes’ observations.",2022,,https://doi.org/10.1016/j.is.2021.101951
1203,Aadia I. Rana and Michael J. Mugavero,How Big Data Science Can Improve Linkage and Retention in Care,"HIV, AIDS, Prevention, Treatment, Continuum, Data, Surveillance",,2019,,https://doi.org/10.1016/j.idc.2019.05.009
1204,Sarah Wordsworth and Brett Doble and Katherine Payne and James Buchanan and Deborah A. Marshall and Christopher McCabe and Dean A. Regier,Using “Big Data” in the Cost-Effectiveness Analysis of Next-Generation Sequencing Technologies: Challenges and Potential Solutions,"Big data, cost-effectiveness, next generation sequencing","Next-generation sequencing (NGS) is considered to be a prominent example of “big data” because of the quantity and complexity of data it produces and because it presents an opportunity to use powerful information sources that could reduce clinical and health economic uncertainty at a patient level. One obstacle to translating NGS into routine health care has been a lack of clinical trials evaluating NGS technologies, which could be used to populate cost-effectiveness analyses (CEAs). A key question is whether big data can be used to partially support CEAs of NGS. This question has been brought into sharp focus with the creation of large national sequencing initiatives. In this article we summarize the main methodological and practical challenges of using big data as an input into CEAs of NGS. Our focus is on the challenges of using large observational datasets and cohort studies and linking these data to the genomic information obtained from NGS, as is being pursued in the conduct of large genomic sequencing initiatives. We propose potential solutions to these key challenges. We conclude that the use of genomic big data to support and inform CEAs of NGS technologies holds great promise. Nevertheless, health economists face substantial challenges when using these data and must be cognizant of them before big data can be confidently used to produce evidence on the cost-effectiveness of NGS.",2018,,https://doi.org/10.1016/j.jval.2018.06.016
1205,Jenifer Pedro and Irwin Brown and Mike Hart,Capabilities and Readiness for Big Data Analytics,"Big data analytics, organisational readiness, organisational capabilities, frameworks, business analytics, thematic analysis","Despite some of the initial hype from marketers and consultants, the use of big data is now firmly established in many organisations worldwide. Big data analytics (BDA) is making use of huge volumes of data from a wide range of structured and unstructured sources. Surveys have however reported a number of barriers to organisational effectiveness with BDA. This research aims to determine what capabilities large organisations require to be ready for a successful BDA initiative. Drawing mainly on relevant results of two published research articles, key informed stakeholders from a large South African telecommunications company were interviewed on this topic. Thematic analysis identified the key themes and sub-themes relating to capabilities needed for the organization to be ready for effective BDA. These proved to be very similar to those given in the earlier research, although a new capability of legal compliance for data protection was now added.",2019,,https://doi.org/10.1016/j.procs.2019.12.147
1206,Badr {Ait Hammou} and Ayoub {Ait Lahcen} and Salma Mouline,Towards a real-time processing framework based on improved distributed recurrent neural network variants with fastText for social big data analytics,"Big data, FastText, Recurrent neural networks, LSTM, BiLSTM, GRU, Natural language processing, Sentiment analysis, Social big data analytics","Big data generated by social media stands for a valuable source of information, which offers an excellent opportunity to mine valuable insights. Particularly, User-generated contents such as reviews, recommendations, and users’ behavior data are useful for supporting several marketing activities of many companies. Knowing what users are saying about the products they bought or the services they used through reviews in social media represents a key factor for making decisions. Sentiment analysis is one of the fundamental tasks in Natural Language Processing. Although deep learning for sentiment analysis has achieved great success and allowed several firms to analyze and extract relevant information from their textual data, but as the volume of data grows, a model that runs in a traditional environment cannot be effective, which implies the importance of efficient distributed deep learning models for social Big Data analytics. Besides, it is known that social media analysis is a complex process, which involves a set of complex tasks. Therefore, it is important to address the challenges and issues of social big data analytics and enhance the performance of deep learning techniques in terms of classification accuracy to obtain better decisions. In this paper, we propose an approach for sentiment analysis, which is devoted to adopting fastText with Recurrent neural network variants to represent textual data efficiently. Then, it employs the new representations to perform the classification task. Its main objective is to enhance the performance of well-known Recurrent Neural Network (RNN) variants in terms of classification accuracy and handle large scale data. In addition, we propose a distributed intelligent system for real-time social big data analytics. It is designed to ingest, store, process, index, and visualize the huge amount of information in real-time. The proposed system adopts distributed machine learning with our proposed method for enhancing decision-making processes. Extensive experiments conducted on two benchmark data sets demonstrate that our proposal for sentiment analysis outperforms well-known distributed recurrent neural network variants (i.e., Long Short-Term Memory (LSTM), Bidirectional Long Short-Term Memory (BiLSTM), and Gated Recurrent Unit (GRU)). Specifically, we tested the efficiency of our approach using the three different deep learning models. The results show that our proposed approach is able to enhance the performance of the three models. The current work can provide several benefits for researchers and practitioners who want to collect, handle, analyze and visualize several sources of information in real-time. Also, it can contribute to a better understanding of public opinion and user behaviors using our proposed system with the improved variants of the most powerful distributed deep learning and machine learning algorithms. Furthermore, it is able to increase the classification accuracy of several existing works based on RNN models for sentiment analysis.",2020,,https://doi.org/10.1016/j.ipm.2019.102122
1207,Yankai Wang and Shilong Wang and Bo Yang and Lingzi Zhu and Feng Liu,"Big data driven Hierarchical Digital Twin Predictive Remanufacturing paradigm: Architecture, control mechanism, application scenario and benefits","multi-life-cycle remanufacturing, Sustainable products, Big data, CPS-Digital-twin(CPSDT), IoT-cloud, Reconfiguration","Remanufacturing is deemed to be an effective method for recycling resources, achieving sustainable production. However, little importance of remanufacturing has been attached in PLM. Surely, there are many problems in implementation of the remanufacturing strategy, such as inability to effectively reduce uncertainty, lack of product multi-life-cycle remanufacturing process tracking management, lack of smart enabling technology application in the full lifecycle that focusing on multi-life-cycle remanufacturing. After analyzing the reasons, through integrating smart enabling technologies, a new PLM paradigm focusing on the multi-life-cycle remanufacturing process: Big Data driven Hierarchical Digital Twin Predictive Remanufacturing (BDHDTPREMfg) is proposed. And the definition of BDHDTPREMfg is proposed. A big data driven layered architecture and the hierarchical CPS-Digital-Twin(CPSDT) reconfiguration control mechanism of BDHDTPREMfg are respectively developed. Then, this paper presents an application scenario of BDHDTPREMfg to validate the feasibility and effectiveness. Based on the above application analysis, the benefits of penetrating BDHDTPREMfg into the entire lifecycle are demonstrated. The summary of this paper and future research work is discussed in the end.",2020,,https://doi.org/10.1016/j.jclepro.2019.119299
1208,Christian Dremel and Matthias M. Herterich and Jochen Wulf and Jan {vom Brocke},Actualizing big data analytics affordances: A revelatory case study,"Big data analytics, Affordance theory, Socio-technical approach, Organizational transformation, Organizational benefits, Affordance actualization","Drawing on a revelatory case study, we identify four big data analytics (BDA) actualization mechanisms: (1) enhancing, (2) constructing, (3) coordinating, and (4) integrating, which manifest in actions on three socio-technical system levels, i.e., the structure, actor, and technology levels. We investigate the actualization of four BDA affordances at an automotive manufacturing company, i.e., establishing customer-centric marketing, provisioning vehicle-data-driven services, data-driven vehicle developing, and optimizing production processes. This study introduces a theoretical perspective to BDA research that explains how organizational actions contribute to actualizing BDA affordances. We further provide practical implications that can help guide practitioners in BDA adoption.",2020,,https://doi.org/10.1016/j.im.2018.10.007
1209,Chiara Bachechi and Laura Po and Federica Rollo,Big Data Analytics and Visualization in Traffic Monitoring,"Traffic, Time series, Air quality maps, Real-time data, Spatio-temporal data, Smart cities","This paper presents a system that employs information visualization techniques to analyze urban traffic data and the impact of traffic emissions on urban air quality. Effective visualizations allow citizens and public authorities to identify trends, detect congested road sections at specific times, and perform monitoring and maintenance of traffic sensors. Since road transport is a major source of air pollution, also the impact of traffic on air quality has emerged as a new issue that traffic visualizations should address. Trafair Traffic Dashboard exploits traffic sensor data and traffic flow simulations to create an interactive layout focused on investigating the evolution of traffic in the urban area over time and space. The dashboard is the last step of a complex data framework that starts from the ingestion of traffic sensor observations, anomaly detection, traffic modeling, and also air quality impact analysis. We present the results of applying our proposed framework on two cities (Modena, in Italy, and Santiago de Compostela, in Spain) demonstrating the potential of the dashboard in identifying trends, seasonal events, abnormal behaviors, and understanding how urban vehicle fleet affects air quality. We believe that the framework provides a powerful environment that may guide the public decision-makers through effective analysis of traffic trends devoted to reducing traffic issues and mitigating the polluting effect of transportation.",2022,,https://doi.org/10.1016/j.bdr.2021.100292
1210,Chiehyeon Lim and Kwang-Jae Kim and Paul P. Maglio,"Smart cities with big data: Reference models, challenges, and considerations","Smart city, Big data, Reference model, Challenge, Consideration","Cities worldwide are attempting to transform themselves into smart cities. Recent cases and studies show that a key factor in this transformation is the use of urban big data from stakeholders and physical objects in cities. However, the knowledge and framework for data use for smart cities remain relatively unknown. This paper reports findings from an analysis of various use cases of big data in cities worldwide and the authors' four projects with government organizations toward developing smart cities. Specifically, this paper classifies the urban data use cases into four reference models and identifies six challenges in transforming data into information for smart cities. Furthermore, building upon the relevant literature, this paper proposes five considerations for addressing the challenges in implementing the reference models in real-world applications. The reference models, challenges, and considerations collectively form a framework for data use for smart cities. This paper will contribute to urban planning and policy development in the modern data-rich economy.",2018,,https://doi.org/10.1016/j.cities.2018.04.011
1211,Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie,Big data analytics and firm performance: Findings from a mixed-method approach,"Big data analytics, Complexity theory, fsQCA, Business value, Mixed-method, Environmental uncertainty","Big data analytics has been widely regarded as a breakthrough technological development in academic and business communities. Despite the growing number of firms that are launching big data initiatives, there is still limited understanding on how firms translate the potential of such technologies into business value. The literature argues that to leverage big data analytics and realize performance gains, firms must develop strong big data analytics capabilities. Nevertheless, most studies operate under the assumption that there is limited heterogeneity in the way firms build their big data analytics capabilities and that related resources are of similar importance regardless of context. This paper draws on complexity theory and investigates the configurations of resources and contextual factors that lead to performance gains from big data analytics investments. Our empirical investigation followed a mixed methods approach using survey data from 175 chief information officers and IT managers working in Greek firms, and three case studies to show that depending on the context, big data analytics resources differ in significance when considering performance gains. Applying a fuzzy-set qualitative comparative analysis (fsQCA) method on the quantitative data, we show that there are four different patterns of elements surrounding big data analytics that lead to high performance. Outcomes of the three case studies highlight the inter-relationships between these elements and outline challenges that organizations face when orchestrating big data analytics resources.",2019,,https://doi.org/10.1016/j.jbusres.2019.01.044
1212,Yuan Guo and Yuan Guo and K. Wu,Research on case retrieval of Bayesian network under big data,"Case retrieval, Big data, BN model, Hadoop platform","Although case retrieval of Bayesian network has greatly promoted the application of CBR technique in engineering fields, it is facing huge challenges with the arrival of the era of big data. First, huge computation task of BN learning caused by big data seriously hampers the efficiency of case retrieval; Second, with the increasing data size, the accuracy of case retrieval becomes poorer and poorer because existing methods of improving probability learning become unfit for new situation. Aiming at the first problem, this paper proposes Within-Cross algorithm to assign computation task to improve the result of parallel data processing and gain better efficiency of case retrieval. For the second problem, this paper proposes a new method called Weighted Index Coefficient of Dirichlet Distribution (WICDD) algorithm, which first measures the influence of different factors on probability learning and then gives a weight to each super parameter of Dirichlet Distribution to adjust the result of probability learning. Thus with WICDD algorithm, the effect of probability learning is greatly improved, which then further enhances the accuracy of case retrieval. Finally, lots of experiments are executed to validate the effectiveness of the proposed method.",2018,,https://doi.org/10.1016/j.datak.2018.08.002
1213,Zhihong Li,Search query of English translation text based on embedded system and big data,"Cross-language information retrieval, Optical character recognition, Embedded applications","Cross-Language Information Retrieval (CLIR) the purpose of another language (target language), a collection of documents written question from one language (source language).CLIR employees publish documents based on user queries, dictionary translation, machine translation methods, and promotion problems. Through various detection methods and translation, this word applies to single words, translation, transliteration of names, including transcription and translation and disambiguation. CLIR Recovery in Question Semantics Technology is the most appropriate) Translation to retrieve documents (dictionary related method) based on English Question Concentrations and Question translation. To the proposed model of translating Arabic to English, and provides the high accuracy Optical Character Recognition (OCR) errors in handling orthography, expanding outside and transliteration dubbed gives higher accuracy in resolving ambiguities. Thus, the single question expands with additional meanings and related words that improve significantly with semantic input. However, the documents related to the questions recover those cross language boundaries. The development of large data systems is completely different from the actual goal of small (traditional, structured) data system development. When the in-depth learning technology is developed, face some space and environmental barriers in different laboratory environments. To describe the requirements when running embedded applications on computers for deep learning.",2021,,https://doi.org/10.1016/j.micpro.2021.103928
1214,Riyaz Ahamed {Ariyaluran Habeeb} and Fariza Nasaruddin and Abdullah Gani and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed and Muhammad Imran,Real-time big data processing for anomaly detection: A Survey,"Real-time, Big data processing, Anomaly detection and machine learning algorithms","The advent of connected devices and omnipresence of Internet have paved way for intruders to attack networks, which leads to cyber-attack, financial loss, information theft in healthcare, and cyber war. Hence, network security analytics has become an important area of concern and has gained intensive attention among researchers, off late, specifically in the domain of anomaly detection in network, which is considered crucial for network security. However, preliminary investigations have revealed that the existing approaches to detect anomalies in network are not effective enough, particularly to detect them in real time. The reason for the inefficacy of current approaches is mainly due the amassment of massive volumes of data though the connected devices. Therefore, it is crucial to propose a framework that effectively handles real time big data processing and detect anomalies in networks. In this regard, this paper attempts to address the issue of detecting anomalies in real time. Respectively, this paper has surveyed the state-of-the-art real-time big data processing technologies related to anomaly detection and the vital characteristics of associated machine learning algorithms. This paper begins with the explanation of essential contexts and taxonomy of real-time big data processing, anomalous detection, and machine learning algorithms, followed by the review of big data processing technologies. Finally, the identified research challenges of real-time big data processing in anomaly detection are discussed.",2019,,https://doi.org/10.1016/j.ijinfomgt.2018.08.006
1215,Shabnam Shadroo and Amir Masoud Rahmani,Systematic survey of big data and data mining in internet of things,"Internet of things, Systematic survey, Big data, Data mining","In recent years, the Internet of Things (IoT) has emerged as a new opportunity. Thus, all devices such as smartphones, transportation facilities, public services, and home appliances are used as data creator devices. All the electronic devices around us help our daily life. Devices such as wrist watches, emergency alarms, and garage doors and home appliances such as refrigerators, microwaves, air conditioning, and water heaters are connected to an IoT network and controlled remotely. Methods such as big data and data mining can be used to improve the efficiency of IoT and storage challenges of a large data volume and the transmission, analysis, and processing of the data volume on the IoT. The aim of this study is to investigate the research done on IoT using big data as well as data mining methods to identify subjects that must be emphasized more in current and future research paths. This article tries to achieve the goal by following the conference and journal articles published on IoT-big data and also IoT-data mining areas between 2010 and August 2017. In order to examine these articles, the combination of Systematic Mapping and literature review was used to create an intended review article. In this research, 44 articles were studied. These articles are divided into three categories: Architecture & Platform, framework, and application. In this research, a summary of the methods used in the area of IoT-big data and IoT-data mining is presented in three categories to provide a starting point for researchers in the future.",2018,,https://doi.org/10.1016/j.comnet.2018.04.001
1216,Nishita Mehta and Anil Pandit,Concurrence of big data analytics and healthcare: A systematic review,"Big data, Analytics, Healthcare, Predictive analytics, Evidence-based medicine","Background
The application of Big Data analytics in healthcare has immense potential for improving the quality of care, reducing waste and error, and reducing the cost of care.
Purpose
This systematic review of literature aims to determine the scope of Big Data analytics in healthcare including its applications and challenges in its adoption in healthcare. It also intends to identify the strategies to overcome the challenges.
Data sources
A systematic search of the articles was carried out on five major scientific databases: ScienceDirect, PubMed, Emerald, IEEE Xplore and Taylor & Francis. The articles on Big Data analytics in healthcare published in English language literature from January 2013 to January 2018 were considered.
Study selection
Descriptive articles and usability studies of Big Data analytics in healthcare and medicine were selected.
Data extraction
Two reviewers independently extracted information on definitions of Big Data analytics; sources and applications of Big Data analytics in healthcare; challenges and strategies to overcome the challenges in healthcare.
Results
A total of 58 articles were selected as per the inclusion criteria and analyzed. The analyses of these articles found that: (1) researchers lack consensus about the operational definition of Big Data in healthcare; (2) Big Data in healthcare comes from the internal sources within the hospitals or clinics as well external sources including government, laboratories, pharma companies, data aggregators, medical journals etc.; (3) natural language processing (NLP) is most widely used Big Data analytical technique for healthcare and most of the processing tools used for analytics are based on Hadoop; (4) Big Data analytics finds its application for clinical decision support; optimization of clinical operations and reduction of cost of care (5) major challenge in adoption of Big Data analytics is non-availability of evidence of its practical benefits in healthcare.
Conclusion
This review study unveils that there is a paucity of information on evidence of real-world use of Big Data analytics in healthcare. This is because, the usability studies have considered only qualitative approach which describes potential benefits but does not take into account the quantitative study. Also, majority of the studies were from developed countries which brings out the need for promotion of research on Healthcare Big Data analytics in developing countries.",2018,,https://doi.org/10.1016/j.ijmedinf.2018.03.013
1217,Heiu-Jou Shaw and Cheng-Kuan Lin,Marine big data analysis of ships for the energy efficiency changes of the hull and maintenance evaluation based on the ISO 19030 standard,"Energy efficiency management, ISO 19030, Hull and propeller maintenance","This study analyzes the energy efficiency of ships based on ISO 19030, which is a standard for the measurement of changes in hull and propeller performance. The goal is to provide energy efficiency management with digital indicators that have not been easily provided. The ship navigation information platform (SNIP) is developed to determine the dynamic information of each ship, including the fuel consumption, ship speed, horsepower of the engine, rotation speed of the engine, wind direction, and wind speed. In addition, model test data and computational fluid dynamics (CFD) calculation data are applied to calculate the energy efficiency performance indicators. Finally, the relationship of the speed through water and the speed over ground enables us to modify the effects of the ocean currents. The results verify that these indicators can be used as a reference for performance monitoring and maintenance prediction of international maritime affairs.",2021,,https://doi.org/10.1016/j.oceaneng.2021.108953
1218,Alessandro Mantelero,"AI and Big Data: A blueprint for a human rights, social and ethical impact assessment","Data protection, Impact assessment, Data protection impact assessment, Human rights, Human rights impact assessment, Ethical impact assessment, Social impact assessment, General Data Protection Regulation","The use of algorithms in modern data processing techniques, as well as data-intensive technological trends, suggests the adoption of a broader view of the data protection impact assessment. This will force data controllers to go beyond the traditional focus on data quality and security, and consider the impact of data processing on fundamental rights and collective social and ethical values. Building on studies of the collective dimension of data protection, this article sets out to embed this new perspective in an assessment model centred on human rights (Human Rights, Ethical and Social Impact Assessment-HRESIA). This self-assessment model intends to overcome the limitations of the existing assessment models, which are either too closely focused on data processing or have an extent and granularity that make them too complicated to evaluate the consequences of a given use of data. In terms of architecture, the HRESIA has two main elements: a self-assessment questionnaire and an ad hoc expert committee. As a blueprint, this contribution focuses mainly on the nature of the proposed model, its architecture and its challenges; a more detailed description of the model and the content of the questionnaire will be discussed in a future publication drawing on the ongoing research.",2018,,https://doi.org/10.1016/j.clsr.2018.05.017
1219,Matthew Jones,What we talk about when we talk about (big) data,,"In common with much contemporary discourse around big data, recent discussion of datafication in the Journal of Strategic Information Systems has focused on its effects on individuals, organisations and society. Generally missing from such analysis, however, is any consideration of data themselves. What is it that is having these effects? In this Viewpoint article I therefore present a critical analysis of a number of widely-held assumptions about data in general and big data in particular. Rather than being a referential, natural, foundational, objective and equal representation of the world, it will be argued, data are partial and contingent and are brought into being through situated practices of conceptualization, recording and use. Big data are also not as revolutionary voluminous, universal or exhaustive as they are often presented. Some initial implications of this reconceptualization of data are explored. A distinction is made between “data in principle” as they are recorded, and the “data in practice” as they are used. It is only the latter, typically a small and not necessarily representative subset of the former, that will contribute directly to the effects of datafication.",2019,,https://doi.org/10.1016/j.jsis.2018.10.005
1220,Bei Li and Jianbin Li and Yuqiao Jiang and Xiaoyun Lan,Experience and reflection from China’s Xiangya medical big data project,"Medical big data, Data sharing, Information security, Cooperation mechanism, Medical data acquisition, Medical data centre","The construction of medical big data includes several problems that need to be solved, such as integration and data sharing of many heterogeneous information systems, efficient processing and analysis of large-scale medical data with complex structure or low degree of structure, and narrow application range of medical data. Therefore, medical big data construction is not only a simple collection and application of medical data but also a complex systematic project. This paper introduces China's experience in the construction of a regional medical big data ecosystem, including the overall goal of the project; establishment of policies to encourage data sharing; handling the relationship between personal privacy, information security, and information availability; establishing a cooperation mechanism between agencies; designing a polycentric medical data acquisition system; and establishing a large data centre. From the experience gained from one of China's earliest established medical big data projects, we outline the challenges encountered during its development and recommend approaches to overcome these challenges to design medical big data projects in China more rationally. Clear and complete top-level design of a project requires to be planned in advance and considered carefully. It is essential to provide a culture of information sharing and to facilitate the opening of data, and changes in ideas and policies need the guidance of the government. The contradiction between data sharing and data security must be handled carefully, that is not to say data openness could be abandoned. The construction of medical big data involves many institutions, and high-level management and cooperation can significantly improve efficiency and promote innovation. Compared with infrastructure construction, it is more challenging and time-consuming to develop appropriate data standards, data integration tools and data mining tools.",2019,,https://doi.org/10.1016/j.jbi.2019.103149
1221,Aboobucker Ilmudeen,Chapter 3 - Big data-based frameworks for healthcare systems,"Big data, Frameworks, Healthcare, Healthcare systems","Today, the term big data involves various applications, technologies, architectures, services, and standards in the healthcare domain. The advanced technology and healthcare systems have been extremely knotted together in recent times. As the adoption of wearable biosensors and their applications have begun across the world, eHealth and mHealth have emerged. Hence, wearable sensor devices produce organized and unorganized big data that cannot be easily processed and analyzed due to its complexity; that hinders effective medical decision making. Modern advances in healthcare systems have increased the size of health records such as through electronic health records, patient care, clinical reports, regulations, and compliance requirements. However, the present data-processing technologies are not capable of handling the growing amount of large datasets. This chapter discusses theoretical illustrations that pinpoint various aspects of big data-related frameworks and proposes a large data-based conceptual framework in healthcare systems.",2021,,https://doi.org/10.1016/B978-0-12-821633-0.00003-9
1222,João Canito and Pedro Ramos and Sérgio Moro and Paulo Rita,Unfolding the relations between companies and technologies under the Big Data umbrella,"Big data companies, Big data technologies, Online news, Gartner magic quadrant","Big Data is dominating the landscape as data originated in many sources keeps piling up. Information Technology (IT) business companies are making tremendous efforts to keep the pace with this wave of innovative technologies. This study aims to identify how the different IT companies are aligned with emerging Big Data technologies. The approach consisted in analyzing 11,505 news published between 2013 and 2016 and aggregated through Google News. The companies were categorized according to their position in the 2017 Gartner Magic Quadrant for advanced analytics. A text mining and topic modeling procedure assisted in summarizing the main findings. Leaders dominated a large fraction of the published news. Challengers are making a significant effort in investing in predictive analytics, overlooking other technologies such as those related to data preparation and integration. The results helped to shed light on the emerging field of Big Data from a corporate perspective.",2018,,https://doi.org/10.1016/j.compind.2018.03.018
1223,Lang Huang and Chao Wu and Bing Wang,"Challenges, opportunities and paradigm of applying big data to production safety management: From a theoretical perspective","Big data, Production safety management, Big-data-driven, Challenges, Opportunities","Big data has caused the scientific community to re-examine the scientific research methodologies and has triggered a revolution in scientific thinking. As a branch of scientific research, production safety management is also exploring methods to take advantage of big data. This research aims to provide a theoretical basis for promoting the application of big data in production safety management. First, four different types of production safety management paradigms were identified, namely small-data-based, static-oriented, interpretation-based and causal-oriented paradigm, and the challenges to these paradigms in the presence of big data were introduced. Second, the opportunities of employing big data in production safety management were identified from four aspects, including better predict the future production safety phenomena, promote production safety management highlight relevance, achieve the balance between deductive and inductive approaches and promote the interdisciplinary development of production safety management. Third, the paradigm shifting trend of production safety management was concluded, and the discipline foundation of the new paradigm was considered as the integration of data science, production management and safety science. Fourth, a new big-data-driven production safety management paradigm was developed, which consists of the logical line of production safety management, the macro-meso-micro data spectrum, the key big data analytics, and the four-dimensional morphology. At last, the strengths (e.g., supporting better-informed safety description, safety inquisition, safety prediction) and future research direction (e.g., theory research focuses on safety-related data mining/capturing/cleansing) of the new paradigm were discussed. The research results not only can provide theoretical and practical basis for big-data-driven production safety management, but also can offer advice to managerial consideration and scholarly investigation.",2019,,https://doi.org/10.1016/j.jclepro.2019.05.245
1224,João Vasco Silva and Tomás R. Tenreiro and Léon Spätjens and Niels P.R. Anten and Martin K. {van Ittersum} and Pytrik Reidsma,Can big data explain yield variability and water productivity in intensive cropping systems?,"Arable crops, Yield gaps, Crop ecology, Crop coefficients (kc), The Netherlands","Yield gaps and water productivity are key indicators to monitor the progress towards more sustainable and productive cropping systems. Individual farmers are collecting increasing amounts of data (‘big data’), which can help monitor the process of sustainable intensification at local level. In this study, we build upon such data to quantify the magnitude and identify the biophysical and management determinants of on-farm yield gaps and water productivity for the main arable crops cultivated in the Netherlands. The analysis focused on ware, seed and starch potatoes, sugar beet, spring onion, winter wheat and spring barley and covered the period 2015–2017. A crop modelling approach based on crop coefficients (kc) and daily weather data was used to estimate the potential yield (Yp), radiation intercepted and potential evapotranspiration (ETP) for each crop. Yield gaps were estimated to be ca. 10% of Yp for sugar beet, 25–30% of Yp for ware, seed and starch potato and spring barley, and 35–40% of Yp for spring onion and winter wheat. Variation in actual yields was associated with water availability in key periods of the growing season as well as with sowing and harvest dates. However, the R2 of the fitted regressions was rather low (20–49%). Current levels of crop water productivity ranged between 13 kg DM ha−1 mm−1 for spring barley, ca. 15 kg DM ha−1 mm−1 for seed potato, spring onion and winter wheat, 23 kg DM ha−1 mm−1 for ware potato and ca. 25 kg DM ha−1 mm−1 for starch potato and sugar beet. These values are about half of their potential, but increasing actual water productivity further is restricted by rainfall amount and distribution. However, doing so should not be prioritized over reducing environmental impacts of these intensive cropping systems in the short-term and may require large investments from farm to regional levels in the long-term. Although these findings are most relevant to similar cropping systems in NW Europe, the underlying methods are generic and can be used to benchmark crop performance in other cropping systems. Based on this work, we argue that ‘big data’ are currently most useful to describe cropping systems at regional scale and derive benchmarks of farm performance but not as much to predict and explain crop yield variability in time and space.",2020,,https://doi.org/10.1016/j.fcr.2020.107828
1225,Francis G.N. Li and Chris Bataille and Steve Pye and Aidan O'Sullivan,"Prospects for energy economy modelling with big data: Hype, eliminating blind spots, or revolutionising the state of the art?","Energy modelling, Climate policy, Energy policy, Decarbonisation, Energy data, Big data","Energy economy models are central to decision making on energy and climate issues in the 21st century, such as informing the design of deep decarbonisation strategies under the Paris Agreement. Designing policies that are aimed at achieving such radical transitions in the energy system will require ever more in-depth modelling of end-use demand, efficiency and fuel switching, as well as an increasing need for regional, sectoral, and agent disaggregation to capture technological, jurisdictional and policy detail. Building and using these models entails complex trade-offs between the level of detail, the size of the system boundary, and the available computing resources. The availability of data to characterise key energy system sectors and interactions is also a key driver of model structure and parameterisation, and there are many blind spots and design compromises that are caused by data scarcity. We may soon, however, live in a world of data abundance, potentially enabling previously impossible levels of resolution and coverage in energy economy models. But while big data concepts and platforms have already begun to be used in a number of selected energy research applications, their potential to improve or even completely revolutionise energy economy modelling has been almost completely overlooked in the existing literature. In this paper, we explore the challenges and possibilities of this emerging frontier. We identify critical gaps and opportunities for the field, as well as developing foundational concepts for guiding the future application of big data to energy economy modelling, with reference to the existing literature on decision making under uncertainty, scenario analysis and the philosophy of science.",2019,,https://doi.org/10.1016/j.apenergy.2019.02.002
1226,Daniel I. McIsaac,Real-world evaluation of enhanced recovery after surgery: big data under the microscope,"big data, enhanced recovery, epidemiology, orthopedic surgery, postoperative outcome, study design",,2020,,https://doi.org/10.1016/j.bja.2020.01.012
1227,Liuqing Li and Jack Geissinger and William A. Ingram and Edward A. Fox,Teaching Natural Language Processing through Big Data Text Summarization with Problem-Based Learning,"information system education, computer science education, problem-based learning, natural language processing, NLP, big data text analytics, machine learning, deep learning","Natural language processing (NLP) covers a large number of topics and tasks related to data and information management, leading to a complex and challenging teaching process. Meanwhile, problem-based learning is a teaching technique specifically designed to motivate students to learn efficiently, work collaboratively, and communicate effectively. With this aim, we developed a problem-based learning course for both undergraduate and graduate students to teach NLP. We provided student teams with big data sets, basic guidelines, cloud computing resources, and other aids to help different teams in summarizing two types of big collections: Web pages related to events, and electronic theses and dissertations (ETDs). Student teams then deployed different libraries, tools, methods, and algorithms to solve the task of big data text summarization. Summarization is an ideal problem to address learning NLP since it involves all levels of linguistics, as well as many of the tools and techniques used by NLP practitioners. The evaluation results showed that all teams generated coherent and readable summaries. Many summaries were of high quality and accurately described their corresponding events or ETD chapters, and the teams produced them along with NLP pipelines in a single semester. Further, both undergraduate and graduate students gave statistically significant positive feedback, relative to other courses in the Department of Computer Science. Accordingly, we encourage educators in the data and information management field to use our approach or similar methods in their teaching and hope that other researchers will also use our data sets and synergistic solutions to approach the new and challenging tasks we addressed.",2020,,https://doi.org/10.2478/dim-2020-0003
1228,Roxanne Connelly and Christopher J. Playford and Vernon Gayle and Chris Dibben,The role of administrative data in the big data revolution in social science research,"Big data, Administrative data, Data management, Data quality, Data access","The term big data is currently a buzzword in social science, however its precise meaning is ambiguous. In this paper we focus on administrative data which is a distinctive form of big data. Exciting new opportunities for social science research will be afforded by new administrative data resources, but these are currently under appreciated by the research community. The central aim of this paper is to discuss the challenges associated with administrative data. We emphasise that it is critical for researchers to carefully consider how administrative data has been produced. We conclude that administrative datasets have the potential to contribute to the development of high-quality and impactful social science research, and should not be overlooked in the emerging field of big data.",2016,,https://doi.org/10.1016/j.ssresearch.2016.04.015
1229,Sara Barja-Martinez and Mònica Aragüés-Peñalba and Íngrid Munné-Collado and Pau Lloret-Gallego and Eduard Bullich-Massagué and Roberto Villafafila-Robles,Artificial intelligence techniques for enabling Big Data services in distribution networks: A review,"Machine learning, Deep learning, Smart grid, Distribution grid, Smart energy service","Artificial intelligence techniques lead to data-driven energy services in distribution power systems by extracting value from the data generated by the deployed metering and sensing devices. This paper performs a holistic analysis of artificial intelligence applications to distribution networks, ranging from operation, monitoring and maintenance to planning. The potential artificial intelligence techniques for power system applications and needed data sources are identified and classified. The following data-driven services for distribution networks are analyzed: topology estimation, observability, fraud detection, predictive maintenance, non-technical losses detection, forecasting, energy management systems, aggregated flexibility services and trading. A review of the artificial intelligence methods implemented in each of these services is conducted. Their interdependencies are mapped, proving that multiple services can be offered as a single clustered service to different stakeholders. Furthermore, the dependencies between the AI techniques with each energy service are identified. In recent years there has been a significant rise of deep learning applications for time series prediction tasks. Another finding is that unsupervised learning methods are mainly being applied to customer segmentation, buildings efficiency clustering and consumption profile grouping for non-technical losses detection. Reinforcement learning is being widely applied to energy management systems design, although more testing in real environments is needed. Distribution network sensorization should be enhanced and increased in order to obtain larger amounts of valuable data, enabling better service outcomes. Finally, the future opportunities and challenges for applying artificial intelligence in distribution grids are discussed.",2021,,https://doi.org/10.1016/j.rser.2021.111459
1230,Blagoj Ristevski and Snezana Savoska,4 - Healthcare and medical Big Data analytics,"Big Data, medical and healthcare Big Data, Big Data Analytics, databases, healthcare information systems","In the era of big data, a huge volume of heterogeneous healthcare and medical data are generated daily. These heterogeneous data, that are stored in diverse data formats, have to be integrated and stored in a standard way and format to perform suitable efficient and effective data analysis and visualization. These data, which are generated from different sources such as mobile devices, sensors, lab tests, clinical notes, social media, demographics data, diverse omics data, etc., can be structured, semistructured, or unstructured. These varieties of data structures require these big data to be stored not only in the standard relational databases but also in NoSQL databases. To provide effective data analysis, suitable classification and standardization of big data in medicine and healthcare are necessary, as well as excellent design and implementation of healthcare information systems. Regarding the security and privacy of the patient’s data, we suggest employing suitable data governance policies. Additionally, we suggest choosing of proper software development frameworks, tools, databases, in-database analytics, stream computing and data mining algorithms (supervised, unsupervised and semisupervised) to reveal valuable knowledge and insights from these healthcare and medical big data. Ultimately we propose the development of not only patient-oriented but also decision- and population-centric healthcare information systems.",2021,,https://doi.org/10.1016/B978-0-12-820203-6.00005-9
1231,Mark D. McCoy,Geospatial Big Data and archaeology: Prospects and problems too great to ignore,"Geospatial, Big Data, Spatial technology, Cyberinfrastructure, Data science","As spatial technology has evolved and become integrated in to archaeology, we face a new set of challenges posed by the sheer size and complexity of data we use and produce. In this paper I discuss the prospects and problems of Geospatial Big Data (GBD) – broadly defined as data sets with locational information that exceed the capacity of widely available hardware, software, and/or human resources. While the datasets we create today remain within available resources, we nonetheless face the same challenges as many other fields that use and create GBD, especially in apprehensions over data quality and privacy. After reviewing the kinds of archaeological geospatial data currently available I discuss the near future of GBD in writing culture histories, making decisions, and visualizing the past. I use a case study from New Zealand to argue for the value of taking a data quantity-in-use approach to GBD and requiring applications of GBD in archaeology be regularly accompanied by a Standalone Quality Report.",2017,,https://doi.org/10.1016/j.jas.2017.06.003
1232,Jianguo Qian and Bingquan Zhu and Ying Li and Zhengchai Shi,Visual recognition processing of power monitoring data based on big data computing,"Power control data, Monitoring, Visual identification, Iterative screening, CARIMA","The operation control of power units is usually carried out by the control personnel with the help of distributed control system. Although it can ensure the safety of unit operation and meet the requirements of power generation loads, the economy of unit operation and the accuracy of control process still need to be further improved. Therefore, by designing multiple view mapping and association, it provides interactive visualization support for relevant experts in the key links of model establishment and evaluation. In the exploration stage of estimating model parameter, the user can get the delay range by line chart and focus + context technology, while in the model screening stage, the user can provide the combination of screening views, selecting the model by its accuracy on different data sets, and finding the model anomalies by the model structure view. Besides, in the model evaluation stage, the user can get the delay range by predicting line chart and model accuracy radar chart. In addition, the method in this paper keeps between 4.2–7.2 in most distributions, and the maximum value is 18. The time series trend of the data segment is consistent, and the absolute value of the weight coefficient is basically 0 after being superimposed, which has great advantages compared with other methods, proving the effective results of the research content in this paper.",2021,,https://doi.org/10.1016/j.egyr.2021.09.205
1233,A.L. Caissie and M.L. Mierzwa and C.D. Fuller and M. Rajaraman and A. Lin and A.M. McDonald and R.A. Popple and Y. Xiao and L. {van Dijk} and P. Balter and H. Fong and H. Ping and M. Kovoor and J. Lee and A. Rao and M.K. Martel and R.F. Thompson and B. Merz and J. Yao and C. Mayo,Radiotherapy (RT) Patterns Of Practice Variability Identified As A Challenge To Real-World Big Data: Recommendations From The Learning From Analysis Of Multicenter Big Data Aggregation (LAMBDA) Consortium,,,2020,,https://doi.org/10.1016/j.ijrobp.2020.07.224
1234,Aparna Kumari and Sudeep Tanwar and Sudhanshu Tyagi and Neeraj Kumar and Michele Maasberg and Kim-Kwang Raymond Choo,Multimedia big data computing and Internet of Things applications: A taxonomy and process model,"Multimedia big data, Data acquisition, Data representation, Data reduction, Data analysis, Data security and privacy, System intelligence, Distributed system, Social media","With an exponential increase in the provisioning of multimedia devices over the Internet of Things (IoT), a significant amount of multimedia data (also referred to as multimedia big data – MMBD) is being generated. Current research and development activities focus on scalar sensor data based IoT or general MMBD and overlook the complexity of facilitating MMBD over IoT. This paper examines the unique nature and complexity of MMBD computing for IoT applications and develops a comprehensive taxonomy for MMBD abstracted into a novel process model reflecting MMBD over IoT. This process model addresses a number of research challenges associated with MMBD, such as scalability, accessibility, reliability, heterogeneity, and Quality of Service (QoS) requirements. A case study is presented to demonstrate the process model.",2018,,https://doi.org/10.1016/j.jnca.2018.09.014
1235,Yaliang Zhao and Samwel K. Tarus and Laurence T. Yang and Jiayu Sun and Yunfei Ge and Jinke Wang,Privacy-preserving clustering for big data in cyber-physical-social systems: Survey and perspectives,"CPSS, Big data, Cloud computing, Privacy preserving, Clustering","Clustering technique plays a critical role in data mining, and has received great success to solve application problems like community analysis, image retrieval, personalized recommendation, activity prediction, etc. This paper first reviews the traditional clustering and the emerging multiple clustering methods, respectively. Although the existing methods have superior performance on some small or certain datasets, they fall short when clustering is performed on CPSS big data because of the high cost of computation and storage. With the powerful cloud computing, this challenge can be effectively addressed, but it brings enormous threat to individual or company’s privacy. Currently, privacy preserving data mining has attracted widespread attention in academia. Compared to other reviews, this paper focuses on privacy preserving clustering technique, guiding a detailed overview and discussion. Specifically, we introduce a novel privacy-preserving tensor-based multiple clustering, propose a privacy-preserving tensor-based multiple clustering analytic and service framework, and give an illustrated case study on the public transportation dataset. Furthermore, we indicate the remaining challenges of privacy preserving clustering and discuss the future significant research in this area.",2020,,https://doi.org/10.1016/j.ins.2019.10.019
1236,Faheem Ullah and Muhammad {Ali Babar},Architectural Tactics for Big Data Cybersecurity Analytics Systems: A Review,"Big data, Cybersecurity, Quality attribute, Architectural tactic","Context
Big Data Cybersecurity Analytics (BDCA) systems leverage big data technologies for analyzing security events data to protect organizational networks, computers, and data from cyber attacks.
Objective
We aimed at identifying the most frequently reported quality attributes and architectural tactics for BDCA systems.
Method
We used Systematic Literature Review (SLR) method for reviewing 74 papers.
Result
Our findings are twofold: (i) identification of 12 most frequently reported quality attributes for BDCA systems; and (ii) identification and codification of 17 architectural tactics for addressing the identified quality attributes. The identified tactics include six performance tactics, four accuracy tactics, two scalability tactics, three reliability tactics, and one security and usability tactic each.
Conclusion
Our study reveals that in the context of BDCA (a) performance, accuracy and scalability are the most important quality concerns (b) data analytics is the most critical architectural component (c) despite the significance of interoperability, modifiability, adaptability, generality, stealthiness, and privacy assurance, these quality attributes lack explicit architectural support (d) empirical investigation is required to evaluate the impact of the codified tactics and explore the quality trade-offs and dependencies among the tactics and (e) the reported tactics need to be modelled using a standardized modelling language such as UML.",2019,,https://doi.org/10.1016/j.jss.2019.01.051
1237,Lianfa Li and Mariam Girguis and Frederick Lurmann and Nathan Pavlovic and Crystal McClure and Meredith Franklin and Jun Wu and Luke D. Oman and Carrie Breton and Frank Gilliland and Rima Habre,Ensemble-based deep learning for estimating PM2.5 over California with multisource big data including wildfire smoke,"PM, Machine learning, Air pollution exposure, Wildfires, Remote sensing, California, High spatiotemporal resolution","Introduction
Estimating PM2.5 concentrations and their prediction uncertainties at a high spatiotemporal resolution is important for air pollution health effect studies. This is particularly challenging for California, which has high variability in natural (e.g, wildfires, dust) and anthropogenic emissions, meteorology, topography (e.g. desert surfaces, mountains, snow cover) and land use.
Methods
Using ensemble-based deep learning with big data fused from multiple sources we developed a PM2.5 prediction model with uncertainty estimates at a high spatial (1 km × 1 km) and temporal (weekly) resolution for a 10-year time span (2008–2017). We leveraged autoencoder-based full residual deep networks to model complex nonlinear interrelationships among PM2.5 emission, transport and dispersion factors and other influential features. These included remote sensing data (MAIAC aerosol optical depth (AOD), normalized difference vegetation index, impervious surface), MERRA-2 GMI Replay Simulation (M2GMI) output, wildfire smoke plume dispersion, meteorology, land cover, traffic, elevation, and spatiotemporal trends (geo-coordinates, temporal basis functions, time index). As one of the primary predictors of interest with substantial missing data in California related to bright surfaces, cloud cover and other known interferences, missing MAIAC AOD observations were imputed and adjusted for relative humidity and vertical distribution. Wildfire smoke contribution to PM2.5 was also calculated through HYSPLIT dispersion modeling of smoke emissions derived from MODIS fire radiative power using the Fire Energetics and Emissions Research version 1.0 model.
Results
Ensemble deep learning to predict PM2.5 achieved an overall mean training RMSE of 1.54 μg/m3 (R2: 0.94) and test RMSE of 2.29 μg/m3 (R2: 0.87). The top predictors included M2GMI carbon monoxide mixing ratio in the bottom layer, temporal basis functions, spatial location, air temperature, MAIAC AOD, and PM2.5 sea salt mass concentration. In an independent test using three long-term AQS sites and one short-term non-AQS site, our model achieved a high correlation (>0.8) and a low RMSE (<3 μg/m3). Statewide predictions indicated that our model can capture the spatial distribution and temporal peaks in wildfire-related PM2.5. The coefficient of variation indicated highest uncertainty over deciduous and mixed forests and open water land covers.
Conclusion
Our method can be generalized to other regions, including those having a mix of major urban areas, deserts, intensive smoke events, snow cover and complex terrains, where PM2.5 has previously been challenging to predict. Prediction uncertainty estimates can also inform further model development and measurement error evaluations in exposure and health studies.",2020,,https://doi.org/10.1016/j.envint.2020.106143
1238,José Ricardo López-Robles and Marisela Rodríguez-Salvador and Nadia Karina Gamboa-Rosales and Selene Ramirez-Rosales and Manuel Jesús Cobo,"The last five years of Big Data Research in Economics, Econometrics and Finance: Identification and conceptual analysis","Type your keywords here, separated by semicolons","Today, the Big Data term has a multidimensional approach where five main characteristics stand out: volume, velocity, veracity, value and variety. It has changed from being an emerging theme to a growing research area. In this respect, this study analyses the literature on Big Data in the Economics, Econometrics and Finance field. To do that, 1.034 publications from 2015 to 2019 were evaluated using SciMAT as a bibliometric and network analysis software. SciMAT offers a complete approach of the field and evaluates the most cited and productive authors, countries and subject areas related to Big Data. Lastly, a science map is performed to understand the intellectual structure and the main research lines (themes).",2019,,https://doi.org/10.1016/j.procs.2019.12.044
1239,Simon Vydra and Bram Klievink,Techno-optimism and policy-pessimism in the public sector big data debate,"Big data, Analytics, Government, Public administration, Policy-making, Decision-making, Science-policy interface, Network governance","Despite great potential, high hopes and big promises, the actual impact of big data on the public sector is not always as transformative as the literature would suggest. In this paper, we ascribe this predicament to an overly strong emphasis the current literature places on technical-rational factors at the expense of political decision-making factors. We express these two different emphases as two archetypical narratives and use those to illustrate that some political decision-making factors should be taken seriously by critiquing some of the core ‘techno-optimist’ tenets from a more ‘policy-pessimist’ angle. In the conclusion we have these two narratives meet ‘eye-to-eye’, facilitating a more systematized interrogation of big data promises and shortcomings in further research, paying appropriate attention to both technical-rational and political decision-making factors. We finish by offering a realist rejoinder of these two narratives, allowing for more context-specific scrutiny and balancing both technical-rational and political decision-making concerns, resulting in more realistic expectations about using big data for policymaking in practice.",2019,,https://doi.org/10.1016/j.giq.2019.05.010
1240,David Bell and Mark Lycett and Alaa Marshan and Asmat Monaghan,Exploring future challenges for big data in the humanitarian domain,"Big data, Veracity, Granularity, Heterogeneous datasets, Humanitarian, Value","This paper examines the challenges of leveraging big data in the humanitarian sector in support of UN Sustainable Development Goal 17 “Partnerships for the Goals”. The full promise of Big Data is underpinned by a tacit assumption that the heterogeneous ‘exhaust trail’ of data is contextually relevant and sufficiently granular to be mined for value. This promise, however, relies on relationality – that patterns can be derived from combining different pieces of data that are of corresponding detail or that there are effective mechanisms to resolve differences in detail. Here, we present empirical work integrating eight heterogeneous datasets from the humanitarian domain to provide evidence of the inherent challenge of complexity resulting from differing levels of data granularity. In clarifying this challenge, we explore the reasons why it is manifest, discuss strategies for addressing it and, as our principal contribution, identify five propositions to guide future research.",2021,,https://doi.org/10.1016/j.jbusres.2020.09.035
1241,Pei Wang and Ming Luo,A digital twin-based big data virtual and real fusion learning reference framework supported by industrial internet towards smart manufacturing,"Virtual and real fusion learning, Big data learning and analysis models, Digital twin, Industrial internet, Smart manufacturing","Digital twin takes Industrial Internet as a carrier deeply coordinating and integrating virtual spaces with physical spaces, which effectively promotes smart factory development. Digital twin-based big data learning and analysis (BDLA) deepens virtual and real fusion, interaction and closed-loop iterative optimization in smart factories. This paper proposes a digital twin-based big data virtual and real fusion (DT-BDVRL) reference framework supported by Industrial Internet towards smart manufacturing. The reference framework is synthetically designed from three perspectives. The first one is an overall framework of DT-BDVRL supported by Industrial Internet. The second one is the establishment method and flow of BDLA models based on digital twin. The final one is digital thread of DT-BDVRL in virtual and real fusion analysis, iteration and closed-loop feedback in product full life cycle processes. For different virtual scenes, iterative optimization and verification methods and processes of BDLA models in virtual spaces are established. Moreover, the BDLA results can drive digital twin running in virtual spaces. By this, the BDLA results can be validated iteratively multiple times in virtual spaces. At same time, the BDLA results that run in virtual spaces are synchronized and executed in physical spaces through Industrial Internet platforms, effectively improving the physical execution effect of BDLA models. Finally, the above contents were applied and verified in the actual production case study of power switchgear equipment.",2021,,https://doi.org/10.1016/j.jmsy.2020.11.012
1242,Jia Liu and Tianrui Li and Peng Xie and Shengdong Du and Fei Teng and Xin Yang,Urban big data fusion based on deep learning: An overview,"Urban computing, Big data, Data fusion, Deep learning","Urban big data fusion creates huge values for urban computing in solving urban problems. In recent years, various models and algorithms based on deep learning have been proposed to unlock the power of knowledge from urban big data. To clarify the methodologies of urban big data fusion based on deep learning (DL), this paper classifies them into three categories: DL-output-based fusion, DL-input-based fusion and DL-double-stage-based fusion. These methods use deep learning to learn feature representation from multi-source big data. Then each category of fusion methods is introduced and some examples are shown. The difficulties and ideas of dealing with urban big data will also be discussed.",2020,,https://doi.org/10.1016/j.inffus.2019.06.016
1243,David Plotkin,Chapter 10 - Big Data Stewardship and Data Lakes,"Big data, data lake, unstructured data, zone","Big Data Governance and big data stewardship are not so different from what we’ve been doing prior to the advent of big data and data lakes. Most of the same roles still need to be filled, and accountability for making data decisions is even more important because of the vast quantity of data, the many ways in which it can be changed, and increased consequences of “getting it wrong” due to not only the large quantities of data and metadata, but also the speed at which the data can change.",2021,,https://doi.org/10.1016/B978-0-12-822132-7.00010-3
1244,Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante,A multi-dimension framework for value creation through Big Data,"Big Data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation","Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.",2020,,https://doi.org/10.1016/j.indmarman.2020.03.015
1245,Ajay Major and Suzanne M. Cox and Samuel L. Volchenboum,Using big data in pediatric oncology: Current applications and future directions,"Pediatric oncology, Pediatric cancer, Big data, Data sharing, Data science, Informatics","Pediatric cancer is a rare disease with a low annual incidence, which presents a significant challenge in being able to collect enough data to fuel clinical discoveries. Big data registry trials hold promise to advance the study of pediatric cancers by allowing for the combination of traditional randomized controlled trials with the power of larger cohort sizes. The emergence of big data resources and data-sharing initiatives are becoming transformative for pediatric cancer diagnosis and treatment. This review discusses the uses of big data in pediatric cancer, existing pediatric cancer registry initiatives and research, the challenges in harmonizing these data to improve accessibility for study, and building pediatric data commons and other important future endeavors.",2020,,https://doi.org/10.1053/j.seminoncol.2020.02.006
1246,Lahcen Tamym and Lyes Benyoucef and Ahmed {Nait Sidi Moh} and Moulay Driss {El Ouadghiri},A big data based architecture for collaborative networks: Supply chains mixed-network,"Big data architecture, Collaborative networks, Enterprises network, Supply chain network, Flexibility, Robustness","Nowadays, the world knows a high-speed development and evolution of technologies, vulnerable economic environments, market changes, and personalised consumer trends. The issue and challenge related to enterprises networks design are more and more critical. These networks are often designed for short terms since their strategies must be competitive and better adapted to the environment, social and economical changes. As a solution, to design a flexible and robust network, it is necessary to deal with the trade-off between conflicting qualitative and quantitative criteria such as cost, quality, delivery time, and competition, etc. To this end, using Big Data (BD) as emerging technology will enhance the real performances of these kinds of networks. Moreover, even if the literature is rich with BD models and frameworks developed for a single supply chain network (SCN), there is a real need to scale and extend these BD models to networked supply chains (NSCs). To do so, this paper proposes a BD architecture to drive a mixed-network of SCs that collaborate in serial and parallel fashions. The collaboration is set up by sharing their resources, capabilities, competencies, and information to imitate a unique organisation. The objective is to increase internal value to their shareholders (where value is seen as wealth) and deliver better external value to the end-customer (where value represents customer satisfaction). Within a mixed-network of SCs, both values are formally calculated considering both serial and parallel networks configurations. Besides, some performance factors of the proposed BD architecture such as security, flexibility, robustness and resilience are discussed.",2021,,https://doi.org/10.1016/j.comcom.2021.05.008
1247,Lihong Zhao,Prediction model of ecological environmental water demand based on big data analysis,"Big data analysis, Ecological environment, Water demand, Prediction","The existing prediction model of eco-environmental water demand has the problem of large prediction error. In order to solve the above problems, the prediction model of eco-environmental water demand is constructed based on big data analysis. In order to reduce the prediction error of the ecological environment water demand prediction model, the framework of the ecological environment water demand prediction model is built. On this basis, the principal component analysis method is used to select the auxiliary variables of the model. Based on the selected auxiliary variables, the minimum monthly average flow method is used to analyze the basic water demand of the ecological environment, the leakage water demand and the water surface evaporation ecological environment water demand, so as to analyze based on the results, the water demand of ecological environment is predicted by big data analysis technology, and the prediction of water demand of ecological environment is realized. The experimental results show that compared with the existing ecological environment water demand prediction model, the prediction error of the model is within 19.3, which fully shows that the constructed ecological environment water demand prediction model has better prediction effect and can provide a certain reference value for the actual use of water resources.",2021,,https://doi.org/10.1016/j.eti.2020.101196
1248,Álvaro Valencia-Parra and Ángel Jesús Varela-Vaca and Luisa Parody and María Teresa Gómez-López,Unleashing Constraint Optimisation Problem solving in Big Data environments,"Big Data, Optimisation problem, Constraint programming, Distributed data, Heterogeneous data format","The application of the optimisation problems in the daily decisions of companies is able to be used for finding the best management according to the necessities of the organisations. However, optimisation problems imply a high computational complexity, increased by the current necessity to include a massive quantity of data (Big Data), for the creation of optimisation problems to customise products and services for their clients. The irruption of Big Data technologies can be a challenge but also an important mechanism to tackle the computational difficulties of optimisation problems, and the possibility to distribute the problem performance. In this paper, we propose a solution that lets the query of a data set supported by Big Data technologies that imply the resolution of Constraint Optimisation Problem (COP). This proposal enables to: (1) model COPs whose input data are obtained from distributed and heterogeneous data; (2) facilitate the integration of different data sources to create the COPs; and, (3) solve the optimisation problems in a distributed way, to improve the performance. It is done by means of a framework and supported by a tool capable of modelling, solving and querying the results of optimisation problems. The tool integrates the Big Data technologies and commercial solvers of constraint programming. The suitability of the proposal and the development have been evaluated with real data sets whose computational study and results are included and discussed.",2020,,https://doi.org/10.1016/j.jocs.2020.101180
1249,Wang Kun and Liu Tong and Xie Xiaodan,Application of Big Data Technology in Scientific Research Data Management of Military Enterprises,"big data technology, scientific research data, data analysis, decision","Scientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology started late in the military enterprises, while military enterprises research data often has the characteristics of decentralization, low relevance, and diverse data types. It cannot fully utilize the advantages of data resources to enhance the core competitiveness of enterprises. To this end, this paper deeply explores the application methods of big data technology in military scientific research data management, and lays a foundation for the construction of scientific research big data platform.",2019,,https://doi.org/10.1016/j.procs.2019.01.221
1250,José Carpio-Pinedo and Javier Gutiérrez,Consumption and symbolic capital in the metropolitan space: Integrating ‘old’ retail data sources with social big data,"Commercial space, Retail, Retail geography, Symbolic capital, Big data, Foursquare, Madrid","While commerce is one of the key activities in cities, its spatial description still requires further attention, especially by considering the different dimensions of commercial space: physical, economic and socio-symbolic. The latter is becoming more and more important in an era where consumption is at the centre of social relations. Further, although data availability has been an enduring obstacle in commercial research, we are witnessing the advent of new data sources, and social-network big data is an opportunity to unveil the places to which consumers attribute prestige or symbolic capital, at the extent of entire metropolitan areas. This paper compares the physical, economic and socio-symbolic dimensions of commercial spaces through the analysis of three different commercial data sources: cadastral micro-data, business register and social-network big data. For the case of Madrid Metropolitan Area, the three databases are compared with correlation analysis and density maps, coming out as partly redundant and partly complementary. Getis-Ord's hotspot statistics integrated into a cluster analysis enable a comprehensive understanding of commercial environments, enriching previous spatial hierarchies. The spatial distribution of symbolic capital unveils a relation with socio-spatial segregation and paves the way to new reflections on the spatiality of consumption as a social practice.",2020,,https://doi.org/10.1016/j.cities.2020.102859
1251,Juan Trujillo and Karen C. Davis and Xiaoyong Du and Ernesto Damiani and Veda C. Storey,Conceptual modeling in the era of Big Data and Artificial Intelligence: Research topics and introduction to the special issue,"Conceptual modeling, Big Data, Machine learning, Artificial Intelligence","Since the first version of the Entity–Relationship (ER) model proposed by Peter Chen over forty years ago, both the ER model and conceptual modeling activities have been key success factors for modeling computer-based systems. During the last decade, conceptual modeling has been recognized as an important research topic in academia, as well as a necessity for practitioners. However, there are many research challenges for conceptual modeling in contemporary applications such as Big Data, data-intensive applications, decision support systems, e-health applications, and ontologies. In addition, there remain challenges related to the traditional efforts associated with methodologies, tools, and theory development. Recently, novel research is uniting contributions from both the conceptual modeling area and the Artificial Intelligence discipline in two directions. The first one is efforts related to how conceptual modeling can aid in the design of Artificial Intelligence (AI) and Machine Learning (ML) algorithms. The second one is how Artificial Intelligence and Machine Learning can be applied in model-based solutions, such as model-based engineering, to infer and improve the generated models. For the first time in the history of Conceptual Modeling (ER) conferences, we encouraged the submission of papers based on AI and ML solutions in an attempt to highlight research from both communities. In this paper, we present some of important topics in current research in conceptual modeling. We introduce the selected best papers from the 37th International Conference on Conceptual Modeling (ER’18) held in Xi’an, China and summarize some of the valuable contributions made based on the discussions of these papers. We conclude with suggestions for continued research.",2021,,https://doi.org/10.1016/j.datak.2021.101911
1252,A Miracolo and M Mills and P Kanavos,POSB319 Predictive Analytic Techniques and Big Data for Improved Health Outcomes in the Context of Value Based Health Care and Coverage Decisions: A Scoping Review,,,2022,,https://doi.org/10.1016/j.jval.2021.11.1002
1253,Yongqiang Lv and Lin Zhou and Guobiao Yao and Xinqi Zheng,Detecting the true urban polycentric pattern of Chinese cities in morphological dimensions: A multiscale analysis based on geospatial big data,"Polycentricity, Urban centers, Multi-scale, Street blocks, Geospatial big data, Chinese cities","With current decentralization trends and polycentric planning efforts, the urban spatial structures of Chinese cities have been changing tremendously. To detect the true urban polycentric pattern of Chinese cities, this article analyzed the urban polycentricity characteristics of 294 cities. The natural cities were delineated by points of interest (POIs), and road networks constituted street blocks. Based on check-in data and new spatial units, centers within both metropolitan areas and central cities were identified and examined. We discovered that all Chinese cities have at least one natural city in their metropolitan areas because of rapid urban sprawl. Although a monocentric structure is still the most common urban spatial structure, 110 Chinese cities displayed different degrees of polycentricity at the metropolitan level. Many natural cities beyond central cities contribute to polycentric development at the metropolitan level. Central cities have maintained their original vitality and importance, most Chinese cities have dispersed urban structures in central cities, and 45 central cities are polycentric. The spatial structures in metropolitan areas are more polycentric than those in central cities. The only 36 cities with polycentric urban structures at both the metropolitan and central city levels are all national or regional central cities in eastern China.",2021,,https://doi.org/10.1016/j.cities.2021.103298
1254,Ines Sbai and Saoussen Krichen,A real-time Decision Support System for Big Data Analytic: A case of Dynamic Vehicle Routing Problems,"Big data analytic, Decision Support System, DVRP, S-GA, Spark","Recently, the explosion of large amounts of traffic data has guided data scientists to create models with big data for a better decision-making. Big Data applications process and analyze this huge amounts of data (collected from a variety of heterogeneous data sources) that cannot be processed with traditional technologies. In this paper, Big Data frameworks are used for solving an optimization problem known as Dynamic Vehicle Routing Problem (DVRP). Hence, due to the NP-Hardness of the problem and to deal with a large size of data, we develop a parallel Spark Genetic Algorithm named (S-GA). This parallelism aims to take the advantage of Spark’s in-memory computing ability (as a master-slave distribution computing) and GA’s iterations operations. Parallel operations were used for fitness evaluation and genetic operations. Based on the parallel S-GA a decision support system is developed for the DVRP in order to generate the best routes. The experiments show that our proposed architecture is improved due to its capacity when coping with Big Data optimization problems by interconnecting components and deploying on different nodes of a cluster.",2020,,https://doi.org/10.1016/j.procs.2020.09.089
1255,Agnieszka Smalec,Big Data as a tool helpful in communication management,"Big Data, marketing communication, management, data processing, collection, communication management","The boundaries between the online and offline worlds have become irretrievably blurred, especially as mobile devices have proliferated. As a result, more and more activities are transferred to the Internet. Every activity in the network leaves a trace, which is why the volume of available data is growing rapidly. The amount of increasing information affects all market participants, and the necessity to constantly collect and process large amounts of data becomes an everyday reality. The aim of the article is to present the concept of big data and to indicate examples of the use of big data to manage marketing communication with the environment. It should be emphasized that not only data transfer devices, but also human interaction contribute to the creation of very large data sets. Acquiring and correctly interpreting them plays an important role in market entities in terms of management, including communication management. Contemporary multi-directional communication, including communication in a hypermedia environment, creates new challenges and threats. The article was prepared based on a literature review, research reports and an analysis of secondary sources. It also outlines the practical implications. The considerations provided are the basis for further activities and empirical research.",2021,,https://doi.org/10.1016/j.procs.2021.09.293
1256,Mohamed Saifeddine {Hadj Sassi} and Faiza Ghozzi Jedidi and Lamia Chaari Fourati,A New Architecture for Cognitive Internet of Things and Big Data,"Internet of Things, Big-Data, Architecture, Cognitive, Data-flow","Big data and the Internet of Things (IoT) are considered as the main paradigms when defining new information architecture projects. Accordingly, technologies that make up these solutions could have an important role to play in business information architecture. Solutions that have approached big data and the IoT as unique technology initiatives, struggle in finding value in such efforts and in the technology itself. A connection to the requirements (volume, velocity, and variety) is mandatory to reach the potential business goals. In this context, we propose a new architecture for Cognitive Internet of Things (CIoT) and big data. The proposed architecture benefits computing mechanisms by combining the data WareHouse (DWH) and Data Lake (DL), and defining a tool for heterogeneous data collection.",2019,,https://doi.org/10.1016/j.procs.2019.09.208
1257,Hourieh Khalajzadeh and Andrew J. Simmons and Mohamed Abdelrazek and John Grundy and John Hosking and Qiang He,An end-to-end model-based approach to support big data analytics development,"Big data analytics, Big data modeling, Big data toolkits, Domain-specific visual languages, Multidisciplinary teams, End-user tools","We present BiDaML 2.0, an integrated suite of visual languages and supporting tool to help multidisciplinary teams with the design of big data analytics solutions. BiDaML tool support provides a platform for efficiently producing BiDaML diagrams and facilitating their design, creation, report and code generation. We evaluated BiDaML using two types of evaluations, a theoretical analysis using the “physics of notations”, and an empirical study with 1) a group of 12 target end-users and 2) five individual end-users. Participants mostly agreed that BiDaML was straightforward to understand/learn, and prefer BiDaML for supporting complex data analytics solution modeling than other modeling languages.",2020,,https://doi.org/10.1016/j.cola.2020.100964
1258,Xin Yin and Quansheng Liu and Xing Huang and Yucong Pan,Perception model of surrounding rock geological conditions based on TBM operational big data and combined unsupervised-supervised learning,"TBM, Surrounding rock class, Perception model, Unsupervised learning, Supervised learning","The perception of surrounding rock geological conditions ahead the tunnel face is essential for TBM safe and efficient tunnelling. This paper developed a perception approach of surrounding rock class based on TBM operational big data and combined unsupervised-supervised learning. In data preprocessing, four data mining techniques (i.e., Z-score, K-NN, Kalman filtering, and wavelet packet decomposition) were used to detect outliers, substitute outliers, suppress noise, and extract features, respectively. Then, GMM was used to revise the original surrounding rock class through clustering TBM load parameters and performance parameters in view of the shortcomings of the HC method in the TBM-excavated tunnel. After that, five various ensemble learning classification models were constructed to identify the surrounding rock class, in which model hyper-parameters were automatically tuned by Bayes optimization. In order to evaluate model performance, balanced accuracy, Kappa, F1-score, and training time were taken into account, and a novel multi-metric comprehensive ranking system was designed. Engineering application results indicated that LightGBM achieved the most superior performance with the highest comprehensive score of 6.9066, followed by GBDT (5.9228), XGBoost (5.4964), RF (3.7581), and AdaBoost (0.9946). Through the weighted purity reduction algorithm, the contributions of input features on the five models were quantitatively analyzed. Finally, the impact of class imbalance on model performance was discussed using the ADASYN algorithm, showing that eliminating class imbalance can further improve the model's perception ability.",2022,,https://doi.org/10.1016/j.tust.2021.104285
1259,Linlin Zhao and Heather L. Ciallella and Lauren M. Aleksunes and Hao Zhu,Advancing computer-aided drug discovery (CADD) by big data and data-driven machine learning modeling,,"Advancing a new drug to market requires substantial investments in time as well as financial resources. Crucial bioactivities for drug candidates, including their efficacy, pharmacokinetics (PK), and adverse effects, need to be investigated during drug development. With advancements in chemical synthesis and biological screening technologies over the past decade, a large amount of biological data points for millions of small molecules have been generated and are stored in various databases. These accumulated data, combined with new machine learning (ML) approaches, such as deep learning, have shown great potential to provide insights into relevant chemical structures to predict in vitro, in vivo, and clinical outcomes, thereby advancing drug discovery and development in the big data era.",2020,,https://doi.org/10.1016/j.drudis.2020.07.005
1260,Paula {de Camargo Fiorini} and Bruno Michel {Roman Pais Seles} and Charbel Jose {Chiappetta Jabbour} and Enzo {Barberio Mariano} and Ana Beatriz Lopes {de Sousa Jabbour},Management theory and big data literature: From a review to a research agenda,"Big data, Big data analytics, Organizational theory, Firms’ performance, Research agenda","The purpose of this study is to enrich the existing state-of-the-art literature on the impact of big data on business growth by examining how dozens of organizational theories can be applied to enhance the understanding of the effects of big data on organizational performance. While the majority of management disciplines have had research dedicated to the conceptual discussion of how to link a variety of organizational theories to empirically quantified research topics, the body of research into big data so far lacks an academic work capable of systematising the organizational theories supporting big data domain. The three main contributions of this work are: (a) it addresses the application of dozens of organizational theories to big data research; (b) it offers a research agenda on how to link organizational theories to empirical research in big data; and (c) it foresees promising linkages between organizational theories and the effects of big data on organizational performance, with the aim of contributing to further research in this field. This work concludes by presenting implications for researchers and managers, and by highlighting intrinsic limitations of the research.",2018,,https://doi.org/10.1016/j.ijinfomgt.2018.07.005
1261,Hanen Balti and Ali {Ben Abbes} and Nedra Mellouli and Imed Riadh Farah and Yanfang Sang and Myriam Lamolle,"A review of drought monitoring with big data: Issues, methods, challenges and research directions","Drought monitoring, Artificial intelligence, Big data, Machine learning, Statistical approach, Remote sensing","Over recent years, the frequency and intensity of droughts have increased and there has been a large drying trend over many parts of the world. Consequently, drought monitoring using big data analytic has gained an explosive interest. Droughts stand among the most damaging natural disasters. It threatens agricultural production, ecological environment, and socio-economic development. For this reason, early warning, accurate evaluation, and efficient prediction are an emergency especially for the nations that are the most menaced by this danger. There are numerous emerging studies addressing big data and its applications in drought monitoring. In fact, big data handle data heterogeneity which is an additive value for the prediction of drought, it offers a view of the different dimensions such as the spatial distribution, the temporal distribution and the severity detection of this phenomenon. Big data analytic and drought are introduced and reviewed in this paper. Besides, this review includes different studies, researches and applications of big data to drought monitoring. Challenges related to data life cycle such as data challenges, data processing challenges and data infrastructure management challenges are also discussed. Finally, we conclude that big data analytic can be beneficial in drought monitoring but there is a need for statistical and artificial intelligence-based approaches.",2020,,https://doi.org/10.1016/j.ecoinf.2020.101136
1262,Denglong Lv and Shibing Zhu,Achieving secure big data collection based on trust evaluation and true data discovery,"Big data collection, Trust evaluation, Trust model, True data discovery, Wireless sensor network","Data collection is an important process in the life cycle of big data processing. It is the key part that must be completed first in all kinds of data applications, which determines the results of data analysis and application service quality. However, untrusted data sources and transmission links expose the data collection process to attacks and malicious threats such as counterfeiting, replay, and denial of service, and ultimately lead to untrustworthy data. In order to cope with the threat of data collection process and ensure data quality, this paper proposes trust evaluation scheme for data security collection based on wireless sensor network, one of the data collection applications, including direct trust, recommendation trust, link trust, and backhaul trust. Meanwhile, in order to realize the dynamic update of the trust of the data sources, a true data discovery and trust dynamic update mechanism based on ω-FCM (Weight Fuzzy C-Mean) algorithm is proposed. The results of a large number of simulation experiments show that the proposed scheme, model and algorithm can effectively evaluate the trust of data sources and ensure the authenticity of the collected data.",2020,,https://doi.org/10.1016/j.cose.2020.101937
1263,Karim Moharm,State of the art in big data applications in microgrid: A review,"Big data, Microgrid","The prospering Big data era is emerging in the power grid. Multiple world-wide studies are emphasizing the big data applications in the microgrid due to the huge amount of produced data. Big data analytics can impact the design and applications towards safer, better, more profitable, and effective power grid. This paper presents the recognition and challenges of the big data and the microgrid. The construction of big data analytics is introduced. The data sources, big data opportunities, and enhancement areas in the microgrid like stability improvement, asset management, renewable energy prediction, and decision-making support are summarized. Diverse case studies are presented including different planning, operation control, decision making, load forecasting, data attacks detection, and maintenance aspects of the microgrid. Finally, the open challenges of big data in the microgrid are discussed.",2019,,https://doi.org/10.1016/j.aei.2019.100945
1264,Ellie Paige and Kerry Doyle and Louisa Jorm and Emily Banks and Meng-Ping Hsu and Lee Nedkoff and Tom Briffa and Dominique A. Cadilhac and Ray Mahoney and Johan W. Verjans and Girish Dwivedi and Michael Inouye and Gemma A. Figtree,A Versatile Big Data Health System for Australia: Driving Improvements in Cardiovascular Health,"Big data, Datasets, Cardiovascular disease, National platform","Cardiovascular diseases (CVD) are leading causes of death and morbidity in Australia and worldwide. Despite improvements in treatment, there remain large gaps in our understanding to prevent, treat and manage CVD events and associated morbidities. This article lays out a vision for enhancing CVD research in Australia through the development of a Big Data system, bringing together the multitude of rich administrative and health datasets available. The article describes the different types of Big Data available for CVD research in Australia and presents an overview of the potential benefits of a Big Data system for CVD research and some of the major challenges in establishing the system for Australia. The steps for progressing this vision are outlined.",2021,,https://doi.org/10.1016/j.hlc.2021.04.023
1265,Jeffrey Hughes and Kirstie Ball,Sowing the seeds of value? Persuasive practices and the embedding of big data analytics,"Big data analytics, Persuasion, Practice, Capabilities, Value","This paper draws on data from three organisational case studies and expert interviews to propose that persuasive practices are the precursors and enablers of analytical capability development. A bundle of seven practices was identified and observed to bridge multiple gaps between technical and non-technical colleagues on big data analytics (BDA) projects. The deployment of these practices varied according to the level of BDA maturity and featured a host of socio-material elements. This paper complements existing technical case studies with a fine-grained qualitative account of the managerial and human elements of BDA implementation. Effective deployment of persuasive practices potentially both embeds the benefits and mitigates the risks of BDA, sowing the seeds of many different forms of value.",2020,,https://doi.org/10.1016/j.techfore.2020.120300
1266,Harika Vanam and Jeberson {Retna Raj R},Analysis of twitter data through big data based sentiment analysis approaches,"Sentiment analysis, Twitter, Unstructured data analysis, Big data analytics, Machine learning algorithm","The common and various forms of Twitter information render this one of the best controlling and recording virtual environments of information. The growth in social media nowadays gives internet users immense interest. In several pups like prediction, advertisement, sentiment analysis …, the data on such a social network platform is used. People exchange good or bad views on problems, items and administrations through the web and informal communities. The capacity to assess such a data productively is presently observed as a noteworthy upper hand in settling on choices all the more proficiently. In this sense, associations use methods, for example, Sentiment Analysis (SA). The utilization of web based life around the globe is growing, however, greatly speeding up mass data generation and stopping us from providing useful insights in conventional SA systems. These data volumes can be processed effectively, using SA and Big Data technology. Big data is not a luxury, in fact, but an important prediction.",2021,,https://doi.org/10.1016/j.matpr.2020.11.486
1267,Yuan Su and Yanni Yu and Ning Zhang,Carbon emissions and environmental management based on Big Data and Streaming Data: A bibliometric analysis,"Big data, Streaming data, Carbon emission, Environmental management, Bibliometric analysis, Net-work analysis","Climate change and environmental management are issues of global concern. The advent of the era of Big Data has created a new research platform for the assessment of environmental governance and policies. However, little is known about Big Data application to climate change and environmental management research. This paper adopts bibliometric analysis in conjunction with network analysis to systematically evaluate the publications on carbon emissions and environmental management based on Big Data and Streaming Data using R package and VOSviewer software. The analysis involves 274 articles after rigorous screening and includes citation analysis, co-citation analysis, and co-word analysis. Main findings include (1) Carbon emissions and environmental management based on big data and streaming data is an emerging multidisciplinary research topic, which has been applied in the fields of computer science, supply chain design, transportation, carbon price assessment, environmental policy evaluation, and CO2 emissions reduction. (2) This field has attracted the attention of nations which are major contributors to the world economy. In particular, European and American scholars have made the main contributions to this topic, and Chinese researchers have also had great impact. (3) The research content of this topic is primarily divided into four categories, including empirical studies of specific industries, air pollution governance, technological innovation, and low-carbon transportation. Our findings suggest that future research should bring greater depth of practical and modeling analysis to environmental policy assessment based on Big Data.",2020,,https://doi.org/10.1016/j.scitotenv.2020.138984
1268,Nick Hajli and Mina Tajvidi and Ayantunji Gbadamosi and Waqar Nadeem,Understanding market agility for new product success with big data analytics,"Big data analytics, Customer agility, Effective use of data, New product success","The complexity that characterises the dynamic nature of the various environmental factors makes it very compelling for firms to be capable of addressing the changing customers' needs. The current study examines the role of big data in new product success. We develop a qualitative research with case study approach to look at this. Specifically, we look at multiple cases to get in-depth understanding of customer agility for new product success with big data analytics. The findings of the study provide insight into the role of customer agility in new product success. This study unpacks the interconnectedness of the effective use of data aggregation tools, the effectiveness of data analysis tools and customer agility. It also explores the link between all of these factors and new product success. The study is reasonably telling in that it shows that the effective use of data aggregation and data analysis tools results in customer agility which in itself explains how an organisation senses and responds speedily to opportunities for innovation in the competitive marketing environment. The current study provides significant theoretical contributions by providing evidence for the role of big data analytics, big data aggregation tools, customer agility, organisational slack and environmental turbulence in new product success.",2020,,https://doi.org/10.1016/j.indmarman.2019.09.010
1269,Alex Neilson and  Indratmo and Ben Daniel and Stevanus Tjandra,Systematic Review of the Literature on Big Data in the Transportation Domain: Concepts and Applications,"Big Data, Smart city, Intelligent transportation system, Connected vehicle, Road traffic safety, Vision Zero","Research in Big Data and analytics offers tremendous opportunities to utilize evidence in making decisions in many application domains. To what extent can the paradigms of Big Data and analytics be used in the domain of transport? This article reports on an outcome of a systematic review of published articles in the last five years that discuss Big Data concepts and applications in the transportation domain. The goal is to explore and understand the current research, opportunities, and challenges relating to the utilization of Big Data and analytics in transportation. The review shows the potential of Big Data and analytics to garner insights and improve transportation systems through the analysis of various forms of data obtained from traffic monitoring systems, connected vehicles, crowdsourcing, and social media. We discuss some platforms and software architecture for the transport domain, along with a wide array of storage, processing, and analytical techniques, and describe challenges associated with the implementation of Big Data and analytics. This review contributes broadly to the various ways in which cities can utilize Big Data in transportation to guide the creation of sustainable and safer traffic systems. Since research in Big Data and transportation is, by and large, at infancy, this article does not prescribe recommendations to the various challenges identified, which also constitutes the limitation of the article.",2019,,https://doi.org/10.1016/j.bdr.2019.03.001
1270,Elaheh Yadegaridehkordi and Mehdi Hourmand and Mehrbakhsh Nilashi and Liyana Shuib and Ali Ahani and Othman Ibrahim,Influence of big data adoption on manufacturing companies' performance: An integrated DEMATEL-ANFIS approach,"Big data, Firm performance, Manufacturing companies, DEMATEL, ANFIS","Big Data is one of the recent technological advances with the strong applicability in almost every industry, including manufacturing. However, despite business opportunities offered by this technology, its adoption is still in early stage in many industries. Thus, this study aimed to identify and rank the significant factors influencing adoption of big data and in turn to predict the influence of big data adoption on manufacturing companies' performance using a hybrid approach of decision-making trial and evaluation laboratory (DEMATEL)- adaptive neuro-fuzzy inference systems (ANFIS). This study identified the critical adoption factors from literature review and categorized them into technological, organizational and environmental dimensions. Data was collected from 234 industrial managers who were involved in the decision-making process regarding IT procurement in Malaysian manufacturing companies. Research results showed that technological factors (perceived benefits, complexity, technology resources, big data quality and integration) have the highest influence on the big data adoption and firms' performance. This study is one of the pioneers in using DEMATEL-ANFIS approach in the big data adoption context. In addition to the academic contribution, findings of this study can hopefully assist manufacturing industries, big data service providers, and governments to precisely focus on vital factors found in this study in order to improve firm performance by adopting big data.",2018,,https://doi.org/10.1016/j.techfore.2018.07.043
1271,Chiaojung Jillian Tsai and Nadeem Riaz and Scarlett Lin Gomez,Big Data in Cancer Research: Real-World Resources for Precision Oncology to Improve Cancer Care Delivery,,"In oncology, the term “big data” broadly describes the rapid acquisition and generation of massive amounts of information, typically from population cancer registries, electronic health records, or large-scale genetic sequencing studies. The challenge of using big data in cancer research lies in interdisciplinary collaboration and information processing to unify diverse data sources and provide valid analytics to harness meaningful information. This article provides an overview of how big data approaches can be applied in cancer research, and how they can be used to translate information into new ways to ultimately make informed decisions that improve cancer care and delivery.",2019,,https://doi.org/10.1016/j.semradonc.2019.05.002
1272,Zikun Feng and Yan Li and Zhao Liu and Ryan Wen Liu,Chapter 10 - Spatiotemporal Big Data-Driven Vessel Traffic Risk Estimation for Promoting Maritime Healthcare: Lessons Learnt from Another Domain than Healthcare,"Data mining, maritime healthcare, maritime risk estimation, automatic identification system (AIS), maritime safety, artificial intelligence","With the rapid development of maritime industries, the vessel traffic density has been gradually increased leading to increasing the potential risk of ship collision accidents in crowded inland waterways. It will bring negative effects on human life safety and global maritime economy. Therefore it is of vital significance to study the risk of ship collision using big data mining techniques. The big data–driven computational results are beneficial for guaranteeing smart maritime healthcare in the fields of ocean engineering and maritime management. This chapter proposes to quantitatively estimate the ship collision risk based on ship domain modeling and real-time vessel trajectory data. In particular, the trajectory data quality is first improved using the cubic spline interpolation method. We assume that the ship collision risk is highly related to the cross areas of ship domains between different ships, which are then computed using the Monte Carlo simulation strategy. For the sake of better understanding, the kernel density estimation method is finally adopted to visually generate the ship collision risk in maps. Experimental results on realistic spatiotemporal big data have illustrated the effectiveness of the proposed method in crowded inland waterways.",2021,,https://doi.org/10.1016/B978-0-12-822060-3.00006-1
1273,Safa Ben Atitallah and Maha Driss and Wadii Boulila and Henda Ben Ghézala,Leveraging Deep Learning and IoT big data analytics to support the smart cities development: Review and future directions,"Internet of Things, Deep Learning, Smart city, Big data analytics, Review","The rapid growth of urban populations worldwide imposes new challenges on citizens’ daily lives, including environmental pollution, public security, road congestion, etc. New technologies have been developed to manage this rapid growth by developing smarter cities. Integrating the Internet of Things (IoT) in citizens’ lives enables the innovation of new intelligent services and applications that serve sectors around the city, including healthcare, surveillance, agriculture, etc. IoT devices and sensors generate large amounts of data that can be analyzed to gain valuable information and insights that help to enhance citizens’ quality of life. Deep Learning (DL), a new area of Artificial Intelligence (AI), has recently demonstrated the potential for increasing the efficiency and performance of IoT big data analytics. In this survey, we provide a review of the literature regarding the use of IoT and DL to develop smart cities. We begin by defining the IoT and listing the characteristics of IoT-generated big data. Then, we present the different computing infrastructures used for IoT big data analytics, which include cloud, fog, and edge computing. After that, we survey popular DL models and review the recent research that employs both IoT and DL to develop smart applications and services for smart cities. Finally, we outline the current challenges and issues faced during the development of smart city services.",2020,,https://doi.org/10.1016/j.cosrev.2020.100303
1274,Lidong Wang and Cheryl Alexander,Chapter 2 - Big data in personalized healthcare,"Big data, Precision healthcare, Personalized healthcare, Big Data analytics, Telepsychiatry, Deep learning","Big data technologies enable correlation of multiple data sources into a coherent view. Big data and Big Data analytics have been used in public health, electronic consultation (e-consultation), real-time telediagnosis, precision healthcare, and personalized healthcare. e-Consultation is one aspect of telemedicine related to remote communication between medical specialists and clinicians, or clinicians and patients. It is generally implemented via the Internet or mobile communication devices (e.g., smartphone) and often generates big data. The concepts, characteristics, methods, emerging technologies, and software platforms or tools of big data and Big Data analytics are introduced in this chapter. Big data and applications in general healthcare are presented. Specifically, big data in precision healthcare and personalized healthcare are introduced. Challenges of big data and Big Data analytics in personalized healthcare are also outlined.",2021,,https://doi.org/10.1016/B978-0-12-822884-5.00017-9
1275,Surajit Bag and Jan Ham Christiaan Pretorius and Shivam Gupta and Yogesh K. Dwivedi,"Role of institutional pressures and resources in the adoption of big data analytics powered artificial intelligence, sustainable manufacturing practices and circular economy capabilities","Big data, Artificial intelligence, Industry 4.0, Circular economy, Sustainable manufacturing","ABSTRACT
The significance of big data analytics-powered artificial intelligence has grown in recent years. The literature indicates that big data analytics-powered artificial intelligence has the ability to enhance supply chain performance, but there is limited research concerning the reasons for which firms engaging in manufacturing activities adopt big data analytics-powered artificial intelligence. To address this gap, our study employs institutional theory and resource-based view theory to elucidate the way in which automotive firms configure tangible resources and workforce skills to drive technological enablement and improve sustainable manufacturing practices and furthermore develop circular economy capabilities. We tested the research hypothesis using primary data collected from 219 automotive and allied manufacturing companies operating in South Africa. The contribution of this work lies in the statistical validation of the theoretical framework, which provides insight regarding the role of institutional pressures on resources and their effects on the adoption of big data analytics-powered artificial intelligence, and how this affects sustainable manufacturing and circular economy capabilities under the moderating effects of organizational flexibility and industry dynamism.",2021,,https://doi.org/10.1016/j.techfore.2020.120420
1276,Uma Narayanan and Varghese Paul and Shelbi Joseph,A novel system architecture for secure authentication and data sharing in cloud enabled Big Data Environment,"Big data outsourcing, Big data sharing, Big data management, SALSA encryption with MapReduce, Fractal index tree, SHA-3","With the rapid growth of data sources, Big data security in Cloud is a big challenge. Different issues have ascended in the area of Big data security such as infrastructure security, data privacy, data management and data integrity. Currently, Big data processing, analytics and storage is secured using cryptography algorithms, which are not appropriate for Big data protection over Cloud. In this paper, we present a solution for addressing the main issues in Big data security over Cloud. We propose a novel system architecture called the Secure Authentication and Data Sharing in Cloud (SADS-Cloud). There are three processes involved in this paper including (i). Big Data Outsourcing, (ii). Big Data Sharing and (iii). Big Data Management. In Big data outsourcing, the data owners are registered to a Trust Center using SHA-3 hashing algorithm. The MapReduce model is used to split the input file into fixed-size of blocks of data and SALSA20 encryption algorithm is applied over each block. In Big data sharing, data users participate in a secure file retrieval. For this purpose, user's credentials (ID, password, secure ID, and current timestamp, email id) are hashed and compared with that stored in a database. In Big data management, there are three important processes implemented to organize data. They are as follows: Compression using Lemperl Ziv Markow Algorithm (LZMA), Clustering using Density-based Clustering of Applications with Noise (DBSCAN), and Indexing using Fractal Index Tree. The proposed scheme for these processes are implemented using Java Programming and performance tested for the following metrics: Information Loss, Compression Ratio, Throughput, Encryption Time and Decryption Time.",2020,,https://doi.org/10.1016/j.jksuci.2020.05.005
1277,Ruiyang Li and Yahan Yang and Haotian Lin,The critical need to establish standards for data quality in intelligent medicine,"Artificial intelligence, Intelligent medicine, Big data, Data collection, Data storage, Data management","Medical artificial intelligence (AI) is an important technical asset to support medical supply-side reforms and national development in the big data era. Clinical data from multiple disciplines represent building blocks for the development and application of AI-aided diagnostic and treatment systems based on medical big data. However, the inconsistent quality of these data resources in AI research leads to waste and inefficiencies. Therefore, it is crucial that the field formulates the requirements and content related to data processing as part of the development of intelligent medicine. To promote medical AI research worldwide, the “Belt and Road” International Ophthalmic Artificial Intelligence Research and Development Alliance will establish a series of expert recommendations for data quality in intelligent medicine.",2021,,https://doi.org/10.1016/j.imed.2021.04.004
1278,Saqib Shamim and Jing Zeng and Syed Muhammad Shariq and Zaheer Khan,Role of big data management in enhancing big data decision-making capability and quality among Chinese firms: A dynamic capabilities view,"Big data management, Dynamic capabilities, Big data decision-making capability, Decision-making quality, China","This study examines the antecedents and influence of big data decision-making capabilities on decision-making quality among Chinese firms. We propose that such capabilities are influenced by big data management challenges such as leadership, talent management, technology, and organisational culture. By using primary data from 108 Chinese firms and utilising partial least squares, we tested the antecedents of big data decision-making capability and its impact on decision-making quality. Findings suggest that big data management challenges are the key antecedents of big data decision-making capability. Furthermore, the latter is vital for big data decision-making quality.",2019,,https://doi.org/10.1016/j.im.2018.12.003
1279,Margaret Cychosz and Alejandrina Cristia,Chapter One - Using big data from long-form recordings to study development and optimize societal impact,"Big data, Wearable technology, Algorithm bias, Automatic measurement, Language, Audio recording, Children","Big data are everywhere. In this chapter, we focus on one source: long-form, child-centered recordings collected using wearable technologies. Because these recordings are simultaneously unobtrusive and encompassing, they may be a breakthrough technology for clinicians and researchers from several diverse fields. We demonstrate this possibility by outlining three applications for the recordings—clinical treatment, large-scale interventions, and language documentation—where we see the greatest potential. We argue that incorporating these recordings into basic and applied research will result in more equitable treatment of patients, more reliable measurements of the effects of interventions on real-world behavior, and deeper scientific insights with less observational bias. We conclude by outlining a proposal for a semistructured online platform where vast numbers of long-form recordings could be hosted and more representative, less biased algorithms could be trained.",2022,,https://doi.org/10.1016/bs.acdb.2021.12.001
1280,Tiago F. Silva and Cleber L. Rodrigues and Manfredo H. Tabacniks and Hugo D.C. Pereira and Thiago B. Saramela and Renato O. Guimarães,Ion beam analysis and big data: How data science can support next-generation instrumentation,"Ion beam analysis, Big data, Data quality assurance, Artificial intelligence","With a growing demand for accurate ion beam analysis on a large number of samples, it becomes an issue of how to ensure the quality standards and consistency over hundreds or thousands of samples. In this sense, a virtual assistant that checks the data quality, emitting certificates of quality, is highly desired. Even the processing of a massive number of spectra is a problem regarding the consistency of the analysis. In this work, we report the design and first results of a virtual layer under implementation in our laboratory. It consists of a series of systems running in the cloud that perform the mentioned tasks and serves as a virtual assistant for member staff and users. We aim to bring the concept of the Internet of Things and artificial intelligence closer to the laboratory to support a new generation of instrumentation.",2020,,https://doi.org/10.1016/j.nimb.2020.05.027
1281,Canchu Lin and Anand Kunnathur,"Strategic orientations, developmental culture, and big data capability","Big data capability, Customer orientation, Entrepreneurial orientation, Technology orientation, And developmental culture","Prior research articulated the importance of developing a big data analytics capability but did not show how to cultivate this development. Drawing on the literature on this topic, this study develops the concept of Big Data capability, which enhances our understanding of Big Data practice beyond that captured in previous literature on the concept of big data analytics capability. This study further highlights the strategic implications of the concept by testing its relationship to three strategic orientations and one aspect of organizational culture. Findings show that customer, entrepreneurial, and technology orientations, and developmental culture are important contributors to the development of Big Data capability.",2019,,https://doi.org/10.1016/j.jbusres.2019.07.016
1282,Qiuping Ma and Hongyan Li and Anders Thorstenson,A big data-driven root cause analysis system: Application of Machine Learning in quality problem solving,"Quality management, Data mining, Machine Learning, Multi-class classification, Neural Network","Root cause analysis for quality problem solving is critical to improve product quality performance and reduce the quality risk for manufacturers. Subjective conventional methods have been applied frequently in past decades. However, due to increasingly complex product and supply chain structures, diverse working conditions, and massive amounts of components, accuracy and efficiency of root cause analysis are progressively challenged in practice. Therefore, data-driven root cause analysis methods have attracted attention lately. In this paper, taking advantage of the availability of big operations data and the rapid development of data science, we design a big data-driven root cause analysis system utilizing Machine Learning techniques to improve the performance of root cause analysis. More specifically, we first propose a conceptual framework of the big data-driven root cause analysis system including three modules of Problem Identification, Root Cause Identification, and Permanent Corrective Action. Furthermore, in the Problem Identification Module, we construct a unified feature-based approach to describe multiple and different types of quality problems by applying a data mining method. In the Root Cause Identification Module, we use supervised Machine Learning (classification) methods to automatically predict the root causes of multiple quality problems. Finally, we illustrate the accuracy and efficiency of the proposed system and algorithms based on actual quality data from a case company. This study contributes to the literature from the following aspects: (i) the integrated system and algorithms can be used directly to develop a computer application to manage and solve quality problems with high concurrences and complexities in any manufacturing process; (ii) a general procedure and method are provided to formulate and describe a large quantity and different types of quality problems; (iii) compared with traditional methods, it is demonstrated using real case data that manufacturing companies can save significant time and cost with our proposed data-driven root cause analysis system; (iv) this study not only aims at improving the quality problem solving practices for a complex manufacturing process but also bridges a gap between the theoretical development of Machining Learning methods and their application in the operations management domain.",2021,,https://doi.org/10.1016/j.cie.2021.107580
1283,Diego García-Gil and Julián Luengo and Salvador García and Francisco Herrera,Enabling Smart Data: Noise filtering in Big Data classification,"Big Data, Smart Data, Classification, Class noise, Label noise.","In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.",2019,,https://doi.org/10.1016/j.ins.2018.12.002
1284,Zhihan Lv and Liang Qiao,Analysis of healthcare big data,"Big data, Health care, Privacy security risk, Privacy measures","In order to explore the development of healthcare in China and the privacy and security risk factors in medical data under the background of big data, the development status of China’s healthcare sector is analyzed. The questionnaire is used to analyze the privacy and security risk factors of healthcare big data and protection measures are put forward based on the data privacy and security risk factors in the context of cloud services in the literature. The results show that in recent years, the number of health institutions, the number of medical personnel, the assets of medical institutions, the per capita hospitalization cost, and the insured population all show a trend of increasing year by year; while in 2017, the crude mortality rate of malignant tumor patients was the highest in China, and the mortality rate of rural patients was higher than that of urban patients. The results of the questionnaire show that the probability of data analysis, medical treatment process, disease diagnosis process, lack of protective measures, and imperfect access system is all greater than 0.8 when medical care big data is oriented to cloud services. Based on this, two levels of privacy protection measures are proposed: technology and management. It indicates that medical institutions need to pay attention to data privacy protection and grasp the use of digital medical data to provide decision support for subsequent medical data analysis.",2020,,https://doi.org/10.1016/j.future.2020.03.039
1285,Pietro Cozzini and Francesca Cavaliere and Giulia Spaggiari and Gianluca Morelli and Marco Riani,Computational methods on food contact chemicals: Big data and in silico screening on nuclear receptors family,"Computational chemistry, Consensus prediction, Database, Nuclear receptors, Toxicology","According to Eurostat, the EU production of chemicals hazardous to health reached 211 million tonnes in 2019. Thus, the possibility that some of these chemical compounds interact negatively with the human endocrine system has received, especially in the last decade, considerable attention from the scientific community. It is obvious that given the large number of chemical compounds it is impossible to use in vitro/in vivo tests for identifying all the possible toxic interactions of these chemicals and their metabolites. In addition, the poor availability of highly curated databases from which to retrieve and download the chemical, structure, and regulative information about all food contact chemicals has delayed the application of in silico methods. To overcome these problems, in this study we use robust computational approaches, based on a combination of highly curated databases and molecular docking, in order to screen all food contact chemicals against the nuclear receptor family in a cost and time-effective manner.",2022,,https://doi.org/10.1016/j.chemosphere.2021.133422
1286,Emily L. Gill and Stephen R. Master,Big Data Everywhere: The Impact of Data Disjunction in the Direct-to-Consumer Testing Model,"Big Data, Laboratory medicine, Machine learning, Direct-to-consumer testing, DTC, Harmonization",,2020,,https://doi.org/10.1016/j.cll.2019.11.009
1287,Renato Arbex and Claudio B. Cunha,Estimating the influence of crowding and travel time variability on accessibility to jobs in a large public transport network using smart card big data,"Public transport, Accessibility, Smart card data, In-vehicle crowding, Travel time reliability","Accessibility metrics are gaining momentum in public transportation planning and policy-making. However, critical user experience issues such as crowding discomfort and travel time unreliability are still not considered in those accessibility indicators. This paper aims to apply a methodology to build spatiotemporal crowding data and estimate travel time variability in a congested public transport network to improve accessibility calculations. It relies on using multiple big data sources available in most transit systems such as smart card and automatic vehicle location (AVL) data. São Paulo, Brazil, is used as a case study to show the impact of crowding and travel time variability on accessibility to jobs. Our results evidence a population-weighted average reduction of 56.8% in accessibility to jobs in a regular workday morning peak due to crowding discomfort, as well as reductions of 6.2% due to travel time unreliability and 59.2% when both are combined. The findings of this study can be of invaluable help to public transport planners and policymakers, as they show the importance of including both aspects in accessibility indicators for better decision making. Despite some limitations due to data quality and consistency throughout the study period, the proposed approach offers a new way to leverage big data in public transport to enhance policy decisions.",2020,,https://doi.org/10.1016/j.jtrangeo.2020.102671
1288,Maria Ijaz Baig and Liyana Shuib and Elaheh Yadegaridehkordi,Big data adoption: State of the art and research challenges,"Big data adoption, Technology–Organization–Environment, Diffusion of Innovations","Big data adoption is a process through which businesses find innovative ways to enhance productivity and predict risk to satisfy customers need more efficiently. Despite the increase in demand and importance of big data adoption, there is still a lack of comprehensive review and classification of the existing studies in this area. This research aims to gain a comprehensive understanding of the current state-of-the-art by highlighting theoretical models, the influence factors, and the research challenges of big data adoption. By adopting a systematic selection process, twenty studies were identified in the domain of big data adoption and were reviewed in order to extract relevant information that answers a set of research questions. According to the findings, Technology–Organization–Environment and Diffusion of Innovations are the most popular theoretical models used for big data adoption in various domains. This research also revealed forty-two factors in technology, organization, environment, and innovation that have a significant influence on big data adoption. Finally, challenges found in the current research about big data adoption are represented, and future research directions are recommended. This study is helpful for researchers and stakeholders to take initiatives that will alleviate the challenges and facilitate big data adoption in various fields.",2019,,https://doi.org/10.1016/j.ipm.2019.102095
1289,Andy S. Alic and Jussara Almeida and Giovanni Aloisio and Nazareno Andrade and Nuno Antunes and Danilo Ardagna and Rosa M. Badia and Tania Basso and Ignacio Blanquer and Tarciso Braz and Andrey Brito and Donatello Elia and Sandro Fiore and Dorgival Guedes and Marco Lattuada and Daniele Lezzi and Matheus Maciel and Wagner Meira and Demetrio Mestre and Regina Moraes and Fabio Morais and Carlos Eduardo Pires and Nádia P. Kozievitch and Walter dos Santos and Paulo Silva and Marco Vieira,BIGSEA: A Big Data analytics platform for public transportation information,,"Analysis of public transportation data in large cities is a challenging problem. Managing data ingestion, data storage, data quality enhancement, modelling and analysis requires intensive computing and a non-trivial amount of resources. In EUBra-BIGSEA (Europe–Brazil Collaboration of Big Data Scientific Research Through Cloud-Centric Applications) we address such problems in a comprehensive and integrated way. EUBra-BIGSEA provides a platform for building up data analytic workflows on top of elastic cloud services without requiring skills related to either programming or cloud services. The approach combines cloud orchestration, Quality of Service and automatic parallelisation on a platform that includes a toolbox for implementing privacy guarantees and data quality enhancement as well as advanced services for sentiment analysis, traffic jam estimation and trip recommendation based on estimated crowdedness. All developments are available under Open Source licenses (http://github.org/eubr-bigsea, https://hub.docker.com/u/eubrabigsea/).",2019,,https://doi.org/10.1016/j.future.2019.02.011
1290,Balan Sundarakani and Aneesh Ajaykumar and Angappa Gunasekaran,Big data driven supply chain design and applications for blockchain: An action research using case study approach,"Big data architecture, Action research, Case study research, Blockchain adoption, Supply chain management","Blockchain appears to still be nascent in its growth and a relatively untapped asset. This research investigates the need of blockchain in Industry 4.0 environment from Big Data perspective in supply chain management. The research method used in this study involves a combination of an Action Research method and Case Study research. More specifically, the action research method was applied in two industry case studies that implemented and tested the designed architecture in a global logistics environment. Case Study A examined the blockchain application in cross-border cargo movements whereas Case Study B investigated the application in a liquid chemical logistics company serving to petroleum industries. Our research analysis has identified that the Case A subject had disconnected systems and services for blockchain wherein the big data interactions had failed (failure case). Whereas in Case B, the company has achieved nearly 25% increase in revenue through its customer service after the blockchain implementation and thereby reduction in paperwork and carbon emissions (success case). This research contributes to the advancement of the body of knowledge to big data and blockchain by identifying key implementation guideline and issues for blockchain in supply chain management. Further, action-based research coupled with a case study approach has been used to evaluate the application aspects of the architecture's scalability and functionality of bigdata and blockchain in supply chain management.",2021,,https://doi.org/10.1016/j.omega.2021.102452
1291,Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante,A multi-dimension framework for value creation through big data,"Big data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation","Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.",2020,,https://doi.org/10.1016/j.indmarman.2019.08.004
1292,Shouyang Wang,An interview with Shouyang Wang: research frontier of big data-driven economic and financial forecasting,"Big data, Economic forecasting, Data mining, Spatio-temporal","The development of big data generation, acquisition, storage, processing, and other technologies has greatly enriched our sensory world and fundamentally changed the basis of traditional economic and financial forecasting. Unexpected events in the economic and financial fields challenge our confidence in the performance of forecasting models. Obviously, the big data-driven decision theories and analysis methods are different from the traditional methods. In view of the important role of big data-driven economic and financial forecasting in social stability, innovative development, and sustainability, the research frontiers of big data-driven economic and financial forecasting in the future include: feature mining of complex economic systems with big data representation; accurate real-time correction of theories and methods of dynamic forecasting and early warning; general paradigm of big data forecasting research; formation and process of big data-driven economic and financial system management mechanism, etc. Systematic research on such issues will contribute to the formation of decision-making theories and research systems in the context of big data, thus improving the adaptability and scientificity of management decisions.",2021,,https://doi.org/10.1016/j.dsm.2021.01.001
1293,António AC Vieira and Luís MS Dias and Maribel Y Santos and Guilherme AB Pereira and José A Oliveira,On the use of simulation as a Big Data semantic validator for supply chain management,"Simulation, Big Data, Data issues, Semantic validation, Supply chain management, Industry 4.0","Simulation stands out as an appropriate method for the Supply Chain Management (SCM) field. Nevertheless, to produce accurate simulations of Supply Chains (SCs), several business processes must be considered. Thus, when using real data in these simulation models, Big Data concepts and technologies become necessary, as the involved data sources generate data at increasing volume, velocity and variety, in what is known as a Big Data context. While developing such solution, several data issues were found, with simulation proving to be more efficient than traditional data profiling techniques in identifying them. Thus, this paper proposes the use of simulation as a semantic validator of the data, proposed a classification for such issues and quantified their impact in the volume of data used in the final achieved solution. This paper concluded that, while SC simulations using Big Data concepts and technologies are within the grasp of organizations, their data models still require considerable improvements, in order to produce perfect mimics of their SCs. In fact, it was also found that simulation can help in identifying and bypassing some of these issues.",2020,,https://doi.org/10.1016/j.simpat.2019.101985
1294,Nelson Lozada and Jose Arias-Pérez and Geovanny Perdomo-Charry,Big data analytics capability and co-innovation: An empirical study,"Business, Economics, Information science, Big data analytics capabilities, Co-innovation, Big data, Co-creation","There are numerous emerging studies addressing big data and its application in different organizational aspects, especially regarding its impact on the business innovation process. This study in particular aims at analyzing the existing relationship between Big Data Analytics Capabilities and Co-innovation. To test the hypothesis model, structural equations by the partial least squares method were used in a sample of 112 Colombian firms. The main findings allow to positively relate Big Data Analytics Capabilities with better and more agile processes of product and service co-creation and with more robust collaboration networks with stakeholders internal and external to the firm.",2019,,https://doi.org/10.1016/j.heliyon.2019.e02541
1295,Hadi Gholizadeh and Hamed Fazlollahtabar and Mohammad Khalilzadeh,A robust fuzzy stochastic programming for sustainable procurement and logistics under hybrid uncertainty using big data,"logistics, Robust fuzzy stochastic programming, Sustainable procurement, Big data, Hybrid uncertainty, ε-Constraint","Today, in many organizations, the debate about the difference in core capabilities has become an important factor for market competition. Companies, based on the field of activity, decide to strengthen some of their capabilities, capacities, and expertise. Therefore, the focus of an organization on the strengths and efforts to develop its sustainability will lead to a competitive advantage in the marketplace. Due to changes in environmental factors, organizations have focused on carbon emissions in procurement and transportation that have the highest carbon footprint. This paper proposes a multi-objective, eco-sustainability model for a supply chain. The objectives are to minimize overall costs, maximize the efficiency of transportation vehicles and minimize information fraud in the process of information sharing within supply chain elements. Big data is considered in the amount of information exchanged between customers and other elements of the proposed supply chain; since there are frauds in information sharing then using big data 5Vs the model is adapted to control the cost of information loss leading to customer dissatisfaction. Since uncertainty is inevitable in the real environments, in this research hybrid uncertainty is considered. Because two sources of uncertainty are considered in most of the parameters, thus it is necessary to robustify the decision-making process. The model is a mixed integer nonlinear program including big data for an optimal sustainable procurement and transportation decision. A heuristic method is used to solve the big data problem that makes use of a robust fuzzy stochastic programming approach. The proposed model can prevent disturbances by using a scenario-based stochastic programming approach. An effective hybrid robust fuzzy stochastic method is also employed for controlling uncertainty in parameters and risk taking out of outbound decisions. To solve the multi-objective model, augmented ε-constraint method is utilized. The model performance is investigated in a comprehensive computational study.",2020,,https://doi.org/10.1016/j.jclepro.2020.120640
1296,Ashish Kumar Jha and Maher A.N. Agi and Eric W.T. Ngai,A note on big data analytics capability development in supply chain,"Big data, Analytics, Capability development, Qualitative study, Supply chain","Big data analytics (BDA) are gaining importance in all aspects of business management. This is driven by both the presence of large-scale data and management's desire to root decisions in data. Extant research demonstrates that supply chain and operations management functions are among the biggest sources and users of data in the company. Therefore, their decision-making processes would benefit from increased use of BDA technologies. However, there is still a lack of understanding of what determines a company's ability to build BDA capability to gain a competitive advantage. In this study, we attempt to answer this fundamental question by identifying the factors that assist a company in or inhibit it from building its BDA capability and maximizing its gains through BDA technologies. We base our findings on a qualitative analysis of data collected from field visits, interviews with senior management, and secondary resources. We find that, in addition to technical capacity, competitive landscape and intra-firm power dynamics play an important role in building BDA capability and using BDA technologies.",2020,,https://doi.org/10.1016/j.dss.2020.113382
1297,Mladen Kezunovic and Pierre Pinson and Zoran Obradovic and Santiago Grijalva and Tao Hong and Ricardo Bessa,Big data analytics for future electricity grids,"Electricity grids, Analytics, Big data, Decision-making","This paper provides a survey of big data analytics applications and associated implementation issues. The emphasis is placed on applications that are novel and have demonstrated value to the industry, as illustrated using field data and practical applications. The paper reflects on the lessons learned from initial implementations, as well as ideas that are yet to be explored. The various data science trends treated in the literature are outlined, while experiences from applying them in the electricity grid setting are emphasized to pave the way for future applications. The paper ends with opportunities and challenges, as well as implementation goals and strategies for achieving impactful outcomes.",2020,,https://doi.org/10.1016/j.epsr.2020.106788
1298,Tibor Koltay,"Chapter 3 - Data quality, the essential “ingredient”","Research data quality, Stakeholders, Trust, Intrinsic and extrinsic data quality, Semiotic representation, Time-related dimensions, Data retrievability, Data reuse, Data governance","This chapter acquaints the reader with the general and often changing nature of research on data quality. It is emphasized that research data quality is closely related to business data; however, the goals of scholarly research have become different, especially as the environments shaping the two are different. From among data quality’s attributes, trust receives particular attention. Technical and scientific quality, the relationship of data quality to data reuse, and other quality factors are also examined, including big data quality, intrinsic and extrinsic data quality, and the semiotic representation of quality attributes, as well as their time-related dimensions and retrievability. Although data reuse was addressed in an earlier chapter, its relationship to data quality is touched on in this chapter as well. Sharing the previously mentioned origin with data quality and being closely associated with it, data governance is also portrayed.",2022,,https://doi.org/10.1016/B978-0-12-824475-3.00004-7
1299,Yesheng Cui and Sami Kara and Ka C. Chan,Manufacturing big data ecosystem: A systematic literature review,"Smart manufacturing, Big data, Cloud computing, Cloud manufacturing, Internet of things, NoSQL","Advanced manufacturing is one of the core national strategies in the US (AMP), Germany (Industry 4.0) and China (Made-in China 2025). The emergence of the concept of Cyber Physical System (CPS) and big data imperatively enable manufacturing to become smarter and more competitive among nations. Many researchers have proposed new solutions with big data enabling tools for manufacturing applications in three directions: product, production and business. Big data has been a fast-changing research area with many new opportunities for applications in manufacturing. This paper presents a systematic literature review of the state-of-the-art of big data in manufacturing. Six key drivers of big data applications in manufacturing have been identified. The key drivers are system integration, data, prediction, sustainability, resource sharing and hardware. Based on the requirements of manufacturing, nine essential components of big data ecosystem are captured. They are data ingestion, storage, computing, analytics, visualization, management, workflow, infrastructure and security. Several research domains are identified that are driven by available capabilities of big data ecosystem. Five future directions of big data applications in manufacturing are presented from modelling and simulation to real-time big data analytics and cybersecurity.",2020,,https://doi.org/10.1016/j.rcim.2019.101861
1300,Patricia {López Martínez} and Ricardo Dintén and José María Drake and Marta Zorrilla,A big data-centric architecture metamodel for Industry 4.0,"Data-centric architecture, Data-intensive applications, Model-based development, Industry 4.0, Big data, Metamodel","The effective implementation of Industry 4.0 requires the reformulation of industrial processes in order to achieve the vertical and horizontal digitalization of the value chain. For this purpose, it is necessary to provide tools that enable their successful implementation. This paper therefore proposes a data-centric, distributed, dynamically scalable reference architecture that integrates cutting-edge technologies being aware of the existence of legacy technology typically present in these environments. In order to make its implementation easier, we have designed a metamodel that collects the description of all the elements involved in a digital platform (data, resources, applications and monitoring metrics) as well as the necessary information to configure, deploy and execute applications on it. Likewise, we provide a tool compliant to the metamodel that automates the generation of configuration, deployment and launch files and their corresponding transference and execution in the nodes of the platform. We show the flexibility, extensibility and validity of our software artefacts through their application in two case studies, one addressed to preprocess and store pollution data and the other one, more complex, which simulates the management of an electric power distribution of a smart city.",2021,,https://doi.org/10.1016/j.future.2021.06.020
1301,Robert Northcott,Big data and prediction: Four case studies,"Big data, Prediction, Case studies, Explanation, Elections, Weather","Has the rise of data-intensive science, or ‘big data’, revolutionized our ability to predict? Does it imply a new priority for prediction over causal understanding, and a diminished role for theory and human experts? I examine four important cases where prediction is desirable: political elections, the weather, GDP, and the results of interventions suggested by economic experiments. These cases suggest caution. Although big data methods are indeed very useful sometimes, in this paper's cases they improve predictions either limitedly or not at all, and their prospects of doing so in the future are limited too.",2020,,https://doi.org/10.1016/j.shpsa.2019.09.002
1302,Manwendra K. Tripathi and Randhir Kumar and Rakesh Tripathi,Big-data driven approaches in materials science: A survey,"Material science, Big data, Machine learning, Data analytics, Predictive Algorithms","The data volume is growing rapidly in material science. Every year data volume is getting double in many context of material science. The growing rate of data in material science is demanding for new computational infrastructures that can speed-up material discovery and deployment. In this survey, we are focusing on the challenges in material science due to growing data rate, and how Big Data technology can play a major role in research of material science. This survey includes various disciplines that can be used with Big Data to provide better analysis in the material science research.",2020,,https://doi.org/10.1016/j.matpr.2020.02.249
1303,Muhammad Faheem and Rizwan Aslam Butt and Rashid Ali and Basit Raza and Md. Asri Ngadi and Vehbi Cagri Gungor,CBI4.0: A cross-layer approach for big data gathering for active monitoring and maintenance in the manufacturing industry 4.0,"Internet of things, Industry 4.0, Big data, Multi-channel communication, Wireless sensor network","Industry 4.0 (I4.0) defines a new paradigm to produce high-quality products at the low cost by reacting quickly and effectively to changing demands in the highly volatile global markets. In Industry 4.0, the adoption of Internet of Things (IoT)-enabled Wireless Sensors (WSs) in the manufacturing processes, such as equipment, machining, assembly, material handling, inspection, etc., generates a huge volume of data known as Industrial Big Data (IBD). However, the reliable and efficient gathering and transmission of this big data from the source sensors to the floor inspection system for the real-time monitoring of unexpected changes in the production and quality control processes is the biggest challenge for Industrial Wireless Sensor Networks (IWSNs). This is because of the harsh nature of the indoor industrial environment that causes high noise, signal fading, multipath effects, heat and electromagnetic interference, which reduces the transmission quality and trigger errors in the IWSNs. Therefore, this paper proposes a novel cross-layer data gathering approach called CBI4.0 for active monitoring and control of manufacturing processes in the Industry 4.0. The key aim of the proposed CBI4.0 scheme is to exploit the multi-channel and multi-radio architecture of the sensor network to guarantee quality of service (QoS) requirements, such as higher data rates, throughput, and low packet loss, corrupted packets, and latency by dynamically switching between different frequency bands in the Multichannel Wireless Sensor Networks (MWSNs). By performing several simulation experiments through EstiNet 9.0 simulator, the performance of the proposed CBI4.0 scheme is compared against existing studies in the automobile Industry 4.0. The experimental outcomes show that the proposed scheme outperforms existing schemes and is suitable for effective control and monitoring of various events in the automobile Industry 4.0.",2021,,https://doi.org/10.1016/j.jii.2021.100236
1304,Shivam Bachhety and Shivani Kapania and Rachna Jain,2 - Big Data Analytics for healthcare: theory and applications,"Big Data, healthcare, Big Data Analytics, Hadoop","In the past 10 years, the healthcare industry is growing at a remarkable rate. The healthcare industry is generating enormous amounts of data in terms of volume, velocity, and variety. Big Data methodologies in healthcare can not only increase the business value but will also add to the improvement of healthcare services. Several techniques can be implemented to develop early disease diagnose systems and improve treatment procedures using detailed analysis over time. In such a situation, Big data Analytics proposes to connect intricate databases to achieve more useful results. In this chapter, we will discuss the procedure of big data analytics in the healthcare sector with some practical applications along with its challenges. We will also have a look at the various big data techniques and their tools for implementation. We conclude this chapter with a discussion on potential opportunities for analytics in the healthcare sector.",2021,,https://doi.org/10.1016/B978-0-12-820203-6.00008-4
1305,Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie,The role of information governance in big data analytics driven innovation,"Big data analytics capabilities, Information governance, Incremental innovation, Radical innovation, Environmental uncertainty, FIMIX-PLS","The age of big data analytics is now here, with companies increasingly investing in big data initiatives to foster innovation and outperform competition. Nevertheless, while researchers and practitioners started to examine the shifts that these technologies entail and their overall business value, it is still unclear whether and under what conditions they drive innovation. To address this gap, this study draws on the resource-based view (RBV) of the firm and information governance theory to explore the interplay between a firm’s big data analytics capabilities (BDACs) and their information governance practices in shaping innovation capabilities. We argue that a firm’s BDAC helps enhance two distinct types of innovative capabilities, incremental and radical capabilities, and that information governance positively moderates this relationship. To examine our research model, we analyzed survey data collected from 175 IT and business managers. Results from partial least squares structural equation modelling analysis reveal that BDACs have a positive and significant effect on both incremental and radical innovative capabilities. Our analysis also highlights the important role of information governance, as it positively moderates the relationship between BDAC’s and a firm’s radical innovative capability, while there is a nonsignificant moderating effect for incremental innovation capabilities. Finally, we examine the effect of environmental uncertainty conditions in our model and find that information governance and BDACs have amplified effects under conditions of high environmental dynamism.",2020,,https://doi.org/10.1016/j.im.2020.103361
1306,Mohamed Aboelmaged and Samar Mouakket,Influencing models and determinants in big data analytics research: A bibliometric analysis,"Big data analytics, Technology adoption, Literature review, Bibliometric analysis, Theoretical models, Adoption frameworks","Incorporating big data analytics into a particular context brings various challenges that rest on the model or framework through which individuals or organisations adopt big data to achieve their objectives. Although these models have recently triggered scholars’ attention in various domains, in-depth knowledge of using each of these models in big data research is still blurred. This study enriches our knowledge on emerging models and theories that shape big data analytics adoption (BDAD) research through a bibliometric analysis of 229 studies (143 journal articles and 86 conference papers) published in indexed sources between 2013 and 2019. As a result, twenty models on BDAD have emerged (e.g., “Dynamic Capabilities”, “Resource-Based View”, “Technology Acceptance Model”, “Diffusion of Innovation”, etc.). The analysis reveals that BDAD research to demonstrate attributes suggestive of a topic at an initial stage of development as it is broadly dispersed across different domains employs a wide range of models, some of which overlap. Most of the applied models are generic in nature focusing on variance-based relationships and snapshot prediction with little consensus. There is a conspicuous dearth of process models, firm-level analysis and cultural orientation in contemporary BDAD research. Insights of this bibliometric study could guide rigorous big data research and practice in various contexts. The study concludes with research implications and limitations that offer promising prospects for forthcoming research.",2020,,https://doi.org/10.1016/j.ipm.2020.102234
1307,Gabriele Morra and David A. Yuen and Henry M. Tufo and Matthew G. Knepley,"Fresh Outlook on Numerical Methods for Geodynamics. Part 2: Big Data, HPC, Education","Big Data, High performance computing, Education, Modeling, Geodynamics","Since 2015 much has developed in geodynamical modeling because of the arrival of Big Data. We present in two parts an overview of numerical techniques but also a scan of the new opportunities in this age of Big Data and prepare the community for the coming decade, the roaring twenties, when Data Analytics will reign. We begin with a review of traditional numerical methods (Part I), followed by a survey of the current techniques used for data analytics and high-performance computing (HPC) (Part II). Our aim is to cover topics of machine learning, neural networks and deep learning, unsupervised learning as well as the role that HPC will play in the Big Data era, especially in hardware of various calibers. Finally, we will address the need for education of students and professionals, in particular, on the use of the emerging programming languages and the importance of scientific software communities.",2021,,https://doi.org/10.1016/B978-0-08-102908-4.00111-9
1308,Simona-Vasilica Oprea and Adela Bâra and Bogdan George Tudorică and Maria Irène Călinoiu and Mihai Alexandru Botezatu,Insights into demand-side management with big data analytics in electricity consumers’ behaviour,"Big data, Machine learning, Smart meters, Electricity consumption, Clustering, Questionnaire analytics","The consumption data from smart meters and complex questionnaires reveals the electricity consumers’ willingness to adapt their lifestyle to reduce or change their behaviour in electricity usage to flatten the peak in electricity consumption and release the stress in the power grid. Thus, the electricity consumption can support the enforcement of tariff and demand response strategies. Although the plethora of complex, unstructured and heterogeneous data is collected from various devices connected to the Internet, smart meters, plugs, sensors and complex questionnaires, there is an undoubted challenge to handle the data flow that does not provide much information as it remains unprocessed. Therefore, in this paper, we propose an innovative methodology that organizes and extracts valuable information from the increasing volume of data, such as data about the electricity consumption measured and recorded at 30 min intervals, as well as data collected from complex questionnaires.",2021,,https://doi.org/10.1016/j.compeleceng.2020.106902
1309,Zhijiong Huang and Zhuangmin Zhong and Qinge Sha and Yuanqian Xu and Zhiwei Zhang and Lili Wu and Yuzheng Wang and Lihang Zhang and Xiaozhen Cui and MingShuang Tang and Bowen Shi and Chuanzeng Zheng and Zhen Li and Mingming Hu and Linlin Bi and Junyu Zheng and Min Yan,An updated model-ready emission inventory for Guangdong Province by incorporating big data and mapping onto multiple chemical mechanisms,"Emission inventory, Guangdong Province, Ship emissions, Big data, VOCs speciation","An accurate characterization of spatial-temporal emission patterns and speciation of volatile organic compounds (VOCs) for multiple chemical mechanisms is important to improving the air quality ensemble modeling. In this study, we developed a 2017-based high-resolution (3 km × 3 km) model-ready emission inventory for Guangdong Province (GD) by updating estimation methods, emission factors, activity data, and allocation profiles. In particular, a full-localized speciation profile dataset mapped to five chemical mechanisms was developed to promote the determination of VOC speciation, and two dynamic approaches based on big data were used to improve the estimation of ship emissions and open fire biomass burning (OFBB). Compared with previous emissions, more VOC emissions were classified as oxygenated volatile organic compound (OVOC) species, and their contributions to the total ozone formation potential (OFP) in the Pearl River Delta (PRD) region increased by 17%. Formaldehyde became the largest OFP species in GD, accounting for 11.6% of the total OFP, indicating that the model-ready emission inventory developed in this study is more reactive. The high spatial-temporal variability of ship sources and OFBB, which were previously underestimated, was also captured by using big data. Ship emissions during typhoon days and holidays decreased by 23–55%. 95% of OFBB emissions were concentrated in 9% of the GD area and 31% of the days in 2017, demonstrating their strong spatial-temporal variability. In addition, this study revealed that GD emissions have changed rapidly in recent years due to the leap-forward control measures implemented, and thus, they needed to be updated regularly. All of these updates led to a 5–17% decrease in the emission uncertainty for most pollutants. The results of this study provide a reference for how to reduce uncertainties in developing model-ready emission inventories.",2021,,https://doi.org/10.1016/j.scitotenv.2020.144535
1310,Mouzhi Ge and Hind Bangui and Barbora Buhnova,Big Data for Internet of Things: A Survey,"Big Data, Data analytics, Internet of Things, Healthcare, Energy, Transportation, Building automation, Smart Cities","With the rapid development of the Internet of Things (IoT), Big Data technologies have emerged as a critical data analytics tool to bring the knowledge within IoT infrastructures to better meet the purpose of the IoT systems and support critical decision making. Although the topic of Big Data analytics itself is extensively researched, the disparity between IoT domains (such as healthcare, energy, transportation and others) has isolated the evolution of Big Data approaches in each IoT domain. Thus, the mutual understanding across IoT domains can possibly advance the evolution of Big Data research in IoT. In this work, we therefore conduct a survey on Big Data technologies in different IoT domains to facilitate and stimulate knowledge sharing across the IoT domains. Based on our review, this paper discusses the similarities and differences among Big Data technologies used in different IoT domains, suggests how certain Big Data technology used in one IoT domain can be re-used in another IoT domain, and develops a conceptual framework to outline the critical Big Data technologies across all the reviewed IoT domains.",2018,,https://doi.org/10.1016/j.future.2018.04.053
1311,Kumar Rahul and Rohitash Kumar Banyal,Data Life Cycle Management in Big Data Analytics,"Data life cycle, Data creation, Data usability, Healthcare, Big data","Data life cycle management is very much useful for any enterprise or application where data is being used and processed for producing results. Data’s appearance for a certain period time ensures accessibility and usability in the system. Data generated through different sources and it is available in various forms for accessibility. A big data-based application such as the healthcare sector generates lots of data through sensors and other electronic devices which can be further classified into a model for report generations and predictions for various purposes for the benefits of patients and hospitals as well. The data life cycle presents the entire data process in the system. The lifecycle of data starts from creation, store, usability, sharing, and archive and destroy in the system and applications. It defines the data flow in an organization. For the successful implementation of the model, there is a need to maintain the life cycle of data under a secured system of data management. This paper deals with the data life cycle with different steps and various works are done for data management in different sectors and benefits of the data life cycle for industrial and healthcare applications including challenges, conclusions, and future scope.",2020,,https://doi.org/10.1016/j.procs.2020.06.042
1312,Huamao Wang and Yumei Yao and Said Salhi,Tension in big data using machine learning: Analysis and applications,"Big data, Machine learning, Data size, Prediction accuracy, Social media","The access of machine learning techniques in popular programming languages and the exponentially expanding big data from social media, news, surveys, and markets provide exciting challenges and invaluable opportunities for organizations and individuals to explore implicit information for decision making. Nevertheless, the users of machine learning usually find that these sophisticated techniques could incur a high level of tensions caused by the selection of the appropriate size of the training data set among other factors. In this paper, we provide a systematic way of resolving such tensions by examining practical examples of predicting popularity and sentiment of posts on Twitter and Facebook, blogs on Mashable, news on Google and Yahoo, the US house survey, and Bitcoin prices. Interesting results show that for the case of big data, using around 20% of the full sample often leads to a better prediction accuracy than opting for the full sample. Our conclusion is found to be consistent across a series of experiments. The managerial implication is that using more is not necessarily the best and users need to be cautious about such an important sensitivity as the simplistic approach may easily lead to inferior solutions with potentially detrimental consequences.",2020,,https://doi.org/10.1016/j.techfore.2020.120175
1313,Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Manoj Dora and Mengqi Liu,"Big Data Analytics as a mediator in Lean, Agile, Resilient, and Green (LARG) practices effects on sustainable supply chains","Big data analytics, Manufacturing firms, Supply chain and logistics management, LARG, Business performance and innovation, Sustainability","The effect of big data on the lean, agile, resilient, and green (LARG) supply chain has not been explored much in the literature. This study investigates the role of ‘Big Data Analytics’ (BDA) as a mediator between ‘sustainable supply chain business performance’ and key factors, namely, lean practices, social practices, environmental practices, organisational practices, supply chain practices, financial practices, and total quality management. A sample of 297 responses from thirty-seven Indian manufacturing firms was collected. The paper is beneficial for managers and practitioners to understand supply chain analytics, and it addresses challenges in the management of LARG practices to contribute to a sustainable supply chain.",2021,,https://doi.org/10.1016/j.tre.2020.102170
1314,Mustafa Yıldırım and Feyza Yıldırım Okay and Suat Özdemir,Big data analytics for default prediction using graph theory,"Big data analytics, Graph theory, Machine learning, Default prediction, SHAP value","With the unprecedented increase in data all over the world, financial sector such as companies and industries try to remain competitive by transforming themselves into data-driven organizations. By analyzing a huge amount of financial data, companies are able to obtain valuable information to determine their strategic plans such as risk control, crisis management, or growth management. However, as the amount of data increase dramatically, traditional data analytic platforms confront with storing, managing, and analyzing difficulties. Emerging Big Data Analytics (BDA) overcome these problems by providing decentralized and distributed processing. In this study, we propose two new models for default prediction. In the first model, called DPModel-1, statistical (logistic regression), and machine learning methods (decision tree, random forest, gradient boosting) are employed to predict company default. Derived from the first model, we propose DPModel-2 based on graph theory. DPModel-2 also comprises new variables obtained from the trading interactions of companies. In both models, grid search optimization and SHapley Additive exPlanations (SHAP) value are utilized in order to determine the best hyperparameters and make the models interpretable, respectively. By leveraging balance sheet, credit, and invoice datasets, default prediction is realized for about one million companies in Turkey between the years 2010–2018. The default rates of companies range between 3%-6% by year. The experimental results are conducted on a BDA platform. According to the DPModel-1 results, the highest AUC score is ensured by random forest with 0.87. In addition, the results are improved for each technique separately by adjusting new variables with graph theory. According to DPModel-2 results, the best AUC score is achieved by random forest with 0.89.",2021,,https://doi.org/10.1016/j.eswa.2021.114840
1315,Michael R. Mathis and Timur Z. Dubovoy and Matthew D. Caldwell and Milo C. Engoren,Making Sense of Big Data to Improve Perioperative Care: Learning Health Systems and the Multicenter Perioperative Outcomes Group,,,2020,,https://doi.org/10.1053/j.jvca.2019.11.012
1316,Romina Rakipi and Federica {De Santis} and Giuseppe D'Onza,Correlates of the internal audit function’s use of data analytics in the big data era: Global evidence,"Internal audit, Data analytics, Big data, Soft skills, Fraud detection, IT audit","In the big data era, internal audit functions (IAFs) should innovate their techniques so as to add value to their organizations. The use of data analytics (DA) increases IAFs’ ability to extract value from big data, helping IAFs to enhance their activities’ efficiency and effectiveness. We use responses from 1,681 Chief Audit Executives (CAEs) in 82 countries to investigate the correlates of IAFs’ DA usage. From the literature, we identify five main variables expected to be associated with IAFs’ DA use. We find a positive and significant association between DA use and (i) the IAF reporting to the audit committee (AC) and (ii) CAEs’ ability to build positive relationships with managers. These findings suggest that IAF independence and CAEs’ soft skills are important to innovate IAF techniques favoring DA use. We also find a positive association between DA use and IAFs’ involvement in the assurance of enterprise risk management, fraud detection, and IT risk audit activities. Our findings contribute to the internal auditing and DA literatures, and should be of interest to CAEs, ACs, corporate boards, and professional associations.",2021,,https://doi.org/10.1016/j.intaccaudtax.2020.100357
1317,Chubing Zhang and Xinchun Wang and Annie Peng Cui and Shenghao Han,Linking big data analytical intelligence to customer relationship management performance,"Big data, Data-driven culture, Competitive pressures, Mass customization, Marketing capability, CRM performance","This study investigates the driving forces of a firm's assimilation of big data analytical intelligence (BDAI) and how the assimilation of BDAI improve customer relationship management (CRM) performance. Drawing on the resource-based view, this study argues that a firm's data-driven culture and the competitive pressure it faces in the industry motivate a firm's assimilation of BDAI. As a firm resource, BDAI enables an organization to develop superior mass-customization capability, which in turn positively influences its CRM performance. In addition, this study proposes that a firm's marketing capability can moderate the impact of BDAI assimilation on its mass-customization capability. Using survey data collected from 147 business-to-business companies, this study finds support for most of the hypotheses. The findings of this study uncover compelling insights about the dynamics involved in the process of using BDAI to improve CRM performance.",2020,,https://doi.org/10.1016/j.indmarman.2020.10.012
1318,Robert X. Gao and Lihui Wang and Moneer Helu and Roberto Teti,Big data analytics for smart factories of the future,"Digital manufacturing system, Information, Learning","Continued advancement of sensors has led to an ever-increasing amount of data of various physical nature to be acquired from production lines. As rich information relevant to the machines and processes are embedded within these “big data”, how to effectively and efficiently discover patterns in the big data to enhance productivity and economy has become both a challenge and an opportunity. This paper discusses essential elements of and promising solutions enabled by data science that are critical to processing data of high volume, velocity, variety, and low veracity, towards the creation of added-value in smart factories of the future.",2020,,https://doi.org/10.1016/j.cirp.2020.05.002
1319,Iris Hausladen and Maximilian Schosser,Towards a maturity model for big data analytics in airline network planning,"Maturity model, Network planning, Big data analytics, Airlines, Case study","The evaluation, acquisition and use of newly available big data sources has become a major strategic and organizational challenge for airline network planners. We address this challenge by developing a maturity model for big data readiness for airline network planning. The development of the maturity model is grounded in literature, expert interviews and case study research involving nine airlines. Four airline business models are represented, namely full-service carriers, low-cost airlines, scheduled charter airlines and cargo airlines. The maturity model has been well received with seven change requests in the model development phase. The revised version has been evaluated as exhaustive and useful by airline network planners. The self-assessment of airlines revealed low to medium maturity for most domains. Organizational factors show the lowest average maturity, IT architecture the highest. Full-service carriers seem to be more mature than airlines with different business models.",2020,,https://doi.org/10.1016/j.jairtraman.2019.101721
1320,Devarshi Shah and Jin Wang and Q. Peter He,Feature engineering in big data analytics for IoT-enabled smart manufacturing – Comparison between deep learning and statistical learning,"Internet-of-Things, Smart manufacturing, Big data, Data analytics, Feature engineering, Deep learning, Statistical learning","As IoT-enabled manufacturing is still in its infancy, there are several key research gaps that need to be addressed. These gaps include the understanding of the characteristics of the big data generated from industrial IoT sensors, the challenges they present to process data analytics, as well as the specific opportunities that the IoT big data could bring to advance manufacturing. In this paper, we use an inhouse-developed IoT-enabled manufacturing testbed to study the characteristics of the big data generated from the testbed. Since the quality of the data usually has the most impact on process modeling, data veracity is often the most challenging characteristic of big data. To address that, we explore the role of feature engineering in developing effective machine learning models for predicting key process variables. We compare complex deep learning approaches to a simple statistical learning approach, with different level or extent of feature engineering, to explore their pros and cons for potential industrial IoT-enabled manufacturing applications.",2020,,https://doi.org/10.1016/j.compchemeng.2020.106970
1321,Dan Zhang and Shan L. Pan and Jiaxin Yu and Wenyuan Liu,Orchestrating big data analytics capability for sustainability: A study of air pollution management in China,"Big data, Big data analytics, Sustainability, Air pollution, Resource orchestration","Under rapid urbanization, cities are facing many societal challenges that impede sustainability. Big data analytics (BDA) gives cities unprecedented potential to address these issues. As BDA is still a new concept, there is limited knowledge on how to apply BDA in a sustainability context. Thus, this study investigates a case using BDA for sustainability, adopting the resource orchestration perspective. A process model is generated, which provides novel insights into three aspects: data resource orchestration, BDA capability development, and big data value creation. This study benefits both researchers and practitioners by contributing to theoretical developments as well as by providing practical insights.",2019,,https://doi.org/10.1016/j.im.2019.103231
1322,Pan Liu and Yue Long and Hai-Cao Song and Yan-Dong He,Investment decision and coordination of green agri-food supply chain considering information service based on blockchain and big data,"Big data, Blockchain, Agri-food supply chain, Investment decision, Coordination","Researches about the fusion application of Big Data and blockchain have appeared for a long time, many information service providers have launched information service business based on Big Data and blockchain (hereafter, ISBD). However, in the green agri-food area, the ISBD application does not popularized. A vital reason is that many decision makers do not know how to make an optimal investment decision and coordinate chain members after adopting ISBD. The core of this problem is to study the issue of investment decision and coordination in a green agri-food supply chain. To solve this problem, firstly, combining with the status of Chinese agricultural development, we proposed a more suitable supply chain structure in the fusion application environment of Big Data and blockchain. Then, we chose a green agri-food supply chain with one producer and one retailer as research object and revised the demand function. Afterwards, considering the changes of agri-food freshness and greenness, we built and analysed the benefit models of producer and retailer before and after using ISBD, and then a cost-sharing and revenue-sharing contract was put forward to coordinate the supply chain. Findings: 1) When the total investment cost payed by producer and retailer is in a certain range, using ISBD will help chain members gain more benefits. 2) If chain members want to gain more benefits after using ISBD, they should try their best to optimize costs by extracting valuable information. Results can offer a theoretical guidance for producer and retailer in investing in ISBD, pricing decision and supply chain coordination after applying ISBD.",2020,,https://doi.org/10.1016/j.jclepro.2020.123646
1323,Dongfang Ren and Xiaopeng Guo and Cunbin Li,Research on big data analysis model of multi energy power generation considering pollutant emission—Empirical analysis from Shanxi Province,"Multi-energy power generation, Pollutant emission, Big data analysis, Renewable energy generation, Thermal power","With the development of the integrated energy Internet, energy structure optimization and emission reduction have led to higher requirements for developing various energy sources to enable coordinated and sustainable development. However, data-mining methods are rarely used to study the coordination of multi-energy generation in published research results. In this study, from the perspective of power industry emissions, coordinated generation of various energy sources, and balance of power generation and consumption, a data-mining algorithm was used to analyze the development of thermal power, hydropower, wind power, waste heat, gas, and other power sources. The chi-square automatic interaction detection tree (CHAID), logistic regression, and two-step clustering methods were applied. The results show that: a) CO2 and SO2 emissions were mainly affected by thermal power generation, whereas NOx emissions were jointly affected by thermal power, garbage power, and gas-fired power, and the emissions of various pollutants increased with an increase in power consumption. The optimal power-generation scheme under minimum emission can be obtained. b) There was a strong correlation between thermal power generation and residential electricity consumption, and renewable energy (wind energy, photovoltaic, hydropower) exhibited the highest correlation with the electricity consumption of the tertiary industry, which indicates that renewable energy generation can be promoted by managing electricity consumption in the tertiary industry. c) When the electricity demand of all users was small, the proportion of renewable energy power generation increased; in contrast, the thermal power generation was larger. This indicates the importance of improving the sustainable and stable power supply of renewable energy. This study provides a data analysis model for the coordinated development of multiple energies, which will contribute to the decision-making basis for controlling power emissions, improving the utilization rate of renewable energy, and optimizing the energy structure.",2021,,https://doi.org/10.1016/j.jclepro.2021.128154
1324,Arfan Majeed and Yingfeng Zhang and Shan Ren and Jingxiang Lv and Tao Peng and Saad Waqar and Enhuai Yin,A big data-driven framework for sustainable and smart additive manufacturing,"Big data, Additive manufacturing, Sustainable manufacturing, Smart manufacturing, Optimization","From the last decade, additive manufacturing (AM) has been evolving speedily and has revealed the great potential for energy-saving and cleaner environmental production due to a reduction in material and resource consumption and other tooling requirements. In this modern era, with the advancements in manufacturing technologies, academia and industry have been given more interest in smart manufacturing for taking benefits for making their production more sustainable and effective. In the present study, the significant techniques of smart manufacturing, sustainable manufacturing, and additive manufacturing are combined to make a unified term of sustainable and smart additive manufacturing (SSAM). The paper aims to develop framework by combining big data analytics, additive manufacturing, and sustainable smart manufacturing technologies which is beneficial to the additive manufacturing enterprises. So, a framework of big data-driven sustainable and smart additive manufacturing (BD-SSAM) is proposed which helped AM industry leaders to make better decisions for the beginning of life (BOL) stage of product life cycle. Finally, an application scenario of the additive manufacturing industry was presented to demonstrate the proposed framework. The proposed framework is implemented on the BOL stage of product lifecycle due to limitation of available resources and for fabrication of AlSi10Mg alloy components by using selective laser melting (SLM) technique of AM. The results indicate that energy consumption and quality of the product are adequately controlled which is helpful for smart sustainable manufacturing, emission reduction, and cleaner production.",2021,,https://doi.org/10.1016/j.rcim.2020.102026
1325,Zehua Xiang and Minli Xu,"Dynamic game strategies of a two-stage remanufacturing closed-loop supply chain considering Big Data marketing, technological innovation and overconfidence","Supply chain management, Big Data marketing, Technological innovation, Closed-loop supply chain, Overconfidence","In the “Internet+” era, involving third-party Internet recycling platforms (IRPs) has revolutionized the operation models of closed-loop supply chains (CLSCs) in China. This study explores the impact of technological innovation, Big Data marketing and overconfidence on supply chain member decision-making. We propose a two-stage remanufacturing CLSC dynamic model consisting of a manufacturer, an IRP, and a supplier based on differential game theory. By comparing the optimal decisions of each member in three scenarios, we find that the IRP’s overconfident behavior is beneficial to both the manufacturer and the IRP but will damage the supplier's profit. Although a suitable cost-sharing ratio can enable the manufacturer and IRP to achieve a “win–win” situation, an excessive level of confidence will inhibit the incentives of the cost-sharing strategy, negatively affecting the manufacturer's interests. Interestingly, a cost-sharing contract will become inefficient under certain conditions, i.e., highly efficient level of technological innovation, highly efficient Big Data marketing, and a high level of overconfidence, negatively affecting the manufacturer’s interests. Additionally, technological innovation efficiency and marketing efficiency will have different effects on the IRP's recycling price. A cost-sharing contract and the IRP’s overconfidence will prompt the IRP to exert more efforts on technological innovation and Big Data marketing and to significantly reduce the manufacturing costs and recycling costs for all members. Notably, although the IRP’s overconfidence and cost-sharing strategies may damage the supplier’s profit, the total profit of the CLSC increases.",2020,,https://doi.org/10.1016/j.cie.2020.106538
1326,Victor Chang and Mohamed Aleem Ali and Alamgir Hossain,Chapter 2 - Investigation of COVID-19 and scientific analysis big data analytics with the help of machine learning,"COVID-19 review, AU methods for COVID-19, Machine learning for COVID-19","This book chapter presents the review of COVID-19 and its status, as well as the scientific Analysis big data analytics with the help of machine learning. We provide in-depth literature review, and provide a summary of the current AI and machine learning methods, which have become increasingly important to provide accurate analyses. Various conceptual diagrams have been used to illustrate how different technologies can contribute to effective analyses for COVID-19. We demonstrate our work from both theoretical contributions and practical implementations.",2022,,https://doi.org/10.1016/B978-0-323-90054-6.00007-6
1327,Matthew Russell and Peng Wang,Physics-informed deep learning for signal compression and reconstruction of big data in industrial condition monitoring,"Physics-informed deep learning, Prognostics and health management, Data compression, Big data","The onset of the Internet of Things enables machines to be outfitted with always-on sensors that can provide health information to cloud-based monitoring systems for prognostics and health management (PHM), which greatly improves reliability and avoids downtime of machines and processes on the shop floor. On the other hand, real-time monitoring produces large amounts of data, leading to significant challenges for efficient and effective data transmission (from the shop floor to the cloud) and analysis (in the cloud). Restricted by industrial hardware capability, especially Internet bandwidth, most solutions approach data transmission from the perspective of data compression (before transmission, at local computing devices) coupled with data reconstruction (after transmission, in the cloud). However, existing data compression techniques may not adapt to domain-specific characteristics of data, and hence have limitations in addressing high compression ratios where full restoration of signal details is important for revealing machine conditions. This study integrates Deep Convolutional Autoencoders (DCAE) with local structure and physics-informed loss terms that incorporate PHM domain knowledge such as the importance of frequency content for machine fault diagnosis. Furthermore, Fault Division Autoencoder Multiplexing (FDAM) is proposed to mitigate the negative effects of multiple disjoint operating conditions on reconstruction fidelity. The proposed methods are evaluated on two case studies, and autocorrelation-based noise analysis provides insight into the relative performance across machine health and operating conditions. Results indicate that physically-informed DCAE compression outperforms prevalent data compression approaches, such as compressed sensing, Principal Component Analysis (PCA), Discrete Cosine Transform (DCT), and DCAE with a standard loss function. FDAM can further improve the data reconstruction quality for certain machine conditions.",2022,,https://doi.org/10.1016/j.ymssp.2021.108709
1328,Mohamed Ahzam Amanullah and Riyaz Ahamed Ariyaluran Habeeb and Fariza Hanum Nasaruddin and Abdullah Gani and Ejaz Ahmed and Abdul Salam Mohamed Nainar and Nazihah Md Akim and Muhammad Imran,Deep learning and big data technologies for IoT security,"Deep learning, Big data, IoT security","Technology has become inevitable in human life, especially the growth of Internet of Things (IoT), which enables communication and interaction with various devices. However, IoT has been proven to be vulnerable to security breaches. Therefore, it is necessary to develop fool proof solutions by creating new technologies or combining existing technologies to address the security issues. Deep learning, a branch of machine learning has shown promising results in previous studies for detection of security breaches. Additionally, IoT devices generate large volumes, variety, and veracity of data. Thus, when big data technologies are incorporated, higher performance and better data handling can be achieved. Hence, we have conducted a comprehensive survey on state-of-the-art deep learning, IoT security, and big data technologies. Further, a comparative analysis and the relationship among deep learning, IoT security, and big data technologies have also been discussed. Further, we have derived a thematic taxonomy from the comparative analysis of technical studies of the three aforementioned domains. Finally, we have identified and discussed the challenges in incorporating deep learning for IoT security using big data technologies and have provided directions to future researchers on the IoT security aspects.",2020,,https://doi.org/10.1016/j.comcom.2020.01.016
1329,Brita Sedlmayr and Andreas Knapp and Michéle Kümmel and Franziska Bathelt and Martin Sedlmayr,Evaluation eines Zukunftsszenarios zur Nutzung von Big-Data-Anwendungen für die Verbesserung der Versorgung von Menschen mit seltenen Erkrankungen,"Evaluation, Akzeptanz, Nutzen, Barrieren, Befragung, Szenario, Big Data, Seltene Erkrankungen, Evaluation, Acceptance, Benefits, Barriers, Survey, Scenario, Big data, Rare diseases","Zusammenfassung
Hintergrund
In Deutschland leben etwa 4 Millionen Menschen mit einer seltenen Erkrankung. Einzelne Studien belegen bereits, dass mit Hilfe von Big Data Verfahren die Diagnostik verbessert und seltene Erkrankungen effektiver erforscht werden können. Deutschlandweit existiert aber bisher kein konkretes, umfassendes Konzept für den Einsatz von Big Data zur Versorgung von Menschen mit seltenen Erkrankungen. Im Rahmen des BMG-geförderten Projekts „BIDA-SE“ wurde ein erstes Szenario entworfen, wie Big-Data-basierte Anwendungen sinnvoll in der Versorgungspraxis von Menschen mit seltenen Erkrankungen einfließen können.
Methode
Ziel der vorliegenden Studie war es, das entwickelte Szenario hinsichtlich der Akzeptanz, des (klinischen) Nutzens, ökonomischer Implikationen sowie Grenzen und Barrieren für dessen mittelfristige Umsetzung zu evaluieren. Für die Bewertung des Szenarios wurde im Zeitraum Oktober-November 2019 eine Online-Befragung innerhalb Deutschlands mit insgesamt N = 9 Ärzt*innen, N = 69 Patient*innen mit seltenen Erkrankungen/Patientenvertreter*innen, N = 14 IT-Expert*innen und N = 21 Versorgungsforscher*innen durchgeführt. Für die Entwicklung des Online-Fragebogens wurden standardisierte, validierte Fragen bereits erprobter Erhebungsinstrumente eingesetzt und eigene Fragen auf Basis einer vorausgegangenen Literaturanalyse konstruiert. Die Auswertung der Befragung erfolgte primär deskriptiv durch eine Analyse von Häufigkeiten, Mittelwerten und Standardabweichungen.
Ergebnisse
Die Ergebnisse zeigen, dass das entwickelte Szenario von allen befragten Gruppen (Ärzt*innen, Patient*innen/Patientenvertreter*innen, IT-Expert*innen und Versorgungsforscher*innen) mehrheitlich Akzeptanz erfährt. Aus Sicht der Ärzt*innen, Patient*innen/Patientenvertreter*innen und Versorgungsforscher*innen hätte das Szenario das Potential, die Diagnosestellung und Therapieeinleitung zu beschleunigen und die sektorenübergreifende Behandlung zu verbessern. Investitionen in die vorgestellte Anwendung würden sich aus Sicht der Ärzt*innen und Versorgungsforscher*innen rentieren. Zur Finanzierung des vorgestellten Szenarios müsste jedoch eine Anpassung der Vergütungssituation erfolgen. Die von allen Gruppen benannten Grenzen und Barrieren für eine mittelfristige Umsetzung des Szenarios lassen sich in sieben Themenfelder mit Handlungsbedarf gruppieren: (1) Finanzierung und Investition, (2) Datenschutz und Datensicherheit, (3) Standards/Datenquellen/Datenqualität, (4) Technologieakzeptanz, (5) Integration in den Arbeitsalltag, (6) Wissen um Verfügbarkeit sowie (7) Gewohnheiten und Präferenzen/Arztrolle.
Diskussion
Mit der vorliegenden Studie wurde ein erstes fachübergreifendes, praxisnahes Szenario unter Nutzung von Big-Data-basierten Anwendungen hinsichtlich Akzeptanz, Nutzen und Grenzen/Barrieren evaluiert und analysiert, inwiefern dieses Szenario zukünftig im Kontext seltener Erkrankungen implementiert werden kann. Das Szenario erfährt von allen befragten Zielgruppen mehrheitlich eine hohe Akzeptanz und wird mehrheitlich als (klinisch) nützlich bewertet, wenngleich noch rechtliche, organisatorische und technische Barrieren für dessen mittelfristige Umsetzung überwunden werden müssen. Die Evaluationsergebnisse tragen dazu bei, Handlungsempfehlungen abzuleiten, um eine mittelfristige Umsetzung des Szenarios zu gewährleiten und den Zugang zu den Zentren für Seltene Erkrankungen zukünftig zu kanalisieren.
Schlussfolgerung
Auf nationaler Ebene wurden zahlreiche Aktivitäten angestoßen, um die Versorgungssituation von Menschen mit seltenen Erkrankungen zu verbessern. Das im Rahmen des Projekts „BIDA-SE“ entwickelte Szenario ergänzt diese Forschungsaktivitäten und verdeutlicht, wie Big-Data-basierte Anwendungen sinnvoll in der Praxis genutzt werden können, um die Diagnosestellung und Therapie von Menschen mit seltenen Erkrankungen nachhaltig verbessern zu können.
Introduction
In Germany there are about 4 million people living with a rare disease. Studies have shown that big data applications can improve diagnosis of and research on rare diseases more effectively. However, no concrete comprehensive concept for the use of big data in the care of people with rare diseases has so far been established in Germany. As part of the project “BIDA-SE”, which is funded by the German Ministry of Health, a first scenario has been designed to show how big data applications can be usefully incorporated into the care of people with rare diseases.
Methods
The aim of the present study was to evaluate this scenario with regard to acceptance, (clinical) benefits, economic aspects, and limitations and barriers to its implementation. To evaluate the scenario, an online survey was conducted in Germany in October/November 2019 amongst a total of N = 9 physicians, N = 69 patients with rare diseases/patient representatives, N = 14 IT experts and N = 21 health care researchers. The online questionnaire consisted of both standardized, validated questions taken from already tested survey instruments and additional questions which were constructed on the basis of a preceding literature analysis. The evaluation of the survey was primarily descriptive, with a calculation of frequencies, mean values and standard deviations.
Results
The results of the evaluation show that the scenario has been accepted by a majority of all groups surveyed (physicians, patients/patient representatives, IT experts and health care researchers). From the point of view of physicians, patients/patient representatives and health care researchers, the scenario has the potential to accelerate the diagnosis and initiation of therapy and to improve cross-sectoral treatment. From the physician’s and health care researcher’s perspective, investments in the application presented in the scenario would be profitable. Financing the scenario would, however, require adjusting the reimbursement situation. The limitations and barriers identified by all groups for a medium-term implementation of the scenario can be grouped into seven thematic areas where action is needed: (1) financing and investment, (2) data protection and data security, (3) standards/data sources/data quality, (4) acceptance of technology, (5) integration into the daily work routine, (6) knowledge about availability as well as (7) habits and preferences/physician's role.
Discussion
With the present study, a first interdisciplinary, practical scenario using big data applications was evaluated with regard to acceptance, benefits and limitations/barriers. The scenario is widely accepted among the majority of all surveyed target groups and is considered (clinically) useful, although legal, organisational and technical barriers still need to be overcome for its medium-term implementation. The evaluation results contribute to the derivation of recommendations for action to ensure the medium-term implementation of the scenario and to channel access to the Centres for Rare Diseases in the future.
Conclusion
Many activities have been initiated at a national level to improve the health care situation of people with rare diseases. The scenario developed in the “BIDA-SE” project complements these research activities and illustrates how big data applications can be usefully implemented into practice to improve the diagnosis and therapy of people with rare diseases in a sustainable way.",2020,,https://doi.org/10.1016/j.zefq.2020.11.002
1330,Stefano Bresciani and Francesco Ciampi and Francesco Meli and Alberto Ferraris,"Using big data for co-innovation processes: Mapping the field of data-driven innovation, proposing theoretical developments and providing a research agenda","Big data, Co-innovation, Open innovation, Bibliometric analysis, Literature review","This is the first systematic literature review concerning the interconnections between big data (BD) and co-innovation. It uses BD as a common perspective of analysis as well as a concept aggregating different research streams (open innovation, co-creation and collaborative innovation). The review is based on the results of a bibliographic coupling analysis performed with 51 peer-reviewed papers published before the end of 2019. Three thematic clusters were discovered, which respectively focused on BD as a knowledge creation enabler within co-innovation contexts, BD as a driver of co-innovation processes based on customer engagement, and the impact of BD on co-innovation within service ecosystems. The paper theoretically argues that the use of BD, in addition to enhancing intentional and direct collaborative innovation processes, allows the development of passive and unintentional co-innovation that can be implemented through indirect relationships between the collaborative actors. This study also makes eleven unique research propositions concerning further theoretical developments and managerial implementations in the field of BD-driven co-innovation.",2021,,https://doi.org/10.1016/j.ijinfomgt.2021.102347
1331,Elaheh Yadegaridehkordi and Mehrbakhsh Nilashi and Liyana Shuib and Mohd {Hairul Nizam Bin Md Nasir} and Shahla Asadi and Sarminah Samad and Nor {Fatimah Awang},The impact of big data on firm performance in hotel industry,"Firm performance, Big data, Hotel industry, Fuzzy logic, Structural equation modelling","Big data has increasingly appeared as a frontier of opportunity in enhancing firm performance. However, it still is in early stages of introduction and many enterprises are still un-decisive in its adoption. The aim of this study is to propose a theoretical model based on integration of Human-Organization-Technology fit and Technology-Organization-Environment frameworks to identify the key factors affecting big data adoption and its consequent impact on the firm performance. The significant factors are gained from the literature and the research model is developed. Data was collected from top managers and/or owners of SMEs hotels in Malaysia using online survey questionnaire. Structural Equation Modelling (SEM) is used to assess the developed model and Adaptive Neuro-Fuzzy Inference Systems (ANFIS) technique is used to prioritize adoption factors based on their importance levels. The results showed that relative advantage, management support, IT expertise, and external pressure are the most important factors in the technological, organizational, human, and environmental dimensions. The results further revealed that technology is the most important influential dimension. The outcomes of this study can assist the policy makers, businesses and governments to make well-informed decisions in adopting big data.",2020,,https://doi.org/10.1016/j.elerap.2019.100921
1332,Marcello M. Mariani and Samuel {Fosso Wamba},Exploring how consumer goods companies innovate in the digital age: The role of big data analytics companies,"Big data analytics, Forecasting, Innovation, Online review crowdsourcing, Consumer goods companies, Digital data","The advent and development of digital technologies have brought about a proliferation of online consumer reviews (OCRs), i.e., real-time customers’ evaluations of products, services, and brands. Increasingly, e-commerce platforms are using them to gain insights from customer feedback. Meanwhile, a new generation of big data analytics (BDA) companies are crowdsourcing large volumes of OCRs by means of controlled ad hoc online experiments and advanced machine learning (ML) techniques to forecast demand and determine the market potential for new products in several industries. We illustrate how this process is taking place for consumer goods companies by exploring the case of UK digital BDA company, SoundOut. Based on an in-depth qualitative analysis, we develop the consumer goods company innovation (CGCI) conceptual framework, which illustrates how digital BDA firms help consumer goods companies to test new products before they are launched on the market, and innovate. Theoretical and managerial implications are discussed.",2020,,https://doi.org/10.1016/j.jbusres.2020.09.012
1333,Sukwon Ji and Bumho Lee and Mun Yong Yi,Building life-span prediction for life cycle assessment and life cycle cost using machine learning: A big data approach,"Building life span, Life cycle cost, Life cycle assessment, Big data, Machine learning, Deep neural network","Life cycle assessment (LCA) and life cycle cost (LCC) are two primary methods used to assess the environmental and economic feasibility of building construction. An estimation of the building's life span is essential to carrying out these methods. However, given the diverse factors that affect the building's life span, it was estimated typically based on its main structural type. However, different buildings have different life spans. Simply assuming that all buildings with the same structural type follow an identical life span can cause serious estimation errors. In this study, we collected 1,812,700 records describing buildings built and demolished in South Korea, analysed the actual life span of each building, and developed a building life-span prediction model using deep-learning and traditional machine learning. The prediction models examined in this study produced root mean square errors of 3.72–4.6 and the coefficients of determination of 0.932–0.955. Among those models, a deep-learning based prediction model was found the most powerful. As anticipated, the conventional method of determining a building's life expectancy using a discrete set of specific factors and associated assumptions of life span did not yield realistic results. This study demonstrates that an application of deep learning to the LCA and LCC of a building is a promising direction, effectively guiding business planning and critical decision making throughout the construction process.",2021,,https://doi.org/10.1016/j.buildenv.2021.108267
1334,Evagelos D. Lioutas and Chrysanthi Charatsari and Giuseppe {La Rocca} and Marcello {De Rosa},Key questions on the use of big data in farming: An activity theory approach,"Big data, Smart farming, Value, Farmers, Cyber-physical-social systems, Activity theory","Big data represent a pioneering development in the field of agriculture. By producing intuition, intelligence, and insights, these data have the potential to recast conventional process-driven agriculture, plotting the course for a smarter, data-driven farming. However, many open issues about the use of big data in agriculture remain unanswered. In this work, conceptualizing smart agricultural systems as cyber-physical-social systems, and building upon activity theory, we aim at highlighting some key questions that need to be addressed. To our view, big data constitute a tool reciprocally produced by all the actors involved in the agrifood supply chains. The constant flux of this tool and the intricate nature of the interactions among the actors who share it complicate the translation of big data into value. Moreover, farmers’ limited capacity to deal with data complexity, along with their dual role as producers and users of big data, impedes the institutionalization of this tool at the farm level. Although the approach used left us with more questions than answers, we suggest that unraveling the institutional arrangements that govern value co-creation, capturing the motivations of farmers and other actors, and detailing the direct and indirect effects that big data (and the technologies used to generate them) have in farms are important preconditions for setting forth rules that facilitate the extraction and equal exchange of value from big data.",2019,,https://doi.org/10.1016/j.njas.2019.04.003
1335,Xiang Li and Ning Guo and Quanzheng Li,Functional Neuroimaging in the New Era of Big Data,"Big data, Neuroimaging, Machine learning, Health informatics, fMRI","The field of functional neuroimaging has substantially advanced as a big data science in the past decade, thanks to international collaborative projects and community efforts. Here we conducted a literature review on functional neuroimaging, with focus on three general challenges in big data tasks: data collection and sharing, data infrastructure construction, and data analysis methods. The review covers a wide range of literature types including perspectives, database descriptions, methodology developments, and technical details. We show how each of the challenges was proposed and addressed, and how these solutions formed the three core foundations for the functional neuroimaging as a big data science and helped to build the current data-rich and data-driven community. Furthermore, based on our review of recent literature on the upcoming challenges and opportunities toward future scientific discoveries, we envisioned that the functional neuroimaging community needs to advance from the current foundations to better data integration infrastructure, methodology development toward improved learning capability, and multi-discipline translational research framework for this new era of big data.",2019,,https://doi.org/10.1016/j.gpb.2018.11.005
1336,António A.C. Vieira and Luís Dias and Maribel Y. Santos and Guilherme A.B. Pereira and José Oliveira,Bypassing Data Issues of a Supply Chain Simulation Model in a Big Data Context,"Simulation, Supply Chain, Big Data, Data issues, Industry 4.0","Supply Chains (SCs) are complex and dynamic networks, where certain events may cause severe problems. To avoid them, simulation can be used, allowing the uncertainty of these systems to be considered. Furthermore, the data that is generated at increasingly high volumes, velocities and varieties by relevant data sources allow, on one hand, the simulation model to capture all the relevant elements. While developing such solution, due to the inherent use of simulation, several data issues were identified and bypassed, so that the incorporated elements comprise a coherent SC simulation model. Thus, the purpose of this paper is to present the main issues that were faced, and discuss how these were bypassed, while working on a SC simulation model in a Big Data context and using real industrial data from an automotive electronics SC. This paper highlights the role of simulation in this task, since it worked as a semantic validator of the data. Moreover, this paper also presents the results that can be obtained from the developed model.",2020,,https://doi.org/10.1016/j.promfg.2020.02.033
1337,Lars Lundberg and Håkan Grahn and Valeria Cardellini and Andreas Polze and Sogand Shirinbab,Editorial to the Special Issue on Big Data in Industrial and Commercial Applications,,,2021,,https://doi.org/10.1016/j.bdr.2021.100244
1338,Todd R. McNutt and Kevin L. Moore and Binbin Wu and Jean L. Wright,Use of Big Data for Quality Assurance in Radiation Therapy,,"The application of big data to the quality assurance of radiation therapy is multifaceted. Big data can be used to detect anomalies and suboptimal quality metrics through both statistical means and more advanced machine learning and artificial intelligence. The application of these methods to clinical practice is discussed through examples of guideline adherence, contour integrity, treatment delivery mechanics, and treatment plan quality. The ultimate goal is to apply big data methods to direct measures of patient outcomes for care quality. The era of big data and machine learning is maturing and the implementation for quality assurance promises to improve the quality of care for patients.",2019,,https://doi.org/10.1016/j.semradonc.2019.05.006
1339,Jorge Merino and Ismael Caballero and Bibiano Rivas and Manuel Serrano and Mario Piattini,A Data Quality in Use model for Big Data,"Data Quality, Big Data, Measurement, Quality-in-Use, Model","Beyond the hype of Big Data, something within business intelligence projects is indeed changing. This is mainly because Big Data is not only about data, but also about a complete conceptual and technological stack including raw and processed data, storage, ways of managing data, processing and analytics. A challenge that becomes even trickier is the management of the quality of the data in Big Data environments. More than ever before the need for assessing the Quality-in-Use gains importance since the real contribution–business value–of data can be only estimated in its context of use. Although there exists different Data Quality models for assessing the quality of regular data, none of them has been adapted to Big Data. To fill this gap, we propose the “3As Data Quality-in-Use model”, which is composed of three Data Quality characteristics for assessing the levels of Data Quality-in-Use in Big Data projects: Contextual Adequacy, Operational Adequacy and Temporal Adequacy. The model can be integrated into any sort of Big Data project, as it is independent of any pre-conditions or technologies. The paper shows the way to use the model with a working example. The model accomplishes every challenge related to Data Quality program aimed for Big Data. The main conclusion is that the model can be used as an appropriate way to obtain the Quality-in-Use levels of the input data of the Big Data analysis, and those levels can be understood as indicators of trustworthiness and soundness of the results of the Big Data analysis.",2016,,https://doi.org/10.1016/j.future.2015.11.024
1340,Yuri A.W. Shardt and Xu Yang and Kevin Brooks and Andrei Torgashov,Data Quality Assessment for System Identification in the Age of Big Data and Industry 4.0,"data quality assessment, system identification, big data, Industry 4.0, soft sensors","As the amount of data stored from industrial processes increases with the demands of Industry 4.0, there is an increasing interest in finding uses for the stored data. However, before the data can be used its quality must be determined and appropriate regions extracted. Initially, such testing was done manually using graphs or basic rules, such as the value of a variable. With large data sets, such an approach will not work, since the amount of data to tested and the number of potential rules is too large. Therefore, there is a need for automated segmentation of the data set into different components. Such an approach has recently been proposed and tested using various types of industrial data. Although the industrial results are promising, there still remain many unanswered questions including how to handle a priori knowledge, over- or undersegmentation of the data set, and setting the appropriate thresholds for a given application. Solving these problems will provide a robust and reliable method for determining the data quality of a given data set.",2020,,https://doi.org/10.1016/j.ifacol.2020.12.103
1341,Victor O.K. Li and Jacqueline C.K. Lam and Yang Han and Kenyon Chow,A Big Data and Artificial Intelligence Framework for Smart and Personalized Air Pollution Monitoring and Health Management in Hong Kong,"Air Pollution Monitoring, Health Management, Artificial Intelligence, Big Data, PM, Personalization, Smart Behavioural Intervention, Health and Well-being Improvement","All people in the world are entitled to enjoy a clean environment and a good quality of life. With big data and artificial intelligence technologies, it is possible to estimate personalized air pollution exposure and synchronize it with activity, health, quality of life and behavioural data, and provide real-time, personalized and interactive alert and advice to improve the health and well-being of individual citizens. In this paper, we propose an overarching framework outlining five major challenges to personalized air pollution monitoring and health management, and respective methodologies in an integrated interdisciplinary manner. First, urban air quality data is sparse, rendering it difficult to provide timely personalized alert and advice. Second, collected data, especially those involving human inputs such as health perception, are often missing and erroneous. Third, the data collected are heterogeneous, and highly complex, not easily comprehensible to facilitate individual and collective decision-making. Fourth, the causal relationships between personal air pollutants exposure (specifically, PM2.5 and PM1.0 and NO2) and personal health conditions, and health-related quality of life perception, of young asthmatics and young healthy citizens in Hong Kong (HK), are yet to be established. Fifth, whether personalized and smart information and advice provided can induce behavioural change and improve health and quality of life are yet to be determined. To overcome these challenges, our first novelty is to develop an AI and big data framework to estimate and forecast air quality in high temporal-spatial resolution and real-time. Our second novelty includes the deployment of mobile pollution sensor platforms to substantially improve the accuracy of estimated and forecasted air quality data, and the collection of activity, health condition and perception data. Our third novelty is the development of visualization tools and comprehensible indexes, by correlating personal exposure with four types of personal data, to provide timely, personalized pollution, health and travel alerts and advice. Our fourth novelty is determining causal relationship, if any, between personal pollutants, PM1.0 and PM2.5, NO2 exposure and personal health condition, and personal health perception, based on a clinical experiment of 150 young asthmatics and 150 young healthy citizens in HK. Our fifth novelty is an intervention study to determine if smart information, presented via our proposed visualized platform, will induce personal behavioural change. Our novel big data AI-driven approach, when integrated with other analytical approaches, provides an integrated interdisciplinary framework for personalized air pollution monitoring and health management, easily transferrable to and applicable in other domains and countries.",2021,,https://doi.org/10.1016/j.envsci.2021.06.011
1342,Yi-Chia Lee and Ying-Ting Chao and Pei-Ju Lin and Yen-Yun Yang and Yu-Cih Yang and Cheng-Chieh Chu and Yu-Chun Wang and Chin-Hao Chang and Shu-Lin Chuang and Wei-Chun Chen and Hsing-Jen Sun and Hsin-Cheng Tsou and Cheng-Fu Chou and Wei-Shiung Yang,Quality assurance of integrative big data for medical research within a multihospital system,"Big data, Electronic health record, Evidence based healthcare management, Validation study","Background
The need is growing to create medical big data based on the electronic health records collected from different hospitals. Errors for sure occur and how to correct them should be explored.
Methods
Electronic health records of 9,197,817 patients and 53,081,148 visits, totaling about 500 million records for 2006–2016, were transmitted from eight hospitals into an integrated database. We randomly selected 10% of patients, accumulated the primary keys for their tabulated data, and compared the key numbers in the transmitted data with those of the raw data. Errors were identified based on statistical testing and clinical reasoning.
Results
Data were recorded in 1573 tables. Among these, 58 (3.7%) had different key numbers, with the maximum of 16.34/1000. Statistical differences (P < 0.05) were found in 34 (58.6%), of which 15 were caused by changes in diagnostic codes, wrong accounts, or modified orders. For the rest, the differences were related to accumulation of hospital visits over time. In the remaining 24 tables (41.4%) without significant differences, three were revised because of incorrect computer programming or wrong accounts. For the rest, the programming was correct and absolute differences were negligible. The applicability was confirmed using the data of 2,730,883 patients and 15,647,468 patient-visits transmitted during 2017–2018, in which 10 (3.5%) tables were corrected.
Conclusion
Significant magnitude of inconsistent data does exist during the transmission of big data from diverse sources. Systematic validation is essential. Comparing the number of data tabulated using the primary keys allow us to rapidly identify and correct these scattered errors.",2022,,https://doi.org/10.1016/j.jfma.2021.12.024
1343,Conor John Cremin and Sabyasachi Dash and Xiaofeng Huang,Big data: Historic advances and emerging trends in biomedical research,"Big data, Big data in biomedicine, Data analytics in biomedical research, Multiomics big data, Biomedical data management, Big data in personalized medicine","Big data is transforming biomedical research by integrating massive amounts of data from laboratory experiments, clinical investigations, healthcare records, and the internet of things. Specifically, the increasing rate at which information is obtained from omics technologies (genomics, epigenomics, transcriptomics, proteomics, metabolomics, and pharmacogenomics) is providing an opportunity for future advances in personalized medicine that are paving the way to improved patient care. The recent advances in omics technologies are profoundly contributing to big data in biomedicine and are anticipated to aid in disease diagnosis and patient care management. Herein, we critically review the major computational techniques, algorithms, and their outcomes that have contributed to recent advances in big data generated from biomedical research in various complex human diseases, such as cancer and infectious diseases. Finally, we discuss trends in the field and the future directions that must be considered to advance the influence of big data on biomedical research and its translation in the healthcare industry.",2022,,https://doi.org/10.1016/j.crbiot.2022.02.004
1344,Yucheng Zhang and Shan Xu and Long Zhang and Mengxi Yang,Big data and human resource management research: An integrative review and new directions for future research,"Human resource management research, Big data, Integrative review, Inductive and deductive paradigms","The lack of sufficient big data-based approaches impedes the development of human resource management (HRM) research and practices. Although scholars have realized the importance of applying a big data approach to HRM research, clear guidance is lacking regarding how to integrate the two. Using a clustering algorithm based on the big data research paradigm, we first conduct a bibliometric review to quantitatively assess and scientifically map the evolution of the current big data HRM literature. Based on this systematic review, we propose a general theoretical framework described as “Inductive (Prediction paradigm: Data mining/Theory building) vs. Deductive (Explanation paradigm: Theory testing)”. In this framework, we discuss potential research questions, their corresponding levels of analysis, relevant methods, data sources and software. We then summarize the general procedures for conducting big data research within HRM research. Finally, we propose a future agenda for applying big data approaches to HRM research and identify five promising HRM research topics at the micro, meso and macro levels along with three challenges and limitations that HRM scholars may face in the era of big data.",2021,,https://doi.org/10.1016/j.jbusres.2021.04.019
1345,Karen H Keddy and Senjuti Saha and Samuel Kariuki and John Bosco Kalule and Farah Naz Qamar and Zoya Haq and Iruka N Okeke,Using big data and mobile health to manage diarrhoeal disease in children in low-income and middle-income countries: societal barriers and ethical implications,,"Summary
Diarrhoea is an important cause of morbidity and mortality in children from low-income and middle-income countries (LMICs), despite advances in the management of this condition. Understanding of the causes of diarrhoea in children in LMICs has advanced owing to large multinational studies and big data analytics computing the disease burden, identifying the important variables that have contributed to reducing this burden. The advent of the mobile phone has further enabled the management of childhood diarrhoea by providing both clinical support to health-care workers (such as diagnosis and management) and communicating preventive measures to carers (such as breastfeeding and vaccination reminders) in some settings. There are still challenges in addressing the burden of diarrhoeal diseases, such as incomplete patient information, underrepresented geographical areas, concerns about patient confidentiality, unequal partnerships between study investigators, and the reactive approach to outbreaks. A transparent approach to promote the inclusion of researchers in LMICs could address partnership imbalances. A big data umbrella encompassing cloud-based centralised databases to analyse interlinked human, animal, agricultural, social, and climate data would provide an informative solution to the development of appropriate management protocols in LMICs.",2022,,https://doi.org/10.1016/S1473-3099(21)00585-5
1346,Awad Elsayed Awad Ibrahim and Ahmed A. Elamer and Amr Nazieh Ezat,The convergence of big data and accounting: innovative research opportunities,"Big data, Analytics, Accounting, Data science, Business intelligence","This study aims to develop accounting standards, curriculums, and research to cope with the rapid development of big data. The study presents several potential convergence points between big data and different accounting techniques and theories. The study discusses how big data can overcome the data limitations of six accounting issues: financial reporting, performance measurement, audit evidence, risk management, corporate budgeting and activity-based techniques. It presents six exciting research questions for future research. Then, the study explains the potential convergence between big data and agency theory, stakeholders theory, and legitimacy theory. This theoretical study develops new convergence points between big data and accounting by reviewing the literature and proposing new ideas and research questions. The conclusion indicates a significant convergence between big data and accounting on the premise that data is the heart of accounting. Big data and advanced analytics have the potential to overcome the data limitations of accounting techniques that require estimations and predictions. A remarkable convergence is argued between big data and three accounting theories. Overall, the study presents helpful insights to members of the accounting and auditing community on the potential of big data.",2021,,https://doi.org/10.1016/j.techfore.2021.121171
1347,Scott A. Brave and R. Andrew Butters and Michael Fogarty,"The perils of working with big data, and a SMALL checklist you can use to recognize them","Big data, Data analysis, Economic forecasting, Selection bias, Reporting lags, High-frequency data, Real-time forecasts, Leading indicator","The use of big data to help explain fluctuations in the broader economy and key business performance indicators is now so commonplace that in some instances it has even begun to rival more traditional measures. Big data sources can very often provide advantages when compared with these more traditional data sources, but with these advantages also come potential pitfalls. We lay out a checklist called SMALL that we have developed in order to help interested parties as they navigate the big data minefield. Based on a set of five questions, the SMALL checklist should help users of big data draw justifiable conclusions and avoid making mistakes in matters of interpretation. To demonstrate, we provide several case studies that demonstrate the subtle nuances of several of these new big data sets and show how the problems they face often closely relate to age-old concerns that more traditional data sources are also forced to tackle.",2021,,https://doi.org/10.1016/j.bushor.2021.06.004
1348,Oliver Y. Tang and Alisa Pugacheva and Ankush I. Bajaj and Krissia M. {Rivera Perla} and Robert J. Weil and Steven A. Toms,The National Inpatient Sample: A Primer for Neurosurgical Big Data Research and Systematic Review,"Big data, Disparities, Health care costs, Health policy, Hospital volume, Machine learning, National Inpatient Sample, Nationwide Inpatient Sample, NIS","Objective
The National Inpatient Sample (NIS) (the largest all-payer inpatient database in the United States) is an important instrument for big data analysis of neurosurgical inquiries. However, earlier research has determined that many NIS studies are limited by common methodological pitfalls. In this study, we provide the first primer of NIS methodological procedures in the setting of neurosurgical research and review all reported neurosurgical studies using the NIS.
Methods
We designed a protocol for neurosurgical big data research using the NIS, based on our subject matter expertise, NIS documentation, and input and verification from the Healthcare Cost and Utilization Project. We subsequently used a comprehensive search strategy to identify all neurosurgical studies using the NIS in the PubMed and MEDLINE, Embase, and Web of Science databases from inception to August 2021. Studies underwent qualitative categorization (years of NIS studied, neurosurgical subspecialty, age group, and thematic focus of study objective) and analysis of longitudinal trends.
Results
We identified a canonical, 4-step protocol for NIS analysis: study population selection; defining additional clinical variables; identification and coding of outcomes; and statistical analysis. Methodological nuances discussed include identifying neurosurgery-specific admissions, addressing missing data, calculating additional severity and hospital-specific metrics, coding perioperative complications, and applying survey weights to make nationwide estimates. Inherent database limitations and common pitfalls of NIS studies discussed include lack of disease process–specific variables and data after the index admission, inability to calculate certain hospital-specific variables after 2011, performing state-level analyses, conflating hospitalization charges and costs, and not following proper statistical methodology for performing survey-weighted regression. In a systematic review, we identified 647 neurosurgical studies using the NIS. Although almost 60% of studies were reported after 2015, <10% of studies analyzed NIS data after 2015. The average sample size of studies was 507,352 patients (standard deviation = 2,739,900). Most studies analyzed cranial procedures (58.1%) and adults (68.1%). The most prevalent topic areas analyzed were surgical outcome trends (35.7%) and health policy and economics (17.8%), whereas patient disparities (9.4%) and surgeon or hospital volume (6.6%) were the least studied.
Conclusions
We present a standardized methodology to analyze the NIS, systematically review the state of the NIS neurosurgical literature, suggest potential future directions for neurosurgical big data inquiries, and outline recommendations to improve the design of future neurosurgical data instruments.",2022,,https://doi.org/10.1016/j.wneu.2022.02.113
1349,Donghui Hu and Yifan Li and Lixuan Pan and Meng Li and Shuli Zheng,A blockchain-based trading system for big data,"Big data trading, Blockchain, Smart contract, Proxy re-encryption, Price negotiation, Value reward","Data are an extremely important asset. Governments around the world encourage big data sharing and trading to promote the big data economy. However, existing data trading platforms are not fully trusted. Such platforms face the problems of a single point of failure (SPOF), opaque transactions, uncontrollability, untraceability, and issues of data privacy. Several blockchain-based big data trading methods have been proposed; however, they do not adequately address the security issues introduced by dishonesty in the data provider and data agent or the fairness of data revenue distribution and price bargaining. In this paper, we propose a blockchain-based decentralized data trading system in which data trading is completed by smart contract-based data matching, price negotiation, and reward assigning. Moreover, the proposed data trading system evaluates the data quality on the basis of three metrics, records the evaluation results in a side-chain, and distributes the data users’ application revenue to the data provider according to the evaluated data quality. We verify the security, usability, and efficiency of the proposed big data trading system.",2021,,https://doi.org/10.1016/j.comnet.2021.107994
1350,Md. Arfanuzzaman,Harnessing artificial intelligence and big data for SDGs and prosperous urban future in South Asia,"Artificial intelligence, Big data, Climate resilience, Data infrastructure, South Asia, SDG, Technological readiness, Urban transformation","Artificial intelligence (AI) and big data solutions are currently being utilized to offer low cost and efficient solutions in solving pressing urban socio-economic and environmental problems globally. The study found big data and AI have the potentiality to solve the common urban problems in South Asia and upsurge the efficiency of urban industries, increase competitiveness and productivity of the human and natural resources, reduce the cost of urban service delivery, and build climate resilience. The study has assessed the current AI and big data initiatives and technologies in mitigating the urban development challenges and their potentiality for scaling up in South Asian cities. The study also examined the latest innovations in AI and big data solutions for SDG monitoring and implementation in South Asia and their implication for transformational change. The study suggested that South Asia can harness the maximum benefit of AI and big data technologies by building big data and associated IT infrastructure, advancing research and innovations with regional cooperation, enhancing technological readiness, and eliminating week enabling conditions.",2021,,https://doi.org/10.1016/j.indic.2021.100127
1351,Maryam Ghasemaghaei and Goran Calic,Can big data improve firm decision quality? The role of data quality and data diagnosticity,"Big data utilization, Data quality, Decision quality, Data diagnosticity","Anecdotal evidence suggests that, despite the large variety of data, the huge volume of generated data, and the fast velocity of obtaining data (i.e., big data), quality of big data is far from perfect. Therefore, many firms defer collecting and integrating big data as they have concerns regarding the impact of utilizing big data on data diagnosticity (i.e., retrieval of valuable information from data) and firm decision making quality. In this study, we use the Organizational Learning Theory and Wang and Strong's data quality framework to explore the impact of processing big data on firm decision quality and the mediating role of data quality (DQ) and data diagnosticity on this relationship. We validate the proposed research model using survey data from 130 firms, obtained from data analysts and IT managers. Results confirm the critical role of DQ in increasing data diagnosticity and improving firm decision quality when processing big data; suggesting important implications for practice and theory. Findings also reveal that while big data utilization positively impacts contextual DQ, accessibility DQ, and representational DQ, interestingly, it negatively impacts intrinsic DQ. Furthermore, findings show that while intrinsic DQ, contextual DQ, and representational DQ significantly increase data diagnosticity, accessibility DQ does not influence it. Most importantly, the findings show that big data utilization does not significantly impact the quality of firm decisions and it is fully mediated through DQ and data diagnosticity. The results of this study contribute to practice by providing important guidelines for managers to improve firm decision quality through the use of big data.",2019,,https://doi.org/10.1016/j.dss.2019.03.008
1352,Giorgia Carra and Jorge I.F. Salluh and Fernando José {da Silva Ramos} and Geert Meyfroidt,Data-driven ICU management: Using Big Data and algorithms to improve outcomes,"Big data, Data mining, Machine learning, Predictive modeling, Intensive care unit","The digitalization of the Intensive Care Unit (ICU) led to an increasing amount of clinical data being collected at the bedside. The term “Big Data” can be used to refer to the analysis of these datasets that collect enormous amount of data of different origin and format. Complexity and variety define the value of Big Data. In fact, the retrospective analysis of these datasets allows to generate new knowledge, with consequent potential improvements in the clinical practice. Despite the promising start of Big Data analysis in medical research, which has seen a rising number of peer-reviewed articles, very limited applications have been used in ICU clinical practice. A close future effort should be done to validate the knowledge extracted from clinical Big Data and implement it in the clinic. In this article, we provide an introduction to Big Data in the ICU, from data collection and data analysis, to the main successful examples of prognostic, predictive and classification models based on ICU data. In addition, we focus on the main challenges that these models face to reach the bedside and effectively improve ICU care.",2020,,https://doi.org/10.1016/j.jcrc.2020.09.002
1353,Saqib Shamim and Yumei Yang and Najam Ul Zia and Mahmood Hussain Shah,Big data management capabilities in the hospitality sector: Service innovation and customer generated online quality ratings,"Big data management, Dynamic capabilities, Service innovation, Knowledge creation, Customer generated online quality rating, Hospitality","Despite the wide usage of big data in tourism and the hospitality sector, little research has been done to understand the role of organizations’ capability of managing big data in value creation. This study bridges this gap by investigating how big data management capabilities lead to service innovation and high online quality ratings. Instead of treating big data management as a whole, we access big data management capabilities at the strategic and operational level. Using a sample of 202 hotels in Pakistan, we collected the primary data for big data capabilities, knowledge creation and service innovation; the secondary data about quality rating were collected from Booking.com. Structural equation modelling through SmartPLS was used for data analysis. The results indicated that big data management capabilities lead to high online quality ratings through the mediation of knowledge creation and service innovation. We contribute to the current literature by empirically testing how strategic level big data capabilities enable the firm to add value in innovativeness and positive online quality ratings through acquiring, contextualizing, experimenting and applying big data.",2021,,https://doi.org/10.1016/j.chb.2021.106777
1354,Ana León and Óscar Pastor,Enhancing Precision Medicine: A Big Data-Driven Approach for the Management of Genomic Data,"Big Data, Genomics, Computer science, Theory and methods","The management of the exponential growth of data that Next Generation Sequencing techniques produce has become a challenge for researchers that are forced to delve into an ocean of complex data in order to extract new insights to unravel the secrets of human diseases. Initially, this can be faced as a Big Data-related problem, but the genomic data have particular and relevant challenges that make them different from other Big Data working domains. Genomic data are much more heterogeneous; they are spread in hundreds of repositories, represented in multiple formats, and have different levels of quality. In addition, getting meaningful conclusions from genomic data requires considering all of the relevant surrounding knowledge that is under continuous evolution. In this scenario, the precise identification of what makes Genome Data Management so different is essential in order to provide effective Big Data-based solutions. Genomic projects require dealing with the technological problems associated with data management, nomenclature standards, and quality issues that only robust Information Systems that use Big Data techniques can provide. The main contribution of this paper is to present a Big Data-driven approach for managing genomic data, that is adapted to the particularities of the domain and to show its applicability to improve genetic diagnoses, which is the core of the development of accurate Precision Medicine.",2021,,https://doi.org/10.1016/j.bdr.2021.100253
1355,Rong Gu and Yang Qi and Tongyu Wu and Zhaokang Wang and Xiaolong Xu and Chunfeng Yuan and Yihua Huang,SparkDQ: Efficient generic big data quality management on distributed data-parallel computation,"Parallel data quality algorithms, Distributed system, Data quality management system, Multi-tasks scheduling, Big data","In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.",2021,,https://doi.org/10.1016/j.jpdc.2021.05.012
1356,M. TALHA and A. ABOU {EL KALAM} and N. ELMARZOUQI,Big Data: Trade-off between Data Quality and Data Security,"Big Data, Data Quality, Data Security, Trade-off between Quality, Security","The essence of an information system lies in the data; if it is not of good quality or not sufficiently protected, the consequences will undoubtedly be harmful. Quality and Security are two essential aspects that add value to data and their implementation has become a real need and must be adopted before any data exploitation. Due to the high volume of data, their diversity and their rapid generation, effective implementation of such systems requires well thought out mechanisms and strategies. This paper provides an overview of Data Quality and Data Security in a Big Data context. We want through this paper to highlight the conflicts that may exist during the implementation of data security and data quality management systems. Such a conflict makes the complexity greater and requires new adapted solutions.",2019,,https://doi.org/10.1016/j.procs.2019.04.127
1357,Haosheng Huang and Xiaobai Angela Yao and Jukka M. Krisp and Bin Jiang,"Analytics of location-based big data for smart cities: Opportunities, challenges, and future directions","Location-based big data (LocBigData), Smart cities, Data analytics, State-of-the-art review, Research agenda, Geodata","The growing ubiquity of location/activity sensing technologies and location-based services (LBS) has led to a large volume and variety of location-based big data (LocBigData), such as location tracking or sensing data, social media data, and crowdsourced geographic information. The increasing availability of such LocBigData has created unprecedented opportunities for research on urban systems and human environments in general. In this article, we first review the common types of LocBigData: mobile phone network data, GPS data, Location-based social media data, LBS usage/log data, smart card travel data, beacon log data (WiFi or Bluetooth), and camera imagery data. Secondly, we describe the opportunities fueled by LocBigData for the realization of smart cities, mainly via answering questions ranging from “what happened” and “why did it happen” to “what's likely to happen in the future” and “what to do next”. Thirdly, pitfalls of dealing with LocBigData are summarized, such as high volume/velocity/variety; non-random sampling; messy and not clean data; and correlations rather than causal relationships. Finally, we review the state-of-the-art research trends in this field, and conclude the article with a list of open research challenges and a research agenda for LocBigData research to help achieve the vision of smart and sustainable cities.",2021,,https://doi.org/10.1016/j.compenvurbsys.2021.101712
1358,Ahmed Mostefaoui and Mohammed Amine Merzoug and Amir Haroun and Anthony Nassar and François Dessables,Big data architecture for connected vehicles: Feedback and application examples from an automotive group,"Connected vehicles, V2I communication, Automotive big data, Big data architecture, Groupe PSA","Nowadays, using their onboard built-in sensors and communication devices, connected vehicles (CVs) can perform numerous measurements (speed, temperature, fuel consumption, etc.) and transmit them, in a real-time fashion, to dedicated infrastructure, usually via 4G/5G wireless communications. This raises many opportunities to develop new innovative telematics services, including, among others, driver safety, customer experience, location-based services and infotainment. Indeed, it is expected that there will be roughly 2 billion connected cars by the end of 2025 on the world’s roadways, where each of which can produce up to 30 terabytes of data per day. Managing this big automotive data, in real and batch modes, imposes tight constraints on the underlying data management platform. To contribute to this research area, in this paper, we report on a real, in-production automotive big data platform; specifically, the one deployed by Groupe PSA (a French car manufacturer known also as Peugeot-Citroën). In particular, we present the technologies and open-source products used within the different components of this CV platform to gather, store, process, and leverage big automotive data. The proposed architecture is then assessed through realistic experiments, and the obtained results are reported and analyzed. Finally, we also provide examples of deployed automotive applications and reveal the implementation details of one of them (an eco-driving service).",2022,,https://doi.org/10.1016/j.future.2022.04.020
1359,Dan Yang and Zihan Su and Min Zhao,Big data and reference intervals,"Indirect method, Reference interval, Data pre-processing, Verification, Big data","Although reference intervals (RIs) play an important role in clinical diagnosis, there remain significant differences with respect to race, gender, age and geographic location. Accordingly, the Clinical Laboratory Standards Institute (CLSI) EP28-A3c has recommended that clinical laboratories establish RIs appropriate to their subject population. Unfortunately, the traditional and direct approach to establish RIs relies on the recruitment of a sufficient number of healthy individuals of various age groups, collection and testing of large numbers of specimens and accurate data interpretation. The advent of the big data era has, however, created a unique opportunity to “mine” laboratory information. Unfortunately, this indirect method lacks standardization, consensus support and CLSI guidance. In this review we provide a historical perspective, comprehensively assess data processing and statistical methods, and post-verification analysis to validate this big data approach in establishing laboratory specific RIs.",2022,,https://doi.org/10.1016/j.cca.2022.01.001
1360,Alexander Binder and Eva-Maria Iwer and Werner Quint,Big Data Management Using Ontologies for CPQ Solutions,"CPQ, Semantic Technologies, Ontologies, Ontology Matching, Data Quality","In recent years, due to a progressive complexity of handling and processing business data, proper big data management has become a challenge, especially for SMEs that have limited resources for investing in the requested business transformation process. As a solution, we suggest an ontology-based CPQ software approach, where we show how the implementation of semantic technologies and ontologies affects data integration processes. We also propose a method called ""ontology-based data matching"", which allows the semiautomatic generation of alignments used to formalize the coherence between ontologies. The proposed method will ensure consistency during integration, significantly improving the productivity of enterprises.",2020,,https://doi.org/10.1016/j.promfg.2020.11.051
1361,Mouna Rhahla and Sahar Allegue and Takoua Abdellatif,Guidelines for GDPR compliance in Big Data systems,"The General Data Protection Regulation (GDPR), Big Data analytics, Privacy, Security","The implementation of the GDPR that aims at protecting European citizens’ privacy is still a real challenge. In particular, in Big Data systems where data are voluminous and heterogeneous, it is hard to track data evolution through its complex life cycle ranging from collection, ingestion, storage and analytics. In this context, from 2016 to 2021 research has been conducted and several security tools designed. However, they are either specific to particular applications or address partially the regulation articles. To identify the covered parts, the missed ones and the necessary metrics for comparing different works, we propose a framework for GDPR compliance. The framework identifies the main components for the regulation implementation by mapping requirements aligned with GDPR’s provisions to IT design requirements. Based on this framework, we compare the main GDPR solutions in the Big Data domain and we propose a guideline for GDPR verification and implementation in Big Data systems.",2021,,https://doi.org/10.1016/j.jisa.2021.102896
1362,Victor O.K. Li and Jacqueline C.K. Lam and Jiahuan Cui,AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making,,"AI and big data technologies have been increasingly deployed to process complex, heterogeneous, high-resolution environmental data, and generate results at greater speeds and higher accuracies to facilitate environmental decision-making. However, current attempts to develop reliable AI and big data technologies for environmental decision-making are still inadequate. In this special issue, AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making, we attempt to address the following important questions: What are the conditions for AI and big data technologies to facilitate environmental decision-making? How can AI and big data be used to facilitate environmental decision-making? Do AI and big data serve those most at risk of environmental pollution? Who should own and govern AI and big data? This special issue brings together researchers in relevant fields of AI and environmental science to address these pertinent questions. First, we will review the existing works which attempt to address these four questions. Second, we summarize the significance and novelty of six articles included in our special issue in addressing these four questions. Finally, we highlight the important principles of AI for Social Good, which can help distinguish good from bad environmental decisions based on AI and big data technologies.",2021,,https://doi.org/10.1016/j.envsci.2021.09.001
1363,Carla Wilkin and Aldónio Ferreira and Kristian Rotaru and Luigi Red Gaerlan,Big data prioritization in SCM decision-making: Its role and performance implications,"Big data, Big data availability, Big data prioritization, Supply chain management, Performance","Given exponential growth in the size of big data, its multi-channel sources and variability in quality that create challenges concerning cost-effective use, firms have invested significantly in databases and analytical tools to inform decision-making. In this regard, one means to avoid the costs associated with producing less than insightful reports and negative effects on performance through wasted resources is prioritizing data in terms of relevance and quality. The aim of this study is to investigate this approach by developing and testing a scale to evaluate Big Data Availability and the role of Big Data Prioritization for more effective use of big data in decision-making and performance. Focusing on the context of supply chain management (SCM), we validate this scale through a survey involving 84 managers. Findings support a positive association between Big Data Availability and its use in SCM decision-making, and suggest that Big Data Prioritization, as conceptualized in the study, has a positive impact on the use of big data in SCM decision-making and SCM performance. Through developing a scale to evaluate association between Big Data Availability and use in SCM decision-making, we make an empirical contribution to value generation from big data.",2020,,https://doi.org/10.1016/j.accinf.2020.100470
1364,Kasuni Weerasinghe and Shane L. Scahill and David J. Pauleen and Nazim Taskin,Big data analytics for clinical decision-making: Understanding health sector perceptions of policy and practice,"Analytics, Clinical decision-making, Big data, Healthcare, Social representation theory","The introduction and use of ‘big data and analytics’ is an on-going issue of discussion in health sectors globally. Healthcare systems of developed countries are trying to create more value and better healthcare through data and use of big data technologies. With an increasing number of articles identifying the value creation of big data and analytics for clinical decision-making, this paper examines how big data is applied, or not applied, in clinical practice. Using social representation theory as a theoretical foundation the paper explores people's perceptions of big data across all levels (policy making, planning, funding, and clinical care) of the New Zealand healthcare sector. The findings show that although adoption of big data technologies is planned for population health and health management, the potential of big data for clinical care has yet to be explored in the New Zealand context. The findings also highlight concern over data quality. The paper provides recommendations for policy and practice particularly around the need for engagement and participation of all levels to discuss data quality as well as big-data-based changes such as precision medicine and technology-assisted clinical decision-making tools. Future avenues of research are suggested.",2022,,https://doi.org/10.1016/j.techfore.2021.121222
1365,Devendra Kumar Mishra and Arvind Kumar Upadhyay and Sanjiv Sharma,An efficient approach for manufacturing process using Big data analytics,"Manufacturing process, Bigdata, Structured data, Unstructured data","Manufacturing is a technique that produce finished goods after taking supplies, raw materials and ingredients. Manufacturing process is frequently used to produce food, chemicals and other things those have very important place in human’s life. This manufacturing process involve data for analysis and management of the process, but in current scenario the data that are generated by the process is increasing day by day. This huge amount of data in known as big data. Big data is difficult to handle by traditional data management tools. Data that are generated by the manufacturing process collected by the logs records and may be as structured or unstructured. Generally analysis is performed by structured data. Unstructured data also provide good insights in the manufacturing process if analysed in proper manner. This paper involved an efficient approach of big data analysis for manufacturing process.",2021,,https://doi.org/10.1016/j.matpr.2021.05.146
1366,Jaqueline de Godoy and Kathrin Otrel-Cass and Kristian Høyer Toft,Transformations of trust in society: A systematic review of how access to big data in energy systems challenges Scandinavian culture,"Surveillance capitalism, Smart meters, Energy transition, Trust, Data Ethics, Big Data","In the era of information technology and big data, the extraction, commodification, and control of personal information is redefining how people relate and interact. However, the challenges that big data collection and analytics can introduce in trust-based societies, like those of Scandinavia, are not yet understood. For instance, in the energy sector, data generated through smart appliances, like smart metering devices, can have collateral implications for the end-users. In this paper, we present a systematic review of scientific articles indexed in Scopus to identify possible relationships between the practices of collecting, processing, analysing, and using people's data and people's responses to such practices. We contextualise this by looking at research about Scandinavian societies and link this to the academic literature on big data and trust, big data and smart meters, data ethics and the energy sector, surveillance capitalism, and subsequently performing a reflexive thematic analysis. We broadly situate our understanding of culture in this context on the interactions between cognitive norms, material culture, and energy practices. Our analysis identified a number of articles discussing problems and solutions to do with the practices of surveillance capitalism. We also found that research addresses these challenges in different ways. While some research focuses on technological amendments to address users’ privacy protection, only few examine the fundamental ethical questions that discuss how big data practices may change societies and increase their vulnerability. The literature suggests that even in highly trusting societies, like the ones found in Scandinavian countries, trust can be undermined and weakened.",2021,,https://doi.org/10.1016/j.egyai.2021.100079
1367,Danilo Ardagna and Cinzia Cappiello and Walter Samá and Monica Vitali,Context-aware data quality assessment for big data,"Data quality, Big data, Context-awareness, Data profiling, DQ assessment","Big data changed the way in which we collect and analyze data. In particular, the amount of available information is constantly growing and organizations rely more and more on data analysis in order to achieve their competitive advantage. However, such amount of data can create a real value only if combined with quality: good decisions and actions are the results of correct, reliable and complete data. In such a scenario, methods and techniques for the Data Quality assessment can support the identification of suitable data to process. If for traditional database numerous assessment methods are proposed, in the Big Data scenario new algorithms have to be designed in order to deal with novel requirements related to variety, volume and velocity issues. In particular, in this paper we highlight that dealing with heterogeneous sources requires an adaptive approach able to trigger the suitable quality assessment methods on the basis of the data type and context in which data have to be used. Furthermore, we show that in some situations it is not possible to evaluate the quality of the entire dataset due to performance and time constraints. For this reason, we suggest to focus the Data Quality assessment only on a portion of the dataset and to take into account the consequent loss of accuracy by introducing a confidence factor as a measure of the reliability of the quality assessment procedure. We propose a methodology to build a Data Quality adapter module, which selects the best configuration for the Data Quality assessment based on the user main requirements: time minimization, confidence maximization, and budget minimization. Experiments are performed by considering real data gathered from a smart city case study.",2018,,https://doi.org/10.1016/j.future.2018.07.014
1368,Subhajit Pal and Sudip Mondal and Gourab Das and Sunirmal Khatua and Zhumur Ghosh,Big data in biology: The hope and present-day challenges in it,"Big data, Cloud computing, Bioinformatics, High throughput data, MapReduce, Machine learning","The wave of new technologies has opened up the opportunity for cost-effective generation of high-throughput profiles of biological systems. This is generating tons of biological data. It is thus leading us towards the “big data” era which is creating a pressing need to bridge the gap between high-throughput technological development and our ability for managing, analyzing, and integrating the biological big data. To harness the maximum out of it, sufficient expertise needs to be developed for big data management and analysis. In this review, we discuss the challenges related to storage, transfer, access and analysis of unstructured and structured biological big data. Subsequently, it provides a comprehensive summary regarding the important strategies adopted for biological big data management which includes a discussion on all the recently used tools or software built for high throughput processing and analysis of biological big data. Finally it discusses the future perspectives of big data bioinformatics.",2020,,https://doi.org/10.1016/j.genrep.2020.100869
1369,Faheem Ullah and M. Ali Babar,On the scalability of Big Data Cyber Security Analytics systems,"Big data, Cyber security, Adaptation, Scalability, Configuration parameter, Spark","Big Data Cyber Security Analytics (BDCA) systems use big data technologies (e.g., Apache Spark) to collect, store, and analyse a large volume of security event data for detecting cyber-attacks. The volume of digital data in general and security event data in specific is increasing exponentially. The velocity with which security event data is generated and fed into a BDCA system is unpredictable. Therefore, a BDCA system should be highly scalable to deal with the unpredictable increase/decrease in the velocity of security event data. However, there has been little effort to investigate the scalability of BDCA systems to identify and exploit the sources of scalability improvement. In this paper, we first investigate the scalability of a Spark-based BDCA system with default Spark settings. We then identify Spark configuration parameters (e.g., execution memory) that can significantly impact the scalability of a BDCA system. Based on the identified parameters, we finally propose a parameter-driven adaptation approach, SCALER, for optimizing a system's scalability. We have conducted a set of experiments by implementing a Spark-based BDCA system on a large-scale OpenStack cluster. We ran our experiments with four security datasets. We have found that (i) a BDCA system with default settings of Spark configuration parameters deviates from ideal scalability by 59.5% (ii) 9 out of 11 studied Spark configuration parameters significantly impact scalability and (iii) SCALER improves the BDCA system's scalability by 20.8% compared to the scalability with default Spark parameter setting. The findings of our study highlight the importance of exploring the parameter space of the underlying big data framework (e.g., Apache Spark) for scalable cyber security analytics.",2022,,https://doi.org/10.1016/j.jnca.2021.103294
1370,Imane El Alaoui and Youssef Gahi,The Impact of Big Data Quality on Sentiment Analysis Approaches,"Big Data Quality Metrics, Big Data Value Chain, Big Data, Big Social Data, Sentiment Analysis, Opinion Mining","Human beings share their good or bad opinions about subjects, products, and services through internet and social networks. The ability to effectively analyze this kind of information is now seen as a key competitive advantage to better inform decisions. In order to do so, organizations employ Sentiment Analysis (SA) techniques on these data. However, the usage of social media around the world is ever-increasing, which considerably accelerates massive data generation and makes traditional SA systems unable to deliver useful insights. Such volume of data can be efficiently analyzed using the combination of SA techniques and Big Data technologies. In fact, big data is not a luxury but an essential necessary to make valuable predictions. However, there are some challenges associated with big data such as quality that could highly affect the SA systems’ accuracy that use huge volume of data. Thus, the quality aspect should be addressed in order to build reliable and credible systems. For this, the goal of our research work is to consider Big Data Quality Metrics (BDQM) in SA that rely of big data. In this paper, we first highlight the most eloquent BDQM that should be considered throughout the Big Data Value Chain (BDVC) in any big data project. Then, we measure the impact of BDQM on a novel SA method accuracy in a real case study by giving simulation results.",2019,,https://doi.org/10.1016/j.procs.2019.11.007
1371,Fadi Muheidat and Dhaval Patel and Spandana Tammisetty and Lo’ai A. Tawalbeh and Mais Tawalbeh,Emerging Concepts Using Blockchain and Big Data,"Blockchain, Bigdata, Data analytics, Smart Cities applications, Healthcare","Blockchain and big data are the emerging technologies that are on the highest agendas of the firms. These are significantly expected to transform the ways in which the business as well as the firm run. These are on the verge of increasing the expectation of the distributed ledgers, which would keep the firms away from struggling challenges. The concept of big data and Blockchain has been used up by various other concepts that would help secure and interpret the information. The ideal solutions offered by these technologies shall address the challenges of big data management as well as for analytics. In addition to that, Blockchain provides its own consensus method, which is the primary means to create an audit trail. This enables users to verify all transactions. The audit trail is a means of verifying the correctness and integrity of every transaction, regardless of who owns the asset. The Blockchain can also verify that different parties of a transaction are following an agreement and not breaking the agreement. Moreover, there have been continuous arguments in the concept of Blockchain at which the bitcoin is fundamental and there are several popular blockchain approaches developed which would deliver performance, security as well as privacy. Apart from this, the use of Blockchain plays a major role in adding an extra data layer for the big data analytics process. Big data is considered secure which cannot be further forged with the network architecture. The current paper shall be discussing the emerging concepts that are using Blockchain as well as big data.",2022,,https://doi.org/10.1016/j.procs.2021.12.206
1372,Ling Tang and Jieyi Li and Hongchuan Du and Ling Li and Jun Wu and Shouyang Wang,Big Data in Forecasting Research: A Literature Review,"Big data, Forecasting, Literature review, Prediction models, Information","With the boom in Internet techniques and computer science, a variety of big data have been introduced into forecasting research, bringing new knowledge and improving prediction models. This paper is the first attempt to conduct a literature review on full-scale big data in forecasting research. By source, big data in forecasting research fell into user-generated content data (from the users on social media in texts, photos, etc.), device-monitored data (by meteorological monitors, smart meters, GPS, etc.) and activity log data (for web searching/visiting, online/offline marketing, clinical treatments, laboratory experiments, etc.). Different data types, bearing distinctive information and characteristics, dominated different forecasting tasks, required different analysis technologies and improved different forecasting models. This survey provides an overall review of big data-based forecasting research, details what (regarding data types and sources), where (forecasting hotspots) and how (analysis and forecasting methods used) big data improved prediction, and offers insights into future prospects.",2022,,https://doi.org/10.1016/j.bdr.2021.100289
1373,Mariagrazia Fugini and Jacopo Finocchi and Paolo Locatelli,A Big Data Analytics Architecture for Smart Cities and Smart Companies,"Big Data platforms, Unstructured data, Text analytics, Machine learning, Virtual enterprises, Smart communities","This paper presents the approach to Big Data Analytics (BDA) developed in the SIBDA (Sistema Innovativo Big Data Analytics) Project. The project aim is to study and develop innovative solutions in the field of BDA for three companies cooperating in a temporary association of enterprises. We discuss elements of Big Data tackled in the project, namely document processing, mass e-mail applications and Internet of Things sensor networks, to be integrated into a shared platform of common assets and services for the three cooperating companies. We comment about the “Big Data Journey” status in Italy reported by Osservatorio Politecnico di Milano. Then, the paper presents the SIBDA project approach and requirements, outlines the adopted architecture and provides implementation hints, along with some experiments and considerations on the use of the proposed architecture for Smart Cities and Smart Enterprises and Communities.",2021,,https://doi.org/10.1016/j.bdr.2021.100192
1374,Chunhui Wen and Jinhai Yang and Liu Gan and Yang Pan,Big data driven Internet of Things for credit evaluation and early warning in finance,"Internet of Things finance, Credit evaluation and early warning, Factor analysis, Particle swarm optimization, Big data driven","The big data technology framework has been successfully used in the Internet of Things, and the financial industry also hopes to use the advanced technology of big data to integrate and improve internal and external data related to credit risks. Relying on more efficient machine learning algorithms to get a reasonable prediction of credit risk can reduce the self-generated losses of the Internet of Things finance and increase profits. This article uses distributed search engine technology to customize web crawlers to obtain the required bank card and transaction data from the multi-source heterogeneous data of the Internet of Things financial industry, design the corresponding Spark parallel algorithm to preprocess the data, and establish an inverted table and two Level index file provides data source for big data analysis platform. After the data source is determined, the Mutually Exclusive Collectively Exhaustive (MECE) analysis method is combined with the scores of many financial business experts in the industry to obtain a set of candidate indicators and quantification methods for the financial credit risk evaluation of the Internet of Things, and analyze the correlation of indicators and risk grading. The random forest algorithm in the big data machine learning library is used to select the feature of the candidate index set, and a multi-level spatial association rule algorithm based on the Hash structure is designed to mine the financial risk information of the Internet of Things, and build a credit risk assessment and intelligent early warning model. This paper selects 26 indicators of Internet of Things finance as the research objects, uses SPSS26.0 software to perform sample Kaiser–Meyer–Olkin (KMO) test and Bartlett sphere test on the original data, and describes the results of factor analysis in detail. The particle swarm algorithm is introduced into the parameter optimization of random forest, and the financial credit risk assessment model of the Internet of Things is established. The results show that this method can significantly reduce the probability of banks making the first and second error rates when evaluating the credit risk of financing the Internet of Things Finance. This is conducive to the smooth development of the Internet of Things financial business for banks, which enables banks to enhance their own profitability while effectively reducing losses due to incorrect credit provision.",2021,,https://doi.org/10.1016/j.future.2021.06.003
1375,Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar,From Big Data to Smart Data: Application to performance management,"Big Data, Smart Data, Performance Management","In the context of digitalization, some companies are considering a transition to Industry 4.0 to ensure greater flexibility, productivity and responsiveness. The implementation of a relevant performance management system is then a real necessity to measure the degree of achievement of these objectives. In the era of Industry 4.0, the potential access to large amounts of data, i.e. Big Data, poses new challenges to the design and implementation of these systems. With the exponential growth of data generated from different sources, there is a need for extensive exploitation of data for performance management. Given the large volume of data, the speed at which it is generated and the variety of data sources, the manufacturing sector is facing with the challenge of creating value from large data sets. This paper introduces some potential benefits of Big Data for business and in particular its role in performance management systems. However, the key idea is that Big Data are not always neither available nor necessary. Authors focus on the concept of smart data, the result of the transformation of Big Data, and define a set of necessary and sufficient conditions the data should satisfy to be considered as Smart. The paper presents some methods of smart data extraction. Such smart data will be used to feed the performance management system in order to obtain more accurate, timely and representative key performance indicators.",2021,,https://doi.org/10.1016/j.ifacol.2021.08.100
1376,Shadi Shahoud and Moritz Winter and Hatem Khalloof and Clemens Duepmeier and Veit Hagenmeyer,An extended Meta Learning Approach for Automating Model Selection in Big Data Environments using Microservice and Container Virtualizationz Technologies,"Meta learning, Machine learning, Microservice, Web-based applications, Big data","For a given specific machine learning task, very often several machine learning algorithms and their right configurations are tested in a trial-and-error approach, until an adequate solution is found. This wastes human resources for constructing multiple models, requires a data analytics expert and is time-consuming. Meta learning addresses these problems and supports non-expert users by recommending a promising learning algorithm based on meta features computed from a given dataset. In the present paper, a new concept for enhancing the predictive performance of meta learning classification models by generating new meta examples is introduced. Our concept is realized and evaluated in a microservice-based meta learning framework. This framework makes use of a powerful Big Data software stack, container visualization, modern web technologies and a microservice architecture. In this demonstration and for evaluation purpose, time series model selection is taken as a use case for applying meta learning. It is shown that the proposed microservice-based meta learning framework introduces an excellent performance in assigning the adequate forecasting model for the chosen time series datasets. Moreover, our new concept for generating new meta examples enhances the predictive performance of the meta learner up to 16.77% and 27.07% in the case of using the original and encoded representation forms of meta features respectively. The recommendation of the most appropriate forecasting model results in a well acceptable low framework overhead demonstrating that the framework can provide an efficient approach to solve the problem of model selection in the context of Big Data.",2021,,https://doi.org/10.1016/j.iot.2021.100432
1377,Vinaya Keskar and Jyoti Yadav and Ajay Kumar,Perspective of anomaly detection in big data for data quality improvement,"Credit card, Validation, LUHN, Big data, Bank","The period of Big Data examination has started in many businesses inside created nations. With expanding headway of Internet technology, expanding measures of data are spilling into contemporary associations. Data are getting bigger and more muddled because of the nonstop age of data from numerous gadgets and sources. In this investigation, we have examined the banking area's inconsistencies because of big data technology, inconsistencies in credit card and afterwards the path how to remove these inconsistencies.",2022,,https://doi.org/10.1016/j.matpr.2021.05.597
1378,Vikrant Sharma and Atul Kumar and Mukesh Kumar,A framework based on BWM for big data analytics (BDA) barriers in manufacturing supply chains,"Big data analytics, Barriers, Manufacturing supply chains, Best worst method (BWM)","Due to its potential utility, Big Data (BD) recently attracted researchers and practitioners in decision-making. Big Data analytics (BDA) becomes more common among manufacturing companies because it lets them gain insight and make decisions based on BD. Given the importance of both BD and BDA, this study aims to identify and analyse essential BDA adoption barriers in supply chains. This study explores the current knowledge base using a BWM (Best Worst Method) to discuss these barriers. Data were obtained from five Indian manufacturing companies. Research findings show that data-related barriers are most significant. The findings will help managers understand the exact nature of the challenges and possible advantages of the BDA and implement BDA policies for the growth and output of supply chain operations.",2021,,https://doi.org/10.1016/j.matpr.2021.03.374
1379,Sonia Cisneros-Cabrera and Anna-Valentini Michailidou and Sandra Sampaio and Pedro Sampaio and Anastasios Gounaris,Experimenting with big data computing for scaling data quality-aware query processing,"Data quality-aware queries, Big data computing, Empirical evaluation","Combining query processing techniques with data quality management approaches enables enforcement of quality constraints, such as timeliness, accuracy and completeness, as part of ad-hoc query specification and execution, improving the quality of query results. Despite the emergence of novel data quality processing tools, there is a dearth of studies assessing performance and scalability in the execution of data quality assessment tasks during query processing. This paper reports on an empirical study aiming to investigate the extent to which a big data computing framework (Spark) can offer significant gains in performance and scalability when executing data quality querying tasks over a range of computational platforms including a single commodity multi-core machine and a cluster-based platform for a wide range of workloads. Our results show that substantial performance and scalability gains can be obtained by using optimized data science libraries combined with the parallel and distributed capabilities of big data computing. We also provide guidelines on choosing the appropriate computational infrastructure for executing DQ-aware queries.",2021,,https://doi.org/10.1016/j.eswa.2021.114858
1380,Jian Wang,A novel oscillation identification method for grid-connected renewable energy based on big data technology,"Oscillation identification, Big data, Evidence theory, Support vector machine","With the development of big data technology, power system has entered the era of data analysis. With the help of the massive data provided by the wide area measurement system, the power system can be easily evaluated, and the abnormal operation status can be detected and positioned. As the increase of renewable energy permeability, more new abnormal operating status have appeared in the system. Aimed at the abnormal operation state in the development of new energy, this paper proposes an oscillation location scheme based on evidence theory and support vector machine, which makes up for the limitation of single oscillation location method. The result of location analysis of oscillation energy method, oscillation phase difference method and forced oscillation phase difference location method is fused by evidence theory.",2022,,https://doi.org/10.1016/j.egyr.2022.02.022
1381,Georgios Georgiadis and Geert Poels,Towards a privacy impact assessment methodology to support the requirements of the general data protection regulation in a big data analytics context: A systematic literature review,"Big data analytics, Data protection, Data protection directive, General data protection regulation, Governance, Information security, Privacy, Privacy impact assessment, Systematic literature review","Big Data Analytics enables today's businesses and organisations to process and utilise the raw data that is generated on a daily basis. While Big Data Analytics has improved efficiency and created many opportunities, it has also increased the risk of personal data being compromised or breached. The General Data Protection Regulation (GDPR) mandates Data Protection Impact Assessment (DPIA) as a means of identifying appropriate controls to mitigate risks associated with the protection of personal data. However, little is currently known about how to conduct such a DPIA in a Big Data Analytics context. To this end, we conducted a systematic literature review with the aim of identifying privacy and data protection risks specific to the Big Data Analytics context that could negatively impact individuals' rights and freedoms when they occur. Based on a sample of 159 articles, we applied a thematic analysis to all identified risks which resulted in the definition of nine Privacy Touch Points that summarise the identified risks. The coverage of these Privacy Touch Points was then analysed for ten Privacy Impact Assessment (PIA) methodologies. The insights gained from our analysis will inform the next phase of our research, in which we aim to develop a comprehensive DPIA methodology that will enable data processors and data controllers to identify, analyse and mitigate privacy and data protection risks when storing and processing data involving Big Data Analytics.",2022,,https://doi.org/10.1016/j.clsr.2021.105640
1382,Jiadi Yin and Jinwei Dong and Nicholas A.S. Hamm and Zhichao Li and Jianghao Wang and Hanfa Xing and Ping Fu,Integrating remote sensing and geospatial big data for urban land use mapping: A review,"Integration methods, Urban functional zone classification, Urban management, Land use","Remote Sensing (RS) has been used in urban mapping for a long time; however, the complexity and diversity of urban functional patterns are difficult to be captured by RS only. Emerging Geospatial Big Data (GBD) are considered as the supplement to RS data, and help to contribute to our understanding of urban lands from physical aspects (i.e., urban land cover) to socioeconomic aspects (i.e., urban land use). Integrating RS and GBD could be an effective way to combine physical and socioeconomic aspects with great potential for high-quality urban land use classification. In this study, we reviewed the existing literature and focused on the state-of-the-art and perspective of the urban land use categorization by integrating RS and GBD. Specifically, the commonly used RS features (e.g., spectral, textural, temporal, and spatial features) and GBD features (e.g., spatial, temporal, semantic, and sequence features) were identified and analyzed in urban land use classification. The integration strategies for RS and GBD features were categorized into feature-level integration (FI) and decision-level integration (DI). To be more specific, the FI method integrates the RS and GBD features and classifies urban land use types using the integrated feature sets; the DI method processes RS and GBD independently and then merges the classification results based on decision rules. We also discussed other critical issues, including analysis unit setting, parcel segmentation, parcel labeling of land use types, and data integration. Our findings provide a retrospect of different features from RS and GBD, strategies of RS and GBD integration, and their pros and cons, which could help to define the framework for future urban land use mapping and better support urban planning, urban environment assessment, urban disaster monitoring and urban traffic analysis.",2021,,https://doi.org/10.1016/j.jag.2021.102514
1383,Saqib Shamim and Jing Zeng and Zaheer Khan and Najam Ul Zia,Big data analytics capability and decision making performance in emerging market firms: The role of contractual and relational governance mechanisms,"Big data, Contractual governance, Relational governance, Big data analytics capability, Culture, Decision-making performance, Emerging markets","This study examines the role of big data contractual and relational governance in big data decision-making performance of firms based in China. It investigates the mediation of big data analytics (BDA) capability in the association of contractual and relational governance with decision-making performance. Furthermore, moderating role of data-driven culture in the relationship of BDA capability and decision-making performance is examined. Data are collected from 108 Chinese firms engaged in big data-related activities. Structural equation modeling is employed to test the hypotheses. This study contributes towards the literature on big data management and governance mechanisms, by establishing the relationship of decision-making performance with big data contractual and relational governance directly and through the mediation of BDA capabilities. It also contributes towards knowledge based dynamic capabilities (KBDCs) view of firms, arguing that dynamic capabilities such as BDA capabilities can be influenced through knowledge sources and activities. We add to the discussions on whether contractual and relational governance are alternatives or they complement each other, by establishing the moderating role of big data relational governance in the relationship of contractual governance and decision-making performance. Finally, we argue that social capital can enhance KBDCs through contractual and relational governance in big data context.",2020,,https://doi.org/10.1016/j.techfore.2020.120315
1384,David G. Rosado and Julio Moreno and Luis E. Sánchez and Antonio Santos-Olmo and Manuel A. Serrano and Eduardo Fernández-Medina,MARISMA-BiDa pattern: Integrated risk analysis for big data,"Big data, Risk assessment, Risk analysis, Information security, Security standards","Data is one of the most important assets for all types of companies, which have undoubtedly grown their quantity and the ways of exploiting them. Big Data appears in this context as a set of technologies that manage data to obtain information that supports decision-making. These systems were not conceived to be secure, resulting in significant risks that must be controlled. Security risks in Big Data must be analyzed and managed in an appropriate manner to protect the system and secure the information and the data being handled. This paper proposes a risk analysis approach for Big Data environments, which is based on a security analysis methodology called MARISMA (Methodology for the Analysis of Risks on Information System), supported by a technological environment in the cloud (eMARISMA tool) already used by numerous clients. Both MARISMA and eMARISMA are specifically designed to be easily adapted to particular contexts, such as Big Data. Our proposal, called MARISMA-BiDa, is based on the main related standards, such as ISO/IEC 27,000 and 31,000, or the NIST Big Data reference architecture or ENISA and CSA recommendations for Big Data.",2021,,https://doi.org/10.1016/j.cose.2020.102155
1385,Ohbyung Kwon and Namyeon Lee and Bongsik Shin,"Data quality management, data usage experience and acquisition intention of big data analytics","Big data analytics, Resource-based view, Data quality management, IT capability, Data usage","Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics mainly from the theoretical perspectives of data quality management and data usage experience. Our empirical investigation reveals that a firm's intention for big data analytics can be positively affected by its competence in maintaining the quality of corporate data. Moreover, a firm's favorable experience (i.e., benefit perceptions) in utilizing external source data could encourage future acquisition of big data analytics. Surprisingly, a firm's favorable experience (i.e., benefit perceptions) in utilizing internal source data could hamper its adoption intention for big data analytics.",2014,,https://doi.org/10.1016/j.ijinfomgt.2014.02.002
1386,Jianzheng Liu and Jie Li and Weifeng Li and Jiansheng Wu,Rethinking big data: A review on the data quality and usage issues,"Big data, Data quality and error, Data ethnics, Spatial information sciences","The recent explosive publications of big data studies have well documented the rise of big data and its ongoing prevalence. Different types of “big data” have emerged and have greatly enriched spatial information sciences and related fields in terms of breadth and granularity. Studies that were difficult to conduct in the past time due to data availability can now be carried out. However, big data brings lots of “big errors” in data quality and data usage, which cannot be used as a substitute for sound research design and solid theories. We indicated and summarized the problems faced by current big data studies with regard to data collection, processing and analysis: inauthentic data collection, information incompleteness and noise of big data, unrepresentativeness, consistency and reliability, and ethical issues. Cases of empirical studies are provided as evidences for each problem. We propose that big data research should closely follow good scientific practice to provide reliable and scientific “stories”, as well as explore and develop techniques and methods to mitigate or rectify those ‘big-errors’ brought by big data.",2016,,https://doi.org/10.1016/j.isprsjprs.2015.11.006
1387,Yujie Yang and Ye Li and Runge Chen and Jing Zheng and Yunpeng Cai and Giancarlo Fortino,Risk Prediction of Renal Failure for Chronic Disease Population Based on Electronic Health Record Big Data,"Renal failure, Risk prediction, Electronic health record, Health big data, Machine learning","Renal failure is a fatal disease raising global concerns. Previous risk models for renal failure mostly rely on the diagnosis of chronic kidney disease, which lacks obvious clinical symptoms and thus is mostly undiagnosed, causing significant omission of high-risk patients. In this paper, we proposed a framework to predict the risk of renal failure directly from a big data repository of chronic disease population without prerequisite diagnosis of chronic kidney disease. The electronic health records of 42,256 patients with hypertension or diabetes in Shenzhen Health Information Big Data Platform were collected, with 398 suffered from renal failure during a 3-year follow-up. Five state-of-the-art machine learning methods are utilized to build risk prediction models of renal failure for chronic disease population. Extensive experimental results show that the proposed framework achieves quite well performance. Particularly, the XGBoost obtains the best performance with an area under receiving-operating-characteristics curve (AUC) of 0.9139. By analyzing the effect of risk factors, we identified that serum creatine, age, urine acid, systolic blood pressure, and blood urea nitrogen are the top five factors associated with renal failure risk. Compared with existing models, our model can be deployed into routine chronic disease management procedures and enable more preemptive, widely-covered screening of renal risks, which would in turn reduce the damage caused by the disease through timely intervention.",2021,,https://doi.org/10.1016/j.bdr.2021.100234
1388,Le Yao and Zhiqiang Ge,Big data quality prediction in the process industry: A distributed parallel modeling framework,"Distributed modeling, MapReduce framework, Parallel computing, Quality prediction, Big data analytics","With the ever increasing data collected from the process, the era of big data has arrived in the process industry. Therefore, the computational effort for data modeling and analytics in standalone modes has become increasingly demanding, particularly for large-scale processes. In this paper, a distributed parallel process modeling approach is presented based on a MapReduce framework for big data quality prediction. Firstly, the architecture for distributed parallel data modeling is formulated under the MapReduce framework. Secondly, a big data quality prediction scheme is developed based on the distributed parallel data modeling approach. As an example, the basic Semi-Supervised Probabilistic Principal Component Regression (SSPPCR) model is deployed to concurrently train a set of local models with split datasets. Meanwhile, Bayesian rule is utilized in a MapReduce way to integrate local models based on their predictive abilities. Two case studies demonstrate the effectiveness of the proposed method for big data quality prediction.",2018,,https://doi.org/10.1016/j.jprocont.2018.04.004
1389,Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira,Leveraging internet of things and big data analytics initiatives in European and American firms: Is data quality a way to extract business value?,"Big data analytics, Internet of things, Strategic management, Knowledge-based theory, Dynamics capability theory","Big data analytics (BDA) and the Internet of Things (IoT) tools are considered crucial investments for firms to distinguish themselves among competitors. Drawing on a strategic management perspective, this study proposes that BDA and IoT capabilities can create significant value in business processes if supported by a good level of data quality, which will lead to a better competitive advantage. Responses are collected from 618 European and American firms that use IoT and BDA applications. Partial least squares results reveal that better data quality is needed to unlock the value of IoT and BDA capabilities.",2020,,https://doi.org/10.1016/j.im.2019.01.003
1390,Jeou-Shyan Horng and Chih-Hsing Liu and Sheng-Fang Chou and Tai-Yi Yu and Da-Chian Hu,Role of big data capabilities in enhancing competitive advantage and performance in the hospitality sector: Knowledge-based dynamic capabilities view,"Knowledge-based dynamic capabilities view, Big data capabilities, Knowledge management, Sustainability marketing, Social media, Big data strategy","To address the unsolved problem of the mechanism underlying the effect of big data analytics capabilities on competitive advantage and performance, this study combined quantitative and qualitative methods to test the examined framework. The results of 257 questionnaires from hotel marketing managers and 19 semistructured interviews, confirm that big data analytics capabilities develop from big data strategies and knowledge management and enhance competitive advantage and performance through sustainability marketing. Moreover, social media enhance sustainability marketing and competitive advantage and performance. The original findings of the current research contribute to the development of big data, sustainability marketing, and social media.",2022,,https://doi.org/10.1016/j.jhtm.2022.02.026
1391,Hamza Ali and Ryad Titah,Is big data used by cities? Understanding the nature and antecedents of big data use by municipalities,"Big data use, Big data use in municipalities, Smart cities, E-government, Municipal use of IT, Digital government","It is estimated that by 2050, 70% of the population will be urban (Nations Unies, 2014). This massive urbanization has created unprecedented challenges for cities and city managers which has led many of them to look for technological solutions to address them, including the use of Big Data, which is among the most considered technological support to help improve the overall operational and service delivery of cities. It is estimated that around 7 billion connected objects will soon be implemented in cities worldwide which will produce an unprecedented and massive amount of real-time data that will have to be managed, used, and analyzed effectively. If this massive amount of data is effectively managed and used, it can provide important benefits and produce real positive impacts on the functioning of cities. Nonetheless, despite these benefits, only a few cities are able to use and exploit big data, and some studies have shown that less than 0.5% of all the available data has been explored. The objective of this study is to understand the factors that influence cities to use big data and the nature of such use. Based on a field survey involving 106 municipalities, this study investigates the antecedents of big data use by cities and shows how different sets of antecedents influence three different types of big data use by cities.",2021,,https://doi.org/10.1016/j.giq.2021.101600
1392,Shivam Gupta and Théo Justy and Shampy Kamboj and Ajay Kumar and Eivind Kristoffersen,Big data and firm marketing performance: Findings from knowledge-based view,"Big data analytics, Artificial intelligence, Marketing performance, Knowledge-based view","A universal trend in advanced manufacturing countries is defining Industry 4.0, industrialized internet and future factories as a recent wave, which may transform the production and its related services. Further, big data analytics has emerged as a game changer in the business world due to its uses for increasing accuracy in decision-making and enhancing performance of sustainable industry 4.0 applications. This study intends to emphasize on how to support Industry 4.0 with knowledge based view. For the same, a conceptual model is framed and presented with essential components that are required for a real world implementation. The study used qualitative analysis and was guided by a knowledge-based theoretical framework. Thematic analysis resulted in the identification of a number of emergent categories. Key findings highlight significant gaps in conventional decision-making systems and demonstrate how big data enhances firms’ strategic and operational decisions as well as facilitates informational access for improved marketing performance. The resulting proposed model can provide managers with a reference point for using big data to line up firms’ activities for more effective marketing efforts and presents a conceptual basis for further empirical studies in this area.",2021,,https://doi.org/10.1016/j.techfore.2021.120986
1393,Harpreet Singh,"Big data, industry 4.0 and cyber-physical systems integration: A smart industry context","Agile management, Heterogeneity, Internet-of-things, Smart factory, Smart manufacturing","The advancements in the industries have paved the way for the distributed establishment of the big data volumes, cyber-physical systems, and industrie 4.0. The perspectives of modules are integrated with the shop-floor monitoring and controlled by computational paradigms, and digital computational spaces. The performance rises after introducing an intelligent and automated manufacturing industry into the next-generation industry. The scope of this paper is to address the state-of-the-art technologies and phases such as digital twins, big data analytics, artificial intelligence, and internet-of-things. The research challenges are examined with attention on data integrity, data quality, data privacy, data availability, data scalability, data transformation, legitimate and monitoring issues, and governance. Lastly, potential research issues that need considerable research efforts are summarized. We believe that this paper is presenting the research directions for researchers in the area of smart industry towards its integration for the advancements of the industrial sector, and agile management. Some surprising development as industry 4.0 integration with socio-technical systems was found in designing the architecture of vertical, horizontal, and end-to-end integration mechanisms.",2021,,https://doi.org/10.1016/j.matpr.2020.07.170
1394,Sepideh {Bazzaz Abkenar} and Mostafa {Haghi Kashani} and Ebrahim Mahdipour and Seyed Mahdi Jameii,"Big data analytics meets social media: A systematic review of techniques, open issues, and future directions","Social networks, Big data, Content analysis, Sentiment analysis, Systematic literature review","Social Networking Services (SNSs) connect people worldwide, where they communicate through sharing contents, photos, videos, posting their first-hand opinions, comments, and following their friends. Social networks are characterized by velocity, volume, value, variety, and veracity, the 5 V’s of big data. Hence, big data analytic techniques and frameworks are commonly exploited in Social Network Analysis (SNA). By the ever-increasing growth of social networks, the analysis of social data, to describe and find communication patterns among users and understand their behaviors, has attracted much attention. In this paper, we demonstrate how big data analytics meets social media, and a comprehensive review is provided on big data analytic approaches in social networks to search published studies between 2013 and August 2020, with 74 identified papers. The findings of this paper are presented in terms of main journals/conferences, yearly distributions, and the distribution of studies among publishers. Furthermore, the big data analytic approaches are classified into two main categories: Content-oriented approaches and network-oriented approaches. The main ideas, evaluation parameters, tools, evaluation methods, advantages, and disadvantages are also discussed in detail. Finally, the open challenges and future directions that are worth further investigating are discussed.",2021,,https://doi.org/10.1016/j.tele.2020.101517
1395,Nadja B. Cech and Marnix H. Medema and Jon Clardy,Benefiting from big data in natural products: importance of preserving foundational skills and prioritizing data quality,,"ABSTRACT
Systematic, large-scale, studies at the genomic, metabolomic, and functional level have transformed the natural product sciences. Improvements in technology and reduction in cost for obtaining spectroscopic, chromatographic, and genomic data coupled with the creation of readily accessible curated and functionally annotated data sets have altered the practices of virtually all natural product research laboratories. Gone are the days when the natural products researchers were expected to devote themselves exclusively to the isolation, purification, and structure elucidation of small molecules. We now also engage with big data in taxonomic, genomic, proteomic, and/or metabolomic collections, and use these data to generate and test hypotheses. While the oft stated aim for the use of large-scale -omics data in the natural products sciences is to achieve a rapid increase in the rate of discovery of new drugs, this has not yet come to pass. At the same time, new technologies have provided unexpected opportunities for natural products chemists to ask and answer new and different questions. With this viewpoint, we discuss the evolution of big data as a part of natural products research and provide a few examples of how discoveries have been enabled by access to big data. We also draw attention to some of the limitations in our existing engagement with large datasets and consider what would be necessary to overcome them.",2021,,https://doi.org/10.1039/d1np00061f
1396,Benjamin T. Hazen and Christopher A. Boone and Jeremy D. Ezell and L. Allison Jones-Farmer,"Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications","Data quality, Statistical process control, Knowledge-based view, Organizational information processing view, Systems theory","Today׳s supply chain professionals are inundated with data, motivating new ways of thinking about how data are produced, organized, and analyzed. This has provided an impetus for organizations to adopt and perfect data analytic functions (e.g. data science, predictive analytics, and big data) in order to enhance supply chain processes and, ultimately, performance. However, management decisions informed by the use of these data analytic methods are only as good as the data on which they are based. In this paper, we introduce the data quality problem in the context of supply chain management (SCM) and propose methods for monitoring and controlling data quality. In addition to advocating for the importance of addressing data quality in supply chain research and practice, we also highlight interdisciplinary research topics based on complementary theory.",2014,,https://doi.org/10.1016/j.ijpe.2014.04.018
1397,El Mehdi Ouafiq and Rachid Saadane and Abdellah Chehri and Seunggil Jeon,AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities,"Smart Farming, Energy Harvesting Capabilities, IoT, Big Data, Agriculture 4.0, Water Management, Sustainability","The use of Internet of Things (IoT) networks offers great advantages over wired networks, especially due to their simple installation, low maintenance costs, and automatic configuration. IoT facilitates the integration of sensing and communication for various industries, including smart farming and precision agriculture. For several years, many researchers have strived to find new sources of energy that are always “cleaner” and more environmentally friendly. Energy harvesting technology is one of the most promising environment-friendly solutions that extend the lifetime of these IoT devices. In this paper, the state-of-art of IoT energy harvesting capabilities and communication technologies in smart agriculture is presented. In addition, this work proposes a comprehensive architecture that includes big data technologies, IoT components, and knowledge-based systems for innovative farm architecture. The solution answers some of the biggest challenges the agriculture industry faces, especially when handling small files in a big data environment without impacting the computation performance. The solution is built on top of a pre-defined big data architecture that includes an abstraction layer of the data lake that handles data quality following a data migration strategy to ensure the data's insights. Furthermore, in this paper, we compared several machine learning algorithms to find the most suitable smart farming analytics tools in terms of forecasting and predictions.",2022,,https://doi.org/10.1016/j.seta.2022.102093
1398,Cangyu Jin and Yamine Bouzembrak and Jiehong Zhou and Qiao Liang and Leonieke M. {van den Bulk} and Anand Gavai and Ningjing Liu and Lukas J. {van den Heuvel} and Wouter Hoenderdaal and Hans J.P. Marvin,Big Data in food safety- A review,,"The massive rise of Big Data generated from smartphones, social media, Internet of Things (IoT), and multimedia, has produced an overwhelming flow of data in either structured or unstructured format. Big Data technologies are being developed and implemented in the food supply chain that gather and analyse these data. Such technologies demand new approaches in data collection, storage, processing and knowledge extraction. In this article, an overview of the recent developments in Big Data applications in food safety are presented. This review shows that the use of Big Data in food safety remains in its infancy but it is influencing the entire food supply chain. Big Data analysis is used to provide predictive insights in several steps in the food supply chain, support supply chain actors in taking real time decisions, and design the monitoring and sampling strategies. Lastly, the main research challenges that require research efforts are introduced.",2020,,https://doi.org/10.1016/j.cofs.2020.11.006
1399,Antonio Benítez-Hidalgo and Cristóbal Barba-González and José García-Nieto and Pedro Gutiérrez-Moncayo and Manuel Paneque and Antonio J. Nebro and María del Mar Roldán-García and José F. Aldana-Montes and Ismael Navas-Delgado,TITAN: A knowledge-based platform for Big Data workflow management,"Big Data analytics, Semantics, Knowledge extraction","Modern applications of Big Data are transcending from being scalable solutions of data processing and analysis, to now provide advanced functionalities with the ability to exploit and understand the underpinning knowledge. This change is promoting the development of tools in the intersection of data processing, data analysis, knowledge extraction and management. In this paper, we propose TITAN, a software platform for managing all the life cycle of science workflows from deployment to execution in the context of Big Data applications. This platform is characterised by a design and operation mode driven by semantics at different levels: data sources, problem domain and workflow components. The proposed platform is developed upon an ontological framework of meta-data consistently managing processes and models and taking advantage of domain knowledge. TITAN comprises a well-grounded stack of Big Data technologies including Apache Kafka for inter-component communication, Apache Avro for data serialisation and Apache Spark for data analytics. A series of use cases are conducted for validation, which comprises workflow composition and semantic meta-data management in academic and real-world fields of human activity recognition and land use monitoring from satellite images.",2021,,https://doi.org/10.1016/j.knosys.2021.107489
1400,Mert Onuralp Gökalp and Ebru Gökalp and Kerem Kayabay and Selin Gökalp and Altan Koçyiğit and P. Erhan Eren,A process assessment model for big data analytics,"Big data, Data analytics, Software development, Software process improvement, Software process assessment","Today, business success is essentially powered by data-centric software. Big data analytics (BDA) grasp the potential of generating valuable insights and empowering businesses to support their strategic decision-making. However, although organizations are aware of BDAs’ potential opportunities, they face challenges to satisfy the BDA-specific processes and integrate them into their daily software development lifecycle. Process capability/ maturity assessment models are used to assist organizations in assessing and realizing the value of emerging capabilities and technologies. However, as a result of the literature review and its analysis, it was observed that none of the existing studies in the BDA domain provides a complete, standardized, and objective capability maturity assessment model. To address this research gap, we focus on developing a BDA process capability assessment model grounded on the well-accepted ISO/IEC 330xx standard series. The proposed model comprises two main dimensions: process and capability. The process dimension covers six BDA-specific processes: business understanding, data understanding, data preparation, model building, evaluation, and deployment and use. The capability dimension has six levels, from not performed to innovating. We conducted case studies in two different organizations to validate the applicability and usability of the proposed model. The results indicate that the proposed model provides significant insights to improve the business value generated by BDA via determining the current capability levels of the organizations' BDA processes, deriving a gap analysis, and creating a comprehensive roadmap for continuous improvement in a standardized way.",2022,,https://doi.org/10.1016/j.csi.2021.103585
1401,Lei Yang and Anqian Jiang and Jiahua Zhang,Optimal timing of big data application in a two-period decision model with new product sales,"Supply chain management, Optimal strategy, Big data application, Two-period model, Social welfare","We study a firm's strategy in adopting big data technology to motivate consumer demand over two periods. In the first period, the firm designs a product to sell to the market and determines whether to apply big data to attract more consumers. In the second period, the firm designs a new product and determines whether to sell the old product and the new product simultaneously, where big data can also be applied in this period to stimulate more demands. We formulate this problem into four models considering whether the firm adopts big data in the first period and/or the second period, and whether the firm only sells the new product or sells both the old and new products in the second period. We find that the firm prefers to apply big data over both periods when the cost is low, only over the second period when the cost is median and will not apply big data when the cost is high. Interestingly, only applying big data over the first period also may bring the most profits with heterogeneous big data coefficients. Furthermore, applying big data in the second period is the better choice for the social welfare.",2021,,https://doi.org/10.1016/j.cie.2021.107550
1402,Rakesh D. Raut and Vinay Surendra Yadav and Naoufel Cheikhrouhou and Vaibhav S. Narwane and Balkrishna E. Narkhede,Big data analytics: Implementation challenges in Indian manufacturing supply chains,"Big data analytics, DEMATEL, Indian manufacturing supply chains, Interpretive structural modeling, MICMAC analysis","Big Data Analytics (BDA) has attracted significant attention from both academicians and practitioners alike as it provides several ways to improve strategic, tactical and operational capabilities to eventually create a positive impact on the economic performance of organizations. In the present study, twelve significant barriers against BDA implementation are identified and assessed in the context of Indian manufacturing Supply Chains (SC). These barriers are modeled using an integrated two-stage approach, consisting of Interpretive Structural Modeling (ISM) in the first stage and Decision-Making Trial and Evaluation Laboratory (DEMATEL) in the second stage. The approach developed provides the interrelationships between the identified constructs and their intensities. Moreover, Fuzzy MICMAC technique is applied to analyze the high impact (i.e., high driving power) barriers. Results show that four constructs, namely lack of top management support, lack of financial support, lack of skills, and lack of techniques or procedures, are the most significant barriers. This study aids policy-makers in conceptualizing the mutual interaction of the barriers for developing policies and strategies to improve the penetration of BDA in manufacturing SC.",2021,,https://doi.org/10.1016/j.compind.2020.103368
1403,Jesus Silva and Omar Bonerge {Pineda Lezama} and Ligia Romero and Darwin Solano and Claudia Fernández,Risk Analysis of Using Big Data in Computer Sciences,"Data management, data quality, decision making, data analysis","Today, as technologies mature and people are encouraged to contribute data to organizations’ databases, more transactions are being captured than ever before. Meanwhile, improvements in data storage technologies have made the cost of evaluating, selecting, and destroying legacy data considerably greater than simply letting it accumulate. On the one hand, the excess of stored data has considerably increased the opportunities to interrelate and analyze them, while the moderate enthusiasm generated by data warehousing and data mining in the 1990s has been replaced by a rampant euphoria about big data and data analytics. But, is this as wonderful as seems? This paper presents a risk analysis of Big Data and Big Data Analytics based on a review of quality factors.",2019,,https://doi.org/10.1016/j.procs.2019.11.052
1404,Dan Zhang and L.G. Pee and Shan L. Pan and Lili Cui,"Big data analytics, resource orchestration, and digital sustainability: A case study of smart city development","Smart city, Big data, Digital sustainability, Resource orchestration, Socio-technical issues","Smart cities are expected to improve the efficiency and effectiveness of urban management, including public services, public security, and environmental protection, and to ultimately achieve Sustainable Development Goal (SDG) 11 for making cities inclusive, safe, resilient, and sustainable. Big data have been identified as a key enabler in the development of smart cities. However, our understanding of how different data sources should be managed and integrated remains limited. By analyzing data applications in the development of a sustainable smart city, this case study identified three phases of development, each requiring a different approach to orchestrating diverse data sources. A framework identifying the phases, data-related issues, data orchestration and its interaction with other resources, focal capabilities, and development approaches is developed. This study benefits both researchers and practitioners by making theoretical contributions and by offering practical insights in the fields of smart cities and big data.",2022,,https://doi.org/10.1016/j.giq.2021.101626
1405,Zhao-ge LIU and Xiang-yang LI and Xiao-han ZHU,scenario modeling for government big data governance decision-making: Chinese experience with public safety services,"Government big data governance, Scenario-based decision-making, Scenario modeling, Model-driven, Data link network, Public safety services","In the public safety service context, government big data governance (GBDG) is a challenging decision-making problem that encompasses uncertainties in the arenas of big data and its complex links. Modeling and collaborating the key scenario information required for GBDG decision-making can minimize system uncertainties. However, existing scenario-building methods are limited by their rigidity as they are employed in various application contexts and the associated high costs of modeling. In this paper, using a design science paradigm, a model-driven scenario modeling approach is proposed to achieve flexible scenario modeling for various applications through the transfer of generic domain knowledge. The key component of the proposed approach is a scenario meta-model that is built from existing literatures and practices by integrating qualitative, quantitative, and meta-modeling analysis. An instantiation mechanism of the scenario meta-model is also proposed to generate customized scenarios under Antecedent-Behavior-Consequence (ABC) theory. Two real-world safety service cases in Wuhan, China were evaluated to find that the proposed approach reduces GBDG decision-making uncertainties significantly by providing key information for GBDG problem identification, solution design, and solution value perception. This scenario-building approach can be further used to develop other GBDG systems for public safety services with reduced uncertainties and complete decision-making functions.",2022,,https://doi.org/10.1016/j.im.2022.103622
1406,Michal Koziara and Andrew Gaukroger and Caroline Hing and Will Eardley,Introduction to Big Data in trauma and orthopaedics,"Big Data, clinical database, GDPR, healthcare, orthopaedics, privacy","The enormous amount of data created daily within healthcare has become so complex, that it cannot be effectively handled by routine analytical methods. Such large data sets can be processed, looking for correlation not otherwise obvious in smaller patient samples. Further advances in terms of data processing as well as significant infrastructure and personnel investments are required to fully reap the benefits of Big Data, in terms of research and financial sense. However, despite its popularity and promise, Big Data in orthopaedics has attracted a number of criticisms, not only in terms of data input and processing, but particularly with regards to analysis of the output, which are explored within the article. Moreover, use of Big Data within healthcare carries the ethical question of privacy and consent.",2021,,https://doi.org/10.1016/j.mporth.2021.01.004
1407,Laura Sebastian-Coleman,Chapter 1 - The Importance of Data Quality Management,"Quality management, process management, data quality management, data governance, costs of poor quality, big data, digital transformation, data privacy, data monetization","This chapter analyzes the role of data quality management in response to the rapid evolution of data in our world. It discusses the impact of poor-quality data on organizations, focusing on the costs and risks associated with poorly managed data. In many organizations, poor-quality data is tolerated to a degree that poor-quality products would not be. Data quality management reduces the costs and risks of poor-quality data and enables the benefits and opportunities of high-quality data, especially in an age of big data, digital transformation, and artificial intelligence.",2022,,https://doi.org/10.1016/B978-0-12-821737-5.00001-8
1408,Zhimei Lei and Yandan Chen and Ming K. Lim,Modelling and analysis of big data platform group adoption behaviour based on social network analysis,"Big data, Platforms, Technology adoption, Corporate group behaviour, Social network analysis","Due to the importance of big data technology in decision-making, production and service provision, enterprises have adopted various big data technologies and platforms to improve their operational efficiency. However, the number of enterprises that have adopted big data is not promising. The purpose of this study is to explore the current status of big data adoption by Chinese enterprises and to reveal the possible factors that hinder big data adoption from the group behaviour network perspective. Based on a real case survey of 54 big data platforms (BDPs), four types of networks—i.e., the enterprise-platform network, enterprise network, platform network and industry similarity and difference (ISD) network—are constructed and analysed on the basis of social network analysis (SNA). This study finds that among Chinese enterprises, the level and scope of big data adoption are generally low and are imbalanced among industries; the cognitive level and adoption behaviour of enterprises on BDPs are inconsistent, the compatibility of BDPs is different, and the density and distance-based cohesion of networks are weak; although the current big data adoption behaviours of Chinese enterprises have formed some structural features, core-periphery structures and maximal complete cliques are found, and the current network structure has little impact on individual enterprises and platforms; enterprises in the same industry prefer to adopt the same kind of big data technology or platform. Based on these findings, several strategies and suggestions to improve big data adoption are provided.",2021,,https://doi.org/10.1016/j.techsoc.2021.101570
1409,Mayada Abd El-Aziz Youssef and Riyad Eid and Gomaa Agag,Cross-national differences in big data analytics adoption in the retail industry,"Big data analytics, Technology adoption, Diffusion of innovations model, Cross-national differences, Retail industry","Big data analytics (BDA) has emerged as a significant area of research for both researchers and practitioners in the retail industry, indicating the importance and influence of solving data-related problems in contemporary business organization. The present study utilised a quantitative-methods approach to investigate factors affecting retailers' adoption of BDA across three countries. A survey questionnaire was used to collect data from managers and decision-makers in the retail industry. Data of 2278 respondents were analysed through structural equation modelling. The findings revealed that security concerns, external support, top management support, and rational decision making culture have a greater effect on BDA adoption in developed countries UK than in UAE and Egypt. However, competition intensity and firm size have a greater effect on BDA adoption in UAE and Egypt than in UK. Finally, human variables (competence of information system's staff and staff's information system knowledge) have a greater effect on BDA adoption in Egypt than UK and UAE. The findings indicate that a “one-size-fits-all” approach is insufficient in capturing the heterogeneity of managers across countries. Implications for practice and theory were demonstrated.",2022,,https://doi.org/10.1016/j.jretconser.2021.102827
1410,Jean-Sébastien Lacam and David Salvetat,Big data and Smart data: two interdependent and synergistic digital policies within a virtuous data exploitation loop,"Big data, Smart data, Volume, Velocity, Variety, Automotive distribution","This research examines for the first time the relationship between Big data and Smart data among French automotive distributors. Many low-tech firms engage in these data policies to improve their decisions and performance through the predictive capacities of their data. A discussion emerges in the literature according to which an effective policy lies in the conversion of a mass of raw data into so-called intelligent data. In order to understand better this digital transition, we question the transformation of data policies practiced in low-tech firms through the founding model of 3Vs (Volume, Variety and Velocity of data). First of all, this empirical study of 112 French automotive distributors develops the existing literature by proposing an original and detailed typology of the data policies practiced (Low data, Big data and Smart data). Secondly, after specifying the elements of the differences between the quantitative nature of Big data and the qualitative nature of Smart data, our results reveal and analyse for the first time the existence of their synergistic relationship. Companies transform their Big data approach into Smart data when they move from massive exploitation to intelligent exploitation of their data. The phenomenon is part of a high-end loop data exploitation. Initially, the exploitation of intelligent data can only be done by extracting a sample from a large raw data pool previously made by a Big data policy. Secondly, the organization's raw data pool is in turn enriched by the repayment of contributions made by the Smart data approach. Thus, this study develops three important ways. First off, we identify, detail and compare the current data policies of a traditional industry. Secondly, we reveal and explain the evolution of digital practices within organizations that now combine both quantitative and qualitative data exploitation. Finally, our results guide decision-makers towards the synergistic and the legitimate association of different forms of data management for better performance.",2021,,https://doi.org/10.1016/j.hitech.2021.100406
1411,Anita Lee-Post and Ram Pakath,"Numerical, secondary Big Data quality issues, quality threshold establishment, & guidelines for journal policy development","Data quality, Big data, Secondary data, Numerical data, Quality threshold","An IS researcher may obtain Big Data from primary or secondary data sources. Sometimes, acquiring primary Big Data is infeasible due to availability, accessibility, cost, time, and/or complexity considerations. In this paper, we focus on Big Data-based IS research and discuss ways in which one may, post hoc, establish quality thresholds for numerical Big Data obtained from secondary sources. We also present guidelines for developing journal policies aimed at ensuring the veracity and verifiability of such data when used for research purposes.",2019,,https://doi.org/10.1016/j.dss.2019.113135
1412,N. Deepa and Quoc-Viet Pham and Dinh C. Nguyen and Sweta Bhattacharya and B. Prabadevi and Thippa Reddy Gadekallu and Praveen Kumar Reddy Maddikunta and Fang Fang and Pubudu N. Pathirana,"A survey on blockchain for big data: Approaches, opportunities, and future directions","Blockchain, Big data, Vertical applications, Smart city, Smart healthcare, Smart transportation, Security","Big data has generated strong interest in various scientific and engineering domains over the last few years. Despite many advantages and applications, there are many challenges in big data to be tackled for better quality of service, e.g., big data analytics, big data management, and big data privacy and security. Blockchain with its decentralization and security nature has the great potential to improve big data services and applications. In this article, we provide a comprehensive survey on blockchain for big data, focusing on up-to-date approaches, opportunities, and future directions. First, we present a brief overview of blockchain and big data as well as the motivation behind their integration. Next, we survey various blockchain services for big data, including blockchain for secure big data acquisition, data storage, data analytics, and data privacy preservation. Then, we review the state-of-the-art studies on the use of blockchain for big data applications in different domains such as smart city, smart healthcare, smart transportation, and smart grid. For a better understanding, some representative blockchain-big data projects are also presented and analyzed. Finally, challenges and future directions are discussed to further drive research in this promising area.",2022,,https://doi.org/10.1016/j.future.2022.01.017
1413,Mohamed Zouheir Kastouni and Ayoub {Ait Lahcen},"Big data analytics in telecommunications: Governance, architecture and use cases","Big data analytics, Big data project’s governance methodology, Big data architecture, Data governance methodology, Big data project’s team, Big data telecommunications use cases","With the upsurge of data traffic due to the change in customer behavior towards the use of telecommunications services, fostered by the current global health situation (mainly due to Covid-19), the telecommunications operators have a golden opportunity to create new sources of revenues using Big Data Analytics (BDA) solutions. Looking to setting up a BDA project, we faced several challenges, notably, in terms of choice of the technical solution from the plethora of the existing tools, and the choice of the governance methodologies for governing the project and the data. The majority of research documents related to the telecommunications industry have not addressed BDA project implementation from start to finish. The purpose of this study focuses on a BDA telecommunications project, namely, Project’s Governance, Architecture, Data Governance and the BDA Project’s Team. The last part of this study presents useful BDA use cases, in terms of applications enabling revenue creation and cost optimization. It appears that this work will facilitate the implementation of BDA projects, and enable telecommunications operators to have a better understanding about the fundamental aspects to be focused on. It is therefore, a study that will contribute positively toward such goal.",2020,,https://doi.org/10.1016/j.jksuci.2020.11.024
1414,Haitham Ghallab and Hanan Fahmy and Mona Nasr,Detection outliers on internet of things using big data technology,"Internet of things, IoT, Big data, Data quality, Outliers Detection, DBSCAN, RDDs","Internet of Things (IoT) is a fundamental concept of a new technology that will be promising and significant in various fields. IoT is a vision that allows things or objects equipped with sensors, actuators, and processors to talk and communicate with each other over the internet to achieve a meaningful goal. Unfortunately, one of the major challenges that affect IoT is data quality and uncertainty, as data volume increases noise, inconsistency and redundancy increases within data and causes paramount issues for IoT technologies. And since IoT is considered to be a massive quantity of heterogeneous networked embedded devices that generate big data, then it is very complex to compute and analyze such massive data. So this paper introduces a new model named NRDD-DBSCAN based on DBSCAN algorithm and using resilient distributed datasets (RDDs) to detect outliers that affect the data quality of IoT technologies. NRDD-DBSCAN has been applied on three different datasets of N-dimensions (2-D, 3-D, and 25-D) and the results were promising. Finally, comparisons have been made between NRDD-DBSCAN and previous models such as RDD-DBSCAN model and DBSCAN algorithm, and these comparisons proved that NRDD-DBSCAN solved the low dimensionality issue of RDD-DBSCAN model and also solved the fact that DBSCAN algorithm cannot handle IoT data. So the conclusion is that NRDD-DBSCAN proposed model can detect the outliers that exist in the datasets of N-dimensions by using resilient distributed datasets (RDDs), and NRDD-DBSCAN can enhance the quality of data exists in IoT applications and technologies.",2020,,https://doi.org/10.1016/j.eij.2019.12.001
1415,Hugo Bergier and Loïc Duron and Christelle Sordet and Lou Kawka and Aurélien Schlencker and François Chasset and Laurent Arnaud,"Digital health, big data and smart technologies for the care of patients with systemic autoimmune diseases: Where do we stand?","Autoimmune diseases, Digital technology, Big data, Delivery of health care, Telemedicine","The past decade has seen tremendous development in digital health, including in innovative new technologies such as Electronic Health Records, telemedicine, virtual visits, wearable technology and sophisticated analytical tools such as artificial intelligence (AI) and machine learning for the deep-integration of big data. In the field of rare connective tissue diseases (rCTDs), these opportunities include increased access to scarce and remote expertise, improved patient monitoring, increased participation and therapeutic adherence, better patient outcomes and patient empowerment. In this review, we discuss opportunities and key-barriers to improve application of digital health technologies in the field of autoimmune diseases. We also describe what could be the fully digital pathway of rCTD patients. Smart technologies can be used to provide real-world evidence about the natural history of rCTDs, to determine real-life drug utilization, advanced efficacy and safety data for rare diseases and highlight significant unmet needs. Yet, digitalization remains one of the most challenging issues faced by rCTD patients, their physicians and healthcare systems. Digital health technologies offer enormous potential to improve autoimmune rCTD care but this potential has so far been largely unrealized due to those significant obstacles. The need for robust assessments of the efficacy, affordability and scalability of AI in the context of digital health is crucial to improve the care of patients with rare autoimmune diseases.",2021,,https://doi.org/10.1016/j.autrev.2021.102864
1416,Michela Piccarozzi and Barbara Aquilani,The role of Big Data in the business challenge of Covid-19: a systematic literature review in managerial studies,"Big Data, Covid-19, systematic literature review, management","2020 was globally greatly affected by the Covid-19 pandemic caused by SARS-CoV-2, which is still today impacting and profoundly changing life globally for people but also for firms. In this context, the need for timely and accurate information has become vital in every area of business management. The spread of the Covid-19 global pandemic has generated an exponential increase and extraordinary volume of data. In this domain, Big Data is one of the digital innovation technologies that can support business organizations during these complex times. Based on these considerations, the aim of this paper is to analyze the managerial literature concerning the issue of Big Data in the management of the Covid-19 pandemic through a systematic literature review. The results show a fundamental role of Big Data in pandemic management for businesses. The paper also provides managerial and theoretical implications.",2022,,https://doi.org/10.1016/j.procs.2022.01.375
1417,Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon},Diagnostic analysis for outlier detection in big data analytics,"Big data, data quality, outlier, Sustainable Development Goals","Recently, Big Data analytics has been one of the most emerging topics in the business field. Data is collected, processed and analyzed to gain useful insight for their organization. Big Data analytics has the potential to improve the quality of life and help to achieve Sustainable Development Goals (SDG). To ensure that SDG goals are achieved, we must utilize existing data to meet those targets and ensure accountability. However, data quality is often left out when dealing with data. Any types of errors presented in the dataset should be properly addressed to ensure the analysis provided is accurate and truthful. In this paper, we have addressed the concept of data quality diagnosis to identify the outlier presented in the dataset. The cause of the outlier is further discussed to identify potential improvements that can be done to the dataset. In addition, recommendations to improve the quality of data and data collection systems are provided.",2022,,https://doi.org/10.1016/j.procs.2021.12.189
1418,Lilia Sfaxi and Mohamed Mehdi Ben Aissa,DECIDE: An Agile event-and-data driven design methodology for decisional Big Data projects,"Big Data, Methodology, Decisional systems, Agile, Data governance, Data quality","Decision making is the lifeblood of the enterprise — from the mundane to the strategically critical. However, the increasing deluge of data makes it more important than ever to understand and use it effectively in every context. Being “data driven” is more aspiration than reality in most organizations due to the complexity, volume, variability and velocity of data streams from every customer and employee interaction. The purpose of this paper is to provide a flexible and adaptable methodology for governing, managing and applying data throughout the enterprise, called DECIDE.",2020,,https://doi.org/10.1016/j.datak.2020.101862
1419,Wilson Wen Bin Goh and Chern Han Yong and Limsoon Wong,Are batch effects still relevant in the age of big data?,"artificial intelligence, batch effect, machine learning, RNA sequencing, single cell","Batch effects (BEs) are technical biases that may confound analysis of high-throughput biotechnological data. BEs are complex and effective mitigation is highly context-dependent. In particular, the advent of high-resolution technologies such as single-cell RNA sequencing presents new challenges. We first cover how BE modeling differs between traditional datasets and the new data landscape. We also discuss new approaches for measuring and mitigating BEs, including whether a BE is significant enough to warrant correction. Even with the advent of machine learning and artificial intelligence, the increased complexity of next-generation biotechnological data means increased complexities in BE management. We forecast that BEs will not only remain relevant in the age of big data but will become even more important.",2022,,https://doi.org/10.1016/j.tibtech.2022.02.005
1420,Mark Birkin,Big Data,"Administrative data, Crowdsourcing, Data ethics, Data quality, Data sharing, Representation and bias, Social media, Spatial analysis, Value, Variety, Velocity, Volume, Volunteered geographical information","Big Data are deeply impactful for research in providing a diverse and rich array of novel sources for academic enquiry. Geographers are benefitting from varied data types including administrative data, social media, volunteered geographic information, and consumer data. The article presents examples and illustrations associated with each of these data types. An additional benefit of Big Data analytics is that it brings geographers into closer contact with real world partners and policy problems. Big Data also attract challenges including representational bias, variable data quality, difficulties of access and ownership, and ethical and legal restrictions in their use. Hence we conclude that Big Data are ripe for fruitful exploitation, but careful investments will be necessary if opportunities are to be realized to the full.",2020,,https://doi.org/10.1016/B978-0-08-102295-5.10616-X
1421,Shuangqi Li and Pengfei Zhao,Big data driven vehicle battery management method: A novel cyber-physical system perspective,"Electric vehicles, Battery energy storage, Cyber-physical battery management system, Big data, Deep learning","The establishment of an accurate battery model is of great significance to improve the reliability of electric vehicles (EVs). However, the battery is a complex electrochemical system with hardly observable and simulatable internal chemical reactions, and it is challenging to estimate the state of battery accurately. This paper proposes a novel flexible and reliable battery management method based on the battery big data platform and Cyber-Physical System (CPS) technology. First of all, to integrate the battery big data resources in the cloud, a Cyber-physical battery management framework is defined and served as the basic data platform for battery modeling issues. And to improve the quality of the collected battery data in the database, this work reports the first attempt to develop an adaptive data cleaning method for the cloud battery management issue. Furthermore, a deep learning algorithm-based feature extraction model, as well as a feature-oriented battery modeling method, is developed to mitigate the under-fitting problem and improve the accuracy of the cloud-based battery model. The actual operation data of electric buses is used to validate the proposed methodologies. The maximum data restoring error can be limited within 1.3% in the experiments, which indicates that the proposed data cleaning method is able to improve the cloud battery data quality effectively. Meanwhile, the maximum SoC estimation error in the proposed feature-oriented battery modeling method is within 2.47%, which highlights the effectiveness of the proposed method.",2021,,https://doi.org/10.1016/j.est.2020.102064
1422,Amanda Caissie and Michelle Mierzwa and Clifton David Fuller and Murali Rajaraman and Alex Lin and Andrew MacDonald and Richard Popple and Ying Xiao and Lisanne VanDijk and Peter Balter and Helen Fong and Heping Xu and Matthew Kovoor and Joonsang Lee and Arvind Rao and Mary Martel and Reid Thompson and Brandon Merz and John Yao and Charles Mayo,Head and Neck Radiotherapy Patterns of Practice Variability Identified as a Challenge to Real-World Big Data: results from the Learning from Analysis of Multicentre Big Data Aggregation (LAMBDA) Consortium,,"Purpose/Objective
Outside of randomized clinical trials, it is difficult to develop clinically relevant evidence-based recommendations for radiotherapy (RT) practice guidelines due to lack of comprehensive real-world data. To address this knowledge gap, we formed the Learning and Analytics from Multicenter Big Data Aggregation (LAMBDA) consortium to cooperatively implement RT data standardization, develop software solutions for data analysis and recommend clinical practice change based on real-world data analyzed. The first phase of this “Big Data” study aimed at characterizing variability in clinical practice patterns of dosimetric data for organs at risk (OAR), that would undermine subsequent use of large scale, electronically aggregated data to characterize associations with outcomes. Evidence from this study was used as the basis for practical recommendations to improve data quality.
Materials/Methods
Dosimetric details of patients with H&N cancer treated with RT between 2014 and 2019 were analyzed. Institutional patterns of practice were characterized including structure nomenclature, volumes and frequency of contouring. Dose volume histogram (DVH) distributions were characterized and compared to institutional constraints and literature values.
Results
Plans for 4664 patients treated to a mean plan dose of 64.4 ± 13.2 Gy in 32 ± 4 fractions were aggregated. Prior to implementation of TG263 guidelines in each institution, there was variability in OAR nomenclature across institutions and structures. With evidence from this study, we identified a targeted and practical set of recommendations aimed at improving the quality of real-world data.
Conclusion
Quantifying similarities and differences among institutions for OAR structures and DVH metrics is the launching point for next steps to investigate potential relationships between DVH parameters and patient outcomes.",2022,,https://doi.org/10.1016/j.adro.2022.100925
1423,Claudio A. Ardagna and Valerio Bellandi and Ernesto Damiani and Michele Bezzi and Cedric Hebert,Big Data Analytics-as-a-Service: Bridging the gap between security experts and data scientists,"Artificial intelligence, Big Data Analytics, Machine learning, Security and privacy","We live in an interconnected and pervasive world where huge amount of data are collected every second. Fully exploiting data through advanced analytics, machine learning and artificial intelligence, becomes crucial for businesses, from micro to large enterprises, resulting in a key advantage (or shortcoming) in the global market competition, as well as in a strong market driver for business analytics solutions. This scenario is deeply changing the security landscape, introducing new risks and threats that affect security and privacy of systems, on one side, and safety of users, on the other side. Many domains that can benefit from novel solutions based on data analytics have stringent security requirements to fulfill. The Energy domain’s Smart Grid is a major example of systems at the crossroads of security and data-driven intelligence. The Smart Grid plays a crucial role in modern energy infrastructure. However, it must face two major challenges related to security: managing front-end intelligent devices such as power assets and smart meters securely, and protecting the huge amount of data received from these devices. Starting from these considerations, setting up proper analytics is a complex problem because security controls could have the undesired side effect of decreasing the accuracy of the analytics themselves. This is even more critical when the configuration of security controls is let to the security expert, who often has only basic skills in data science. In this paper, we propose a solution based on the concept of Model-Based Big Data Analytics-as-a-Service (MBDAaaS) that bridges the gap between security experts and data scientists. Our solution acts as a middleware allowing a security expert and a data scientist to collaborate to the deployment of an analytics addressing their needs.",2021,,https://doi.org/10.1016/j.compeleceng.2021.107215
1424,Feliks P. Sejahtera Surbakti and Wei Wang and Marta Indulska and Shazia Sadiq,Factors influencing effective use of big data: A research framework,"Big data, Effective use, Factors, Framework","Information systems (IS) research has explored “effective use” in a variety of contexts. However, it is yet to specifically consider it in the context of the unique characteristics of big data. Yet, organizations have a high appetite for big data, and there is growing evidence that investments in big data solutions do not always lead to the derivation of intended value. Accordingly, there is a need for rigorous academic guidance on what factors enable effective use of big data. With this paper, we aim to guide IS researchers such that the expansion of the body of knowledge on the effective use of big data can proceed in a structured and systematic manner and can subsequently lead to empirically driven guidance for organizations. Namely, with this paper, we cast a wide net to understand and consolidate from literature the potential factors that can influence the effective use of big data, so they may be further studied. To do so, we first conduct a systematic literature review. Our review identifies 41 factors, which we categorize into 7 themes, namely data quality; data privacy and security and governance; perceived organizational benefit; process management; people aspects; systems, tools, and technologies; and organizational aspects. To explore the existence of these themes in practice, we then analyze 45 published case studies that document insights into how specific companies use big data successfully. Finally, we propose a framework for the study of effective use of big data as a basis for future research. Our contributions aim to guide researchers in establishing the relevance and relationships within the identified themes and factors and are a step toward developing a deeper understanding of effective use of big data.",2020,,https://doi.org/10.1016/j.im.2019.02.001
1425,Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon},A Review on Data Cleansing Methods for Big Data,"data cleansing, big data, data quality","Massive amounts of data are available for the organization which will influence their business decision. Data collected from the various resources are dirty and this will affect the accuracy of prediction result. Data cleansing offers a better data quality which will be a great help for the organization to make sure their data is ready for the analyzing phase. However, the amount of data collected by the organizations has been increasing every year, which is making most of the existing methods no longer suitable for big data. Data cleansing process mainly consists of identifying the errors, detecting the errors and corrects them. Despite the data need to be analyzed quickly, the data cleansing process is complex and time-consuming in order to make sure the cleansed data have a better quality of data. The importance of domain expert in data cleansing process is undeniable as verification and validation are the main concerns on the cleansed data. This paper reviews the data cleansing process, the challenge of data cleansing for big data and the available data cleansing methods.",2019,,https://doi.org/10.1016/j.procs.2019.11.177
1426,Xin Li and Rob Law,Network analysis of big data research in tourism,"Big data, Tourism studies, Co-citation analysis, Network analysis, Research trends","This study aims to provide a comprehensive network analysis to understand the current state of big data research in tourism by investigating multi-disciplinary contributions relevant to big data. A comprehensive network analytical method, which includes co-citation, clustering and trend analysis, is applied to systematically analyse publications from 2008 to 2017. Two unique data sets from Web of Science are collected. The first data set focuses on big data research in tourism and hospitality. The second data set involves other disciplines, such as computer science, for a comparison with tourism. Results suggest that applications of social media and user-generated content are gaining momentum, whereas theory-based studies on big data in tourism remain limited. Tourism and other relevant domains have similar concerns with the challenges involved in big data, such as privacy, data quality and appropriate data use. This comparative network analysis has implications for future big data research in tourism.",2020,,https://doi.org/10.1016/j.tmp.2019.100608
1427,Morten Brinch and Angappa Gunasekaran and Samuel {Fosso Wamba},Firm-level capabilities towards big data value creation,"Big data, Supply chain management, Operations management, Value creation, Business analytics, Capabilities","Big data has played an increasingly important role in using data to improve business value. In response to several big data challenges, the purpose of this study is to identify firm-level capabilities required to create value from big data. The adjacent theories of business process management and IT business value underpinned the study, together with an in-depth case study that led to the identification of twenty-four types of capabilities related to IT, process, performance, human, strategic, and organizational practices. The findings confirmed the application of practices and capabilities of adjacent theories, as well as certain practices and attributes that were both changed and reinforced at the intersection of big data. As an outstanding additional support to the extant big data studies, this work empirically confirms and portrays hitherto unexplored capabilities of big data and set their roles, thus providing a holistic overview of firm-level capabilities that are required for big data value creation.",2021,,https://doi.org/10.1016/j.jbusres.2020.07.036
1428,Yuguang Ye and Jianshe Shi and Daxin Zhu and Lianta Su and Jianlong Huang and Yifeng Huang,Management of medical and health big data based on integrated learning-based health care system: A review and comparative analysis,"Integrated learning, Health care system, Elaboration Likelihood Machine, System design, Medical big data, Internet of Medical Things","Purpose
We present a Health Care System (HCS) based on integrated learning to achieve high-efficiency and high-precision integration of medical and health big data, and compared it with an internet-based integrated system.
Method
The method proposed in this paper adopts the Bagging integrated learning method and the Extreme Learning Machine (ELM) prediction model to obtain a high-precision strong learning model. In order to verify the integration efficiency of the system, we compare it with the Internet-based health big data integration system in terms of integration volume, integration efficiency, and storage space capacity.
Results
The HCS based on integrated learning relies on the Internet in terms of integration volume, integration efficiency, and storage space capacity. The amount of integration is proportional to the time and the integration time is between 170-450 ms, which is only half of the comparison system; whereby the storage space capacity reaches 8.3×28TB.
Conclusion
The experimental results show that the integrated learning-based HCS integrates medical and health big data with high integration volume and integration efficiency, and has high space storage capacity and concurrent data processing performance.",2021,,https://doi.org/10.1016/j.cmpb.2021.106293
1429,Mokhtar Sellami and Haithem Mezni and Mohand Said Hacid,On the use of big data frameworks for big service composition,"Big data, Big service, Big service composition, Quality of big services, Fuzzy RCA, Spark","Over the last years, big data has emerged as a new paradigm for the processing and analysis of massive volumes of data. Big data processing has been combined with service and cloud computing, leading to a new class of services called “Big Services”. In this new model, services can be seen as an abstract layer that hides the complexity of the processed big data. To meet users' complex and heterogeneous needs in the era of big data, service reuse is a natural and efficient means that helps orchestrating available services' operations, to provide customer on-demand big services. However different from traditional Web service composition, composing big services refers to the reuse of, not only existing high-quality services, but also high-quality data sources, while taking into account their security constraints (e.g., data provenance, threat level and data leakage). Moreover, composing heterogeneous and large-scale data-centric services faces several challenges, apart from security risks, such as the big services' high execution time and the incompatibility between providers' policies across multiple domains and clouds. Aiming to solve the above issues, we propose a scalable approach for big service composition, which considers not only the quality of reused services (QoS), but also the quality of their consumed data sources (QoD). Since the correct representation of big services requirements is the first step towards an effective composition, we first propose a quality model for big services and we quantify the data breaches using L-Severity metrics. Then to facilitate processing and mining big services' related information during composition, we exploit the strong mathematical foundation of fuzzy Relational Concept Analysis (fuzzy RCA) to build the big services' repository as a lattice family. We also used fuzzy RCA to cluster services and data sources based on various criteria, including their quality levels, their domains, and the relationships between them. Finally, we define algorithms that parse the lattice family to select and compose high-quality and secure big services in a parallel fashion. The proposed method, which is implemented on top of Spark big data framework, is compared with two existing approaches, and experimental studies proved the effectiveness of our big service composition approach in terms of QoD-aware composition, scalability, and security breaches.",2020,,https://doi.org/10.1016/j.jnca.2020.102732
1430,Hannah D Budnitz and Emmanouil Tranos and Lee Chapman,Transport Modes and Big Data,"Application programming interfaces, Automation, Big data, Crowd-sourced, Digitization, Geolocation, Information and communication technology, Intelligent transport systems, Internet of things, Location-based services, Mobility as a service, Real time information systems, Timestamp, Vehicle telematics","Transport modes and big data considers the characteristics of “Big Data” as described by the 5 “Vs,” Volume, Velocity, Variety, Veracity, and Value. It reviews the many sources of big data within the transport sector by modal group, and the sources of big data from the Information and Communication Technology (ICT) sector that may be applied to the understanding of transport. It briefly considers the uses, advantages, and disadvantages of these data sources, and the challenges to analyzing and interpreting them. It concludes that Big Data is necessary to prepare for the uncertain future of transport, but recognizes the challenges to applying it accurately and effectively to transport problems and policies.",2021,,https://doi.org/10.1016/B978-0-08-102671-7.10601-3
1431,Chunquan Li and Yaqiong Chen and Yuling Shang,A review of industrial big data for decision making in intelligent manufacturing,"Intelligent manufacturing, Artificial intelligence, Industrial big data, Big data-driven technology, Decision-making","Under the trend of economic globalization, intelligent manufacturing has attracted a lot of attention from academic and industry. Related enabling technologies make manufacturing industry more intelligent. As one of the key technologies in artificial intelligence, big data driven analysis improves the market competitiveness of manufacturing industry by mining the hidden knowledge value and potential ability of industrial big data, and helps enterprise leaders make wise decisions in various complex manufacturing environments. This paper provides a theoretical analysis basis for big data-driven technology to guide decision-making in intelligent manufacturing, fully demonstrating the practicability of big data-driven technology in the intelligent manufacturing industry, including key advantages and internal motivation. A conceptual framework of intelligent decision-making based on industrial big data-driven technology is proposed in this study, which provides valuable insights and thoughts for the severe challenges and future research directions in this field.",2022,,https://doi.org/10.1016/j.jestch.2021.06.001
1432,Lingqiang Kong and Zhifeng Liu and Jianguo Wu,A systematic review of big data-based urban sustainability research: State-of-the-science and future directions,"Big data, Social media data, Urban landscape sustainability, Smart city, Urban planning","The future of humanity depends increasingly on the performance of cities. Big data provide new and powerful ways of studying and improving coupled urban environmental, social, and economic systems to achieve urban sustainability. However, the term big data has been defined variably, and its urban applications have so far been sporadic in terms of research topic and location. A comprehensive review of big data-based urban environment, society, and sustainability (UESS) research is much needed. The aim of this study was to summarize the big data-based UESS research using a systematic review approach in combination with bibliometric and thematic analyses. The results showed that the numbers of publications and citations of related articles have been increasing exponentially in recent years. The most frequently used big data in UESS research are human behavior data, and the major analytical methods are of five types: classification, clustering, regression, association rules, and social network analysis. The major research topics of big data-based UESS research include urban mobility, urban land use and planning, environmental sustainability, public health and safety, social equity, tourism, resources and energy utilization, real estate, and retail, accommodation and catering. Big data benefit UESS research by proving a people-oriented perspective, timely and real-time information, and fine-resolution spatial dynamics. In addition, several obstacles were identified to applying big data in UESS research, which are related to data quality and acquisition, data storage and management, data security and privacy, data cleaning and preprocessing, and data analysis and information mining. To move forward, future research should integrate multiple big data sources, develop and utilize new methods such as deep learning and cloud computing, and expand the application fields to focus on the interactions between human activities and urban environments. This review can contribute to understanding the current situation of big data-based UESS research, and provide a reference for studies of this topic in the future.",2020,,https://doi.org/10.1016/j.jclepro.2020.123142
1433,Yu Liu and Yangtao Wang and Ke Zhou and Yujuan Yang and Yifei Liu,Semantic-aware data quality assessment for image big data,"Semantic-aware, Quality assessment, Image big data, IDSTH, SHR","Data quality (DQ) assessment is essential for realizing the promise of big data by judging the value of data in advance. Relevance, an indispensable dimension of DQ, focusing on “fitness for requirement”, can arouse the user’s interest in exploiting the data source. It has two-level evaluations: (1) the amount of data that meets the user’s requirements; (2) the matching degree of these relevant data. However, there lack works of DQ assessment at dimension of relevance, especially for unstructured image data which focus on semantic similarity. When we try to evaluate semantic relevance between an image data source and a query (requirement), there are three challenges: (1) how to extract semantic information with generalization ability for all image data? (2) how to quantify relevance by fusing the quantity of relevant data and the degree of similarity comprehensively? (3) how to improve assessing efficiency of relevance in a big data scenario by design of an effective architecture? To overcome these challenges, we propose a semantic-aware data quality assessment (SDQA) architecture which includes off-line analysis and on-line assessment. In off-line analysis, for an image data source, we first transform all images into hash codes using our improved Deep Self-taught Hashing (IDSTH) algorithm which can extract semantic features with generalization ability, then construct a graph using hash codes and restricted Hamming distance, next use our designed Semantic Hash Ranking (SHR) algorithm to calculate the importance score (rank) for each node (image), which takes both the quantity of relevant images and the degree of semantic similarity into consideration, and finally rank all images in descending order of score. During on-line assessment, we first convert the user’s query into hash codes using IDSTH model, then retrieve matched images to collate their importance scores, and finally help the user determine whether the image data source is fit for his requirement. The results on public dataset and real-world dataset show effectiveness, superiority and on-line efficiency of our SDQA architecture.",2020,,https://doi.org/10.1016/j.future.2019.07.063
1434,Lei Li and Jiabao Lin and Ye Ouyang and Xin (Robert) Luo,Evaluating the impact of big data analytics usage on the decision-making quality of organizations,"Big data analytics usage, Data analytics capabilities, Decision-making quality, Agricultural firms","Big data initiatives are critical for transforming traditional organizational decision making into data-driven decision making. However, prior information systems research has not paid enough attention to the impact of big data analytics usage on decision-making quality. Drawing on the dynamic capability theory, this study investigated the impact of big data analytics usage on decision-making quality and tested the mediating effect of data analytics capabilities. We collected data from 240 agricultural firms in China. The empirical results showed that big data analytics usage had a positive impact on decision-making quality and that data analytics capabilities played a mediating role in the relationship between big data analytics usage and decision-making quality. Hence, firms should not only popularize big data analytics usage in their business activities but also take measures to improve their data analytics capabilities, which will improve their decision-making quality toward competitive advantages.",2022,,https://doi.org/10.1016/j.techfore.2021.121355
1435,Angelo Corallo and Anna Maria Crespino and Mariangela Lazoi and Marianna Lezzi,Model-based Big Data Analytics-as-a-Service framework in smart manufacturing: A case study,"Big Data Analytics, BDA, MBDAaaS framework, Smart manufacturing, Industry 4.0, Anomaly detection","Today, in a smart manufacturing environment based on the Industry 4.0 paradigm, people, technological infrastructure and machinery equipped with sensors can constantly generate and communicate a huge volume of data, also known as Big Data. The manufacturing industry takes advantage of Big Data and analytics evolution by improving its capability to bring out valuable information and knowledge from industrial processes, production systems and sensors. The adoption of model-based frameworks in the Big Data Analytics pipeline can better address user configuration requirements (e.g. type of analysis to perform, type of algorithm to be applied) and also provide more transparency and clearness on the execution of workflows and data processing. In the current state of art, an application of a model-based framework in a manufacturing scenario is missing. Therefore, in this study, by means of a case study research focused on data from sensors associated with Computer Numerical Control machines, the configuration and execution of a Big Data Analytics pipeline with a Model-based Big Data Analytics-as-a-Service framework is described. The case study provides to theoreticians and managerial experts useful evidence for managing real-time data analytics and deploying a workflow that addresses specific analytical goals, driven by user requirements and developer models, in a complex manufacturing domain.",2022,,https://doi.org/10.1016/j.rcim.2022.102331
1436,Nikolay Nikolov and Yared Dejene Dessalk and Akif Quddus Khan and Ahmet Soylu and Mihhail Matskin and Amir H. Payberah and Dumitru Roman,Conceptualization and scalable execution of big data workflows using domain-specific languages and software containers,"Big data workflows, Internet of Things, Domain-specific languages, Software containers","Big Data processing, especially with the increasing proliferation of Internet of Things (IoT) technologies and convergence of IoT, edge and cloud computing technologies, involves handling massive and complex data sets on heterogeneous resources and incorporating different tools, frameworks, and processes to help organizations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, requires taking advantage of Cloud infrastructures’ elasticity for scalability. In this article, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies, message-oriented middleware (MOM), and a domain-specific language (DSL) to enable highly scalable workflow execution and abstract workflow definition. We demonstrate our system in a use case and a set of experiments that show the practical applicability of the proposed approach for the specification and scalable execution of Big Data workflows. Furthermore, we compare our proposed approach’s scalability with that of Argo Workflows – one of the most prominent tools in the area of Big Data workflows – and provide a qualitative evaluation of the proposed DSL and overall approach with respect to the existing literature.",2021,,https://doi.org/10.1016/j.iot.2021.100440
1437,Yuncheng Shen and Bing Guo and Yan Shen and Xuliang Duan and Xiangqian Dong and Hong Zhang and Chuanwu Zhang and Yuming Jiang,Personal big data pricing method based on differential privacy,"Personal big data, Data privacy, Privacy protection, Differential privacy, Positive pricing, Reverse pricing, Privacy budget, Privacy compensation","Personal big data can greatly promote social management, business applications, and personal services, and bring certain economic benefits to users. The difficulty with personal big data security and privacy protection lies in realizing the maximization of the value of personal big data and in striking a balance between data privacy protection and sharing on the premise of satisfying personal big data security and privacy protection. Thus, in this paper, we propose a personal big data pricing method based on differential privacy (PMDP). We design two different mechanisms of positive and reverse pricing to reasonbly price personal big data. We perform aggregate statistics on an open dataset and extensively evaluated its performance. The experimental results show that PMDP can provide reasonable pricing for personal big data and fair compensation to data owners, ensuring an arbitrage-free condition and finding a balance between privacy protection and data utility.",2022,,https://doi.org/10.1016/j.cose.2021.102529
1438,Dhanya Therese Jose and Jørgen Holme and Antorweep Chakravorty and Chunming Rong,Integrating big data and blockchain to manage energy smart grid - TOTEM framework,"Blockchain, Big data analytic, Hyperledger fabric, Hadoop, MapReduce, Docker, PIVT","The demand for electricity is increasing exponentially day by day especially with the arrival of electric vehicles. In smart community neighborhood project, it demands electricity should be produced at the household or community level and sell or buy according to the demands. Since the actors can produce, sell and also buy according to the demands, thus the name prosumers. ICT solutions can contribute to this in several ways such as machine learning for analysing the household data for customer demand and peak hour for the usage of electricity, blockchain as a trustworthy platform for selling or buying, data hub, and ensure data security and privacy of prosumers. TOTEM: Token for controlled computation is a framework that allows the users to analyze the data without moving the data from the data owner's environment. It also ensures the data security and privacy of the data. Here in this article, we will show the importance of TOTEM architecture in the EnergiX project and how the extended version of TOTEM can be efficiently merged with the demands of the current and similar projects.",2022,,https://doi.org/10.1016/j.bcra.2022.100081
1439,"El Alaoui, Imane and Gahi, Youssef and Messoussi, Rochdi",Big Data Quality Metrics for Sentiment Analysis Approaches,"Big data quality metrics, Big data, Sentiment analysis","In a world increasingly connected, and in which information flows quickly and affects a very large number of people, sentiment analysis has seen a spectacular development over the past ten years. This is due to the fact that the explosion of social networks has allowed anyone with internet access to publicly express his opinion. Moreover, the emergence of big data has brought enormous opportunities and powerful storage and analytics tools to the field of sentiment analysis. However, big data introduces new variables and constraints that could radically affect the traditional models of sentiment analysis. Therefore, new concerns, such as big data quality, have to be addressed to get the most out of big data. To the best of our knowledge, no contributions have been published so far which address big data quality in SA throughout its different processes. In this paper, we first highlight the most important big data quality metrics to consider in any big data project. Then, we show how these metrics could be specifically considered in SA approaches and this for each phase in the big data value chain.",2019,,10.1145/3341620.3341629
1440,"Reda, Oumaima and Sassi, Imad and Zellou, Ahmed and Anter, Samir",Towards a Data Quality Assessment in Big Data,"Data Quality evaluation, Big Data, Data Quality, Quality Models","In recent years, as more and more data sources have become available and the volumes of data potentially accessible have increased, the assessment of data quality has taken a central role whether at the academic, professional or any other sector. Given that users are often concerned with the need to filter a large amount of data to better satisfy their requirements and needs, and that data analysis can be based on inaccurate, incomplete, ambiguous, duplicated and of poor quality, it makes everyone wonder what the results of these analyses will really be like. However, there is a very complex process involved in the identification of new, valid, potentially useful and meaningful data from a large data collection and various information systems, and is critically dependent on a number of measures to be developed to ensure data quality. To this end, the main objective of this paper is to introduce a general study on data quality related with big data, by providing what other researchers came up with on that subject. The paper will be finalized by a comparative study between the different existing data quality models.",2020,,10.1145/3419604.3419803
1441,"Emmanuel, Isitor and Stanier, Clare",Defining Big Data,"Big Data characteristics, Data Quality Dimensions, Data Quality, Big Data","As Big Data becomes better understood, there is a need for a comprehensive definition of Big Data to support work in fields such as data quality for Big Data. Existing definitions of Big Data define Big Data by comparison with existing, usually relational, definitions, or define Big Data in terms of data characteristics or use an approach which combines data characteristics with the Big Data environment. In this paper we examine existing definitions of Big Data and discuss the strengths and limitations of the different approaches, with particular reference to issues related to data quality in Big Data. We identify the issues presented by incomplete or inconsistent definitions. We propose an alternative definition and relate this definition to our work on quality in Big Data.",2016,,10.1145/3010089.3010090
1442,"Baldassarre, Maria Teresa and Caballero, Ismael and Caivano, Danilo and Rivas Garcia, Bibiano and Piattini, Mario",From Big Data to Smart Data: A Data Quality Perspective,"Big Data, Smart Data, Data Quality","Big Data (BD) solutions are designed to better support decision-making processes in order to optimize organizational performance. These BD solutions use company’s core business data, using typically large datasets. However, data that doesn’t meet adequate quality levels will lead to BD solutions that will not produce useful results, and consequently may not be used to make adequate business decisions. For a long time, companies have collected and stored large amounts of data without being able to exploit the advantage of exploring it. Nowadays, and thanks to the Big Data explosion, organizations have begun to recognize the need for estimating the value of their data and, vice-versa, managing data accordingly to their value. This need of managing the Value of data has led to the concept of Smart Data. It not only involves the datasets, but also the set of technologies, tools, processes and methodologies that enable all the Values from the data to the End-users (Business, data scientist, BI…). Consequently, Smart data is data actionable. We discovered that data quality is one of the most important issues when it comes to “smartizing” data. In this paper, we introduce a methodology to make data smarter, taking as a reference point, the quality level of the data itself.",2018,,10.1145/3281022.3281026
1443,"Cuzzocrea, Alfredo and Sacc\`{a}, Domenico and Ullman, Jeffrey D.",Big Data: A Research Agenda,"big data posting, OLAP over big data, privacy of big data, big data","Recently, a great deal of interest for Big Data has risen, mainly driven from a widespread number of research problems strongly related to real-life applications and systems, such as representing, modeling, processing, querying and mining massive, distributed, large-scale repositories (mostly being of unstructured nature). Inspired by this main trend, in this paper we discuss three important aspects of Big Data research, namely OLAP over Big Data, Big Data Posting, and Privacy of Big Data. We also depict future research directions, hence implicitly defining a research agenda aiming at leading future challenges in this research field.",2013,,10.1145/2513591.2527071
1444,"Pti\v{c}ek, Marina and Vrdoljak, Boris",Big Data and New Data Warehousing Approaches,"MapReduce, databases, big data, data warehouse, NewSQL, NoSQL","Big data are a data trend present around us mainly through Internet -- social networks and smart devices and meters -- mostly without us being aware of them. Also they are a fact that both industry and scientific research needs to deal with. They are interesting from analytical point of view, for they contain knowledge that cannot be ignored and left unused. Traditional system that supports the advanced analytics and knowledge extraction -- data warehouse -- is not able to cope with large amounts of fast incoming various and unstructured data, and may be facing a paradigm shift in terms of utilized concepts, technologies and methodologies, which have become a very active research area in the last few years. This paper provides an overview of research trends important for the big data warehousing, concepts and technologies used for data storage and (ETL) processing, and research approaches done in attempts to empower traditional data warehouses for handling big data.",2017,,10.1145/3141128.3141139
1445,"Neamtu, Rodica and Ahsan, Ramoza and Stokes, Jeff and Hoxha, Armend and Bao, Jialiang and Gvozdenovic, Stefan and Meyer, Ted and Patel, Nilesh and Rangan, Raghu and Wang, Yumou and Zhang, Dongyun and Rundensteiner, Elke A.",Taming Big Data: Integrating Diverse Public Data Sources for Economic Competitiveness Analytics,"diverse data sources, data integration, Big data","In an era where Big Data can greatly impact a broad population, many novel opportunities arise, chief among them the ability to integrate data from diverse sources and ""wrangle"" it to extract novel insights. Conceived as a tool that can help both expert and non-expert users better understand public data, MATTERS was collaboratively developed by the Massachusetts High Tech Council, WPI and other institutions as an analytic platform offering dynamic modeling capabilities. MATTERS is an integrative data source on high fidelity cost and talent competitiveness metrics. Its goal is to extract, integrate and model rich economic, financial, educational and technological information from renowned heterogeneous web data sources ranging from The US Census Bureau, The Bureau of Labor Statistics to the Institute of Education Sciences, all known to be critical factors influencing economic competitiveness of states. This demonstration of MATTERS illustrates how we tackle challenges of data acquisition, cleaning, integration and wrangling into appropriate representations, visualization and story-telling with data in the context of state competitiveness in the high-tech sector.",2014,,10.1145/2658840.2658845
1446,"Khan, Nawsher and Alsaqer, Mohammed and Shah, Habib and Badsha, Gran and Abbasi, Aftab Ahmad and Salehian, Soulmaz","The 10 Vs, Issues and Challenges of Big Data","Big Data, Data Management","In this emerging computing and digital globe, information and Knowledge are created and then collected with a rapid approach by wide range of applications through scientific computing and commercial workloads. Over 3.8 billion people out of 7.6 billion population of the world are connected to the internet. Out of 13.4 billion devices, 8.06 billion devices have a mobile connection. In 2020, 38.5 billion devices will be connected and globally internet traffic will be 92 times greater than it was in 2005. The use of such devices and internet not only increase the data volume but the velocity of market brings in fast-track and accelerates as information is transferred and shared with light speed on optic fiber and wireless networks. This fast generation of huge data creates numerous challenges. The existing approaches addressing issues such as, Volume, Variety, Velocity and Value in big data research perspective. The objectives of the paper are to investigate and analyze the current status of Big Data and furthermore a comprehensive overview of various aspects has discussed, and additionally has been described all 10 Vs' (Issues) of Big Data.",2018,,10.1145/3206157.3206166
1447,"Cho, Wonhee and Lim, Yoojin and Lee, Hwangro and Varma, Mohan Krishna and Lee, Moonsoo and Choi, Eunmi",Big Data Analysis with Interactive Visualization Using R Packages,"Mining, Hadoop, R, Visualization, Big data","Compared to the traditional data storing, processing, analyzing and visualization which have been performed, Big data requires evolutionary technologies of massive data processing on distributed and parallel systems, such as Hadoop system. Big data analytic systems, thus, have been popular to derive important decision making in various areas. However, visualization on analytic system faces various limitation due to the huge amount of data. This brings the necessity of interactive visualization techniques beyond the traditional static visualization. R has been used and improved for a big data analysis and mining tool. Also, R is supported with various and abundant packages for different targets with visualization. However interactive visualization packages are not easily found in the market. This paper compares and analyzes interactive web packages with visualization packages for R. This paper also proposes interactive web visualized analysis environment for big data with a combination of interactive web packages and visualization packages. In particular, Big data analysis techniques with sensed data are presented as the result by reflecting the decision view on sensing field.",2014,,10.1145/2640087.2644168
1448,"Raza, Muhammad Umair and XuJian, Zhao",A Comprehensive Overview of BIG DATA Technologies: A Survey,"MapReduce, Apache Hadoop, HDFS, Big Data Technology","In as much as the approaches of the new revolution, machines including transmission media like social media sites, nowadays quantity of data swell hastily. So, size is the core and only facet that leaps the mention of BIG DATA. In this article, an effort to touch a comprehensive view of big data technologies, because of the swift evolution of data by an industry trying the academic press to catch up. This paper also offers a unified explanation of big data as well as the analytics methods. A practical discriminate characteristic of this paper is core analytics associated with unstructured data which is more than 90% of big data. To deal with complicated Big Data problems, great work has been done. This paper analyzes contemporary Big Data technologies. Therein article further strengthens the necessity to formulate new tools for analytics. It bestows not sole an intercontinental overview of big data techniques even though the valuation according to big data Hadoop Ecosystem. It classifies and debates the main technologies feature, challenges, and usage as well.",2020,,10.1145/3404687.3404694
1449,"Cao, Shuangshuang",Opportunities and Challenges of Marketing in the Context of Big Data,"Marketing, Big data, Personalized service","In the era of big data, under the conditions of rapid economic development in our country, various enterprises have also vigorously carried out marketing. In the context of big data, marketing research should be strengthened to effectively improve market. Market issues ensure that marketing has improved its status in the era of big data. This article has conducted a research and analysis on marketing in the context of big data. And then the opportunities and challenges of marketing in the context of big data has been explained, which gradually optimize the marketing implementation effect. The challenges faced by marketing has been understood which ensures that the big data model plays its best role in it.",2021,,10.1145/3456389.3456390
1450,"Chen, Hong-Mei and Kazman, Rick and Garbajosa, Juan and Gonzalez, Eloy",Toward Big Data Value Engineering for Innovation,"innovation, value engineering, architecture landscape, ecosystem, energy industry, big data, value discovery","This article articulates the requirements for an effective big data value engineering method. It then presents a value discovery method, called Eco-ARCH (Eco-ARCHitecture), tightly integrated with the BDD (Big Data Design) method for addressing these requirements, filling a methodological void. Eco-ARCH promotes a fundamental shift in design thinking for big data system design -- from ""bounded rationality"" for problem solving to ""expandable rationality"" for design for innovation. The Eco-ARCH approach is most suitable for big data value engineering when system boundaries are fluid, requirements are ill-defined, many stakeholders are unknown and design goals are not provided, where no architecture pre-exists, where system behavior is non-deterministic and continuously evolving, and where co-creation with consumers and prosumers is essential to achieving innovation goals. The method was augmented and empirically validated in collaboration with an IT service company in the energy industry to generate a new business model that we call ""eBay in the Grid"".",2016,,10.1145/2896825.2896837
1451,"Chen, Hong-Mei and Kazman, Rick and Haziyev, Serge and Hrytsay, Olha",Big Data System Development: An Embedded Case Study with a Global Outsourcing Firm,"system engineering, software architecture, embedded case study methodology, data system design methods, collaborative practice research, big data","Big data system development is dramatically different from small (traditional, structured) data system development. At the end of 2014, big data deployment is still scarce and failures abound. Outsourcing has become a main strategy for many enterprises. We therefore selected an outsourcing company who has successfully deployed big data projects for our study. Our research results from analyzing 10 outsourced big data projects provide a glimpse into early adopters of big data, illuminates the challenges for system development that stem from the 5Vs of big data and crystallizes the importance of architecture design choices and technology selection. We followed a collaborative practice research (CPR) method to develop and validate a new method, called BDD. BDD is the first attempt to systematically combine architecture design with data modeling approaches to address big data system development challenges. The use of reference architectures and a technology catalog are advancements to architecture design methods and are proving to be well-suited for big data system architecture design and system development.",2015,,
1452,"Zhi-peng, Sun and Gui-ming, Chen and Hui, Zhang",Evaluation of Large-Scale Complex Systems Effectiveness Based on Big Data,"big data, large-scale complex systems, evaluation, effectiveness","With the advent of the information age, big data technology came into being. The wide application of big data brings new opportunities and challenges to the construction of national defense and military information. Under the background of information-based joint operations characterized by large complex systems, how to scientifically and rationally plan the construction of large complex systems, and maximize the effectiveness of the complex system has become a key concern for system construction decision makers and researchers. This paper combines the application of big data in the construction of large complex systems, and focuses on the evaluation of the effectiveness of large complex systems based on big data, which can be used for reference by relevant researchers.",2019,,10.1145/3335484.3335545
1453,"Davoudian, Ali and Liu, Mengchi",Big Data Systems: A Software Engineering Perspective,"software engineering, quality assurance, software reference architecture, requirements engineering, Big Data, Big Data systems","Big Data Systems (BDSs) are an emerging class of scalable software technologies whereby massive amounts of heterogeneous data are gathered from multiple sources, managed, analyzed (in batch, stream or hybrid fashion), and served to end-users and external applications. Such systems pose specific challenges in all phases of software development lifecycle and might become very complex by evolving data, technologies, and target value over time. Consequently, many organizations and enterprises have found it difficult to adopt BDSs. In this article, we provide insight into three major activities of software engineering in the context of BDSs as well as the choices made to tackle them regarding state-of-the-art research and industry efforts. These activities include the engineering of requirements, designing and constructing software to meet the specified requirements, and software/data quality assurance. We also disclose some open challenges of developing effective BDSs, which need attention from both researchers and practitioners.",2020,,10.1145/3408314
1454,"Abell\'{o}, Alberto",Big Data Design,"database design, nosql, big data","It is widely accepted today that Relational databases are not appropriate in highly distributed shared-nothing architectures of commodity hardware, that need to handle poorly structured heterogeneous data. This has brought the blooming of NoSQL systems with the purpose of mitigating such problem, specially in the presence of analytical workloads. Thus, the change in the data model and the new analytical needs beyond OLAP take us to rethink methods and models to design and manage these newborn repositories. In this paper, we will analyze state of the art and future research directions.",2015,,10.1145/2811222.2811235
1455,"Safhi, Hicham Moad and Frikh, Bouchra and Ouhbi, Brahim",Data Source Selection in Big Data Context,"Source reliability, Big Data integration, Data quality, Big Data Source Selection","Big Data presents promising technological and economical opportunities. In fact, it has become the raw material of production for many organizations. Data is available in large quantities, and it continues generating abundantly. However, not all the data will have valuable knowledge. Unreliable sources provide misleading and biased information, and even reliable sources could suffer from low data quality.In this paper, we propose a novel methodology for the selectability of data sources, by both considering the presence and the absence of users' preferences. The proposed model integrates multiple factors that affect the reliability of data sources, including their quality, gain, cost and coverage. Experimental results on real world data-sets, show its capability to find the subset of relevant and reliable sources with the lowest cost.",2019,,10.1145/3366030.3366121
1456,"Mohammad, Atif and Mcheick, Hamid and Grant, Emanuel",Big Data Architecture Evolution: 2014 and Beyond,"cloud computing, big data","This paper aims at developing the Big Data Architecture, and its relation with Analytics, Cloud Services as well as Business Intelligence. The chief aim from all mentioned is to enable the Enterprise Architecture and the Vision of an Organizational target to utilize all the data they are ingesting and regressing data for their short-term or long-terms analytical needs, while making sure that they are addressing during the design phase of such data architecture for both directly and indirectly related stakeholder. Since all stakeholders have their relative interests to utilize the transformed data-sets. This paper also identifies most of the Big Data Architecture, threat analysis within a Big Data System and Big Data Analytic Roadmaps, in terms of smaller components by conducting a gap-analysis that has significant importance as Baseline Big Data Architecture, targeting the end resultant Architectures, once the distillation process of main Big Data Architecture is completed by the Data Architects.",2014,,10.1145/2656346.2656358
1457,"Heer, Jeffrey and Kandel, Sean",Interactive Analysis of Big Data,,"New user interfaces can transform how we work with big data, and raise exciting research problems that span human-computer interaction, machine learning, and distributed systems.",2012,,10.1145/2331042.2331058
1458,"Thuraisingham, Bhavani",Big Data Security and Privacy,"security, privacy, big data","This paper describes the issues surrounding big data security and privacy and provides a summary of the National Science Foundation sponsored workshop on this topic held in Dallas, Texas on September 16-17, 2014. Our goal is to build a community in big data security and privacy to explore the challenging research problems.",2015,,10.1145/2699026.2699136
1459,"Gopalkrishnan, Vivekanand and Steier, David and Lewis, Harvey and Guszcza, James","Big Data, Big Business: Bridging the Gap",,"Business analytics, occupying the intersection of the worlds of management science, computer science and statistical science, is a potent force for innovation in both the private and public sectors. The successes of business analytics in strategy, process optimization and competitive advantage has led to data being increasingly recognized as a valuable asset in many organizations. In recent years, thanks to a dramatic increase in the volume, variety and velocity of data, the loosely defined concept of ""Big Data"" has emerged as a topic of discussion in its own right -- with different viewpoints in both the business and technical worlds. From our perspective, it is important for discussions of ""Big Data"" to start from a well-defined business goal, and remain moored to fundamental principles of both cost/benefit analysis as well as core statistical science. This note discusses some business case considerations for analytics projects involving ""Big Data"", and proposes key questions that businesses should ask. With practical lessons from Big Data deployments in business, we also pose a number of research challenges that may be addressed to enable the business analytics community bring best data analytic practices when confronted with massive data sets.",2012,,10.1145/2351316.2351318
1460,"Wang, Jianwu and Tang, Yan and Nguyen, Mai and Altintas, Ilkay",A Scalable Data Science Workflow Approach for Big Data Bayesian Network Learning,"Kepler, Distributed computing, Ensemble learning, Bayesian network, Big Data, Scientific workflow, Hadoop","In the Big Data era, machine learning has more potential to discover valuable insights from the data. As an important machine learning technique, Bayesian Network (BN) has been widely used to model probabilistic relationships among variables. To deal with the challenges of Big Data PN learning, we apply the techniques in distributed data-parallelism (DDP) and scientific workflow to the BN learning process. We first propose an intelligent Big Data pre-processing approach and a data quality score to measure and ensure the data quality and data faithfulness. Then, a new weight based ensemble algorithm is proposed to learn a BN structure from an ensemble of local results. To easily integrate the algorithm with DDP engines, such as Hadoop, we employ Kepler scientific workflow to build the whole learning process. We demonstrate how Kepler can facilitate building and running our Big Data BN learning application. Our experiments show good scalability and learning accuracy when running the application in real distributed environments.",2014,,10.1109/BDC.2014.10
1461,"Kuschicke, Felix and Thiele, Thomas and Meisen, Tobias and Jeschke, Sabina",A Data-Based Method for Industrial Big Data Project Prioritization,"Project Selection, Framework, Project Prioritization, Manufacturing, Industrial Big Data","The application of Big Data Techniques (BDT) in discrete manufacturing appears to be very promising, considering lighthouse projects in this area. In general, the goal is to collect all data from manufacturing systems comprehensively, in order to enable new findings and decision support by means of appropriate Industrial Big Data (IBD) analysis procedures. However, due to limited human and economic resources, potential IBD projects need to get prioritized -- in the best case according to their cost-benefit ratio. Available methods for this purpose are insufficient, due to their limited ability to be operationalized, error-proneness, and lack of scientific evidence. In this paper, we discuss how cost-benefit-analysis frameworks can be applied to the preliminary selection of production use cases for the implementation of BDT in larger production systems. It supports the use case selection process from information about production needs, available BDT, and given condition(s) per use case. This concept paper attempts to consolidate the hitherto fragmented discourse on how to prioritize IBD projects, evaluates the challenges of prioritization in this field, and presents a prioritization concept to overcome these challenges.",2017,,10.1145/3175684.3175687
1462,"Cappiello, Cinzia and Sam\'{a}, Walter and Vitali, Monica",Quality Awareness for a Successful Big Data Exploitation,,"The combination of data and technology is having a high impact on the way we live. The world is getting smarter thanks to the quantity of collected and analyzed data. However, it is necessary to consider that such amount of data is continuously increasing and it is necessary to deal with novel requirements related to variety, volume, velocity, and veracity issues. In this paper we focus on veracity that is related to the presence of uncertain or imprecise data: errors, missing or invalid data can compromise the usefulness of the collected values. In such a scenario, new methods and techniques able to evaluate the quality of the available data are needed. In fact, the literature provides many data quality assessment and improvement techniques, especially for structured data, but in the Big Data era new algorithms have to be designed. We aim to provide an overview of the issues and challenges related to Data Quality assessment in the Big Data scenario. We also propose a possible solution developed by considering a smart city case study and we describe the lessons learned in the design and implementation phases.",2018,,
1463,"Sinaeepourfard, Amir and Garcia, Jordi and Masip-Bruin, Xavier and Mar\'{\i}n-Torder, Eva",Towards a Comprehensive Data Lifecycle Model for Big Data Environments,"vs challenges, data lifecycle, data management, big data, data complexity, data organization","A huge amount of data is constantly being produced in the world. Data coming from the IoT, from scientific simulations, or from any other field of the eScience, are accumulated over historical data sets and set up the seed for future Big Data processing, with the final goal to generate added value and discover knowledge. In such computing processes, data are the main resource; however, organizing and managing data during their entire life cycle becomes a complex research topic. As part of this, Data LifeCycle (DLC) models have been proposed to efficiently organize large and complex data sets, from creation to consumption, in any field, and any scale, for an effective data usage and big data exploitation.Several DLC frameworks can be found in the literature, each one defined for specific environments and scenarios. However, we realized that there is no global and comprehensive DLC model to be easily adapted to different scientific areas. For this reason, in this paper we describe the Comprehensive Scenario Agnostic Data LifeCycle (COSA-DLC) model, a DLC model which: i) is proved to be comprehensive as it addresses the 6Vs challenges (namely Value, Volume, Variety, Velocity, Variability and Veracity; and ii), it can be easily adapted to any particular scenario and, therefore, fit the requirements of a specific scientific field. In this paper we also include two use cases to illustrate the ease of the adaptation in different scenarios. We conclude that the comprehensive scenario agnostic DLC model provides several advantages, such as facilitating global data management, organization and integration, easing the adaptation to any kind of scenario, guaranteeing good data quality levels and, therefore, saving design time and efforts for the scientific and industrial communities.",2016,,10.1145/3006299.3006311
1464,"Mockus, Audris",Engineering Big Data Solutions,"Data Engineering, Data Science, Game Theory, Analytics, Statistics, Data Quality, Operational Data"," Structured and unstructured data in operational support tools have long been prevalent in software engineering. Similar data is now becoming widely available in other domains. Software systems that utilize such operational data (OD) to help with software design and maintenance activities are increasingly being built despite the difficulties of drawing valid conclusions from disparate and low-quality data and the continuing evolution of operational support tools. This paper proposes systematizing approaches to the engineering of OD-based systems. To prioritize and structure research areas we consider historic developments, such as big data hype; synthesize defining features of OD, such as confounded measures and unobserved context; and discuss emerging new applications, such as diverse and large OD collections and extremely short development intervals. To sustain the credibility of OD-based systems more research will be needed to investigate effective existing approaches and to synthesize novel, OD-specific engineering principles. ",2014,,10.1145/2593882.2593889
1465,"Savas, Onur and Sagduyu, Yalin and Deng, Julia and Li, Jason","Tactical Big Data Analytics: Challenges, Use Cases, and Solutions","cloud computing, algorithms, tactical environment, analytics, big data","We discuss tactical challenges of the Big Data analytics regarding the underlying data, application space, and com- puting environment, and present a comprehensive solution framework motivated by the relevant tactical use cases. First, we summarize the unique characteristics of the Big Data problem in the Department of Defense (DoD) context and underline the main differences from the commercial Big Data problems. Then, we introduce two use cases, (i) Big Data analytics with multi-intelligence (multi-INT) sensor data and (ii) man-machine crowdsourcing using MapReduce framework. For these two use cases, we introduce Big Data analytics and cloud computing solutions in a coherent frame- work that supports tactical data, application, and computing needs.",2014,,10.1145/2627534.2627561
1466,"Cuzzocrea, Alfredo and Damiani, Ernesto",Pedigree-Ing Your Big Data: Data-Driven Big Data Privacy in Distributed Environments,,"This paper introduces a general framework for supporting data-driven privacy-preserving big data management in distributed environments, such as emerging Cloud settings. The proposed framework can be viewed as an alternative to classical approaches where the privacy of big data is ensured via security-inspired protocols that check several (protocol) layers in order to achieve the desired privacy. Unfortunately, this injects considerable computational overheads in the overall process, thus introducing relevant challenges to be considered. Our approach instead tries to recognize the ""pedigree"" of suitable summary data representatives computed on top of the target big data repositories, hence avoiding computational overheads due to protocol checking. We also provide a relevant realization of the framework above, the so-called Data-dRIven aggregate-PROvenance privacy-preserving big Multidimensional data (DRIPROM) framework, which specifically considers multidimensional data as the case of interest.",2018,,10.1109/CCGRID.2018.00100
1467,"Bertot, John Carlo and Choi, Heeyoon","Big Data and E-Government: Issues, Policies, and Recommendations","open government, big data","The promises and potential of Big Data in transforming digital government services, governments, and the interaction between governments, citizens, and the business sector, are substantial. From ""smart"" government to transformational government, Big Data can foster collaboration; create real-time solutions to challenges in agriculture, health, transportation, and more; and usher in a new era of policy- and decision-making. There are, however, a range of policy challenges to address regarding Big Data, including access and dissemination; digital asset management, archiving and preservation; privacy; and security. This paper selectively reviews and analyzes the U.S. policy context regarding Big Data and offers recommendations aimed at facilitating Big Data initiatives.",2013,,10.1145/2479724.2479730
1468,"Liou, Teau-Jiuan and Weng, Ming-Wei and Lee, Liza",The Effect of Big Data Platforms on Multi-Stage Production System in Industrie 4.0,"Industrie 4.0, Big data, Digital transformation, Multi-stage assembly","The aim of this paper is to analyze how Industrie 4.0 triggers changes in the business models of manufacturing SMEs (small and medium-sized enterprises) by big data platforms in selected casting manufacturer in Taiwan. A generalized model is presented to determine the optimal production run time, production rate, the advertising effort and demand with observation features that minimize the total cost per unit time. Advances in science and technology such as IoT technology, big data platform to investigate information asymmetry between manufacturer and customers. Numerical examples and sensitivity analysis are then provided by the collecting real data from Taiwan. Finally, concluding remarks are offered.",2020,,10.1145/3418688.3418697
1469,"Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Shyu, Mei-Ling and Iyengar, S. S.",Multimedia Big Data Analytics: A Survey,"multimedia databases, retrieval, survey, data mining, machine learning, indexing, 5V challenges, Big data analytics, mobile multimedia, multimedia analysis","With the proliferation of online services and mobile technologies, the world has stepped into a multimedia big data era. A vast amount of research work has been done in the multimedia area, targeting different aspects of big data analytics, such as the capture, storage, indexing, mining, and retrieval of multimedia big data. However, very few research work provides a complete survey of the whole pine-line of the multimedia big data analytics, including the management and analysis of the large amount of data, the challenges and opportunities, and the promising research directions. To serve this purpose, we present this survey, which conducts a comprehensive overview of the state-of-the-art research work on multimedia big data analytics. It also aims to bridge the gap between multimedia challenges and big data solutions by providing the current big data frameworks, their applications in multimedia analyses, the strengths and limitations of the existing methods, and the potential future directions in multimedia big data analytics. To the best of our knowledge, this is the first survey that targets the most recent multimedia management techniques for very large-scale data and also provides the research studies and technologies advancing the multimedia analyses in this big data era.",2018,,10.1145/3150226
1470,"Cuzzocrea, Alfredo","Warehousing and Protecting Big Data: State-Of-The-Art-Analysis, Methodologies, Future Challenges","Big Data, Warehousing Big Data, Big Data Analytics, Protecting Big Data","This paper proposes a comprehensive critical survey on the issues of warehousing and protecting big data, which are recognized as critical challenges of emerging big data research. Indeed, both are critical aspects to be considered in order to build truly, high-performance and highly-flexible big data management systems. We report on state-of-the-art approaches, methodologies and trends, and finally conclude by providing open problems and challenging research directions to be considered by future efforts.",2016,,10.1145/2896387.2900335
1471,"Chatfield, Akemi Takeoka and Ojo, Adegboyega and Puron-Cid, Gabriel and Reddick, Christopher G.",Census Big Data Analytics Use: International Cross Case Analysis,"big data analytics, census big data, electronic census, big data challenges, cross case analysis, public value creation, use","Despite the growing practices in big data and big data analytics use, there is still the paucity of research on links between government big data analytics use and public value creation. This multi-case study of Australia, Ireland, Mexico, and U.S.A. examines the state of big data and big data analytics use in the national census context. The census agencies are at varying stages in digitally transforming their national census process, products and services through assimilating and using big data and big data analytics. The cross-case analysis of government websites and documents identified emerging agency challenges in creating public value in the national census context: (1) big data analytics capability development, (2) cross agency data access and data integration, and (3) data security, privacy &amp; trust. Based on the insights gained, a research model aims to postulate the possible links among challenges, big data/big data analytics use, and public value creation.",2018,,10.1145/3209281.3209372
1472,"Bahadi, Jihane and El Asri, Bouchra and Courtine, M\'{e}lanie and Rhanoui, Maryem and Kergosien, Yannick",Towards Efficient Big Data: Hadoop Data Placing and Processing,"Multidimensional approach, Intelligent processing, Data placing, Hadoop, Big Data, MapReduce jobs","Currently, the generated data flow is growing at a high rate resulting to the problem of data obesity and abundance, but yet a lack of pertinent information. To handle this Big Data, Hadoop is a distributed framework that facilitates data storage and processing. Although Hadoop is designed to deal with demands of storage and analysis of ever-growing Data, its performance characteristics are still to improve. In this regard, many approaches have been proposed to enhance Hadoop capabilities. Nevertheless, an overview of these approaches shows that several aspects need to be improved in terms of performance and data relevancy. The main challenge is how to extract efficiently value from the big data sources. For this purpose, we propose in this paper to discuss Hadoop architecture and intelligent data discovery, and propose an effective on-demand Big Data contribution enabling to process relevant data in efficient and effective way according to the stakeholder's needs, and aiming to boost Data appointment by integrating multidimensional approach.",2018,,10.1145/3289100.3289108
1473,"Johnson, Justin M and Khoshgoftaar, Taghi M",A Survey on Classifying Big Data with Label Noise,"deep learning, data streams, machine learning, label noise, classification, data quality, big data","Class label noise is a critical component of data quality that directly inhibits the predictive performance of machine learning algorithms. While many data-level and algorithm-level methods exist for treating label noise, the challenges associated with big data call for new and improved methods. This survey addresses these concerns by providing an extensive literature review on treating label noise within big data. We begin with an introduction to the class label noise problem and traditional methods for treating label noise. Next, we present 30 methods for treating class label noise in a range of big data contexts, i.e. high volume, high variety, and high velocity problems. The surveyed works include distributed solutions capable of operating on data sets of arbitrary sizes, deep learning techniques for large-scale data sets with limited clean labels, and streaming techniques for detecting class noise in the presence of concept drift. Common trends and best practices are identified in each of these areas, implementation details are reviewed, empirical results are compared across studies when applicable, and references to 17 open-source projects and programming packages are provided. An emphasis on label noise challenges, solutions, and empirical results as they relate to big data distinguishes this work as a unique contribution that will inspire future research and guide machine learning practitioners.",2021,,10.1145/3492546
1474,"Xin, Li and Tianyun, Shi and Xiaoning, Ma",Research on the Big Data Platform and Its Key Technologies for the Railway Locomotive System,"Railway, Key technology, Application platform, Locomotive system, Big data","In order to improve the efficiency of locomotive organization and the quality of locomotive operation, this paper analyzes and discusses the big data platform and some key technologies suitable for the big data application of the railway locomotive system. Firstly, the definition of big data of the railway locomotive system is proposed, and the current data characteristics of the railway locomotive system are summarized, then the status quo and demands of big data application of the railway locomotive system are analyzed. Secondly, the overall architecture of the big data platform for the railway locomotive system is proposed. Furthermore, seven application scenarios available for the big data platform are analyzed, including locomotive running organization, high-speed railway, repair, maintenance and other fields. Finally, some key technologies, which consist of data collection system of front-line operations, locomotive equipment portrait analysis, staff portrait analysis, transmission and analysis of locomotive video, intelligent auxiliary driving system, are provided to increase efficiency of the locomotive organization and capability of safety management. The obtained results can play a positive role in the construction and application of big data of the railway locomotive system.",2020,,10.1145/3404687.3404693
1475,"Liu, Yi and Peng, Jiawen and Yu, Zhihao",Big Data Platform Architecture under The Background of Financial Technology: In The Insurance Industry As An Example,"insurance industry, big data platform, time and space data, platform architecture, Financial technology","With the rise of the concept of financial technology, financial and technology gradually in-depth integration, scientific and technological means to become financial product innovation, improve financial efficiency and reduce financial transaction costs an important driving force. In this context, the new technology platform is from the business philosophy, business model, technical means, sales, internal management and other dimensions to re-shape the financial industry. In this paper, the existing big data platform architecture technology innovation, adding space-time data elements, combined with the insurance industry for practical analysis, put forward a meaningful product circle and customer circle.",2018,,10.1145/3297730.3297743
1476,"Schilling, Lisa M. and Pena-Jackson, Griselda and Russell, Seth and Corral, Janet and Kwan, Bethany and Ressalam, Julie",Co-Designing Learning Materials to Empower Laypersons to Better Understand Big Data and Big Data Methods,"community engagement, big data, co-design, education","University of Colorado Anschutz Medical Campus' Data Science to Patient Value Program and 2040 Partners for Health sought to create open learning materials for engaged citizens and community leaders regarding big data and big data methods to support their collaboration in patient-centered and participatorybased community research and evaluation. 2040 is a local nonprofit organization that cultivates partnerships in Aurora, Colorado neighborhoods to tackle critical health needs. Our goal was to co-design and co-create a series of big data learning modules accessible to community laypeople, so they might better understand big data topics and be empowered more actively engage in health research and evaluation that uses big data methods.",2019,,10.1145/3331651.3331659
1477,"Chen, Rui-Yang",Target Data Optimization Based on Big Data-Streaming for Two-Stage Fuzzy Extraction System,"fuzzy decision tree, big data-streaming, Target data optimization, fuzzy case-based reasoning","How to extract target data effectively and intelligently is key point in the big data-streaming operation. Data extractions need focus on priority of data selection to reduce impact of 4Vs because of requirement of real-time computation. Corresponding big data-streaming for three-stage extraction system is presented in terms of hierarchal base and fuzzy representations. The proposed approach is based on a combination of clustering, classification and relationships method with fuzzy weighted similarity under hierarchical feature-based model. Moreover, heuristic fuzzy CBR-FDT- algorithms are provided to explore the target data optimization. Successful case study and experiment with simulations demonstrated the performance of the proposed approach.",2018,,10.1145/3297730.3297731
1478,"Puangpontip, Supadchaya and Hewett, Rattikorn",Assessing Reliability of Big Data Stream for Smart City,"IoT, Data Reliability, Theory of evidence, Smart City","Proliferation of IoT (Internet of Things) and sensor technology has expedited the realization of Smart City. To enable necessary functions, sensors distributed across the city generate a huge volume of stream data that are crucial for controlling Smart City devices. However, due to conditions such as wears and tears, battery drain, or malicious attacks, not all data are reliable even when they are accurately measured. These data could lead to invalid and devastating consequences (e.g., failed utility or transportation services). The assessment of data reliability is necessary and challenging especially for Smart City, as it has to keep up with velocity of big data stream to provide up-to-date results. Most research on data reliability has focused on data fusion and anomaly detection that lack a quantified measure of how much the data over a period of time are adequately reliable for decision-makings. This paper alleviates these issues and presents an online approach to assessing Big stream data reliability in a timely manner. By employing a well-studied evidence-based theory, our approach provides a computational framework that assesses data reliability in terms of belief likelihoods. The framework is lightweight and easy to scale, deeming fit for streaming data. We evaluate the approach using a real application of light sensing data of 1,323,298 instances. The preliminary results are consistent with logical rationales, confirming validity of the approach.",2019,,10.1145/3372454.3372478
1479,"Auer, Florian and Felderer, Michael",Addressing Data Quality Problems with Metamorphic Data Relations,"quality assessment, data quality, metamorphic testing, big data, metamorphic data relations","In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.",2019,,10.1109/MET.2019.00019
1480,"Huang, Yiqing and Zhu, Fangzhou and Yuan, Mingxuan and Deng, Ke and Li, Yanhua and Ni, Bing and Dai, Wenyuan and Yang, Qiang and Zeng, Jia",Telco Churn Prediction with Big Data,"customer retention, telco churn prediction, big data","We show that telco big data can make churn prediction much more easier from the $3$V's perspectives: Volume, Variety, Velocity. Experimental results confirm that the prediction performance has been significantly improved by using a large volume of training data, a large variety of features from both business support systems (BSS) and operations support systems (OSS), and a high velocity of processing new coming data. We have deployed this churn prediction system in one of the biggest mobile operators in China. From millions of active customers, this system can provide a list of prepaid customers who are most likely to churn in the next month, having $0.96$ precision for the top $50000$ predicted churners in the list. Automatic matching retention campaigns with the targeted potential churners significantly boost their recharge rates, leading to a big business value.",2015,,10.1145/2723372.2742794
1481,"Netten, Niels and Bargh, Mortaza S. and Choenni, Sunil and Meijer, Ronald",Exploiting Big Data for Evaluation Studies,"data linkage, counterfactuals, ex-post policy evaluation, Big data","The collection and analysis of relevant data for evaluating public policies is not a straightforward task. An important type of such studies is the so-called ex-post evaluation. The main objective of ex-post evaluations is to determine to what extent a realized intervention is successful in tackling a societal challenge, e.g., youth unemployment. At a first glance an obvious method is to collect some baseline measurements for a set of relevant variables, apply the intervention for a while and collect the new measurement values for the same set of variables. Then, comparing the measurement values of the variables before and after the intervention provides an insight into the extent of successfulness of the intervention. This, however, is only true if the ""ceteris paribus"" condition holds. In practice it is infeasible to enforce this condition for societal challenges. Often, after having the baseline measurements, several phenomena emerge that may impact the new measurements without being taken into account. This makes it difficult to determine how much of the measured differences between the values of the variables before and after the intervention should be attributed to the emerging phenomena (or the so-called counterfactuals) and how much of the differences can be attributed to the applied intervention.This paper discusses how exploiting big data may contribute to the task of elucidating the influences of counterfactuals (and interventions) in ex-post evaluation studies. The paper proposes a framework to utilize big data for accounting for the impact of emerging phenomena in ex-post evaluation studies.",2017,,10.1145/3047273.3047377
1482,"Adenuga, Kayode I. and Muniru, Idris O. and Sadiq, Fatai I. and Adenuga, Rahmat O. and Solihudeen, Muhammad J.",Big Data in Healthcare: Are We Getting Useful Insights from This Avalanche of Data?,"Big Data, Challenges, Benefits, Analytics","The benefits of deriving useful insights from avalanche of data available everywhere cannot be overemphasized. Big Data analytics can revolutionize the healthcare industry. It can also ensure functional productivity, help forecast and suggest feedbacks to disease outbreaks, enhance clinical practice, and optimize healthcare expenditure which cuts across all stakeholders in healthcare sectors. Notwithstanding these immense capabilities available in the general application of big data; studies on derivation of useful insights from healthcare data that can enhance medical practice have received little academic attention. Therefore, this study highlighted the possibility of making very insightful healthcare outcomes with big data through a simple classification problem which classifies the tendency of individuals towards specific drugs based on personality measures. Our model though trained with less than 2000 samples and with a simple neural network architecture achieved mean accuracies of 76.87% (sd=0.0097) and 75.86% (sd=0.0123) for the 0.15 and 0.05 validation sets respectively. The relatively acceptable performance recorded by our model despite the small dataset could largely be attributed to number of attributes in our dataset. It is essential to uncover some of the many complexities in our societies in relations to healthcare; and through many machine learning architectures like the neural networks these complex relationships can be discovered",2019,,10.1145/3328833.3328841
1483,"Petrova-Antonova, Dessislava and Krasteva, Iva and Ilieva, Sylvia and Pavlova, Irena",Conceptual Architecture of GATE Big Data Platform,"Big Data Value Chain, GATE Platform, Smart City, Emerging Architectures, Big Data","Today we experience a data-driven society. All human activities, industrial processes and research lead to data generation of unprecedented scale, spurring new products, services and businesses. Big Data and its application have been a target for European Commission -- with more than 100 FP7 and about 50 H2020 funded projects under Big Data domain. GATE project aims to establish and sustain in the long run a Centre of Excellence as collaborative environment for conducting Big Data research and innovation, facilitated by GATE platform and Innovation Labs. This paper proposes a conceptual architecture of GATE platform, that is holistic, symbiotic, open, evolving and data-integrated. It is also modular and with component-based design that allows to position a mix of products and tools from different providers. GATE platform will enable start-ups, SMEs and large enterprises, as well as other organizations in a wide range of sectors, to build advanced Data driven services and applications. The usability of the proposed architecture is proven through a development of a sample time series data visualization application. Its architecture follows the proposed one through implementation of required components using open technology stack.",2019,,10.1145/3345252.3345282
1484,"Heinrich, Bernd and Hristova, Diana and Klier, Mathias and Schiller, Alexander and Szubartowicz, Michael",Requirements for Data Quality Metrics,"Data quality, data quality assessment, data quality metrics, requirements for metrics","Data quality and especially the assessment of data quality have been intensively discussed in research and practice alike. To support an economically oriented management of data quality and decision making under uncertainty, it is essential to assess the data quality level by means of well-founded metrics. However, if not adequately defined, these metrics can lead to wrong decisions and economic losses. Therefore, based on a decision-oriented framework, we present a set of five requirements for data quality metrics. These requirements are relevant for a metric that aims to support an economically oriented management of data quality and decision making under uncertainty. We further demonstrate the applicability and efficacy of these requirements by evaluating five data quality metrics for different data quality dimensions. Moreover, we discuss practical implications when applying the presented requirements.",2018,,10.1145/3148238
1485,"Fan, Wenfei",Data Quality: From Theory to Practice,,"Data quantity and data quality, like two sides of a coin, are equally important to data management. This paper provides an overview of recent advances in the study of data quality, from theory to practice. We also address challenges introduced by big data to data quality management.",2015,,10.1145/2854006.2854008
1486,"Wang, Hongzhi and Gao, Hong and Yin, Shenjun and Zhu, Jie",The Design of Course Architecture for Big Data,"big data, data science, course architecture","Big data is one of the hottest topic in not only academic but also enterprise, which provide grate requirements for the people with knowledge and experiences of big data. However, current education architecture of computer science could not provide sufficient training for big data. For the education for people suitable for big data era, we attempt to design a novel course architecture. Such course architecture will not change the skeleton of traditional course architecture of computer science but just add content and subjects that is adaptive for big data. In this paper, we discuss the goal, architecture and content of the course architecture.",2017,,10.1145/3063955.3063968
1487,"Amudhavel, J. and Padmapriya, V. and Gowri, V. and Lakshmipriya, K. and Kumar, K. Prem and Thiyagarajan, B.","Perspectives, Motivations and Implications Of Big Data Analytics","Data analytics, infringement, unstructured data, exploration, application, Big data, monitor","As today there is an enormous volume of data, examining these large sets contains structure and unstructured data of different types and sizes; big data analytics is used. Data Analytics allows the user to analyze the unusable data to make a faster and better decision. The Latest supply chain professionals are suffused with data, which provokes various new ways of thoughts regarding how the data are produced, ordered, controlled and analyzed. Data Quality in Supply-Chain Management is used to monitor and control the data. This paper presents the knowledge infrastructure about trends in big data analytics. Big Data can also be given as an all-encompassing term for any collection of data sets so large or complex that it becomes difficult to process using traditional data processing applications. The challenges include examination, confine, extent, exploration, sharing, storage, relocate, and visualization and privacy infringement. The Big Data have their application in various fields like in Tourism, Climate Research and many other fields.",2015,,10.1145/2743065.2743099
1488,"Al-Qirim, Nabeel and Tarhini, Ali and Rouibah, Kamel",Determinants of Big Data Adoption and Success,"big data challenges, big data success factors, Big data analytics, big data strategy","This research investigates the large hype surrounding big data (BD) and Analytics (BDA) in both academia and the business world. Initial insights pointed to large and complex amalgamations of different fields, techniques and tools. Above all, BD as a research field and as a business tool found to be under developing and is fraught with many challenges. The intention here in this research is to develop an adoption model of BD that could detect key success predictors. The research finds a great interest and optimism about BD value that fueled this current buzz behind this novel phenomenon. Like any disruptive innovation, its assimilation in organizations oppressed with many challenges at various contextual levels. BD would provide different advantages to organizations that would seriously consider all its perspectives alongside its lifecycle in the pre-adoption or adoption or implementation phases. The research attempts to delineate the different facets of BD as a technology and as a management tool highlighting different contributions, implications and recommendations. This is of great interest to researchers, professional and policy makers.",2017,,10.1145/3127942.3127961
1489,"Liu, Yingjian and Qiu, Meng and Liu, Chao and Guo, Zhongwen",Big Data Challenges in Ocean Observation: A Survey,"Ocean observation, Marine big data, Data analysis, Data storage, Data computing","Ocean observation plays an essential role in ocean exploration. Ocean science is entering into big data era with the exponentially growth of information technology and advances in ocean observatories. Ocean observatories are collections of platforms capable of carrying sensors to sample the ocean over appropriate spatiotemporal scales. Data collected by these platforms help answer a range of fundamental and applied research questions. Many countries are spending considerable amount of resources on ocean observing programs for various purposes. Given the huge volume, diverse types, sustained measurement, and potential uses of ocean observing data, it is a typical kind of big data, namely marine big data. The traditional data-centric infrastructure is insufficient to deal with new challenges arising in ocean science. New distributed, large-scale modern infrastructure backbone is urgently required. This paper discusses some possible strategies to solve marine big data challenges in the phases of data storage, data computing, and analysis. Some applications in physics, chemistry, geology, and biology illustrate the significant uses of marine big data. Finally, we highlight some challenges and key issues in marine big data.",2017,,10.1007/s00779-016-0980-2
1490,"Fugini, Mariagrazia and Finocchi, Jacopo",Data and Process Quality Evaluation in a Textual Big Data Archiving System,"content management, unstructured Big Data, machine learning, data quality, Big Data analytics, text analytics",The article presents a textual Big Data analytics solution developed in a real setting as a part of a high-capacity document digitization and storage system. A software based on machine learning techniques performs automated extraction and processing of textual contents. The work focuses on performance and data confidence evaluation and describes the approach to computing a set of indicators for textual data quality. It then presents experimental results.,2022,,10.1145/3461015
1491,"Ke, Changwen and Wang, Kuisheng",Research and Application of Enterprise Big Data Governance,"Data governance, governance framework, data quality","With1 the further development of information technology, data has become one of the core resources of enterprises. In the current era of big data, data governance has gradually become an important means for enterprises to make intelligent decisions, helping enterprises to occupy a favorable position in a highly competitive market environment. This paper briefly describes the current situation of enterprise big data governance, and analyzes the problems of current data governance from the perspectives of enterprise management and data itself. To deal with these problems, this paper proposes several suggestions and strategies from the perspectives of organization and management system construction, construction of a data standard system, improve the level of data quality management, data technology surpport, etc. Fully combining the advanced theories and methods of domestic data governance, this paper designs a data governance framework from the perspective of data application and innovation. The framework includes supervision and control modules, data governance staff organization modules, application and service modules, and data processing and integration modules. And the framework is applied to the data governance of electric power enterprises, which has important reference significance and value for enterprises to carry out data governance.",2018,,10.1145/3207677.3278000
1492,"Ramasamy, Ramachandran",Towards Big Data Analytics Framework: ICT Professionals Salary Profile Compilation Perspective,"business intelligence, big data analytics, ICT salary profile","This paper elucidates the opportunity on expanding the on-going preparation of salary profile of information communications technology (ICT) professionals of Malaysia into a big data analytics (BDA) activity. The current activity is based on structured database provided by the online job service providers. In essence, BDA framework entailing 5 Vs namely, volume, variety, velocity, veracity and value used in gauging the gaps and potential areas to consider in next stages.",2014,,10.1145/2691195.2691196
1493,"Ordonez, Carlos",Can We Analyze Big Data inside a DBMS?,"sql, parallel algorithms, big data, mapreduce, dbms","Relational DBMSs remain the main data management technology, despite the big data analytics and no-SQL waves. On the other hand, for data analytics in a broad sense, there are plenty of non-DBMS tools including statistical languages, matrix packages, generic data mining programs and large-scale parallel systems, being the main technology for big data analytics. Such large-scale systems are mostly based on the Hadoop distributed file system and MapReduce. Thus it would seem a DBMS is not a good technology to analyze big data, going beyond SQL queries, acting just as a reliable and fast data repository. In this survey, we argue that is not the case, explaining important research that has enabled analytics on large databases inside a DBMS. However, we also argue DBMSs cannot compete with parallel systems like MapReduce to analyze web-scale text data. Therefore, each technology will keep influencing each other. We conclude with a proposal of long-term research issues, considering the ""big data analytics"" trend.",2013,,10.1145/2513190.2513198
1494,"Mohammed, Tareq Abed and Ghareeb, Ahmed and Al-bayaty, Hussein and Aljawarneh, Shadi",Big Data Challenges and Achievements: Applications on Smart Cities and Energy Sector,"machin learning, data mining, data processing, OLAP, big data","In this paper, the Big Data challenges and the processing is analyzed, recently great attention has been paid to the challenges for great data, largely due to the wide spread of applications and systems used in real life, such as presentation, modeling, processing and large (often unlimited) data storage. Mass Data Survey, OLAP Mass Data, Mass Data Dissemination and Mass Data Protection. Consequently, we focus on further research trends and, as a default, we will explore a future research challenge research project in this area of research.",2019,,10.1145/3368691.3368717
1495,"Khan, Nawsher and Naim, Arshi and Hussain, Mohammad Rashid and Naveed, Quadri Noorulhasan and Ahmad, Naim and Qamar, Shamimul","The 51 V's Of Big Data: Survey, Technologies, Characteristics, Opportunities, Issues and Challenges","data storage, data characteristics, data generation, Big Data","Currently Big Data is the biggest buzzword, and definitely, we believe that Big Data is changing the world. Some researchers say Big Data will be even bigger buzzword than the Internet. With fast-growing computing resources, information and knowledge a new digital globe has emerged. Information is being created and stored at a fast rate and is being accessed by a vast range of applications through scientific computing, commercial workloads, and social media. In 2018, over 28 billion devices globally, are connected to the internet. In 2020, more than 50 billion smart appliances will be connected worldwide and internet traffic flow will be 92 times greater than it was in 2005. The usage of such a massive number of connected devices not only increase the data volume but also the velocity of data addition with speed of light on fiber optic and various wireless networks. This fast generation of enormous data creates numerous threats and challenges. There exist various approaches that are addressing issues and challenges of Big Data with the theory of Vs such as 3 V's, 5 V's, 7 V's etc. The objective of this work is to explore and investigate the status of the current Big Data domain. Further, a comprehensive overview of Big Data, its characteristics, opportunities, issues, and challenges have been explored and described with the help of 51 V's. The outcome of this research will help in understanding the Big Data in a systematic way.",2019,,10.1145/3312614.3312623
1496,"Zhu, Fangzhou and Luo, Chen and Yuan, Mingxuan and Zhu, Yijian and Zhang, Zhengqing and Gu, Tao and Deng, Ke and Rao, Weixiong and Zeng, Jia",City-Scale Localization with Telco Big Data,"regression models, telco big data, localization","It is still challenging in telecommunication (telco) industry to accurately locate mobile devices (MDs) at city-scale using the measurement report (MR) data, which measure parameters of radio signal strengths when MDs connect with base stations (BSs) in telco networks for making/receiving calls or mobile broadband (MBB) services. In this paper, we find that the widely-used location based services (LBSs) have accumulated lots of over-the-top (OTT) global positioning system (GPS) data in telco networks, which can be automatically used as training labels for learning accurate MR-based positioning systems. Benefiting from these telco big data, we deploy a context-aware coarse-to-fine regression (CCR) model in Spark/Hadoop-based telco big data platform for city-scale localization of MDs with two novel contributions. First, we design map-matching and interpolation algorithms to encode contextual information of road networks. Second, we build a two-layer regression model to capture coarse-to-fine contextual features in a short time window for improved localization performance. In our experiments, we collect 108 GPS-associated MR records in the centroid of Shanghai city with 12 x 11 square kilometers for 30 days, and measure four important properties of real-world MR data related to localization errors: stability, sensitivity, uncertainty and missing values. The proposed CCR works well under different properties of MR data and achieves a mean error of 110m and a median error of $80m$, outperforming the state-of-art range-based and fingerprinting localization methods.",2016,,10.1145/2983323.2983345
1497,"Petrou, Charilaos and Paraskevas, Michael",Signal Processing Techniques Restructure The Big Data Era,"convex optimization, signal processing techniques, statistical learning tools, stochastic approximation, big data","Big data science has been developed into a topic that attracts attention from industry, academia and governments. The main objective in Big Data science is to recognize and extract meaningful information from huge amounts of heterogeneous data and unstructured data (which constitute 95% of big data). Signal Processing (SP) techniques and related statistical learning (SL) tools such as Principal Component Analysis (PCA), R-PCA (Robust PCA), Compressive Sampling (CS), convex optimization (CO), stochastic approximation (SA), kernel based learning (KBL) tasks are used for robustness, compression and dimensionality reduction in Big Data arising challenges. This review paper introduces Big Data related SP techniques and presents applications of this emerging field.",2016,,10.1145/3003733.3003767
1498,"Debattista, Jeremy and Auer, S\""{O}ren and Lange, Christoph",Luzzu—A Methodology and Framework for Linked Data Quality Assessment,"linked data, Data quality, quality assessment","The increasing variety of Linked Data on the Web makes it challenging to determine the quality of this data and, subsequently, to make this information explicit to data consumers. Despite the availability of a number of tools and frameworks to assess Linked Data Quality, the output of such tools is not suitable for machine consumption, and thus consumers can hardly compare and rank datasets in the order of fitness for use. This article describes a conceptual methodology for assessing Linked Datasets, and Luzzu; a framework for Linked Data Quality Assessment. Luzzu is based on four major components: (1) an extensible interface for defining new quality metrics; (2) an interoperable, ontology-driven back-end for representing quality metadata and quality problems that can be re-used within different semantic frameworks; (3) scalable dataset processors for data dumps, SPARQL endpoints, and big data infrastructures; and (4) a customisable ranking algorithm taking into account user-defined weights. We show that Luzzu scales linearly against the number of triples in a dataset. We also demonstrate the applicability of the Luzzu framework by evaluating and analysing a number of statistical datasets against a variety of metrics. This article contributes towards the definition of a holistic data quality lifecycle, in terms of the co-evolution of linked datasets, with the final aim of improving their quality.",2016,,10.1145/2992786
1499,"Sahri, Soror and Moussa, Rim",Customized Eager-Lazy Data Cleansing for Satisfactory Big Data Veracity,"Veracity, Big data","Big data systems are becoming mainstream for big data management either for batch processing or real-time processing. In order to extract insights from data, quality issues are very important to address, particularly. A veracity assessment model is consequently needed. In this paper, we propose a model which ties quality of datasets and quality of query resultsets. We particularly examine quality issues raised by a given dataset, order attributes along their fitness for use and correlate veracity metrics to business queries. We validate our work using the open dataset NYC taxi’ trips.",2021,,10.1145/3472163.3472195
1500,"Berti-Equille, Laure and Ba, Mouhamadou Lamine",Veracity of Big Data: Challenges of Cross-Modal Truth Discovery,"Truth discovery, data quality, information extraction, data fusion, fact checking",,2016,,10.1145/2935753
1501,"Samosir, Ridha Sefina and Hendric, Harco Leslie and Gaol, Ford Lumban and Abdurachman, Edi and Soewito, Benfano",Measurement Metric Proposed For Big Data Analytics System,"Metric, Measurement, Software, Big Data Analytics","Big data is defined as a very large data set (volume), velocity and variety. Big data analytics systems must be supports for parallel processing and large storage. The problem of this research is how to identify measurement metric based on big data analytics system characteristic. One device that support big data platform is Hadoop. Measurement is a process for assigning values or symbols to the attributes of an entity. The purpose of measurement is to distinguish between entities one to another. Indicator for software measurement represented with a metric. The aim of this research is to proposes some measurement metric for big data analytics system. This research using UML exactly a class diagram in system modelling to identify the measurement metric. Both of dynamic and static metric is proposed as solution to measure big data analytics system. Result for this researh are some measurement ndicator both of dynamic and static metric based on class diagram for big data analytics.",2017,,10.1145/3168390.3168425
1502,"Barzan Abdalla, Hemn and Mustafa, Nasser and Ihnaini, Baha",Big Data: Finding Frequencies of Faulty Multimedia Data,"Classification using SVM, Map-reduce, Pre-Processing, Fault data detection, Big Data","In many health care domains, big data has arrived. How to manage and use big data better has become the focus of all walks of life. Many data sources provide the repeated fault data—the repeated fault data forming the delay of processing time and storage capacity. Big data includes properties like volume, velocity, variety, variability, value, complexity, and performance put forward more challenges. Most healthcare domains face the problem of testing for structured and unstructured data validation in big data. It provides low-quality data and delays in response. In testing process is delay and not provide the correct response. In Proposed, pre-testing and post-testing are used for big data testing. In pre-testing, classify fault data from different data sources. After Classification to group big data using SVM algorithms such as Text, Image, Audio, and Video file. In post-testing, to implement the pre-processing, remove the zero file size, unrelated file extension, and de-duplication after pre-processing to implement the Map-reduce algorithm to find out the big data efficiently. This process reduces the pre-processing time, reduces the server energy, and increases the processing time. To remove the fault data before pre-processing means to increase the processing time and data storage.",2021,,10.1145/3503928.3503929
1503,"Bansal, Maggi and Chana, Inderveer and Clarke, Siobh\'{a}n","A Survey on IoT Big Data: Current Status, 13 V’s Challenges, and Future Directions","V’s challenges for IoT big data, cloud IoT services, big data 2.0, IoT big data, cloud computing in IoT, IoT big data survey","Driven by the core technologies, i.e., sensor-based autonomous data acquisition and the cloud-based big data analysis, IoT automates the actuation of data-driven intelligent actions on the connected objects. This automation enables numerous useful real-life use-cases, such as smart transport, smart living, smart cities, and so on. However, recent industry surveys reflect that data-related challenges are responsible for slower growth of IoT in recent years. For this reason, this article presents a systematic and comprehensive survey on IoT Big Data (IoTBD) with the aim to identify the uncharted challenges for IoTBD. This article analyzes the state-of-the-art academic works in IoT and big data management across various domains and proposes a taxonomy for IoTBD management. Then, the survey explores the IoT portfolio of major cloud vendors and provides a classification of vendor services for the integration of IoT and IoTBD on their cloud platforms. After that, the survey identifies the IoTBD challenges in terms of 13 V’s challenges and envisions IoTBD as “Big Data 2.0.” Then the survey provides comprehensive analysis of recent works that address IoTBD challenges by highlighting their strengths and weaknesses to assess the recent trends and future research directions. Finally, the survey concludes with discussion on open research issues for IoTBD.",2020,,10.1145/3419634
1504,"Sumbaly, Roshan and Kreps, Jay and Shah, Sam",The Big Data Ecosystem at LinkedIn,"hadoop, data pipeline, machine learning, offline processing, data mining, big data","The use of large-scale data mining and machine learning has proliferated through the adoption of technologies such as Hadoop, with its simple programming semantics and rich and active ecosystem. This paper presents LinkedIn's Hadoop-based analytics stack, which allows data scientists and machine learning researchers to extract insights and build product features from massive amounts of data. In particular, we present our solutions to the ``last mile'' issues in providing a rich developer ecosystem. This includes easy ingress from and egress to online systems, and managing workflows as production processes. A key characteristic of our solution is that these distributed system concerns are completely abstracted away from researchers. For example, deploying data back into the online system is simply a 1-line Pig command that a data scientist can add to the end of their script. We also present case studies on how this ecosystem is used to solve problems ranging from recommendations to news feed updates to email digesting to descriptive analytical dashboards for our members.",2013,,10.1145/2463676.2463707
1505,"Bhardwaj, Dave and Ormandjieva, Olga",Rigorous Measurement Model for Validity of Big Data: MEGA Approach,"Representational Theory of Measurement,, Quality Characteristics (V's), Validity, Measurement Hierarchical Model, Big Data","Big Data is becoming a substantial part of the decision-making processes in both industry and academia, especially in areas where Big Data may have a profound impact on businesses and society. However, as more data is being processed, data quality is becoming a genuine issue that negatively affects credibility of the systems we build because of the lack of visibility and transparency of the underlying data. Therefore, Big Data quality measurement is becoming increasingly necessary in assessing whether data can serve its purpose in a particular context (such as Big Data analytics, for example). This research addresses Big Data quality measurement modelling and automation by proposing a novel quality measurement framework for Big Data (MEGA) that objectively assesses the underlying quality characteristics of Big Data (also known as the V's of Big Data) at each step of the Big Data Pipelines. Five of the Big Data V's (Volume, Variety, Velocity, Veracity and Validity) are currently automated by the MEGA framework. In this paper, a new theoretically valid quality measurement model is proposed for an essential quality characteristic of Big Data, called Validity. The proposed measurement information model for Validity of Big Data is a hierarchy of 4 derived measures / indicators and 5 based measures. Validity measurement is illustrated on a running example.",2021,,10.1145/3472163.3472171
1506,"Sadiq, Shazia and Dasu, Tamraparni and Dong, Xin Luna and Freire, Juliana and Ilyas, Ihab F. and Link, Sebastian and Miller, Miller J. and Naumann, Felix and Zhou, Xiaofang and Srivastava, Divesh",Data Quality: The Role of Empiricism,,"We outline a call to action for promoting empiricism in data quality research. The action points result from an analysis of the landscape of data quality research. The landscape exhibits two dimensions of empiricism in data quality research relating to type of metrics and scope of method. Our study indicates the presence of a data continuum ranging from real to synthetic data, which has implications for how data quality methods are evaluated. The dimensions of empiricism and their inter-relationships provide a means of positioning data quality research, and help expose limitations, gaps and opportunities.",2018,,10.1145/3186549.3186559
1507,"Anagnostopoulos, Christos and Triantafillou, Peter",Scaling out Big Data Missing Value Imputations: Pythia vs. Godzilla,"clustering, missing value, big data","Solving the missing-value (MV) problem with small estimation errors in big data environments is a notoriously resource-demanding task. As datasets and their user community continuously grow, the problem can only be exacerbated. Assume that it is possible to have a single machine (`Godzilla'), which can store the massive dataset and support an ever-growing community submitting MV imputation requests. Is it possible to replace Godzilla by employing a large number of cohort machines so that imputations can be performed much faster, engaging cohorts in parallel, each of which accesses much smaller partitions of the original dataset? If so, it would be preferable for obvious performance reasons to access only a subset of all cohorts per imputation. In this case, can we decide swiftly which is the desired subset of cohorts to engage per imputation? But efficiency and scalability is just one key concern! Is it possible to do the above while ensuring comparable or even better than Godzilla's imputation estimation errors? In this paper we derive answers to these fundamentals questions and develop principled methods and a framework which offer large performance speed-ups and better, or comparable, errors to that of Godzilla, independently of which missing-value imputation algorithm is used. Our contributions involve Pythia, a framework and algorithms for providing the answers to the above questions and for engaging the appropriate subset of cohorts per MV imputation request. Pythia functionality rests on two pillars: (i) dataset (partition) signatures, one per cohort, and (ii) similarity notions and algorithms, which can identify the appropriate subset of cohorts to engage. Comprehensive experimentation with real and synthetic datasets showcase our efficiency, scalability, and accuracy claims.",2014,,10.1145/2623330.2623615
1508,"Arruda, Darlan",Requirements Engineering in the Context of Big Data Applications,"big data requirements engineering, business goals, empirical software engineering., empirical studies, big data applications","Requirements Engineering (RE) plays an essential role in the software engineering process, being considered as one of the most critical phases of the software development life-cycle. As we might expect, then, the Requirements Engineering would play a similar role in the context of Big Data applications. However, practicing Requirements Engineering is a challenging and complex task. It involves (i) stakeholders with diverse backgrounds and levels of knowledge, (ii) different application domains, (iii) it is expensive and error-prone, (iii) it is important to be aligned with business goals, to name a few. Because it involves such complex activities, a lot has to be understood in order to properly address Requirements Engineering. Especially, when the technology domain (e.g., Big Data) is not yet well explored. In this context, this paper describes a research plan on Requirements Engineering involving the development of Big Data applications. The high-level goal is to investigate: (i) On the technical front, the Requirements Engineering activities with respect to the analysis and specification of Big Data requirements and, (ii) on the management side, the relationship between RE and Business Goals in the development of Big Data Software applications.",2018,,10.1145/3178315.3178323
1509,"Cuzzocrea, Alfredo and Bellatreche, Ladjel and Song, Il-Yeol",Data Warehousing and OLAP over Big Data: Current Challenges and Future Research Directions,"big data, data warehousing, big multidimensional data, olap","In this paper, we highlight open problems and actual research trends in the field of Data Warehousing and OLAP over Big Data, an emerging term in Data Warehousing and OLAP research. We also derive several novel research directions arising in this field, and put emphasis on possible contributions to be achieved by future research efforts.",2013,,10.1145/2513190.2517828
1510,"Potiguara Carvalho, Artur and Potiguara Carvalho, Fernanda and Dias Canedo, Edna and Potiguara Carvalho, Pedro Henrique","Big Data, Anonymisation and Governance to Personal Data Protection","Anonymisation, Privacy, Big Data, Governance, Personal Data Protection","In a massive processing data era, an emerging impasse has taking scenario: privacy. In this context, personal data receive particular attention, witch its laws and guidelines that ensure better and legal use of data. The General Data Protection Regulation (GDPR) - in the European Union - and the Brazilian General Data Protection Law (LGPD) - in Brazil - lead to anonymisation (and its processes and techniques) as a way to reach secure use of personal data. However, expectations placed on this tool must be reconsidered according to risks and limits of its use, mainly when this technique is applied to Big Data. We discussed whether anonymisation used in conjunction with good data governance practices could provide greater protection for privacy. We conclude that good governance practices can strengthen privacy in anonymous data belonging to a Big Data, and we present a suggestive governance framework aimed at privacy. ",2020,,10.1145/3396956.3398253
1511,"Dessalk, Yared Dejene and Nikolov, Nikolay and Matskin, Mihhail and Soylu, Ahmet and Roman, Dumitru",Scalable Execution of Big Data Workflows Using Software Containers,"Domain-specific languages, Big Data workflows, Software containers","Big Data processing involves handling large and complex data sets, incorporating different tools and frameworks as well as other processes that help organisations make sense of their data collected from various sources. This set of operations, referred to as Big Data workflows, require taking advantage of the elasticity of cloud infrastructures for scalability. In this paper, we present the design and prototype implementation of a Big Data workflow approach based on the use of software container technologies and message-oriented middleware (MOM) to enable highly scalable workflow execution. The approach is demonstrated in a use case together with a set of experiments that demonstrate the practical applicability of the proposed approach for the scalable execution of Big Data workflows. Furthermore, we present a scalability comparison of our proposed approach with that of Argo Workflows - one of the most prominent tools in the area of Big Data workflows.",2020,,10.1145/3415958.3433082
1512,"Gong, Yiwei and Janssen, Marijn",Enterprise Architectures for Supporting the Adoption of Big Data,"enterprise architecture, ICT-architecture, e-government, open data, infrastructure, big data, BOLD","Governments from all over the world are struggling to take advantage of big data developments. Enterprise Architecture (EA) can be used as an instrument to integrate big data (BD) in the existing business processes and ICT-landscape. In this policy paper, we explore the role of EA in the adoption of BD. For this, we adopted a qualitative case study approach and investigated a large administrative organization that was in the process of adopting BD. We found in our case study that the first attempts were focused on integrating big data in the current landscape, but this encountered too many challenges that halt progress. To overcome the challenges, a separate BD department and accompanying infrastructure was created. The strategy was first to reap the benefits of BD and to understand what should be done, and thereafter integrating the working systems in the existing landscape. The findings suggest that current infrastructures might not be suitable for integrating BD and substantial changes are needed first. In the case the role of BD needed to be first clarified before EA could play a role in adopting BD. EA should deal with the uncertainties and complexities by ensuring a configurable landscape, by providing an incremental approach for adapting the infrastructure step-by-step, before the benefits of big data can be gained. Developing an incremental migration plan was found to be a key aspect for the adoption of BD.",2017,,10.1145/3085228.3085275
1513,"Hu, Zhifeng and Zhao, Feng and Zhao, Xiaona",Research on Smart Education Service Platform Based on Big Data,"Smart education, Big data, Information-oriented education","The big data technology can be applied to build the education service platforms and construct the big data analysis and application system as well as the multi-dimensional perception system. The big data analysis assists in the teaching process and breaks the temporal and spatial restrictions of educational resources, to realize the diversification of educational resources and improve the effectiveness of teaching feedback. This paper proposes a smart education service platform based on big data, which can promote the organic integration of educational communication, educational research, learning activities, teaching affairs administration, and information infrastructures. At the same time, the platform provides smarter, more efficient, and accurate services for teaching.",2020,,10.1145/3453187.3453340
1514,"Zhu, Yada and Xiong, Jinjun","Modern Big Data Analytics for ""Old-Fashioned"" Semiconductor Industry Applications","analytics, Big data, semiconductor, manufacturing","Big data analytics is the latest spotlight with all the glare of fame ranging from media coverage to booming start-up companies to eye-catching merges and acquisitions. On the contrary, the $336 billion industry of semiconductor was seen as an ""old-fashioned"" business, with fading interests from the best and brightest among young graduates and engineers. How will modern big data analytics help the semiconductor industry walk through this transition? This paper answers this question via a number of practical but challenging problems arising from semiconductor manufacturing process. We show that many existing machine learning algorithms are not well positioned to solve these problems, and novel techniques involving temporal, structural and hierarchical properties need to be developed to solve these problems.",2015,,
1515,"Ding, Shifu and Liu, Yan and Zhang, Jianjun and Tan, Yaqi and Li, Xiaoxia and Tang, RuiChun",The Planning and Construction of Healthcare Big Data Platform,"Big Data, Healthcare, Electronic Health Record, EMR, SOA","Healthcare Big Data Platform is the important content in the process of medical information industry. To a certain extent, it represents the overall level of the regional informatization. It is also a data exchange and sharing platform connecting the basic systems of local various medical and health institutions, and it is also the base and carrier to integrate the regional information system. This paper introduces the local regional medical informatization construction, planning architecture, data center construction mode and technical realization methods. Through this project, the informatization level of basic health agencies and all hospitals will have been greatly improved. It can provide more convenient and high-quality medical service for patients, alleviates ""difficulty and expensive"" problem effectively.",2020,,10.1145/3433996.3434027
1516,"Chen, Xin and Vo, Hoang and Aji, Ablimit and Wang, Fusheng",High Performance Integrated Spatial Big Data Analytics,"spatial analytics, database, data warehouse, GIS, MapReduce","The growth of spatial big data has been explosive thanks to cost-effective and ubiquitous positioning technologies, and the generation of data from multiple sources in multi-forms. Such emerging spatial data has high potential to create new insights and values for our life through spatial analytics. However, spatial data analytics faces two major challenges. First, spatial data is both data-and compute-intensive due to the massive amounts of data and the multi-dimensional nature, which requires high performance spatial computing infrastructure and methods. Second, spatial big data sources are often isolated, for example, OpenStreetMap, census data and Twitter tweets are independent data sources. This leads to incompleteness of information and sometimes limited data accuracy, thus limited values from the data. Integrating spatial big data analytics by consolidating multiple data sources provides significant potential for data quality improvement in terms of completeness and accuracy, and much increased values derived from the data. In this paper, we present our vision of a high performance integrated spatial big data analytics framework. We provide a scalable spatial query based data integration engine with MapReduce, and demonstrate integrated spatial data analytics through a few use cases in our preliminary work. We then present our future plan on integrated spatial big data analytics for improving public health research and applications.",2014,,10.1145/2676536.2676538
1517,"Man, Rui and Zhou, Guomin and Fan, Jingchao",Research on Scientific Data Management in Big Data Era,"Opening and sharing of data resource, Scientific data management, Big data, Scientific data","Scientific data is an important strategic resource in the era of big data. Efficient management and wide circulation are the key ways to enhance the value of scientific data resources. With the transformation of the industrial society into the information society, the importance of scientific data management is also increasing all over the world, which continuously promotes the maturity of scientific data management and sharing. In this article, through comprehensive research of scientific data management ideas, policies, practices and results, the analysis summarizes the advanced experience of international scientific data management, for the similar problems and challenges existing in the research in China, puts forward the future a period of time the direction and suggestions on the development of scientific data management: 1. the specification of various kinds of degree of the standardization of scientific data resources; 2. To strengthen data mining capacity; 3. To strengthen the cultivation of talents in data science; 4. To strengthen international cooperation and enhance core competitiveness in the big data era.",2020,,10.1145/3424978.3425010
1518,"Labouseur, Alan G. and Matheus, Carolyn C.",An Introduction to Dynamic Data Quality Challenges,"graph systems, Dynamic data quality, relational systems, internet of things, big data",,2017,,10.1145/2998575
1519,"Zhao, Liangbin and Fu, Xiuju",A Visual Method for Ship Close Encounter Pattern Recognition Based on Fuzzy Theory and Big Data Intelligence,"AIS data, Ship encounter, Fuzzy theory, Visualization","As an important application of big data technology in the maritime field, big data driven visualization of ship encounter patterns helps to intuitively understand the risk situation in the water traffic. However traditional methods based on fixed thresholds do not consider the fuzziness of classification on ship encounter situations. We proposed a visual method to visualize the risk situations caused by the interaction between vessel traffic flows in more detail based on fuzzy theory and big data intelligence on large scale AIS data. A case study is conducted to verify the applicability based on the AIS data from Singapore Strait. Visualization results of density in grids show that the proposed method can effectively reflect the ship encounter patterns, which are consistent with the real situation and can show more valuable details for the safety assessment of water traffic.",2020,,10.1145/3445945.3445962
1520,"Sun, Yu and Niu, Yanfang and Lu, Le",Research on Influencing Factors of Government Audit Big Data Capability,"Government audit, Big data analysis capability, Influencing factors","The widespread application of big data has had a profound impact on social and economic development. Government auditing is the guarantee for the modernization of national governance, and the development of big data audit capability has become the key to improving national governance capability. This paper summarizes the concept of government audit big data capability, and constructs the influencing factor model of government audit big data capability. This study finds that the construction degree of audit big data platform, big data management ability, big data audit technology and auditors' big data technology ability have a significant positive impact on the government audit big data ability, and the audit organization coordination ability plays a positive moderating effect in the whole impact process. This study provides guidance for the improvement and development of government audit big data capability.",2021,,10.1145/3508259.3508284
1521,"Fang, Ruogu and Pouyanfar, Samira and Yang, Yimin and Chen, Shu-Ching and Iyengar, S. S.",Computational Health Informatics in the Big Data Age: A Survey,"4V challenges, machine learning, survey, data mining, computational health informatics, Big data analytics, clinical decision support","The explosive growth and widespread accessibility of digital health data have led to a surge of research activity in the healthcare and data sciences fields. The conventional approaches for health data management have achieved limited success as they are incapable of handling the huge amount of complex data with high volume, high velocity, and high variety. This article presents a comprehensive overview of the existing challenges, techniques, and future directions for computational health informatics in the big data age, with a structured analysis of the historical and state-of-the-art methods. We have summarized the challenges into four Vs (i.e., volume, velocity, variety, and veracity) and proposed a systematic data-processing pipeline for generic big data in health informatics, covering data capturing, storing, sharing, analyzing, searching, and decision support. Specifically, numerous techniques and algorithms in machine learning are categorized and compared. On the basis of this material, we identify and discuss the essential prospects lying ahead for computational health informatics in this big data age.",2016,,10.1145/2932707
1522,"Zeng, Xuezhi and Garg, Saurabh and Barika, Mutaz and Zomaya, Albert Y. and Wang, Lizhe and Villari, Massimo and Chen, Dan and Ranjan, Rajiv",SLA Management for Big Data Analytical Applications in Clouds: A Taxonomy Study,"big data analytics application, SLA, Big data, service layer, service level agreement, SLA metrics","Recent years have witnessed the booming of big data analytical applications (BDAAs). This trend provides unrivaled opportunities to reveal the latent patterns and correlations embedded in the data, and thus productive decisions may be made. This was previously a grand challenge due to the notoriously high dimensionality and scale of big data, whereas the quality of service offered by providers is the first priority. As BDAAs are routinely deployed on Clouds with great complexities and uncertainties, it is a critical task to manage the service level agreements (SLAs) so that a high quality of service can then be guaranteed. This study performs a systematic literature review of the state of the art of SLA-specific management for Cloud-hosted BDAAs. The review surveys the challenges and contemporary approaches along this direction centering on SLA. A research taxonomy is proposed to formulate the results of the systematic literature review. A new conceptual SLA model is defined and a multi-dimensional categorization scheme is proposed on its basis to apply the SLA metrics for an in-depth understanding of managing SLAs and the motivation of trends for future research.",2020,,10.1145/3383464
1523,"Lin, Jimmy and Ryaboy, Dmitriy",Scaling Big Data Mining Infrastructure: The Twitter Experience,,"The analytics platform at Twitter has experienced tremendous growth over the past few years in terms of size, complexity, number of users, and variety of use cases. In this paper, we discuss the evolution of our infrastructure and the development of capabilities for data mining on ""big data"". One important lesson is that successful big data mining in practice is about much more than what most academics would consider data mining: life ""in the trenches"" is occupied by much preparatory work that precedes the application of data mining algorithms and followed by substantial effort to turn preliminary models into robust solutions. In this context, we discuss two topics: First, schemas play an important role in helping data scientists understand petabyte-scale data stores, but they're insufficient to provide an overall ""big picture"" of the data available to generate insights. Second, we observe that a major challenge in building data analytics platforms stems from the heterogeneity of the various components that must be integrated together into production workflows---we refer to this as ""plumbing"". This paper has two goals: For practitioners, we hope to share our experiences to flatten bumps in the road for those who come after us. For academic researchers, we hope to provide a broader context for data mining in production environments, pointing out opportunities for future work.",2013,,10.1145/2481244.2481247
1524,"Han, Ping",Research on Foreign Exchange Management Model Based on Big Data,"Big data background, foreign exchange management, mode","The rapid development of Internet technology has promoted the process of economic globalization, and the application of big data technology has injected new vitality into the innovation of foreign exchange management models. Businesses such as foreign exchange deposits and loans and foreign currency exchange have encountered new development opportunities. Under the effect of big data technology, it can quickly process massive amounts of information, respond to various exchange rate changes, and achieve the improvement of foreign exchange business management. Especially under the circumstances of the current RMB marketization, exchange rate reform and the diversification of the international situation, the difficulty of foreign exchange management is gradually increasing. How to better improve the efficiency of foreign exchange management has become a problem that must be solved at present. Therefore, it is of great significance to explore the foreign exchange management model based on the background of big data, build a big data computing mechanism, give full play to its advantages in foreign exchange management, and promote the improvement of foreign exchange management.",2020,,10.1145/3402569.3409041
1525,"Zakerzadeh, Hessam and Aggarwal, Charu C. and Barker, Ken",Privacy-Preserving Big Data Publishing,,"The problem of privacy-preserving data mining has been studied extensively in recent years because of its importance as a key enabler in the sharing of massive data sets. Most of the work in privacy has focussed on issues involving the quality of privacy preservation and utility, though there has been little focus on the issue of scalability in privacy preservation. The reason for this is that anonymization has generally been seen as a batch and one-time process in the context of data sharing. However, in recent years, the sizes of data sets have grown tremendously to a point where the effective application of the current algorithms is becoming increasingly difficult. Furthermore, the transient nature of recent data sets has resulted in an increased need for the repeated application of such methods on the newer data sets which have been collected. Repeated application demands even greater computational efficiency in order to be practical. For example, an algorithm with quadratic complexity is unlikely to be implementable in reasonable time over terabyte scale data sets. A bigger issue is that larger data sets are likely to be addressed by distributed frameworks such as MapReduce. In such frameworks, one has to address the additional issue of minimizing data transfer across different nodes, which is the bottleneck. In this paper, we discuss the first approach towards privacy-preserving data mining of very massive data sets using MapReduce. We study two most widely-used privacy models k-anonymity and l-diversity for anonymization, and present experimental results illustrating the effectiveness of the approach.",2015,,10.1145/2791347.2791380
1526,"Malaka, Iman and Brown, Irwin",Challenges to the Organisational Adoption of Big Data Analytics: A Case Study in the South African Telecommunications Industry,"Technology Adoption, Big Data Analytics, Big Data, South Africa","The purpose of this interpretive study was to explore the challenges to the adoption of Big Data Analytics (BDA) in organisations. The Technology-Organisation-Environment (TOE) model was used to guide the study. Data was collected from a large telecommunication organization in South Africa. Seven participants, from both Information Technology (IT) and business were interviewed to gain a holistic overview of challenges towards the adoption of BDA. An inductive approach was used for analysis. Findings revealed technological challenges to the adoption of BDA as being Data Integration; Data Privacy; Return on Investment; Data Quality; Cost; Data Integrity; and Performance and Scalability. From the organizational perspective, the major challenges were Ownership and Control; Skills Shortages; Business Focus and Prioritisation; Training and Exposure; Silos; and Unclear Processes. From the environmental context there were no major challenges highlighted. Organisational challenges were deemed to be the major inhibitors to adoption of BDA",2015,,10.1145/2815782.2815793
1527,"Shen, Shaoyi and Li, Bin and Li, Situo",Construction and Application of Big Data Analysis Platform for Enterprise,"Construction, Shared, Analysis of big data, Distributed, Data asset","A data revolution has been leading by big data, which have the extremely profound influence on the economic, social development and public life. This paper introduces the meaning of big data, and discusses the innovation and opportunity of enterprise under the perspective of big data. According to the information architecture, this paper supplies the basic construction of enterprise big data analysis platform, and suggests the strategy of application, which have certain realistic directive significance.",2019,,10.1145/3374587.3374650
1528,"Diao, Yanhua",Tourism Prediction Based on Multi-Source Big Data Fusion Technology,"Data fusion technology, Multi-source big data, Tourism prediction","In the practical application of existing big data tourism prediction, there are some practical problems, such as complicated data sources and difficult fusion, low prediction accuracy and poor guiding practice effect. In view of this situation, this paper intends to build a tourism big data index prediction model suitable for the characteristics of tourism development through core data extraction, multi-source data fusion, complex data modeling and other key technologies. With the help of the improved tourism prediction model based on multi-source big data fusion technology, the tourist flow and consumption characteristics of Shandong province are more accurately identified and predicted. It can provide help for optimizing public service of tourism, strengthening early warning of tourist flow and improving marketing strategy of tourist destination. This study innovatively supplements the effective integration theory of multi-source tourism big data and the organic integration theory of big data and traditional sampling survey data. At the same time, the relevant methods of tourism big data forecasting model are extended.",2021,,10.1145/3501409.3501593
1529,"Kun-fa, Li and Jing-chun, Chen and Yan-xi, Wang",Big Data Informatization Applied to Optimization of Human Resource Performance Management,"Big data, human resources, performance management","With the development of technology in the era of digital big data in the network and the promotion of network technology, big data is simultaneously integrated into different industry sectors to achieve Internet performance management, and enhance the new perspective of enterprise human resources performance management activities. Today's Internet, cloud computing, Internet of Things and other industrial technologies have undergone repeated changes, showing an unprecedented picture. At present, the subjective awareness of enterprise human resources performance management is too strong, lack of objective data understanding, and the theoretical framework of big data human resource management is not fully applied. This paper reconstructs the data system from four aspects: data source, collection, integration and analysis. Innovate the human resources performance management method from the system to provide more scientific and specific ideas for human resource performance management.",2019,,10.1145/3357292.3357302
1530,"Cheng, Susu and Zhao, Haijun",An Overview of Techniques for Confirming Big Data Property Rights,"Big Data, Confirmation of Information Property, Information property index, Method for confirming information property rights","The major premise of big data circulation is to identify the ownership of data resource. This paper summed some feasible techniques and methods for confirming big data property which are data citation technology, data provenance technology, data reversible hiding technology, computer forensic technology and block chain technology. The ownership of information property which from different sizes, different formats and different storage condition on distributed heterogeneous platforms can be confirmed by comprehensive application of these techniques and methods based on the coupling interface between them in the practice of big data.",2018,,10.1145/3193063.3193069
1531,"Al-Mansoori, Ahmed and Yu, Shui and Xiang, Yong and Sood, Keshav",A Survey on Big Data Stream Processing in SDN Supported Cloud Environment,"big data stream processing, SDN, resource optimization, cloud computing, cost optimization, big data","Big data is the term which denotes data with features such as voluminous data, a variety of data and streaming data as well. Processing big data became essential for enterprises to garner general intelligence and avoid biased conclusions. Due to these features, big data processing is considered to be a challenging task. Big data Processing should rely on a robust network. Cloud computing offers a suitable environment for these processes. However, it is more challenging when we move big data to the cloud, as managing the cloud resources is the main issue. Software Defined Network (SDN) has a potential solution to this issue. In this paper, first, we survey the present state of the art of SDN, cloud computing, and Big data Stream processing (BDSP). Then, we discuss SDN in the context of Big Data Stream Processing in Cloud environment. Finally, critical issues and research opportunity are discussed.",2018,,10.1145/3167918.3167924
1532,"Saraee, Mo and Silva, Charith",A New Data Science Framework for Analysing and Mining Geospatial Big Data,"big data, machine learning, data mining, data science, geospatial big data","Geospatial Big Data analytics are changing the way that businesses operate in many industries. Although a good number of research works have reported in the literature on geospatial data analytics and real-time data processing of large spatial data streams, only a few have addressed the full geospatial big data analytics project lifecycle and geospatial data science project lifecycle. Big data analysis differs from traditional data analysis primarily due to the volume, velocity and variety characteristics of the data being processed. One of a motivation of introducing new framework is to address these big data analysis challenges. Geospatial data science projects differ from most traditional data analysis projects because they could be complex and in need of advanced technologies in comparison to the traditional data analysis projects. For this reason, it is essential to have a process to govern the project and ensure that the project participants are competent enough to carry on the process. To this end, this paper presents, new geospatial big data mining and machine learning framework for geospatial data acquisition, data fusion, data storing, managing, processing, analysing, visualising and modelling and evaluation. Having a good process for data analysis and clear guidelines for comprehensive analysis is always a plus point for any data science project. It also helps to predict required time and resources early in the process to get a clear idea of the business problem to be solved.",2018,,10.1145/3220228.3220236
1533,"Musto, Jiri and Dahanayake, Ajantha",Integrating Data Quality Requirements to Citizen Science Application Design,"Data Quality requirements, Data Quality Characteristics, Data quality, Conceptual model, Citizen science","Data quality is an important aspect in many fields. In citizen science application databases, data quality is often found lacking, which is why there needs to be a method of integrating data quality into the design. This paper tackles the problem by dividing data quality into separate characteristics according to the ISO / IEC 25012 standard. These characteristics are integrated into a conceptual model of the system and data model for citizen science applications. Furthermore, the paper describes a way to measure data quality using the data quality characteristics. The models and measuring methods are theoretical and can be adapted into case specific designs.",2019,,10.1145/3297662.3365797
1534,"Zhichao, Xu and Jiandong, Zhao and Huan, Huang",Based on Hadoop's Tech Big Data Combination and Mining Technology Framework,"tech big data, combination, Hadoop, mining","With the advent of the Internet + era in the field of Tech big data, the big data of Tech big data has a large amount of data and various characteristics. It is an important means to carry out research on the big data of Tech big data to realize the combination and mining of efficient multi-source foreign technology data. However, at present, the big data of Tech big data are divided into disciplines and different formats, which are difficult to realize the intersection of effective scientific and technological information and realize data sharing. This paper puts forward a kind of big data combined with Tech big data and mining technology based on the Hadoop framework.It includes a unified collection and preprocessing method of big data of Tech big data and the design of storage and management platform for data sources. It is based on Map/Reduce Tech big data parallelization computing model and system.Its correlation with important scientific data mining services.The framework has good practicability and expansibility.",2018,,10.1145/3194206.3194229
1535,"Bizer, Christian and Boncz, Peter and Brodie, Michael L. and Erling, Orri",The Meaningful Use of Big Data: Four Perspectives -- Four Challenges,,"Twenty-five Semantic Web and Database researchers met at the 2011 STI Semantic Summit in Riga, Latvia July 6-8, 2011[1] to discuss the opportunities and challenges posed by Big Data for the Semantic Web, Semantic Technologies, and Database communities. The unanimous conclusion was that the greatest shared challenge was not only engineering Big Data, but also doing so meaningfully. The following are four expressions of that challenge from different perspectives.",2012,,10.1145/2094114.2094129
1536,"Markus, M. Lynne and Topi, Heikki","Big Data, Big Decisions for Science, Society, and Business: Report on a Research Agenda Setting Workshop",,"The report from the workshop, ""Big Data, Big Decisions for Government, Business and Society,"" makes a number of astute contributions. There is no need to replicate them in this foreword - they are in the report. What might be missed comes between the lines, where provocative points are made. Big Data means big opinions and big stakes. Those who think Big Data important want to be proven right, those who think Big Data a passing fad want Big Data to fade, and those who think Big Data will bring profound change hope for change. Big Data, like everything important, is political.",2015,,
1537,"Yu, Bangbo and Zhao, Haijun",Research on the Construction of Big Data Trading Platform in China,"big data trading platform, regulatory construction, Data assets","As a new type of asset, the value of big data resources can only be realized in the transaction circulation. Establishing and improving the big data trading platform market system is a systematic project to transform data from resources into assets. Through the comparative analysis of several typical big data trading platform construction practices in China, this research finds some problems, such as unclear positioning of some platforms leading to overlapping functions, extensive data transaction, lack of unified data pricing methods, unclear data ownership. In addition, there is no difference between the data escrow transaction mode and the aggregate transaction mode, and the rights of the data supply parties cannot be guaranteed. And it also discusses how to promote the construction and improvement of China's big data trading market more systematically, normally and institutionally. Finally, it proposes to build local big data trading platforms according to local conditions, establish a data transaction system based on blockchain, establish a big data transaction pricing index system, and establish a big data standard system.",2019,,10.1145/3321454.3321474
1538,"El Bousty, Hicham and krit, Salah-ddine and Elasikri, Mohamed and Dani, Hassan and Karimi, Khaoula and Bendaoud, Kaoutar and Kabrane, Mustapha","Investigating Business Intelligence in the Era of Big Data: Concepts, Benefits and Challenges","Data Warehouse, Big Data, Business Intelligence, Cloud Computing","Business intelligence suppose retrieving value from data floating in the organization environment. It provides methods and tools for collecting, storing, formatting and analyzing data for the purpose of helping managers in decision-making. At the start, only data from enterprise internal activities were examined. Now and in this turbulent business environment, organizations should incorporate analysis of the huge amount of external data gathered from multifarious sources. It is argued that BI systems accuracy depends on quantity of data at their disposal, yet some storage and analysis methods are phased out and should be reviewed by academics and practitioners.This paper presents an overview of BI challenges in the context of Big Data (BD) and some available solutions provided, either by using Cloud Computing (CC) or improving Data Warehouse (DW) efficiency.",2018,,10.1145/3234698.3234723
1539,"Zhang, He and Zhou, Xin and Huang, Xin and Huang, Huang and Babar, Muhammad Ali",An Evidence-Based Inquiry into the Use of Grey Literature in Software Engineering,"evidence-based software engineering, systematic (literature) review, survey, empirical software engineering, grey literature","Context: Following on other scientific disciplines, such as health sciences, the use of Grey Literature (GL) has become widespread in Software Engineering (SE) research. Whilst the number of papers incorporating GL in SE is increasing, there is little empirically known about different aspects of the use of GL in SE research.Method: We used a mixed-methods approach for this research. We carried out a Systematic Literature Review (SLR) of the use of GL in SE, and surveyed the authors of the selected papers included in the SLR (as GL users) and the invited experts in SE community on the use of GL in SE research. Results: We systematically selected and reviewed 102 SE secondary studies that incorporate GL in SE research, from which we identified two groups based on their reporting: 1) 76 reviews only claim their use of GL; 2) 26 reviews report the results by including GL. We also obtained 20 replies from the GL users and 24 replies from the invited SE experts. Conclusion: There is no common understanding of the meaning of GL in SE. Researchers define the scopes and the definitions of GL in a variety of ways. We found five main reasons of using GL in SE research. The findings have enabled us to propose a conceptual model for how GL works in SE research lifecycle. There is an apparent need for research to develop guidelines for using GL in SE and for assessing quality of GL. The current work can provide a panorama of the state-of-the-art of using GL in SE for the follow-up research, as to determine the important position of GL in SE research.",2020,,10.1145/3377811.3380336
1540,"Li, Xiaocan and Xie, Kun and Wang, Xin and Xie, Gaogang and Xie, Dongliang and Li, Zhenyu and Wen, Jigang and Diao, Zulong and Wang, Tian",Quick and Accurate False Data Detection in Mobile Crowd Sensing,,"The attacks, faults, and severe communication/system conditions in Mobile Crowd Sensing (MCS) make false data detection a critical problem. Observing the intrinsic low dimensionality of general monitoring data and the sparsity of false data, false data detection can be performed based on the separation of normal data and anomalies. Although the existing separation algorithm based on Direct Robust Matrix Factorization (DRMF) is proven to be effective, requiring iteratively performing Singular Value Decomposition (SVD) for low-rank matrix approximation would result in a prohibitively high accumulated computation cost when the data matrix is large. In this work, we observe the quick false data location feature from our empirical study of DRMF, based on which we propose an intelligent Light weight Low Rank and False Matrix Separation algorithm (LightLRFMS) that can reuse the previous result of the matrix decomposition to deduce the one for the current iteration step. Depending on the type of data corruption, random or successive/mass, we design two versions of LightLRFMS. From a theoretical perspective, we validate that LightLRFMS only requires one round of SVD computation and thus has very low computation cost. We have done extensive experiments using a PM 2.5 air condition trace and a road traffic trace. Our results demonstrate that LightLRFMS can achieve very good false data detection performance with the same highest detection accuracy as DRMF but with up to 20 times faster speed thanks to its lower computation cost.",2020,,10.1109/TNET.2020.2982685
1541,"Shin, Jaeho and Wu, Sen and Wang, Feiran and De Sa, Christopher and Zhang, Ce and R\'{e}, Christopher",Incremental Knowledge Base Construction Using DeepDive,,"Populating a database with unstructured information is a long-standing problem in industry and research that encompasses problems of extraction, cleaning, and integration. Recent names used for this problem include dealing with dark data and knowledge base construction (KBC). In this work, we describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems, and we present techniques to make the KBC process more efficient. We observe that the KBC process is iterative, and we develop techniques to incrementally produce inference results for KBC systems. We propose two methods for incremental inference, based respectively on sampling and variational techniques. We also study the tradeoff space of these methods and develop a simple rule-based optimizer. DeepDive includes all of these contributions, and we evaluate DeepDive on five KBC systems, showing that it can speed up KBC inference tasks by up to two orders of magnitude with negligible impact on quality.",2015,,10.14778/2809974.2809991
1542,"Linxen, Sebastian and Sturm, Christian and Br\""{u}hlmann, Florian and Cassau, Vincent and Opwis, Klaus and Reinecke, Katharina",How WEIRD is CHI?,"HCI research, sample bias, generalizability, WEIRD, geographic diversity"," Computer technology is often designed in technology hubs in Western countries, invariably making it “WEIRD”, because it is based on the intuition, knowledge, and values of people who are Western, Educated, Industrialized, Rich, and Democratic. Developing technology that is universally useful and engaging requires knowledge about members of WEIRD and non-WEIRD societies alike. In other words, it requires us, the CHI community, to generate this knowledge by studying representative participant samples. To find out to what extent CHI participant samples are from Western societies, we analyzed papers published in the CHI proceedings between 2016-2020. Our findings show that 73% of CHI study findings are based on Western participant samples, representing less than 12% of the world’s population. Furthermore, we show that most participant samples at CHI tend to come from industrialized, rich, and democratic countries with generally highly educated populations. Encouragingly, recent years have seen a slight increase in non-Western samples and those that include several countries. We discuss suggestions for further broadening the international representation of CHI participant samples.",2021,,10.1145/3411764.3445488
1543,"Tian, Jiannan and Di, Sheng and Zhang, Chengming and Liang, Xin and Jin, Sian and Cheng, Dazhao and Tao, Dingwen and Cappello, Franck",WaveSZ: A Hardware-Algorithm Co-Design of Efficient Lossy Compression for Scientific Data,,"Error-bounded lossy compression is critical to the success of extreme-scale scientific research because of ever-increasing volumes of data produced by today's high-performance computing (HPC) applications. Not only can error-controlled lossy compressors significantly reduce the I/O and storage burden but they can retain high data fidelity for post analysis. Existing state-of-the-art lossy compressors, however, generally suffer from relatively low compression and decompression throughput (up to hundreds of megabytes per second on a single CPU core), which considerably restrict the adoption of lossy compression by many HPC applications especially those with a fairly high data production rate. In this paper, we propose a highly efficient lossy compression approach based on field programmable gate arrays (FPGAs) under the state-of-the-art lossy compression model SZ. Our contributions are fourfold. (1) We adopt a wavefront memory layout to alleviate the data dependency during the prediction for higher-dimensional predictors, such as the Lorenzo predictor. (2) We propose a co-design framework named waveSZ based on the wavefront memory layout and the characteristics of SZ algorithm and carefully implement it by using high-level synthesis. (3) We propose a hardware-algorithm co-optimization method to improve the performance. (4) We evaluate our proposed waveSZ on three real-world HPC simulation datasets from the Scientific Data Reduction Benchmarks and compare it with other state-of-the-art methods on both CPUs and FPGAs. Experiments show that our waveSZ can improve SZ's compression throughput by 6.9X ~ 8.7X over the production version running on a state-of-the-art CPU and improve the compression ratio and throughput by 2.1X and 5.8X on average, respectively, compared with the state-of-the-art FPGA design.",2020,,
1544,"Chowdhury, Hussain Ahmed and Bhattacharyya, Dhruba Kumar and Kalita, Jugal Kumar","Differential Expression Analysis of RNA-Seq Reads: Overview, Taxonomy, and Tools",,"Analysis of RNA-sequence (RNA-seq) data is widely used in transcriptomic studies and it has many applications. We review RNA-seq data analysis from RNA-seq reads to the results of differential expression analysis. In addition, we perform a descriptive comparison of tools used in each step of RNA-seq data analysis along with a discussion of important characteristics of these tools. A taxonomy of tools is also provided. A discussion of issues in quality control and visualization of RNA-seq data is also included along with useful tools. Finally, we provide some guidelines for the RNA-seq data analyst, along with research issues and challenges which should be addressed.",2020,,10.1109/TCBB.2018.2873010
1545,"Wiese, Jason",Personal Context from Mobile Phones—Case Study 5,,,2021,,
1546,"Hilman, Muhammad H. and Rodriguez, Maria A. and Buyya, Rajkumar",Multiple Workflows Scheduling in Multi-Tenant Distributed Systems: A Taxonomy and Future Directions,"multiple workflows scheduling, multi-tenant platforms, Scientific workflows","Workflows are an application model that enables the automated execution of multiple interdependent and interconnected tasks. They are widely used by the scientific community to manage the distributed execution and dataflow of complex simulations and experiments. As the popularity of scientific workflows continue to rise, and their computational requirements continue to increase, the emergence and adoption of multi-tenant computing platforms that offer the execution of these workflows as a service becomes widespread. This article discusses the scheduling and resource provisioning problems particular to this type of platform. It presents a detailed taxonomy and a comprehensive survey of the current literature and identifies future directions to foster research in the field of multiple workflow scheduling in multi-tenant distributed computing systems.",2020,,10.1145/3368036
1547,"Cai, Zhipeng and Xiong, Zuobin and Xu, Honghui and Wang, Peng and Li, Wei and Pan, Yi",Generative Adversarial Networks: A Survey Toward Private and Secure Applications,"deep learning, Generative adversarial networks, privacy and security","Generative Adversarial Networks (GANs) have promoted a variety of applications in computer vision and natural language processing, among others, due to its generative model’s compelling ability to generate realistic examples plausibly drawn from an existing distribution of samples. GAN not only provides impressive performance on data generation-based tasks but also stimulates fertilization for privacy and security oriented research because of its game theoretic optimization strategy. Unfortunately, there are no comprehensive surveys on GAN in privacy and security, which motivates this survey to summarize systematically. The existing works are classified into proper categories based on privacy and security functions, and this survey conducts a comprehensive analysis of their advantages and drawbacks. Considering that GAN in privacy and security is still at a very initial stage and has imposed unique challenges that are yet to be well addressed, this article also sheds light on some potential privacy and security applications with GAN and elaborates on some future research directions.",2021,,10.1145/3459992
1548,"Lu, Xi and L. Reynolds, Tera and Jo, Eunkyung and Hong, Hwajung and Page, Xinru and Chen, Yunan and A. Epstein, Daniel",Comparing Perspectives Around Human and Technology Support for Contact Tracing,"Self-tracking, Contact tracing, COVID-19, Personal informatics, Public health, Crisis informatics","Various contact tracing approaches have been applied to help contain the spread of COVID-19, with technology-based tracing and human tracing among the most widely adopted. However, governments and communities worldwide vary in their adoption of digital contact tracing, with many instead choosing the human approach. We investigate how people perceive the respective benefits and risks of human and digital contact tracing through a mixed-methods survey with 291 respondents from the United States. Participants perceived digital contact tracing as more beneficial for protecting privacy, providing convenience, and ensuring data accuracy, and felt that human contact tracing could help provide security, emotional reassurance, advice, and accessibility. We explore the role of self-tracking technologies in public health crisis situations, highlighting how designs must adapt to promote societal benefit rather than just self-understanding. We discuss how future digital contact tracing can better balance the benefits of human tracers and technology amidst the complex contact tracing process and context.",2021,,10.1145/3411764.3445669
1549,"Khayyat, Zuhair and Lucia, William and Singh, Meghna and Ouzzani, Mourad and Papotti, Paolo and Quian\'{e}-Ruiz, Jorge-Arnulfo and Tang, Nan and Kalnis, Panos",Fast and Scalable Inequality Joins,"Spark SQL, Inequality join, PostgreSQL, Incremental, Selectivity estimation","Inequality joins, which is to join relations with inequality conditions, are used in various applications. Optimizing joins has been the subject of intensive research ranging from efficient join algorithms such as sort-merge join, to the use of efficient indices such as $$B^+$$B+-tree, $$R^*$$R\'{z}-tree and Bitmap. However, inequality joins have received little attention and queries containing such joins are notably very slow. In this paper, we introduce fast inequality join algorithms based on sorted arrays and space-efficient bit-arrays. We further introduce a simple method to estimate the selectivity of inequality joins which is then used to optimize multiple predicate queries and multi-way joins. Moreover, we study an incremental inequality join algorithm to handle scenarios where data keeps changing. We have implemented a centralized version of these algorithms on top of PostgreSQL, a distributed version on top of Spark SQL, and an existing data cleaning system, Nadeef. By comparing our algorithms against well-known optimization techniques for inequality joins, we show our solution is more scalable and several orders of magnitude faster.",2017,,10.1007/s00778-016-0441-6
1550,"Ismail, Azra and Kumar, Neha",AI in Global Health: The View from the Front Lines,"AI, Qualitative, India, Healthcare, HCI4D, Social Good"," There has been growing interest in the application of AI for Social Good, motivated by scarce and unequal resources globally. We focus on the case of AI in frontline health, a Social Good domain that is increasingly a topic of significant attention. We offer a thematic discourse analysis of scientific and grey literature to identify prominent applications of AI in frontline health, motivations driving this work, stakeholders involved, and levels of engagement with the local context. We then uncover design considerations for these systems, drawing from data from three years of ethnographic fieldwork with women frontline health workers and women from marginalized communities in Delhi (India). Finally, we outline an agenda for AI systems that target Social Good, drawing from literature on HCI4D, post-development critique, and transnational feminist theory. Our paper thus offers a critical and ethnographic perspective to inform the design of AI systems that target social impact.",2021,,10.1145/3411764.3445130
1551,"Heise, Arvid and Quian\'{e}-Ruiz, Jorge-Arnulfo and Abedjan, Ziawasch and Jentzsch, Anja and Naumann, Felix",Scalable Discovery of Unique Column Combinations,,"The discovery of all unique (and non-unique) column combinations in a given dataset is at the core of any data profiling effort. The results are useful for a large number of areas of data management, such as anomaly detection, data integration, data modeling, duplicate detection, indexing, and query optimization. However, discovering all unique and non-unique column combinations is an NP-hard problem, which in principle requires to verify an exponential number of column combinations for uniqueness on all data values. Thus, achieving efficiency and scalability in this context is a tremendous challenge by itself.In this paper, we devise Ducc, a scalable and efficient approach to the problem of finding all unique and non-unique column combinations in big datasets. We first model the problem as a graph coloring problem and analyze the pruning effect of individual combinations. We then present our hybrid column-based pruning technique, which traverses the lattice in a depth-first and random walk combination. This strategy allows Ducc to typically depend on the solution set size and hence to prune large swaths of the lattice. Ducc also incorporates row-based pruning to run uniqueness checks in just few milliseconds. To achieve even higher scalability, Ducc runs on several CPU cores (scale-up) and compute nodes (scale-out) with a very low overhead. We exhaustively evaluate Ducc using three datasets (two real and one synthetic) with several millions rows and hundreds of attributes. We compare Ducc with related work: Gordian and HCA. The results show that Ducc is up to more than 2 orders of magnitude faster than Gordian and HCA (631x faster than Gordian and 398x faster than HCA). Finally, a series of scalability experiments shows the efficiency of Ducc to scale up and out.",2013,,10.14778/2732240.2732248
1552,"Chu, Xu and Ilyas, Ihab F. and Koutris, Paraschos",Distributed Data Deduplication,,"Data deduplication refers to the process of identifying tuples in a relation that refer to the same real world entity. The complexity of the problem is inherently quadratic with respect to the number of tuples, since a similarity value must be computed for every pair of tuples. To avoid comparing tuple pairs that are obviously non-duplicates, blocking techniques are used to divide the tuples into blocks and only tuples within the same block are compared. However, even with the use of blocking, data deduplication remains a costly problem for large datasets. In this paper, we show how to further speed up data deduplication by leveraging parallelism in a shared-nothing computing environment. Our main contribution is a distribution strategy, called Dis-Dedup, that minimizes the maximum workload across all worker nodes and provides strong theoretical guarantees. We demonstrate the effectiveness of our proposed strategy by performing extensive experiments on both synthetic datasets with varying block size distributions, as well as real world datasets.",2016,,10.14778/2983200.2983203
1553,"Jain, Ayush and Sarma, Akash Das and Parameswaran, Aditya and Widom, Jennifer","Understanding Workers, Developing Effective Tasks, and Enhancing Marketplace Dynamics: A Study of a Large Crowdsourcing Marketplace",,"We conduct an experimental analysis of a dataset comprising over 27 million microtasks performed by over 70,000 workers issued to a large crowdsourcing marketplace between 2012--2016. Using this data---never before analyzed in an academic context---we shed light on three crucial aspects of crowdsourcing: (1) Task design---helping requesters understand what constitutes an effective task, and how to go about designing one; (2) Marketplace dynamics --- helping marketplace administrators and designers understand the interaction between tasks and workers, and the corresponding marketplace load; and (3) Worker behavior --- understanding worker attention spans, lifetimes, and general behavior, for the improvement of the crowdsourcing ecosystem as a whole.",2017,,10.14778/3067421.3067431
1554,"Lang, Willis and Nehme, Rimma V. and Robinson, Eric and Naughton, Jeffrey F.",Partial Results in Database Systems,"result semantics, failures, partial results","As the size and complexity of analytic data processing systems continue to grow, the effort required to mitigate faults and performance skew has also risen. However, in some environments we have encountered, users prefer to continue query execution even in the presence of failures (e.g., the unavailability of certain data sources), and receive a ""partial"" answer to their query. We explore ways to characterize and classify these partial results, and describe an analytical framework that allows the system to perform coarse to fine-grained analysis to determine the semantics of a partial result. We propose that if the system is equipped with such a framework, in some cases it is better to return and explain partial results than to attempt to avoid them.",2014,,10.1145/2588555.2612176
1555,"Zou, Jie and Aliannejadi, Mohammad and Kanoulas, Evangelos and Pera, Maria Soledad and Liu, Yiqun",Users Meet Clarifying Questions: Toward a Better Understanding of User Interactions for Search Clarification,User Study; Information Seeking Systems; Clarifying Questions,"The use of clarifying questions (CQs) is a fairly new and useful technique to aid systems in recognizing the intent, context, and preferences behind user queries. Yet, understanding the extent of the effect of CQs on user behavior and the ability to identify relevant information remains relatively unexplored. In this work, we conduct a large user study to understand the interaction of users with CQs in various quality categories, and the effect of CQ quality on user search performance in terms of finding relevant information, search behavior, and user satisfaction. Analysis of implicit interaction data and explicit user feedback demonstrates that high-quality CQs improve user performance and satisfaction. By contrast, low- and mid-quality CQs are harmful, and thus allowing the users to complete their tasks without CQ support may be preferred in this case. We also observe that user engagement, and therefore the need for CQ support, is affected by several factors, such as search result quality or perceived task difficulty. The findings of this study can help researchers and system designers realize why, when, and how users interact with CQs, leading to a better understanding and design of search clarification systems.",2022,,10.1145/3524110
1556,"He, Dengbo and Risteska, Martina and Donmez, Birsen and Chen, Kaiyang",Driver Cognitive Load Classification Based on Physiological Data—Case Study 7,,,2021,,
1557,"Priedhorsky, Reid and Osthus, Dave and Daughton, Ashlynn R. and Moran, Kelly R. and Generous, Nicholas and Fairchild, Geoffrey and Deshpande, Alina and Del Valle, Sara Y.","Measuring Global Disease with Wikipedia: Success, Failure, and a Research Agenda","epidemiology, disease, modeling, forecasting, wikipedia","Effective disease monitoring provides a foundation for effective public health systems. This has historically been accomplished with patient contact and bureaucratic aggregation, which tends to be slow and expensive. Recent internet-based approaches promise to be real-time and cheap, with few parameters. However, the question of when and how these approaches work remains open. We addressed this question using Wikipedia access logs and category links. Our experiments, replicable and extensible using our open source code and data, test the effect of semantic article filtering, amount of training data, forecast horizon, and model staleness by comparing across 6 diseases and 4 countries using thousands of individual models. We found that our minimal-configuration, language-agnostic article selection process based on semantic relatedness is effective for improving predictions, and that our approach is relatively insensitive to the amount and age of training data. We also found, in contrast to prior work, very little forecasting value, and we argue that this is consistent with theoretical considerations about the nature of forecasting. These mixed results lead us to propose that the currently observational field of internet-based disease surveillance must pivot to include theoretical models of information flow as well as controlled experiments based on simulations of disease.",2017,,10.1145/2998181.2998183
1558,"Chatzilygeroudis, Konstantinos and Hatzilygeroudis, Ioannis and Perikos, Isidoros",Machine Learning Basics,,,2021,,
1559,"Zhao, Liping and Loucopoulos, Pericles and Kavakli, Evangelia and Letsholo, Keletso J.",User Studies on End-User Service Composition: A Literature Review and a Design Framework,"review framework, design guideline, service-oriented computing, empirical studies, end-user service composition, systematic review, mapshups, web services, qualitative studies, User studies","Context: End-user service composition (EUSC) is a service-oriented paradigm that aims to empower end users and allow them to compose their own web applications from reusable service components. User studies have been used to evaluate EUSC tools and processes. Such an approach should benefit software development, because incorporating end users’ feedback into software development should make software more useful and usable. Problem: There is a gap in our understanding of what constitutes a user study and how a good user study should be designed, conducted, and reported. Goal: This article aims to address this gap. Method: The article presents a systematic review of 47 selected user studies for EUSC. Guided by a review framework, the article systematically and consistently assesses the focus, methodology and cohesion of each of these studies. Results: The article concludes that the focus of these studies is clear, but their methodology is incomplete and inadequate, their overall cohesion is poor. The findings lead to the development of a design framework and a set of questions for the design, reporting, and review of good user studies for EUSC. The detailed analysis and the insights obtained from the analysis should be applicable to the design of user studies for service-oriented systems as well and indeed for any user studies related to software artifacts.",2019,,10.1145/3340294
1560,"Sallam, Samar and Sakamoto, Yumiko and Leboe-McGowan, Jason and Latulipe, Celine and Irani, Pourang","Towards Design Guidelines for Effective Health-Related Data Videos: An Empirical Investigation of Affect, Personality, and Video Content","attitude change, persuasive technology, data storytelling, Data Video, physical activity, narrative visualization, actionable, personality, solution, affect"," Data Videos (DVs), or animated infographics that tell stories with data, are becoming increasingly popular. Despite their potential to induce attitude change, little is explored about how to produce effective DVs. This paper describes two studies that explored factors linked to the potential of health DVs to improve viewers’ behavioural change intentions. We investigated: 1) how viewers’ affect is linked to their behavioural change intentions; 2) how these affect are linked to the viewers’ personality traits; 3) which attributes of DVs are linked to their persuasive potential. Results from both studies indicated that viewers’ negative affect lowered their behavioural change intentions. Individuals with higher neuroticism exhibited higher negative affect and were harder to convince. Finally, Study 2 proved that providing any solutions to the health problem, presented in the DV, made the viewers perceive the videos as more actionable while lowering their negative affect, and importantly, induced higher behavioural change intentions. ",2022,,10.1145/3491102.3517727
1561,"Benarous, Maya and Toch, Eran and Ben-Gal, Irad",Synthesis of Longitudinal Human Location Sequences: Balancing Utility and Privacy,"privacy, long short term memory network (LSTM), Synthetic data, location sequences","People’s location data is continuously tracked from various devices and sensors, enabling an ongoing analysis of sensitive information that can violate people’s privacy and reveal confidential information. Synthetic data has been used to generate representative location sequences yet to maintain the users’ privacy. Nonetheless, the privacy-accuracy tradeoff between these two measures has not been addressed systematically. In this paper, we analyze the use of different synthetic data generation models for long location sequences, including extended short-term memory networks (LSTMs), Markov Chains, and variable-order Markov models (VMMs). We employ different performance measures, such as data similarity and privacy, and discuss the inherent tradeoff. Furthermore, we introduce other measurements to quantify each of these measures. Based on the anonymous data of 300 thousand cellular-phone users, our work offers a road map for developing policies for synthetic data generation processes. We propose a framework for building data generation models and evaluating their effectiveness regarding those accuracy and privacy measures.",2022,,10.1145/3529260
1562,"Koutris, Paraschos and Upadhyaya, Prasang and Balazinska, Magdalena and Howe, Bill and Suciu, Dan",Toward Practical Query Pricing with QueryMarket,"data pricing, integer linear programming","We develop a new pricing system, QueryMarket, for flexible query pricing in a data market based on an earlier theoretical framework (Koutris et al., PODS 2012). To build such a system, we show how to use an Integer Linear Programming formulation of the pricing problem for a large class of queries, even when pricing is computationally hard. Further, we leverage query history to avoid double charging when queries purchased over time have overlapping information, or when the database is updated. We then present a technique that fairly shares revenue when multiple sellers are involved. Finally, we implement our approach in a prototype and evaluate its performance on several query workloads.",2013,,10.1145/2463676.2465335
1563,"Law, Edith and Gajos, Krzysztof Z. and Wiggins, Andrea and Gray, Mary L. and Williams, Alex",Crowdsourcing as a Tool for Research: Implications of Uncertainty,"interviews, crowdsourcing for research, citizen science","Numerous crowdsourcing platforms are now available to support research as well as commercial goals. However, crowdsourcing is not yet widely adopted by researchers for generating, processing or analyzing research data. This study develops a deeper understanding of the circumstances under which crowdsourcing is a useful, feasible or desirable tool for research, as well as the factors that may influence researchers' decisions around adopting crowdsourcing technology. We conducted semi-structured interviews with 18 researchers in diverse disciplines, spanning the humanities and sciences, to illuminate how research norms and practitioners' dispositions were related to uncertainties around research processes, data, knowledge, delegation and quality. The paper concludes with a discussion of the design implications for future crowdsourcing systems to support research.",2017,,10.1145/2998181.2998197
1564,Task Group on Information Technology Curricula,Information Technology Curricula 2017: Curriculum Guidelines for Baccalaureate Degree Programs in Information Technology,,,2017,,
1565,"Bari, Rummana and Adams, Roy J. and Rahman, Md. Mahbubur and Parsons, Megan Battles and Buder, Eugene H. and Kumar, Santosh",RConverse: Moment by Moment Conversation Detection Using a Mobile Respiration Sensor,"Conversation Modeling, Wearable Sensing, Respiration Signal, Machine Learning","Monitoring of in-person conversations has largely been done using acoustic sensors. In this paper, we propose a new method to detect moment-by-moment conversation episodes by analyzing breathing patterns captured by a mobile respiration sensor. Since breathing is affected by physical and cognitive activities, we develop a comprehensive method for cleaning, screening, and analyzing noisy respiration data captured in the field environment at individual breath cycle level. Using training data collected from a speech dynamics lab study with 12 participants, we show that our algorithm can identify each respiration cycle with 96.34% accuracy even in presence of walking. We present a Conditional Random Field, Context-Free Grammar (CRF-CFG) based conversation model, called rConverse, to classify respiration cycles into speech or non-speech, and subsequently infer conversation episodes. Our model achieves 82.7% accuracy for speech/non-speech classification and it identifies conversation episodes with 95.9% accuracy on lab data using a leave-one-subject-out cross-validation. Finally, the system is validated against audio ground-truth in a field study with 32 participants. rConverse identifies conversation episodes with 71.7% accuracy on 254 hours of field data. For comparison, the accuracy from a high-quality audio-recorder on the same data is 71.9%.",2018,,10.1145/3191734
1566,"Riegler, Michael and Pogorelov, Konstantin and Eskeland, Sigrun Losada and Schmidt, Peter Thelin and Albisser, Zeno and Johansen, Dag and Griwodz, Carsten and Halvorsen, P\r{a}l and Lange, Thomas De",From Annotation to Computer-Aided Diagnosis: Detailed Evaluation of a Medical Multimedia System,"Medical multimedia system, evaluation, gastrointestinal tract","Holistic medical multimedia systems covering end-to-end functionality from data collection to aided diagnosis are highly needed, but rare. In many hospitals, the potential value of multimedia data collected through routine examinations is not recognized. Moreover, the availability of the data is limited, as the health care personnel may not have direct access to stored data. However, medical specialists interact with multimedia content daily through their everyday work and have an increasing interest in finding ways to use it to facilitate their work processes. In this article, we present a novel, holistic multimedia system aiming to tackle automatic analysis of video from gastrointestinal (GI) endoscopy. The proposed system comprises the whole pipeline, including data collection, processing, analysis, and visualization. It combines filters using machine learning, image recognition, and extraction of global and local image features. The novelty is primarily in this holistic approach and its real-time performance, where we automate a complete algorithmic GI screening process. We built the system in a modular way to make it easily extendable to analyze various abnormalities, and we made it efficient in order to run in real time. The conducted experimental evaluation proves that the detection and localization accuracy are comparable or even better than existing systems, but it is by far leading in terms of real-time performance and efficient resource consumption.",2017,,10.1145/3079765
1567,"Gursoy, Mehmet Emre and Liu, Ling and Truex, Stacey and Yu, Lei and Wei, Wenqi",Utility-Aware Synthesis of Differentially Private and Attack-Resilient Location Traces,"data privacy, location privacy, mobile computing","As mobile devices and location-based services become increasingly ubiquitous, the privacy of mobile users' location traces continues to be a major concern. Traditional privacy solutions rely on perturbing each position in a user's trace and replacing it with a fake location. However, recent studies have shown that such point-based perturbation of locations is susceptible to inference attacks and suffers from serious utility losses, because it disregards the moving trajectory and continuity in full location traces. In this paper, we argue that privacy-preserving synthesis of complete location traces can be an effective solution to this problem. We present AdaTrace, a scalable location trace synthesizer with three novel features: provable statistical privacy, deterministic attack resilience, and strong utility preservation. AdaTrace builds a generative model from a given set of real traces through a four-phase synthesis process consisting of feature extraction, synopsis learning, privacy and utility preserving noise injection, and generation of differentially private synthetic location traces. The output traces crafted by AdaTrace preserve utility-critical information existing in real traces, and are robust against known location trace attacks. We validate the effectiveness of AdaTrace by comparing it with three state of the art approaches (ngram, DPT, and SGLT) using real location trace datasets (Geolife and Taxi) as well as a simulated dataset of 50,000 vehicles in Oldenburg, Germany. AdaTrace offers up to 3-fold improvement in trajectory utility, and is orders of magnitude faster than previous work, while preserving differential privacy and attack resilience.",2018,,10.1145/3243734.3243741
1568,"Fan, Wenfei and Hu, Chunming and Liu, Xueli and Lu, Ping",Discovering Graph Functional Dependencies,"fixed-parameter tractability, gfd discovery, parallel scalable","This paper studies discovery of GFDs, a class of functional dependencies defined on graphs. We investigate the fixed-parameter tractability of three fundamental problems related to GFD discovery. We show that the implication and satisfiability problems are fixed-parameter tractable, but the validation problem is co-W[1]-hard. We introduce notions of reduced GFDs and their topological support, and formalize the discovery problem for GFDs. We develop algorithms for discovering GFDs and computing their covers. Moreover, we show that GFD discovery is feasible over large-scale graphs, by providing parallel scalable algorithms for discovering GFDs that guarantee to reduce running time when more processors are used. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of the algorithms.",2018,,10.1145/3183713.3196916
1569,"Jun, Eunice and Jo, Blue A. and Oliveira, Nigini and Reinecke, Katharina",Digestif: Promoting Science Communication in Online Experiments,,"Online experiments allow researchers to collect data from large, demographically diverse global populations. Unlike in-lab studies, however, online experiments often fail to inform participants about the research to which they contribute. This paper is the first to investigate barriers that prevent researchers from providing such science communication in online experiments. We found that the main obstacles preventing researchers from including such information are assumptions about participant disinterest, limited time, concerns about losing anonymity, and concerns about experimental bias. Researchers also noted the dearth of tools to help them close the information loop with their study participants. Based on these findings, we formulated design requirements and implemented Digestif, a new web-based tool that supports researchers in providing their participants with science communication pages. Our evaluation shows that Digestif's scaffolding, examples, and nudges to focus on participants make researchers more aware of their participants' curiosity about research and more likely to disclose pertinent research information.",2018,,10.1145/3274353
1570,,SIGMOD '15: Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,,"Welcome to SIGMOD 2015 -- officially, the 2015 ACM SIGMOD International Conference on the Management of Data! This year's conference is being held in the beautiful cultural capital of Australia, Melbourne. During the Gold Rush period of the 19th Century, Melbourne was the richest city in the world, and as a result it is filled with many unique neighborhoods and distinctive buildings. In addition to wonderful neighborhoods to explore, the city has great museums and other cultural attractions, as well as a fine multi-cultural atmosphere. For those who would like to explore the outdoors, popular highlights are the Phillip Island Nature Park (90 minutes away), which features wild penguins who return in a parade each day at sunset, and the Great Ocean Road, one of the world's most scenic coastal drives, including the famous towering 12 Apostles.SIGMOD 2015's exciting technical program reflects not only traditional topics, but the database community's role in broader data science and data analytics. The keynote from Laura Haas, ""The Power Behind the Throne: Information Integration in the Age of Data-Driven Discovery"" highlights the role of database and data integration techniques in the growing field of data science. Jignesh Patel's talk, ""From Data to Insights @ Bare Metal Speed,"" explains how hardware and software need to be co-evolved to support the needs of scalable data analytics. Jennifer Widom, winner of the 2015 ACM-W Athena Lecturer Award for fundamental contributions to computer science, will give her award talk, ""Three Favorite Results,"" on Tuesday. Christopher R\'{e} will lead a panel on ""Machine Learning and Databases: The Sound of Things to Come or a Cacophony of Hype?,"" with participants Divyakant Agrawal, Magdalena Balazinska, Michael Cafarella, Michael Jordan, Tim Kraska, and Raghu Ramakrishnan. Of course, there are also 106 research paper presentations, 4 tutorials, 30 demonstrations, and 18 industrial papers. Papers will be presented both as talks during the research sessions, and as part of plenary Poster Sessions.SIGMOD 2015 is preceded by the PhD Workshop, as well as workshops on leading-edge topics like data analytics (DanaC), databases and the Web (WebDB), exploratory search (ExploreDB), managing and mining spatial data (GeoRich), and graph data (GRADES); the New Researcher Symposium will take place on Wednesday. The banquet will be held in a Melbourne landmark, the Town Hall.As in recent years, we had two submission deadlines for SIGMOD this year, one in August and one in November. The review process was journal-style, with multiple rounds of reviews coordinated by the Group Leaders. We accepted 34 of 137 papers from the first deadline and 72 of 278 from the second deadline. The total acceptance rate was about 25.5%, and we believe that the revision processhas improved the quality of the technical program.",2015,,
1571,"Grandhi, Sukeshini A. and Plotnick, Linda",Do I Spit or Do I Pass? Perceived Privacy and Security Concerns of Direct-to-Consumer Genetic Testing,"rational, privacy, direct-to-consumer genetic testing, security, decision making, heuristics, concerns, information privacy, information disclosure","This study explores privacy concerns perceived by people with respect to having their DNA tested by direct-to-consumer (DTC) genetic testing companies such as 23andMe and Ancestry.com. Data collected from 510 respondents indicate that those who have already obtained a DTC genetic test have significantly lower levels of privacy and security concerns than those who have not obtained a DTC genetic test. Qualitative data from respondents of both these groups show that the concerns are mostly similar. However, the factors perceived to alleviate privacy concerns are more varied and nuanced amongst those who have obtained a DTC genetic test. Our data suggest that privacy concerns or lack of concerns are based on complex and multiple considerations including data ownership, access control of data and regulatory authorities of social, political and legal systems. Respondents do not engage in a full cost/benefit analysis of having their DNA tested.",2022,,10.1145/3492838
1572,"Fan, Wenfei and Lu, Ping",Dependencies for Graphs,"validation, built-in predicates, TGDs, disjunction, satisfiability, axiom system, keys, Graph dependencies, conditional functional dependencies, implication, EGDs","This article proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is defined as a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs can express graph functional dependencies with constant literals to catch inconsistencies, and keys carrying id literals to identify entities (vertices) in a graph. We revise the chase for GEDs and prove its Church-Rosser property. We characterize GED satisfiability and implication, and establish the complexity of these problems and the validation problem for GEDs, in the presence and absence of constant literals and id literals. We also develop a sound, complete and independent axiom system for finite implication of GEDs. In addition, we extend GEDs with built-in predicates or disjunctions, to strike a balance between the expressive power and complexity. We settle the complexity of the satisfiability, implication, and validation problems for these extensions.",2019,,10.1145/3287285
1573,"Sa, Christopher and Ratner, Alex and R\'{e}, Christopher and Shin, Jaeho and Wang, Feiran and Wu, Sen and Zhang, Ce",Incremental Knowledge Base Construction Using DeepDive,"Incremental, Knowledge base construction, Performance","Populating a database with information from unstructured sources--also known as knowledge base construction (KBC)--is a long-standing problem in industry and research that encompasses problems of extraction, cleaning, and integration. In this work, we describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems, and we present techniques to make the KBC process more efficient. We observe that the KBC process is iterative, and we develop techniques to incrementally produce inference results for KBC systems. We propose two methods for incremental inference, based, respectively, on sampling and variational techniques. We also study the trade-off space of these methods and develop a simple rule-based optimizer. DeepDive includes all of these contributions, and we evaluate DeepDive on five KBC systems, showing that it can speed up KBC inference tasks by up to two orders of magnitude with negligible impact on quality.",2017,,10.1007/s00778-016-0437-2
1574,"Alexander, Jason and Thanh Vi, Chi",DSP Basics,,,2021,,
1575,"Boukhelifa, N. and Bezerianos, A. and Cancino, W. and Lutton, E.",Evolutionary Visual Exploration: Evaluation of an Iec Framework for Guided Visual Search,"visual analytics, information visualization., data mining, Interactive evolutionary algorithms, interactive evolutionary computation, genetic programming","We evaluate and analyse a framework for evolutionary visual exploration EVE that guides users in exploring large search spaces. EVE uses an interactive evolutionary algorithm to steer the exploration of multidimensional data sets toward two-dimensional projections that are interesting to the analyst. Our method smoothly combines automatically calculated metrics and user input in order to propose pertinent views to the user. In this article, we revisit this framework and a prototype application that was developed as a demonstrator, and summarise our previous study with domain experts and its main findings. We then report on results from a new user study with a clearly predefined task, which examines how users leverage the system and how the system evolves to match their needs. While we previously showed that using EVE, domain experts were able to formulate interesting hypotheses and reach new insights when exploring freely, our new findings indicate that users, guided by the interactive evolutionary algorithm, are able to converge quickly to an interesting view of their data when a clear task is specified. We provide a detailed analysis of how users interact with an evolutionary algorithm and how the system responds to their exploration strategies and evaluation patterns. Our work aims at building a bridge between the domains of visual analytics and interactive evolution. The benefits are numerous, in particular for evaluating interactive evolutionary computation IEC techniques based on user study methodologies.",2017,,10.1162/EVCO_a_00161
1576,"Vourvopoulos, A. and Niforatos, E. and Bermudez i Badia, S. and Liarokapis, Fotis",Brain–Computer Interfacing with Interactive Systems—Case Study 2,,,2021,,
1577,"Topi, Heikki and Karsten, Helena and Brown, Sue A. and Carvalho, Jo\~{a}o Alvaro and Donnellan, Brian and Shen, Jun and Tan, Bernard C. Y. and Thouin, Mark F.",MSIS 2016: Global Competency Model for Graduate Degree Programs in Information Systems,,"This document, ""MSIS 2016: Global Competency Model for Graduate Degree Programs in Information Systems"", is the latest in the series of reports that provides guidance for degree programs in the Information Systems (IS) academic discipline. The first of these reports (Ashenhurst, 1972) was published in the early 1970s, and the work has continued ever since both at the undergraduate and master's levels. The Association for Computing Machinery (ACM) has sponsored the reports from the beginning. Since the Association for Information Systems (AIS) was established in the mid-1990s, the two organizations have collaborated on the production of curriculum recommendations for the IS discipline. At the undergraduate level, both the Association for Information Technology Professionals (AITP) (formerly DPMA) and the International Federation for Information Processing (IFIP) have also made significant contributions to the curriculum recommendations.",2017,,
1578,"Buschek, Daniel and Alt, Florian",Building Adaptive Touch Interfaces—Case Study 6,,,2021,,
1579,"Alam, Iqbal and Sharif, Kashif and Li, Fan and Latif, Zohaib and Karim, M. M. and Biswas, Sujit and Nour, Boubakr and Wang, Yu",A Survey of Network Virtualization Techniques for Internet of Things Using SDN and NFV,"software-defined IoT, software-defined network, Internet of Things, network function virtualization, network softwarization","Internet of Things (IoT) and Network Softwarization are fast becoming core technologies of information systems and network management for the next-generation Internet. The deployment and applications of IoT range from smart cities to urban computing and from ubiquitous healthcare to tactile Internet. For this reason, the physical infrastructure of heterogeneous network systems has become more complicated and thus requires efficient and dynamic solutions for management, configuration, and flow scheduling. Network softwarization in the form of Software Defined Networks and Network Function Virtualization has been extensively researched for IoT in the recent past. In this article, we present a systematic and comprehensive review of virtualization techniques explicitly designed for IoT networks. We have classified the literature into software-defined networks designed for IoT, function virtualization for IoT networks, and software-defined IoT networks. These categories are further divided into works that present architectural, security, and management solutions. Besides, the article highlights several short-term and long-term research challenges and open issues related to the adoption of software-defined Internet of Things.",2020,,10.1145/3379444
1580,"Pimentel, Jo\~{a}o Felipe and Freire, Juliana and Murta, Leonardo and Braganholo, Vanessa","A Survey on Collecting, Managing, and Analyzing Provenance from Scripts","managing, Provenance, survey, collecting, scripts, analyzing","Scripts are widely used to design and run scientific experiments. Scripting languages are easy to learn and use, and they allow complex tasks to be specified and executed in fewer steps than with traditional programming languages. However, they also have important limitations for reproducibility and data management. As experiments are iteratively refined, it is challenging to reason about each experiment run (or trial), to keep track of the association between trials and experiment instances as well as the differences across trials, and to connect results to specific input data and parameters. Approaches have been proposed that address these limitations by collecting, managing, and analyzing the provenance of scripts. In this article, we survey the state of the art in provenance for scripts. We have identified the approaches by following an exhaustive protocol of forward and backward literature snowballing. Based on a detailed study, we propose a taxonomy and classify the approaches using this taxonomy.",2019,,10.1145/3311955
1581,"Utz, Christine and Becker, Steffen and Schnitzler, Theodor and Farke, Florian M. and Herbert, Franziska and Schaewitz, Leonie and Degeling, Martin and D\""{u}rmuth, Markus",Apps Against the Spread: Privacy Implications and User Acceptance of COVID-19-Related Smartphone Apps on Three Continents,"COVID-19, digital contact tracing, privacy"," The COVID-19 pandemic has fueled the development of smartphone applications to assist disease management. Many “corona apps” require widespread adoption to be effective, which has sparked public debates about the privacy, security, and societal implications of government-backed health applications. We conducted a representative online study in Germany (n = 1003), the US (n = 1003), and China (n = 1019) to investigate user acceptance of corona apps, using a vignette design based on the contextual integrity framework. We explored apps for contact tracing, symptom checks, quarantine enforcement, health certificates, and mere information. Our results provide insights into data processing practices that foster adoption and reveal significant differences between countries, with user acceptance being highest in China and lowest in the US. Chinese participants prefer the collection of personalized data, while German and US participants favor anonymity. Across countries, contact tracing is viewed more positively than quarantine enforcement, and technical malfunctions negatively impact user acceptance.",2021,,10.1145/3411764.3445517
1582,"Ma, Qian and Gu, Yu and Lee, Wang-Chien and Yu, Ge and Liu, Hongbo and Wu, Xindong",REMIAN: Real-Time and Error-Tolerant Missing Value Imputation,"real-time imputation, Missing value, poor-quality streaming data","Missing value (MV) imputation is a critical preprocessing means for data mining. Nevertheless, existing MV imputation methods are mostly designed for batch processing, and thus are not applicable to streaming data, especially those with poor quality. In this article, we propose a framework, called Real-time and Error-tolerant Missing vAlue ImputatioN (REMAIN), to impute MVs in poor-quality streaming data. Instead of imputing MVs based on all the observed data, REMAIN first initializes the MV imputation model based on a-RANSAC which is capable of detecting and rejecting anomalies in an efficient manner, and then incrementally updates the model parameters upon the arrival of new data to support real-time MV imputation. As the correlations among attributes of the data may change over time in unforseenable ways, we devise a deterioration detection mechanism to capture the deterioration of the imputation model to further improve the imputation accuracy. Finally, we conduct an extensive evaluation on the proposed algorithms using real-world and synthetic datasets. Experimental results demonstrate that REMAIN achieves significantly higher imputation accuracy over existing solutions. Meanwhile, REMAIN improves up to one order of magnitude in time cost compared with existing approaches.",2020,,10.1145/3412364
1583,"Komninos, Andreas and Dunlop, Mark D. and Wilson, John N.",Combining Infrastructure Sensor and Tourism Market Data in a Smart City Project—Case Study 1,,,2021,,
1584,"Vertanen, Keith",Probabilistic Text Entry—Case Study 3,,,2021,,
1585,"Guo, Guangming and Zhu, Feida and Chen, Enhong and Liu, Qi and Wu, Le and Guan, Chu",From Footprint to Evidence: An Exploratory Study of Mining Social Data for Credit Scoring,"Personal credit scoring, consumer finance, P2P lending, user profiling, features, social data","With the booming popularity of online social networks like Twitter and Weibo, online user footprints are accumulating rapidly on the social web. Simultaneously, the question of how to leverage the large-scale user-generated social media data for personal credit scoring comes into the sight of both researchers and practitioners. It has also become a topic of great importance and growing interest in the P2P lending industry. However, compared with traditional financial data, heterogeneous social data presents both opportunities and challenges for personal credit scoring. In this article, we seek a deep understanding of how to learn users’ credit labels from social data in a comprehensive and efficient way. Particularly, we explore the social-data-based credit scoring problem under the micro-blogging setting for its open, simple, and real-time nature. To identify credit-related evidence hidden in social data, we choose to conduct an analytical and empirical study on a large-scale dataset from Weibo, the largest and most popular tweet-style website in China. Summarizing results from existing credit scoring literature, we first propose three social-data-based credit scoring principles as guidelines for in-depth exploration. In addition, we glean six credit-related insights arising from empirical observations of the testbed dataset. Based on the proposed principles and insights, we extract prediction features mainly from three categories of users’ social data, including demographics, tweets, and networks. To harness this broad range of features, we put forward a two-tier stacking and boosting enhanced ensemble learning framework. Quantitative investigation of the extracted features shows that online social media data does have good potential in discriminating good credit users from bad. Furthermore, we perform experiments on the real-world Weibo dataset consisting of more than 7.3 million tweets and 200,000 users whose credit labels are known through our third-party partner. Experimental results show that (i) our approach achieves a roughly 0.625 AUC value with all the proposed social features as input, and (ii) our learning algorithm can outperform traditional credit scoring methods by as much as 17% for social-data-based personal credit scoring.",2016,,10.1145/2996465
1586,"Arif, Ahmed Sabbir",Statistical Grounding,,,2021,,
1587,"Papadakis, George and Skoutas, Dimitrios and Thanos, Emmanouil and Palpanas, Themis",Blocking and Filtering Techniques for Entity Resolution: A Survey,"Blocking, filtering, entity resolution","Entity Resolution (ER), a core task of Data Integration, detects different entity profiles that correspond to the same real-world object. Due to its inherently quadratic complexity, a series of techniques accelerate it so that it scales to voluminous data. In this survey, we review a large number of relevant works under two different but related frameworks: Blocking and Filtering. The former restricts comparisons to entity pairs that are more likely to match, while the latter identifies quickly entity pairs that are likely to satisfy predetermined similarity thresholds. We also elaborate on hybrid approaches that combine different characteristics. For each framework we provide a comprehensive list of the relevant works, discussing them in the greater context. We conclude with the most promising directions for future work in the field.",2020,,10.1145/3377455
1588,"Ghosh, Aindrila and Nashaat, Mona and Miller, James and Quader, Shaikh",Context-Based Evaluation of Dimensionality Reduction Algorithms—Experiments and Statistical Significance Analysis,"statistical significance analysis, context-based evaluation, quality metrics, Dimensionality reduction","Dimensionality reduction is a commonly used technique in data analytics. Reducing the dimensionality of datasets helps not only with managing their analytical complexity but also with removing redundancy. Over the years, several such algorithms have been proposed with their aims ranging from generating simple linear projections to complex non-linear transformations of the input data. Subsequently, researchers have defined several quality metrics in order to evaluate the performances of different algorithms. Hence, given a plethora of dimensionality reduction algorithms and metrics for their quality analysis, there is a long-existing need for guidelines on how to select the most appropriate algorithm in a given scenario. In order to bridge this gap, in this article, we have compiled 12 state-of-the-art quality metrics and categorized them into 5 identified analytical contexts. Furthermore, we assessed 15 most popular dimensionality reduction algorithms on the chosen quality metrics using a large-scale and systematic experimental study. Later, using a set of robust non-parametric statistical tests, we assessed the generalizability of our evaluation on 40 real-world datasets. Finally, based on our results, we present practitioners’ guidelines for the selection of an appropriate dimensionally reduction algorithm in the present analytical contexts.",2021,,10.1145/3428077
1589,"Fan, Wenfei and Hu, Chunming and Liu, Xueli and Lu, Ping",Discovering Graph Functional Dependencies,"graphs, implication, validation, discovery, Functional dependencies","This article studies discovery of Graph Functional Dependencies (GFDs), a class of functional dependencies defined on graphs. We investigate the fixed-parameter tractability of three fundamental problems related to GFD discovery. We show that the implication and satisfiability problems are fixed-parameter tractable, but the validation problem is co-W[1]-hard in general. We introduce notions of reduced GFDs and their topological support, and formalize the discovery problem for GFDs. We develop algorithms for discovering GFDs and computing their covers. Moreover, we show that GFD discovery is feasible over large-scale graphs, by providing parallel scalable algorithms that guarantee to reduce running time when more processors are used. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of the algorithms.",2020,,10.1145/3397198
1590,,"Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice",,"Intelligent Computing for Interactive System Design provides a comprehensive resource on what has become the dominant paradigm in designing novel interaction methods, involving gestures, speech, text, touch and brain-controlled interaction, embedded in innovative and emerging human–computer interfaces. These interfaces support ubiquitous interaction with applications and services running on smartphones, wearables, in-vehicle systems, virtual and augmented reality, robotic systems, the Internet of Things (IoT), and many other domains that are now highly competitive, both in commercial and in research contexts.This book presents the crucial theoretical foundations needed by any student, researcher, or practitioner working on novel interface design, with chapters on statistical methods, digital signal processing (DSP), and machine learning (ML). These foundations are followed by chapters that discuss case studies on smart cities, brain–computer interfaces, probabilistic mobile text entry, secure gestures, personal context from mobile phones, adaptive touch interfaces, and automotive user interfaces. The case studies chapters also highlight an in-depth look at the practical application of DSP and ML methods used for processing of touch, gesture, biometric, or embedded sensor inputs. A common theme throughout the case studies is ubiquitous support for humans in their daily professional or personal activities.In addition, the book provides walk-through examples of different DSP and ML techniques and their use in interactive systems. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal and multi-sensor systems. In a series of short additions to each chapter, an expert on the legal and ethical issues explores the emergent deep concerns of the professional community, on how DSP and ML should be adopted and used in socially appropriate ways, to most effectively advance human performance during ubiquitous interaction with omnipresent computers.This carefully edited collection is written by international experts and pioneers in the fields of DSP and ML. It provides a textbook for students and a reference and technology roadmap for developers and professionals working on interaction design on emerging platforms.",2021,,
1591,,WWW '19: Companion Proceedings of The 2019 World Wide Web Conference,,"It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.",2019,,
1592,,ASE 2016: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,,,2016,,
1593,,SA '16: SIGGRAPH ASIA 2016 Courses,,"The SIGGRAPH Asia Courses program will feature a variety of instructional sessions catered to the different levels of expertise of our attendees. Sessions from introductory to advanced topics in computer graphics and interactive techniques will be conducted by speakers from renowned organizations and academic research institutions from over the world.The program has been the premier source for practitioners, developers, researchers, artists, and students who want to learn about the state-of-the-art technologies in computer graphics and their related topics.",2016,,
1594,,CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing,,"Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous ""revise and resubmit"" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on ""Conversational AI &amp; Lessons Learned."" Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, ""The Science Gap."" We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, ""Mobility in Collaboration.""",2017,,
1595,,CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems,,,2021,,
1596,,"The Handbook of Multimodal-Multisensor Interfaces: Foundations, User Modeling, and Common Modality Combinations - Volume 1",," The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces-user input involving new media (speech, multi-touch, gestures, writing) embedded in multimodal-multisensor interfaces. These interfaces support smartphones, wearables, in-vehicle, robotic, and many other applications that are now highly competitive commercially.   This edited collection is written by international experts and pioneers in the field. It provides a textbook for students, and a reference and technology roadmap for professionals working in this rapidly emerging area.    Volume 1 of the handbook presents relevant theory and neuroscience foundations for guiding the development of high-performance systems. Additional chapters discuss approaches to user modeling, interface design that supports user choice, synergistic combination of modalities with sensors, and blending of multimodal input and output. They also highlight an in-depth look at the most common multimodal-multisensor combinations- for example, touch and pen input, haptic and non-speech audio output, and speech co-processed with visible lip movements, gaze, gestures, or pen input. A common theme throughout is support for mobility and individual differences among users-including the world's rapidly growing population of seniors.    These handbook chapters provide walk-through examples and video illustrations of different system designs and their interactive use. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal-multisensor systems. In the final chapter, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces should be designed in the future to most effectively advance human performance. ",2017,,
1597,"McMenemy, David",Ethics and Personal Context,,,2021,,
1598,"Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.",Challenges in Deploying Machine Learning: A Survey of Case Studies,"Machine learning applications, sofware deployment","In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow we show that practitioners face issues at each stage of the deployment process. The goal of this paper is to lay out a research agenda to explore approaches addressing these challenges.",2022,,10.1145/3533378
1599,"Leznik, Mark and Grohmann, Johannes and Kliche, Nina and Bauer, Andr\'{e} and Seybold, Daniel and Eismann, Simon and Kounev, Samuel and Domaschka, J\""{o}rg","Same, Same, but Dissimilar: Exploring Measurements for Workload Time-Series Similarity","distance metrics, data sets, workload analysis, time series similarity, time series","Benchmarking is a core element in the toolbox of most systems researchers and is used for analyzing, comparing, and validating complex systems. In the quest for reliable benchmark results, a consensus has formed that a significant experiment must be based on multiple runs. To interpret these runs, mean and standard deviation are often used. In case of experiments where each run produces a time series, applying and comparing the mean is not easily applicable and not necessarily statistically sound. Such an approach ignores the possibility of significant differences between runs with a similar average. In order to verify this hypothesis, we conducted a survey of 1,112 publications of selected performance engineering and systems conferences canvassing open data sets from performance experiments. The identified 3 data sets purely rely on average and standard deviation. Therefore, we propose a novel analysis approach based on similarity analysis to enhance the reliability of performance evaluations. Our approach evaluates 12 (dis-)similarity measures with respect to their applicability in analysing performance measurements and identifies four suitable similarity measures. We validate our approach by demonstrating the increase in reliability for the data sets found in the survey.",2022,,10.1145/3489525.3511699
1600,"McMenemy, David",Ethics and Secure Gestures,,,2021,,
1601,,Preface,,"Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice provides a comprehensive resource on what has become the dominant paradigm for novel interaction design methods involving gesture, speech, text, and touch embedded in novel and emerging interfaces. These interfaces support smartphones, wearables, in-vehicle devices, virtual reality, robotic, the Internet of Things (IoT), brain–computer interaction, and many other applications that are now highly competitive commercially.This edited collection is written by international experts and pioneers in the field of digital signal processing (DSP) and machine learning (ML) for interactive systems. It provides a textbook for students, and a reference and technology roadmap for developers and professionals working in interaction design on emerging platforms. This introductory textbook presents theory chapters on statistical grounding, signal processing, and ML foundations for guiding the development of novel interactive systems. Additional chapters discuss case studies on smart cities, brain–computer interfaces (BCI), probabilistic text entry, secure gestures, personal context from mobile phones, building adaptive touch interfaces, and automotive user interfaces (UIs). The chapters on case studies also highlight an in-depth look at domain-specific language (DSL) and ML methods used, for example, in touch, gesture, electroencephalography (EEG), electrocardiography (ECG), and galvanic skin response (GSR) signals, or embedded sensor inputs. A common theme throughout is the ubiquitous support for humans as they go about their daily professional or personal activities.This introductory book provides walk-through examples of different DSP and ML techniques and their use in interactive systems. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal–multisensor systems. After each chapter an expert on the legal and ethical issues explores the wider ethical issues on how DSP and ML should be adopted and used in socially appropriate ways, to most effectively advance human performance during interaction with novel platforms.Parisa Eslambolchilar, Andreas Komninos, and Mark D. Dunlop, March 2020AcknowledgmentsWe would like to thank our external reviewers for their valuable feedback throughout the writing process.",2021,,
1602,"McMenemy, David",Ethical Issues in Probabilistic Text Entry,,,2021,,
1603,"Lu, Yi and Yin, Jun and Ge, Shilun",Analysis of Dynamic Complexity Feature of Information System Data Based on Visualization,"visualization, feature analysis, management information system, dynamic complexity","The volume and complexity of the data in the information system continues to increase during the operation of the information system. The phenomenon leads to the fact that decision-makers are faced with data-rich and information-deficient situations. How to intuitively grasp the dynamic complexity of information system data has become a concern of scholars. Therefore, this paper collected and organized the system log data of two shipping companies, adopted multiple visualization forms, and combined the dynamic complexity measurement methods of information systems, focusing on the overall distribution status and development trend of the data dynamic complexity and internal information system and the influence of complexity of subsystems within the information system on overall data dynamic operation complexity, and then analyzed the overall feature of the dynamic complexity of information system data, and summarized the feature regulars of the complexity of the visualization method. The results show that the visual feature analysis of the dynamic complexity of information systems can improve the ability of enterprises to analyze and make decisions, and provide a basis for enterprise managers to develop corresponding management measures.",2018,,10.1145/3277139.3277179
1604,"Batubara, F. Rizal and Ubacht, Jolien and Janssen, Marijn",Challenges of Blockchain Technology Adoption for E-Government: A Systematic Literature Review,"adoption, literature review, government, blockchain, public service","The ability of blockchain technology to record transactions on distributed ledgers offers new opportunities for governments to improve transparency, prevent fraud, and establish trust in the public sector. However, blockchain adoption and use in the context of e-Government is rather unexplored in academic literature. In this paper, we systematically review relevant research to understand the current research topics, challenges and future directions regarding blockchain adoption for e-Government. The results show that the adoption of blockchain-based applications in e-Government is still very limited and there is a lack of empirical evidence. The main challenges faced in blockchain adoption are predominantly presented as technological aspects such as security, scalability and flexibility. From an organizational point of view, the issues of acceptability and the need of new governance models are presented as the main barriers to adoption. Moreover, the lack of legal and regulatory support is identified as the main environmental barrier of adoption. Based on the challenges presented in the literature, we propose future research questions that need to be addressed to inform how the public sector should approach the blockchain technology adoption.",2018,,10.1145/3209281.3209317
1605,"Musciotto, Federico and Delpriori, Saverio and Castagno, Paolo and Pournaras, Evangelos",Mining Social Interactions in Privacy-Preserving Temporal Networks,,"The opportunities to empirically study temporal networks nowadays are immense thanks to Internet of Things technologies along with ubiquitous and pervasive computing that allow a real-time fine-grained collection of social network data. This empowers data analytics and data scientists to reason about complex temporal phenomena, such as disease spread, residential energy consumption, political conflicts etc., using systematic methologies from complex networks and graph spectra analysis. However, a misuse of these methods may result in privacy-intrusive and discriminatory actions that may threaten citizens' autonomy and put their life under surveillance. This paper studies highly sparse temporal networks that model social interactions such as the physical proximity of participants in conferences. When citizens can self-determine the anonymized proximity data they wish to share via privacy-preserving platforms, temporal networks may turn out to be highly sparse and have low quality. This paper shows that even in this challenging scenario of privacy-by-design, significant information can be mined from temporal networks such as the correlation of events happening during a conference or stable groups interacting over time. The findings of this paper contribute to the introduction of privacy-preserving data analytics in temporal networks and their applications.",2016,,
1606,"Tu, Zhen and Xu, Fengli and Li, Yong and Zhang, Pengyu and Jin, Depeng",A New Privacy Breach: User Trajectory Recovery From Aggregated Mobility Data,,"Human mobility data have been ubiquitously collected through cellular networks and mobile applications, and publicly released for academic research and commercial purposes for the last decade. Since releasing individual’s mobility records usually gives rise to privacy issues, data sets owners tend to only publish aggregated mobility data, such as the number of users covered by a cellular tower at a specific timestamp, which is believed to be sufficient for preserving users’ privacy. However, in this paper, we argue and prove that even publishing aggregated mobility data could lead to privacy breach in individuals’ trajectories. We develop an attack system that is able to exploit the uniqueness and regularity of human mobility to recover individual’s trajectories from the aggregated mobility data without any prior knowledge. By conducting experiments on two real-world data sets collected from both the mobile application and cellular network, we reveal that the attack system is able to recover users’ trajectories with an accuracy of about 73%~91% at the scale of thousands to ten thousands of mobile users, which indicates severe privacy leakage in such data sets. Our extensive analysis also reveals that by generalization and perturbation, this kind of privacy leakage can only be mitigated. Through the investigation on aggregated mobility data, this paper recognizes a novel privacy problem in publishing statistic data, which appeals for immediate attentions from both the academy and industry.",2018,,10.1109/TNET.2018.2829173
1607,"Grosset, Pascal and Biwer, Christopher M. and Pulido, Jesus and Mohan, Arvind T. and Biswas, Ayan and Patchett, John and Turton, Terece L. and Rogers, David H. and Livescu, Daniel and Ahrens, James",Foresight: Analysis That Matters for Data Reduction,"multi-layer neural network, performance evaluation, data compression","As the computation power of supercomputers increases, so does simulation size, which in turn produces orders-of-magnitude more data. Because generated data often exceed the simulation's disk quota, many simulations would stand to benefit from data-reduction techniques to reduce storage requirements. Such techniques include autoencoders, data compression algorithms, and sampling. Lossy compression techniques can significantly reduce data size, but such techniques come at the expense of losing information that could result in incorrect post hoc analysis results. To help scientists determine the best compression they can get while keeping their analyses accurate, we have developed Foresight, an analysis framework that enables users to evaluate how different data-reduction techniques will impact their analyses. We use particle data from a cosmology simulation, turbulence data from Direct Numerical Simulation, and asteroid impact data from xRage to demonstrate how Foresight can help scientists determine the best data-reduction technique for their simulations.",2020,,
1608,"Young, Meg and Rodriguez, Luke and Keller, Emily and Sun, Feiyang and Sa, Boyang and Whittington, Jan and Howe, Bill",Beyond Open vs. Closed: Balancing Individual Privacy and Public Accountability in Data Sharing,"data sharing, privacy, data ethics, data governance, algorithmic bias","Data too sensitive to be ""open"" for analysis and re-purposing typically remains ""closed"" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.",2019,,10.1145/3287560.3287577
1609,"Singh, Jatinder and Pasquier, Thomas and Bacon, Jean and Powles, Julia and Diaconu, Raluca and Eyers, David",Big Ideas Paper: Policy-Driven Middleware for a Legally-Compliant Internet of Things,"policy specification and enforcement, audit, Law, regulation","Internet of Things (IoT) applications, systems and services are subject to law. We argue that for the IoT to develop lawfully, there must be technical mechanisms that allow the enforcement of specified policy, such that systems align with legal realities. The audit of policy enforcement must assist the apportionment of liability, demonstrate compliance with regulation, and indicate whether policy correctly captures legal responsibilities. As both systems and obligations evolve dynamically, this cycle must be continuously maintained.This poses a huge challenge given the global scale of the IoT vision. The IoT entails dynamically creating new services through managed and flexible data exchange. Data management is complex in this dynamic environment, given the need to both control and share information, often across federated domains of administration.We see middleware playing a key role in managing the IoT. Our vision is for a middleware-enforced, unified policy model that applies end-to-end, throughout the IoT. This is because policy cannot be bound to things, applications, or administrative domains, since functionality is the result of composition, with dynamically formed chains of data flows.We have investigated the use of Information Flow Control (IFC) to manage and audit data flows in cloud computing; a domain where trust can be well-founded, regulations are more mature and associated responsibilities clearer. We feel that IFC has great potential in the broader IoT context. However, the sheer scale and the dynamic, federated nature of the IoT pose a number of significant research challenges.",2016,,10.1145/2988336.2988349
1610,"Gupta, Srishti and Jablonski, Julia and Tsai, Chun-Hua and Carroll, John M.",Instagram of Rivers: Facilitating Distributed Collaboration in Hyperlocal Citizen Science,"distributed collaboration, collaboratory, citizen science, sustainability","Citizen science project leaders collecting field data in a hyperlocal community often face common socio-technical challenges, which can potentially be addressed by sharing innovations across different groups through peer-to-peer collaboration. However, most citizen science groups practice in isolation, and end up re-inventing the wheel when it comes to addressing these common challenges. This study seeks to investigate distributed collaboration between different water monitoring citizen science groups. We discovered a unique social network application called Water Reporter that mediated distributed collaboration by creating more visibility and transparency between groups using the app. We interviewed 8 citizen science project leaders who were users of this app, and 6 other citizen science project leaders to understand how distributed collaboration mediated by this app differed from collaborative practices of Non Water Reporter users. We found that distributed collaboration was an important goal for both user groups, however, the tasks that support these collaboration activities differed for the two user groups.",2022,,10.1145/3512944
1611,"McMenemy, David",Ethics and Smart Cities,,,2021,,
1612,"Ding, Junhua and Kang, Xiaojun and Hu, Xin-Hua",Validating a Deep Learning Framework by Metamorphic Testing,"metamorphic testing, deep learning, support vector machine, software validation, neural network","Deep learning has become an important tool for image classification and natural language processing. However, the effectiveness of deep learning is highly dependent on the quality of the training data as well as the net model for the learning. The training data set for deep learning normally is fairly large, and the net model is pretty complex. It is necessary to validate the deep learning framework including the net model, executing environment, and training data set before it is used for any applications. In this paper, we propose an approach for validating the classification accuracy of a deep learning framework that includes a convolutional neural network, a deep learning executing environment, and a massive image data set. The framework is first validated with a classifier built on support vector machine, and then it is tested using a metamorphic validation approach. The effectiveness of the approach is demonstrated by validating a deep learning classifier for automated classification of biology cell images. The proposed approach can be used for validating other deep learning framework for different applications.",2017,,
1613,"McMenemy, David",Ethical Issues in Brain–Computer Interfaces,,,2021,,
1614,"Rieder, Bernhard",Studying Facebook via Data Extraction: The Netvizz Application,"research tool, media studies, social network analysis, Facebook, data extraction, social networking services","This paper describes Netvizz, a data collection and extraction application that allows researchers to export data in standard file formats from different sections of the Facebook social networking service. Friendship networks, groups, and pages can thus be analyzed quantitatively and qualitatively with regards to demographical, post-demographical, and relational characteristics. The paper provides an overview over analytical directions opened up by the data made available, discusses platform specific aspects of data extraction via the official Application Programming Interface, and briefly engages the difficult ethical considerations attached to this type of research.",2013,,10.1145/2464464.2464475
1615,"McGinnis, Leon F. and Rose, Oliver",History and Perspective of Simulation in Manufacturing,,"Manufacturing systems incorporate many semi-independent, yet strongly interacting processes, usually exhibiting some stochastic behavior. As a consequence, overall system behavior, in the long run but also in the short run, is very difficult to predict. Not surprisingly, both practitioners and academics recognized in the 1950's the potential value of discrete event simulation technology in supporting manufacturing system decision-making. This short history is one perspective on the development and evolution of discrete event simulation technology and applications, specifically focusing on manufacturing applications. This assessment is based on an examination of the literature, our own experiences, and interviews with leading practitioners. History is interesting, but it's useful only if it helps us see a way forward, so we offer some opinions on the state of the research and practice of simulation in manufacturing, and the opportunities to further advance the field.",2017,,
1616,"Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret",Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure,"machine learning, requirements engineering, datasets","Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.",2021,,10.1145/3442188.3445918
1617,"Bansal, Chahat and Jain, Arpit and Barwaria, Phaneesh and Choudhary, Anuj and Singh, Anupam and Gupta, Ayush and Seth, Aaditeshwar",Temporal Prediction of Socio-Economic Indicators Using Satellite Imagery,"socio-economic development, Landsat, temporal prediction, census, poverty mapping, satellite imagery","Machine learning models based on satellite data have been actively researched to serve as a proxy for the prediction of socio-economic development indicators. Such models have however rarely been tested for transferability over time, i.e. whether models learned on data for a certain year are able to make accurate predictions on data for another year. Using a dataset from the Indian census at two time points, for the years 2001 and 2011, we evaluate the temporal transferability of a simple machine learning model at sub-national scales of districts and propose a generic method to improve its performance. This method can be especially relevant when training datasets are small to train a robust prediction model. Then, we go further to build an aggregate development index at the district-level, on the lines of the Human Development Index (HDI) and demonstrate high accuracy in predicting the index based on satellite data for different years. This can be used to build applications to guide data-driven policy making at fine spatial and temporal scales, without the need to conduct frequent expensive censuses and surveys on the ground.",2020,,10.1145/3371158.3371167
1618,"Basole, Rahul C. and Russell, Martha G. and Huhtam\""{a}ki, Jukka and Rubens, Neil and Still, Kaisa and Park, Hyunwoo",Understanding Business Ecosystem Dynamics: A Data-Driven Approach,"information visualization, interorganizational networks, Data triangulation, business ecosystem","Business ecosystems consist of a heterogeneous and continuously evolving set of entities that are interconnected through a complex, global network of relationships. However, there is no well-established methodology to study the dynamics of this network. Traditional approaches have primarily utilized a single source of data of relatively established firms; however, these approaches ignore the vast number of relevant activities that often occur at the individual and entrepreneurial levels. We argue that a data-driven visualization approach, using both institutionally and socially curated datasets, can provide important complementary, triangulated explanatory insights into the dynamics of interorganizational networks in general and business ecosystems in particular. We develop novel visualization layouts to help decision makers systemically identify and compare ecosystems. Using traditionally disconnected data sources on deals and alliance relationships (DARs), executive and funding relationships (EFRs), and public opinion and discourse (POD), we empirically illustrate our data-driven method of data triangulation and visualization techniques through three cases in the mobile industry Google’s acquisition of Motorola Mobility, the coopetitive relation between Apple and Samsung, and the strategic partnership between Nokia and Microsoft. The article concludes with implications and future research opportunities.",2015,,10.1145/2724730
1619,"Morton, Kristi and Bunker, Ross and Mackinlay, Jock and Morton, Robert and Stolte, Chris",Dynamic Workload Driven Data Integration in Tableau,"data integration, visualization","Tableau is a commercial business intelligence (BI) software tool that supports interactive, visual analysis of data. Armed with a visual interface to data and a focus on usability, Tableau enables a wide audience of end-users to gain insight into their datasets. The user experience is a fluid process of interaction in which exploring and visualizing data takes just a few simple drag-and-drop operations (no programming or DB experience necessary). In this context of exploratory, ad-hoc visual analysis, we describe a novel approach to integrating large, heterogeneous data sources. We present a new feature in Tableau called data blending, which gives users the ability to create data visualization mashups from structured, heterogeneous data sources dynamically without any upfront integration effort. Users can author visualizations that automatically integrate data from a variety of sources, including data warehouses, data marts, text files, spreadsheets, and data cubes. Because our data blending system is workload driven, we are able to bypass many of the pain-points and uncertainty in creating mediated schemas and schema-mappings in current pay-as-you-go integration systems.",2012,,10.1145/2213836.2213961
1620,"Wang, Xiaolan and Dong, Xin Luna and Meliou, Alexandra",Data X-Ray: A Diagnostic Tool for Data Errors,"data cleaning, error diagnosis, data profiling","A lot of systems and applications are data-driven, and the correctness of their operation relies heavily on the correctness of their data. While existing data cleaning techniques can be quite effective at purging datasets of errors, they disregard the fact that a lot of errors are systematic, inherent to the process that produces the data, and thus will keep occurring unless the problem is corrected at its source. In contrast to traditional data cleaning, in this paper we focus on data diagnosis: explaining where and how the errors happen in a data generative process.We develop a large-scale diagnostic framework called DATA X-RAY. Our contributions are three-fold. First, we transform the diagnosis problem to the problem of finding common properties among erroneous elements, with minimal domain-specific assumptions. Second, we use Bayesian analysis to derive a cost model that implements three intuitive principles of good diagnoses. Third, we design an efficient, highly-parallelizable algorithm for performing data diagnosis on large-scale data. We evaluate our cost model and algorithm using both real-world and synthetic data, and show that our diagnostic framework produces better diagnoses and is orders of magnitude more efficient than existing techniques.",2015,,10.1145/2723372.2750549
1621,"Fan, Wenfei and Wu, Yinghui and Xu, Jingbo",Functional Dependencies for Graphs,"graphs, satisfiability, functional dependencies, validation, implication","We propose a class of functional dependencies for graphs, referred to as GFDs. GFDs capture both attribute-value dependencies and topological structures of entities, and subsume conditional functional dependencies (CFDs) as a special case. We show that the satisfiability and implication problems for GFDs are coNP-complete and NP-complete, respectively, no worse than their CFD counterparts. We also show that the validation problem for GFDs is coNP-complete. Despite the intractability, we develop parallel scalable algorithms for catching violations of GFDs in large-scale graphs. Using real-life and synthetic data, we experimentally verify that GFDs provide an effective approach to detecting inconsistencies in knowledge and social graphs.",2016,,10.1145/2882903.2915232
1622,"Wu, Jian and Sheng, Victor S. and Zhang, Jing and Li, Hua and Dadakova, Tetiana and Swisher, Christine Leon and Cui, Zhiming and Zhao, Pengpeng",Multi-Label Active Learning Algorithms for Image Classification: Overview and Future Promise,"Image classification, multi-label image, annotation, active learning, sampling strategy","Image classification is a key task in image understanding, and multi-label image classification has become a popular topic in recent years. However, the success of multi-label image classification is closely related to the way of constructing a training set. As active learning aims to construct an effective training set through iteratively selecting the most informative examples to query labels from annotators, it was introduced into multi-label image classification. Accordingly, multi-label active learning is becoming an important research direction. In this work, we first review existing multi-label active learning algorithms for image classification. These algorithms can be categorized into two top groups from two aspects respectively: sampling and annotation. The most important component of multi-label active learning is to design an effective sampling strategy that actively selects the examples with the highest informativeness from an unlabeled data pool, according to various information measures. Thus, different informativeness measures are emphasized in this survey. Furthermore, this work also makes a deep investigation on existing challenging issues and future promises in multi-label active learning with a focus on four core aspects: example dimension, label dimension, annotation, and application extension.",2020,,10.1145/3379504
1623,"Lee, En-Shiun Annie and Sze-To, Ho-Yin Antonio and Wong, Man-Hon and Leung, Kwong-Sak and Lau, Terrence Chi-Kong and Wong, Andrew K. C.",Discovering Protein-DNA Binding Cores by Aligned Pattern Clustering,,"Understanding binding cores is of fundamental importance in deciphering Protein-DNA TF-TFBS binding and gene regulation. Limited by expensive experiments, it is promising to discover them with variations directly from sequence data. Although existing computational methods have produced satisfactory results, they are one-to-one mappings with no site-specific information on residue/nucleotide variations, where these variations in binding cores may impact binding specificity. This study presents a new representation for modeling binding cores by incorporating variations and an algorithm to discover them from only sequence data. Our algorithm takes protein and DNA sequences from TRANSFAC a Protein-DNA Binding Database as input; discovers from both sets of sequences conserved regions in Aligned Pattern Clusters APCs; associates them as Protein-DNA Co-Occurring APCs; ranks the Protein-DNA Co-Occurring APCs according to their co-occurrence, and among the top ones, finds three-dimensional structures to support each binding core candidate. If successful, candidates are verified as binding cores. Otherwise, homology modeling is applied to their close matches in PDB to attain new chemically feasible binding cores. Our algorithm obtains binding cores with higher precision and much faster runtime (≥1,600x) than that of its contemporaries, discovering candidates that do not co-occur as one-to-one associated patterns in the raw data. Availability: http://www.pami.uwaterloo.ca/~ealee/files/tcbbPnDna2015/Release.zip.",2017,,10.1109/TCBB.2015.2474376
1624,"Viscusi, Gianluigi and Collins, Aengus and Florin, Marie-Valentine",Governments' Strategic Stance toward Artificial Intelligence: An Interpretive Display on Europe,"Governance, Public Sector, Strategic Types, Strategy, Artificial Intelligence, Policies, Digitalization, Miles and Snow","This article aims to provide an interpretive display of the strategic stance toward innovation enabled by Artificial Intelligence (AI) of different governments based in Europe. The analysis includes the European Union (EU), some of its members as well as a non-member country, which we argue presents interesting characteristics. Based on a comprehensive analysis of a corpus of documents, which includes national strategies, external reports as well as web resources, the different countries considered in this article are subsequently classified using as interpretive lens, among other frameworks, the strategic types identified by Miles and Snow (defender, prospector, analyzer, reactor). The results show a prevalence of a ""prospector"" stance, interested in differentiation and the search of novelty. However, the results also show that similar strategic types may be driven by different values as well as governance orientation among the considered countries, thus leading to different potential ways to implement the expected AI-enabled innovation.",2020,,10.1145/3428502.3428508
1625,"Mart\'{\i}nez-Fern\'{a}ndez, Silverio and Bogner, Justus and Franch, Xavier and Oriol, Marc and Siebert, Julien and Trendowicz, Adam and Vollmer, Anna Maria and Wagner, Stefan",Software Engineering for AI-Based Systems: A Survey,"systematic mapping study, artificial intelligence, AI-based systems, Software engineering","AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.",2022,,10.1145/3487043
1626,"Tsai, Yi-Shan and Singh, Shaveen and Rakovic, Mladen and Lim, Lisa-Angelique and Roychoudhury, Anushka and Gasevic, Dragan",Charting Design Needs and Strategic Approaches for Academic Analytics Systems through Co-Design,"co-design, educational quality, academic analytics, implementation strategy, higher education"," Academic analytics focuses on collecting, analysing and visualising educational data to generate institutional insights and improve decision-making for academic purposes. However, challenges that arise from navigating a complex organisational structure when introducing analytics systems have called for the need to engage key stakeholders widely to cultivate a shared vision and ensure that implemented systems create desired value. This paper presents a study that takes co-design steps to identify design needs and strategic approaches for the adoption of academic analytics, which serves the purpose of enhancing the measurement of educational quality utilising institutional data. Through semi-structured interviews with 54 educational stakeholders at a large research university, we identified particular interest in measuring student engagement and the performance of courses and programmes. Based on the observed perceptions and concerns regarding data use to measure or evaluate these areas, implications for adoption strategy of academic analytics, such as leadership involvement, communication, and training, are discussed. ",2022,,10.1145/3506860.3506939
1627,"Missier, Paolo and Bajoudah, Shaimaa and Capossele, Angelo and Gaglione, Andrea and Nati, Michele",Mind My Value: A Decentralized Infrastructure for Fair and Trusted IoT Data Trading,,"Internet of Things (IoT) data are increasingly viewed as a new form of massively distributed and large scale digital assets, which are continuously generated by millions of connected devices. The real value of such assets can only be realized by allowing IoT data trading to occur on a marketplace that rewards every single producer and consumer, at a very granular level. Crucially, we believe that such a marketplace should not be owned by anybody, and should instead fairly and transparently self-enforce a well defined set of governance rules. In this paper we address some of the technical challenges involved in realizing such a marketplace. We leverage emerging blockchain technologies to build a decentralized, trusted, transparent and open architecture for IoT traffic metering and contract compliance, on top of the largely adopted IoT brokered data infrastructure. We discuss an Ethereum-based prototype implementation and experimentally evaluate the overhead cost associated with Smart Contract transactions, concluding that a viable business model can indeed be associated with our technical approach.",2017,,10.1145/3131542.3131564
1628,"De Sa, Christopher and Ratner, Alex and R\'{e}, Christopher and Shin, Jaeho and Wang, Feiran and Wu, Sen and Zhang, Ce",DeepDive: Declarative Knowledge Base Construction,,"The dark data extraction or knowledge base construction (KBC) problem is to populate a SQL database with information from unstructured data sources including emails, webpages, and pdf reports. KBC is a long-standing problem in industry and research that encompasses problems of data extraction, cleaning, and integration. We describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems. The key idea in DeepDive is that statistical inference and machine learning are key tools to attack classical data problems in extraction, cleaning, and integration in a unified and more effective manner. DeepDive programs are declarative in that one cannot write probabilistic inference algorithms; instead, one interacts by defining features or rules about the domain. A key reason for this design choice is to enable domain experts to build their own KBC systems. We present the applications, abstractions, and techniques of DeepDive employed to accelerate construction of KBC systems.",2016,,10.1145/2949741.2949756
1629,"Bala Bisandu, Desmond and Salih Homaid, Mohammed and Moulitsas, irene and Filippone, Salvatore",A Deep Feedforward Neural Network and Shallow Architectures Effectiveness Comparison: Flight Delays Classification Perspective,"Support Vector Machine, Classification, flight delays, deep learning, deep neural network","Flight delays have negatively impacted the socio-economics state of passengers, airlines and airports, resulting in huge economic losses. Hence, it has become necessary to correctly predict their occurrences in decision-making because it is important for the effective management of the aviation industry. Developing accurate flight delays classification models depends mostly on the air transportation system complexity and the infrastructure available in airports, which may be a region-specific issue. However, no specific prediction or classification model can handle the individual characteristics of all airlines and airports at the same time. Hence, the need to further develop and compare predictive models for the aviation decision system of the future cannot be over-emphasised. In this research, flight on-time data records from the United State Bureau of Transportation Statistics was employed to evaluate the performances of Deep Feedforward Neural Network, Neural Network, and Support Vector Machine models on a binary classification problem. The research revealed that the models achieved different accuracies of flight delay classifications. The Support Vector Machine had the worst average accuracy than Neural Network and Deep Feedforward Neural Network in the initial experiment. The Deep Feedforward Neural Network outperformed Support Vector Machines and Neural Network with the best average percentage accuracies. Going further to investigate the Deep Feedforward Neural Network architecture on different parameters against itself suggest that training a Deep Feedforward Neural Network algorithm, regardless of data training size, the classification accuracy peaks. We examine which number of epochs works best in our flight delay classification settings for the Deep Feedforward Neural Network. Our experiment results demonstrate that having many epochs affects the convergence rate of the model; unlike when hidden layers are increased, it does not ensure better or higher accuracy in a binary classification of flight delays. Finally, we recommended further studies on the applicability of the Deep Feedforward Neural Network in flight delays prediction with specific case studies of either airlines or airports to check the impact on the model's performance.",2021,,10.1145/3505711.3505712
1630,"Tufte, Kristin and Datta, Kushal and Jindal, Alekh and Maier, David and Bertini, Robert L.",Challenges and Opportunities in Transportation Data,"Smart Cities, Data Management, Transportation Data","From the time and money lost sitting in congestion and waiting for traffic signals to change, to the many people injured and killed in traffic crashes each year, to the emissions and energy consumption from our vehicles, the effects of transportation on our daily lives are immense. A wealth of transportation data is available to help address these problems; from data from sensors installed to monitor and operate the roadways and traffic signals to data from cell phone apps and -- just over the horizon -- data from connected vehicles and infrastructure. However, this wealth of data has yet to be effectively leveraged, thus providing opportunities in areas such as improving traffic safety, reducing congestion, improving traffic signal timing, personalizing routing, coordinating across transportation agencies and more. This paper presents opportunities and challenges in applying data management technology to the transportation domain.",2018,,10.1145/3236461.3241971
1631,"Taylor, Simon J. E. and Anagnostou, Anastasia and Fabiyi, Adedeji and Currie, Christine and Monks, Thomas and Barbera, Roberto and Becker, Bruce",Open Science: Approaches and Benefits for Modeling &amp; Simulation,,"Open Science is the practice of making scientific research accessible to all. It promotes open access to the artefacts of research, the software, data, results and the scientific articles in which they appear, so that others can validate, use and collaborate. Open Science is also being mandated by many funding bodies. The concept of Open Science is new to many Modelling &amp; Simulation (M&amp;S) researchers. To introduce Open Science to our field, this paper unpacks Open Science to understand some of its approaches and benefits. Good practice in the reporting of simulation studies is discussed and the Strengthening the Reporting of Empirical Simulation Studies (STRESS) standardized checklist approach is presented. A case study shows how Digital Object Identifiers, Researcher Registries, Open Access Data Repositories and Scientific Gateways can support Open Science practices for M&amp;S research. The article concludes with a set of guidelines for adopting Open Science for M&amp;S.",2017,,
1632,"Park, Jaeyeon and Nam, Woojin and Choi, Jaewon and Kim, Taeyeong and Yoon, Dukyong and Lee, Sukhoon and Paek, Jeongyeup and Ko, JeongGil",Glasses for the Third Eye: Improving the Quality of Clinical Data Analysis with Motion Sensor-Based Data Filtering,"Clinical Decision Support System, Wireless Sensor Network, Noise Filter, Motion Sensing, Health Care Information Systems","Recent advances in machine learning based data analytics are opening opportunities for designing effective clinical decision support systems (CDSS) which can become the ""third-eye"" in the current clinical procedures and diagnosis. However, common patient movements in hospital wards may lead to faulty measurements in physiological sensor readings, and training a CDSS from such noisy data can cause misleading predictions, directly leading to potentially dangerous clinical decisions. In this work, we present MediSense, a system to sense, classify, and identify noise-causing motions and activities that affect physiological signal when made by patients on their hospital beds. Essentially, such a system can be considered as ""glasses"" for the clinical third eye in correctly observing medical data. MediSense combines wirelessly connected embedded platforms for motion detection with physiological signal data collected from patients to identify faulty physiological signal measurements and filters such noisy data from being used in CDSS training or testing datasets. We deploy our system in real intensive care units (ICUs), and evaluate its performance from real patient traces collected at these ICUs through a 4-month pilot study at the Ajou University Hospital Trauma Center, a major hospital facility located in Suwon, South Korea. Our results show that MediSense successfully classifies patient motions on the bed with &gt;90% accuracy, shows 100% reliability in determining the locations of beds within the ICU, and each bed-attached sensor achieves a lifetime of more than 33 days, which satisfies the application-level requirements suggested by our clinical partners. Furthermore, a simple case-study with arrhythmia patient data shows that MediSense can help improve the clinical diagnosis accuracy.",2017,,10.1145/3131672.3131690
1633,"Li, Zeyu and Wang, Hongzhi and Shao, Wei and Li, Jianzhong and Gao, Hong",Repairing Data through Regular Expressions,,"Since regular expressions are often used to detect errors in sequences such as strings or date, it is natural to use them for data repair. Motivated by this, we propose a data repair method based on regular expression to make the input sequence data obey the given regular expression with minimal revision cost. The proposed method contains two steps, sequence repair and token value repair.For sequence repair, we propose the Regular-expression-based Structural Repair (RSR in short) algorithm. RSR algorithm is a dynamic programming algorithm that utilizes Nondeterministic Finite Automata (NFA) to calculate the edit distance between a prefix of the input string and a partial pattern regular expression with time complexity of O(nm2) and space complexity of O(mn) where m is the edge number of NFA and n is the input string length. We also develop an optimization strategy to achieve higher performance for long strings. For token value repair, we combine the edit-distance-based method and associate rules by a unified argument for the selection of the proper method. Experimental results on both real and synthetic data show that the proposed method could repair the data effectively and efficiently.",2016,,10.14778/2876473.2876478
1634,"Anand, Sanjay Kumar and Kumar, Suresh",Experimental Comparisons of Clustering Approaches for Data Representation,"optimal score, Clustering approach, internal validation, external validation, stability validation","Clustering approaches are extensively used by many areas such as IR, Data Integration, Document Classification, Web Mining, Query Processing, and many other domains and disciplines. Nowadays, much literature describes clustering algorithms on multivariate data sets. However, there is limited literature that presented them with exhaustive and extensive theoretical analysis as well as experimental comparisons. This experimental survey paper deals with the basic principle, and techniques used, including important characteristics, application areas, run-time performance, internal, external, and stability validity of cluster quality, etc., on five different data sets of eleven clustering algorithms. This paper analyses how these algorithms behave with five different multivariate data sets in data representation. To answer this question, we compared the efficiency of eleven clustering approaches on five different data sets using three validity metrics-internal, external, and stability and found the optimal score to know the feasible solution of each algorithm. In addition, we have also included four popular and modern clustering algorithms with only their theoretical discussion. Our experimental results for only traditional clustering algorithms showed that different algorithms performed different behavior on different data sets in terms of running time (speed), accuracy and, the size of data set. This study emphasized the need for more adaptive algorithms and a deliberate balance between the running time and accuracy with their theoretical as well as implementation aspects.",2022,,10.1145/3490384
1635,"M\""{a}kitalo, Niko and Flores-Martin, Daniel and Flores, Huber and Lagerspetz, Eemil and Christophe, Francois and Ihantola, Petri and Babazadeh, Masiar and Hui, Pan and Murillo, Juan Manuel and Tarkoma, Sasu and Mikkonen, Tommi",Human Data Model: Improving Programmability of Health and Well-Being Data for Enhanced Perception and Interaction,"data mashups, ubiquitous computing, programmable world, Internet of Things, data management, Mobile computing, pervasive computing, Human Data Model, wearable computers, IoT","Today, an increasing number of systems produce, process, and store personal and intimate data. Such data has plenty of potential for entirely new types of software applications, as well as for improving old applications, particularly in the domain of smart healthcare. However, utilizing this data, especially when it is continuously generated by sensors and other devices, with the current approaches is complex—data is often using proprietary formats and storage, and mixing and matching data of different origin is not easy. Furthermore, many of the systems are such that they should stimulate interactions with humans, which further complicates the systems. In this article, we introduce the Human Data Model—a new tool and a programming model for programmers and end users with scripting skills that help combine data from various sources, perform computations, and develop and schedule computer-human interactions. Written in JavaScript, the software implementing the model can be run on almost any computer either inside the browser or using Node.js. Its source code can be freely downloaded from GitHub, and the implementation can be used with the existing IoT platforms. As a whole, the work is inspired by several interviews with professionals, and an online survey among healthcare and education professionals, where the results show that the interviewed subjects almost entirely lack ideas on how to benefit the ever-increasing amount of data measured of the humans. We believe that this is because of the missing support for programming models for accessing and handling the data, which can be satisfied with the Human Data Model.",2020,,10.1145/3402524
1636,"Nambiar, Athira and Bernardino, Alexandre and Nascimento, Jacinto C.",Gait-Based Person Re-Identification: A Survey,"gait analysis, biometrics, person re-identification, machine learning, computer vision, Video surveillance","The way people walk is a strong correlate of their identity. Several studies have shown that both humans and machines can recognize individuals just by their gait, given that proper measurements of the observed motion patterns are available. For surveillance applications, gait is also attractive, because it does not require active collaboration from users and is hard to fake. However, the acquisition of good-quality measures of a person’s motion patterns in unconstrained environments, (e.g., in person re-identification applications) has proved very challenging in practice. Existing technology (video cameras) suffer from changes in viewpoint, daylight, clothing, accessories, and other variations in the person’s appearance. Novel three-dimensional sensors are bringing new promises to the field, but still many research issues are open. This article presents a survey of the work done in gait analysis for re-identification in the past decade, looking at the main approaches, datasets, and evaluation methodologies. We identify several relevant dimensions of the problem and provide a taxonomic analysis of the current state of the art. Finally, we discuss the levels of performance achievable with the current technology and give a perspective of the most challenging and promising directions of research for the future.",2019,,10.1145/3243043
1637,"Esteves, Diego and Mendes, Pablo N. and Moussallem, Diego and Duarte, Julio Cesar and Zaveri, Amrapali and Lehmann, Jens",MEX Interfaces: Automating Machine Learning Metadata Generation,,"Despite recent efforts to achieve a high level of interoperability of Machine Learning (ML) experiments, positively collaborating with the Reproducible Research context, we still run into problems created due to the existence of different ML platforms: each of those have a specific conceptualization or schema for representing data and metadata. This scenario leads to an extra coding-effort to achieve both the desired interoperability and a better provenance level as well as a more automatized environment for obtaining the generated results. Hence, when using ML libraries, it is a common task to re-design specific data models (schemata) and develop wrappers to manage the produced outputs. In this article, we discuss this gap focusing on the solution for the question: ""What is the cleanest and lowest-impact solution, i.e., the minimal effort to achieve both higher interoperability and provenance metadata levels in the Integrated Development Environments (IDE) context and how to facilitate the inherent data querying task?"". We introduce a novel and low-impact methodology specifically designed for code built in that context, combining Semantic Web concepts and reflection in order to minimize the gap for exporting ML metadata in a structured manner, allowing embedded code annotations that are, in run-time, converted in one of the state-of-the-art ML schemas for the Semantic Web: MEX Vocabulary.",2016,,
1638,"Nazi, Azade and Ding, Bolin and Narasayya, Vivek and Chaudhuri, Surajit",Efficient Estimation of Inclusion Coefficient Using Hyperloglog Sketches,,"Efficiently estimating the inclusion coefficient - the fraction of values of one column that are contained in another column - is useful for tasks such as data profiling and foreign-key detection. We present a new estimator, BML, for inclusion coefficient based on Hyperloglog sketches that results in significantly lower error compared to the state-of-the art approach that uses Bottom-k sketches. We evaluate the error of the BML estimator using experiments on industry benchmarks such as TPC-H and TPC-DS, and several real-world databases. As an independent contribution, we show how Hyperloglog sketches can be maintained incrementally with data deletions using only a constant amount of additional memory.",2018,,10.14778/3231751.3231759
1639,"Chen, I-Min A. and Markowitz, Victor M. and Szeto, Ernest and Palaniappan, Krishna and Chu, Ken",Maintaining a Microbial Genome &amp; Metagenome Data Analysis System in an Academic Setting,"genome data analysis system, data warehouse","The Integrated Microbial Genomes (IMG) system integrates microbial community aggregate genomes (metagenomes) with genomes from all domains of life. IMG provides tools for analyzing and reviewing the structural and functional annotations of metagenomes and genomes in a comparative context. At the core of the IMG system is a data warehouse that contains genome and metagenome datasets provided by scientific users, as well as public bacterial, archaeal, eukaryotic, and viral genomes from the US National Center for Biotechnology Information genomic archive and a rich set of engineered, environmental and host associated metagenomes. Genomes and metagenome datasets are processed using IMG's microbial genome and metagenome sequence data processing pipelines and then are integrated into the data warehouse using IMG's data integration toolkit. Microbial genome and metagenome application specific user interfaces provide access to different subsets of IMG's data and analysis toolkits. Genome and metagenome analysis is a gene centric iterative process that involves a sequence (composition) of data exploration and comparative analysis operations, with individual operations expected to have rapid response time.From its first release in 2005, IMG has grown from an initial content of about 300 genomes with a total of 2 million genes, to 22,578 bacterial, archaeal, eukaryotic and viral genomes, and 4,188 metagenome samples, with about 24.6 billion genes as of May 1st, 2014. IMG's database architecture is continuously revised in order to cope with the rapid increase in the number and size of the genome and metagenome datasets, maintain good query performance, and accommodate new data types. We present in this paper IMG's new database architecture developed over the past three years in the context of limited financial, engineering and data management resources customary to academic database systems. We discuss the alternative commercial and open source database management systems we considered and experimented with and describe the hybrid architecture we devised for sustaining IMG's rapid growth.",2014,,10.1145/2618243.2618244
1640,"Sharma, Ms Promila and Meena, Uma and Sharma, Girish Kumar",Intelligent Data Analysis Using Optimized Support Vector Machine Based Data Mining Approach for Tourism Industry,"tourism industry, Intelligent data","Data analysis involves the deployment of sophisticated approaches from data mining methods, information theory, and artificial intelligence in various fields like tourism, hospitality, and so on for the extraction of knowledge from the gathered and preprocessed data. In tourism, pattern analysis or data analysis using classification is significant for finding the patterns that represent new and potentially useful information or knowledge about the destination and other data. Several data mining techniques are introduced for the classification of data or patterns. However, overfitting, less accuracy, local minima, sensitive to noise are the drawbacks in some existing data mining classification methods. To overcome these challenges, Support vector machine with Red deer optimization (SVM-RDO) based data mining strategy is proposed in this article. Extended Kalman filter (EKF) is utilized in the first phase, i.e., data cleaning to remove the noise and missing values from the input data. Mantaray foraging algorithm (MaFA) is used in the data selection phase, in which the significant data are selected for the further process to reduce the computational complexity. The final phase is the classification, in which SVM-RDO is proposed to access the useful pattern from the selected data. PYTHON is the implementation tool used for the experiment of the proposed model. The experimental analysis is done to show the efficacy of the proposed work. From the experimental results, the proposed SVM-RDO achieved better accuracy, precision, recall, and F1 score than the existing methods for the tourism dataset. Thus, it is showed the effectiveness of the proposed SVM-RDO for pattern analysis.",2022,,10.1145/3494566
1641,"Hus\'{a}k, Martin and La\v{s}tovi\v{c}ka, Martin and Tovar\v{n}\'{a}k, Daniel",System for Continuous Collection of Contextual Information for Network Security Management and Incident Handling,"Cyber Situational Awareness, Incident Response, Incident Handling, Cybersecurity, Network Monitoring"," In this paper, we describe a system for the continuous collection of data for the needs of network security management. When a cybersecurity incident occurs in the network, the contextual information on the involved assets facilitates estimating the severity and impact of the incident and selecting an appropriate incident response. We propose a system based on the combination of active and passive network measurements and the correlation of the data with third-party systems. The system enumerates devices and services in the network and their vulnerabilities via fingerprinting of operating systems and applications. Further, the system pairs the hosts in the network with contacts on responsible administrators and highlights critical infrastructure and its dependencies. The system concentrates all the information required for common incident handling procedures and aims to speed up incident response, reduce the time spent on the manual investigation, and prevent errors caused by negligence or lack of information.",2021,,10.1145/3465481.3470037
1642,"Tang, Rong and Mon, Lorri and Beheshti, Jamshid and Li, Yuelin and Pollock, Danielle and Ni, Chaoqun and Chu, Samuel and Xiao, Lu and Caffrey, Julia and Gentry, Steven",Needs Assessment of ASIS&amp;T Publications: Bridging Information Research and Practice,"ASIS&amp;T publications, publication format and processes, knowledge transfer, user groups","This study reports the results of a 2016 online survey on perceptions and uses of ASIS&amp;T publications. The 190 survey respondents represented 26 countries and 5 continents, with 77% of participants coming from academia rather than practitioners. Among the emerging themes were calls for a wider scope of research from information science to be reflected in the publications (including JASIS&amp;T and the ASIS&amp;T Proceedings), and ongoing challenges in the role of the Bulletin as a bridge between research and practice. The study provides insights into the scholarly publishing practices of the ASIS&amp;T community and highlights key issues for the future direction of ASIS&amp;T's scholarly communication.",2016,,
1643,"McMenemy, David",Ethical Issues in Digital Signal Processing and Machine Learning,,,2021,,
1644,"Zegura, Ellen and DiSalvo, Carl and Meng, Amanda",Care and the Practice of Data Science for Social Good,"care, HCI, Data science for social good, community engagement","Data science is an interdisciplinary field that extracts insights from data through a multi-stage process of data collection, analysis and use. When data science is applied for social good, a variety of stakeholders are introduced to the process with an intention to inform policies or programs to improve well-being. Our goal in this paper is to propose an orientation to care in the practice of data science for social good. When applied to data science, a logic of care can improve the data science process and reveal outcomes of ""good"" throughout. Consideration of care in practice has its origins in Science and Technology Studies (STS) and has recently been applied by Human Computer Interaction (HCI) researchers to understand technology repair and use in under-served environments as well as care in remote health monitoring. We bring care to the practice of data science through a detailed examination of our engaged research with a community group that uses data as a strategy to advocate for permanently affordable housing. We identify opportunities and experiences of care throughout the stages of the data science process. We bring greater detail to the notion of human-centered systems for data science and begin to describe what these look like.",2018,,10.1145/3209811.3209877
1645,"Partington, Andrew and Wynn, Moe and Suriadi, Suriadi and Ouyang, Chun and Karnon, Jonathan",Process Mining for Clinical Processes: A Comparative Analysis of Four Australian Hospitals,"data preparation, health care delivery, comparative analysis, Process mining, patient pathways","Business process analysis and process mining, particularly within the health care domain, remain under-utilized. Applied research that employs such techniques to routinely collected health care data enables stakeholders to empirically investigate care as it is delivered by different health providers. However, cross-organizational mining and the comparative analysis of processes present a set of unique challenges in terms of ensuring population and activity comparability, visualizing the mined models, and interpreting the results. Without addressing these issues, health providers will find it difficult to use process mining insights, and the potential benefits of evidence-based process improvement within health will remain unrealized. In this article, we present a brief introduction on the nature of health care processes, a review of process mining in health literature, and a case study conducted to explore and learn how health care data and cross-organizational comparisons with process-mining techniques may be approached. The case study applies process-mining techniques to administrative and clinical data for patients who present with chest pain symptoms at one of four public hospitals in South Australia. We demonstrate an approach that provides detailed insights into clinical (quality of patient health) and fiscal (hospital budget) pressures in the delivery of health care. We conclude by discussing the key lessons learned from our experience in conducting business process analysis and process mining based on the data from four different hospitals.",2015,,10.1145/2629446
1646,"Shaw, Mary","Myths and Mythconceptions: What Does It Mean to Be a Programming Language, Anyhow?","domain-specific programming language, open resource coalition, vernacular software developer, exploratory programming, fitness to task, general-purpose programming language, problem-setting design, formal specifications, problem-solving design, generality-power tradeoffs, software credentials, sufficient correctness, closed software system","Modern software does not stand alone; it is embedded in complex physical and sociotechnical systems. It relies on computational support from interdependent subsystems as well as non-code resources such as data, communications, sensors, and interactions with humans. Both general-purpose programming languages and mainstream programming language research focus on symbolic notations with well-defined abstractions that are intended for use by professionals to write programs that solve precisely specified problems. There is a strong emphasis on correctness of the resulting programs, preferably by formal reasoning. However, these languages, despite their careful design and formal foundations, address only a modest portion of modern software and only a minority of software developers.  Several persistent myths reinforce this focus. These myths express an idealized model of software and software development. They provide a lens for examining modern software and software development practice: highly trained professionals are outnumbered by vernacular developers. Writing new code is dominated by composition of ill-specified software and non-software components. General-purpose languages may be less appropriate for a task than domain-specific languages, and functional correctness is often a less appropriate goal than overall fitness for task. Support for programming to meet a specification is of little help to people who are programming in order to understand their problems. Reasoning about software is challenged by uncertainty and nondeterminism in the execution environment and by the increasingly dominant role of data, especially with the advent of systems that rely on machine learning. The lens of our persistent myths illuminates the dissonance between our idealized view of software development and common practice, which enables us to identify emerging opportunities and challenges for programming language research.",2022,,10.1145/3480947
1647,"Baltodano, Sonia and Garcia-Mancilla, Jesus and Ju, Wendy",Eliciting Driver Stress Using Naturalistic Driving Scenarios on Real Roads,"Driver Benchmarking, Driver Evaluation, Interaction Design, Stress, Design Methods, Wizard of Oz","We propose a novel method for reliably inducing stress in drivers for the purpose of generating real-world participant data for machine learning, using both scripted in-vehicle stressor events and unscripted on-road stressors such as pedestrians and construction zones. On-road drives took place in a vehicle outfitted with an experimental display that lead drivers to believe they had prematurely ran out of charge on an isolated road. We describe the elicitation method, course design, instrumentation, data collection procedure and the post-hoc labeling of unplanned road events to illustrate how rich data about a variety of stress-related events can be elicited from study participants on-road. We validate this method with data including psychophysiological measurements, video, voice, and GPS data from (N=20) participants. Results from algorithmic psychophysiological stress analysis were validated using participant self-reports. Results of stress elicitation analysis show that our method elicited a stress-state in 89% of participants.",2018,,10.1145/3239060.3239090
1648,"Ronaghi, Zahra and Thomas, Rollin and Deslippe, Jack and Bailey, Stephen and Gursoy, Doga and Kisner, Theodore and Keskitalo, Reijo and Borrill, Julian",Python in the NERSC Exascale Science Applications Program for Data,,"We describe a new effort at the National Energy Research Scientific Computing Center (NERSC) in performance analysis and optimization of scientific Python applications targeting the Intel Xeon Phi (Knights Landing, KNL) manycore architecture. The Python-centered work outlined here is part of a larger effort called the NERSC Exascale Science Applications Program (NESAP) for Data. NESAP for Data focuses on applications that process and analyze high-volume, high-velocity data sets from experimental or observational science (EOS) facilities supported by the US Department of Energy Office of Science. We present three case study applications from NESAP for Data that use Python. These codes vary in terms of ""Python purity"" from applications developed in pure Python to ones that use Python mainly as a convenience layer for scientists without expertise in lower level programming languages like C, C++ or Fortran. The science case, requirements, constraints, algorithms, and initial performance optimizations for each code are discussed. Our goal with this paper is to contribute to the larger conversation around the role of Python in high-performance computing today and tomorrow, highlighting areas for future work and emerging best practices.",2017,,10.1145/3149869.3149873
1649,"Kagklis, Vasileios and Verykios, Vassilios S. and Tzimas, Giannis and Tsakalidis, Athanasios K.",Knowledge Sanitization on the Web,"LP-Based Hiding Approaches, Knowledge Hiding, Frequent Itemset Hiding, Privacy Preserving Data Mining","The widespread use of the Internet caused the rapid growth of data on the Web. But as data on the Web grew larger in numbers, so did the perils due to the applications of data mining. Privacy preserving data mining (PPDM) is the field that investigates techniques to preserve the privacy of data and patterns. Knowledge Hiding, a subfield of PPDM, aims at preserving the sensitive patterns included in the data, which are going to be published. A wide variety of techniques fall under the umbrella of Knowledge Hiding, such as frequent pattern hiding, sequence hiding, classification rule hiding and so on.In this tutorial we create a taxonomy for the frequent itemset hiding techniques. We also provide as examples for each category representative works that appeared recently and fall into each one of these categories. Then, we focus on the detailed overview of a specific category, the so called linear programming-based techniques. Finally, we make a quantitative and qualitative comparison among some of the existing techniques that are classified into this category.",2014,,10.1145/2611040.2611044
1650,"Jin, Zhuochen and Cui, Shuyuan and Guo, Shunan and Gotz, David and Sun, Jimeng and Cao, Nan",CarePre: An Intelligent Clinical Decision Assistance System,"Personal health records, neural networks, user interface design, reasoning about belief and knowledge, visual analytics","Clinical decision support systems are widely used to assist with medical decision making. However, clinical decision support systems typically require manually curated rules and other data that are difficult to maintain and keep up to date. Recent systems leverage advanced deep learning techniques and electronic health records to provide a more timely and precise result. Many of these techniques have been developed with a common focus on predicting upcoming medical events. However, although the prediction results from these approaches are promising, their value is limited by their lack of interpretability. To address this challenge, we introduce CarePre, an intelligent clinical decision assistance system. The system extends a state-of-the-art deep learning model to predict upcoming diagnosis events for a focal patient based on his or her historical medical records. The system includes an interactive framework together with intuitive visualizations designed to support diagnosis, treatment outcome analysis, and the interpretation of the analysis results. We demonstrate the effectiveness and usefulness of the CarePre&nbsp;system by reporting results from a quantities evaluation of the prediction algorithm, two case studies, and interviews with senior physicians and pulmonologists.",2020,,10.1145/3344258
1651,"Gram, Dennis and Karapanagiotis, Pantelis and Liebald, Marius and Walz, Uwe",Design and Implementation of a Historical German Firm-Level Financial Database,"databases, germany, financial data, cliometrics, economic history","Broad, long-term financial, and economic datasets are scarce resources, particularly in the European context. In this paper, we present an approach for an extensible data model that is adaptable to future changes in technologies and sources. This model may constitute a basis for digitized and structured long-term historical datasets for different jurisdictions and periods. The data model covers the specific peculiarities of historical financial and economic data and is flexible enough to reach out for data of different types (quantitative as well as qualitative) from different historical sources, hence, achieving extensibility. Furthermore, we outline a relational implementation of this approach based on historical German firm and stock market data from 1920 to 1932.",2022,,10.1145/3531533
1652,"Fiesler, Casey and Dye, Michaelanne and Feuston, Jessica L. and Hiruncharoenvate, Chaya and Hutto, C.J. and Morrison, Shannon and Khanipour Roshan, Parisa and Pavalanathan, Umashanthi and Bruckman, Amy S. and De Choudhury, Munmun and Gilbert, Eric",What (or Who) Is Public? Privacy Settings and Social Media Content Sharing,"mixed methods, machine learning, content analysis, facebook, social media, research methods, privacy, dataset, prediction, mechanical turk","When social networking sites give users granular control over their privacy settings, the result is that some content across the site is public and some is not. How might this content--or characteristics of users who post publicly versus to a limited audience--be different? If these differences exist, research studies of public content could potentially be introducing systematic bias. Via Mechanical Turk, we asked 1,815 Facebook users to share recent posts. Using qualitative coding and quantitative measures, we characterize and categorize the nature of the content. Using machine learning techniques, we analyze patterns of choices for privacy settings. Contrary to expectations, we find that content type is not a significant predictor of privacy setting; however, some demographics such as gender and age are predictive. Additionally, with consent of participants, we provide a dataset of nearly 9,000 public and non-public Facebook posts.",2017,,10.1145/2998181.2998223
1653,"Sadoghi, Mohammad and Jacobsen, Hans-Arno",Analysis and Optimization for Boolean Expression Indexing,"publish/subscribe, data structure, complex event processing, Boolean expressions","BE-Tree is a novel dynamic data structure designed to efficiently index Boolean expressions over a high-dimensional discrete space. BE Tree-copes with both high-dimensionality and expressiveness of Boolean expressions by introducing an effective two-phase space-cutting technique that specifically utilizes the discrete and finite domain properties of the space. Furthermore, BE-Tree employs self-adjustment policies to dynamically adapt the tree as the workload changes. Moreover, in BE-Tree, we develop two novel cache-conscious predicate evaluation techniques, namely, lazy and bitmap evaluations, that also exploit the underlying discrete and finite space to substantially reduce BE-Tree's matching time by up to 75%BE-Tree is a general index structure for matching Boolean expression which has a wide range of applications including (complex) event processing, publish/subscribe matching, emerging applications in cospaces, profile matching for targeted web advertising, and approximate string matching. Finally, the superiority of BE-Tree is proven through a comprehensive evaluation with state-of-the-art index structures designed for matching Boolean expressions.",2013,,10.1145/2487259.2487260
1654,"Mahyar, Narges and Nguyen, Diana V. and Chan, Maggie and Zheng, Jiayi and Dow, Steven P.",The Civic Data Deluge: Understanding the Challenges of Analyzing Large-Scale Community Input,"public inpu, community engagement, qualitative dataanalysis, digital civics","Advancements in digital civics have enabled leaders to engage and gather input from a broader spectrum of the public. However, less is known about the analysis process around community input and the challenges faced by civic leaders as engagement practices scale up. To understand these challenges, we conducted 21 interviews with leaders on civic-oriented projects. We found that at a small-scale, civic leaders manage to facilitate sensemaking through collaborative or individual approaches. However, as civic leaders scale engagement practices to account for more diverse perspectives, making sense of the large quantity of qualitative data becomes a challenge. Civic leaders could benefit from training in qualitative data analysis and simple, scalable collaborative analysis tools that would help the community form a shared understanding. Drawing from these insights, we discuss opportunities for designing tools that could improve civic leaders' ability to utilize and reflect public input in decisions.",2019,,10.1145/3322276.3322354
1655,,Data Deduplication,,"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.",2019,,
1656,"Jin, Hong and Miao, Yunting and Jung, Jae-Rim and Li, Dongjin",Construction of Information Search Behavior Based on Data Mining,"Data mining, Mobile social network, Model construction, Information search behavior","With the increasing maturity of Web 2.0–related technologies and the expansion of applications, a large number of social network services have emerged at home and abroad. These network platforms have greatly enriched the lives of netizens and become an important platform for studying user information behavior. At the same time, the development of technologies such as global positioning system technology, search engine, and data mining has made users’ data in the mobile social network platform receive extensive attention. This paper constructs a model of information search behavior based on data mining technology in mobile socialized networks and tests it with empirical methods. The results show that the usefulness of information plays a mediating role in the path of information usability impact on information search behavior. Product interaction and human-computer interaction can significantly affect the information search behavior. Trust plays a mediating role in the interaction of virtual community interaction (product interaction, interpersonal interaction, and human-computer interaction) on information search behavior. Using data mining technology to mine user needs, mining relevant data in search and mining information utilization, can improve user information search efficiency and efficiency. What’s more, it can provide basis and support for users and website decision-making.",2022,,10.1007/s00779-019-01239-8
1657,"Karafili, Erisa and Lupu, Emil C.",Enabling Data Sharing in Contextual Environments: Policy Representation and Analysis,"cloud, abductive reasoning, data access, argumentation reasoning, policy language, usage control, data sharing","Internet of Things environments enable us to capture more and more data about the physical environment we live in and about ourselves. The data enable us to optimise resources, personalise services and offer unprecedented insights into our lives. However, to achieve these insights data need to be shared (and sometimes sold) between organisations imposing rights and obligations upon the sharing parties and in accordance with multiple layers of sometimes conflicting legislation at international, national and organisational levels. In this work, we show how such rules can be captured in a formal representation called ""Data Sharing Agreements"". We introduce the use of abductive reasoning and argumentation based techniques to work with context dependent rules, detect inconsistencies between them, and resolve the inconsistencies by assigning priorities to the rules. We show how through the use of argumentation based techniques use-cases taken from real life application are handled flexibly addressing trade-offs between confidentiality, privacy, availability and safety.",2017,,10.1145/3078861.3078876
1658,"Wang, Xianzhi and Sheng, Quan Z. and Fang, Xiu Susie and Li, Xue and Xu, Xiaofei and Yao, Lina",Approximate Truth Discovery via Problem Scale Reduction,"truth discovery, consistency assurance, problem scale reduction, recursive method","Many real-world applications rely on multiple data sources to provide information on their interested items. Due to the noises and uncertainty in data, given a specific item, the information from different sources may conflict. To make reliable decisions based on these data, it is important to identify the trustworthy information by resolving these conflicts, i.e., the truth discovery problem. Current solutions to this problem detect the veracity of each value jointly with the reliability of each source for each data item. In this way, the efficiency of truth discovery is strictly confined by the problem scale, which in turn limits truth discovery algorithms from being applicable on a large scale. To address this issue, we propose an approximate truth discovery approach, which divides sources and values into groups according to a user-specified approximation criterion. The groups are then used for efficient inter-value influence computation to improve the accuracy. Our approach is applicable to most existing truth discovery algorithms. Experiments on real-world datasets show that our approach improves the efficiency compared to existing algorithms while achieving similar or even better accuracy. The scalability is further demonstrated by experiments on large synthetic datasets.",2015,,10.1145/2806416.2806444
1659,"Zoumpatianos, Konstantinos and Palpanas, Themis and Mylopoulos, John and Mat\'{e}, Alejandro and Trujillo, Juan",Monitoring and Diagnosing Indicators for Business Analytics,,"Modeling the strategic objectives has been shown to be useful both for understanding a business as well as planning and guiding the overall activities within an enterprise. Business strategy is modeled according to human expertise, setting up the goals as well as the indicators that monitor activities and goals. However, usually indicators provide high-level aggregated views of data, making it difficult to pinpoint problems within specific sub-areas until they have a significant impact into the aggregated value. By the time these problems become evident, they have already hindered the performance of the organization. However, performing a detailed analysis manually can be a daunting task, due to the size of the data space. In order to solve this problem, we propose a user-driven method to analyze the data related to each business indicator by means of data mining. We illustrate our approach with a real world example based on the Europe 2020 framework. Our approach allows us not only to identify latent problems, but also to highlight deviations from anticipated trends that may represent opportunities and exceptional situations, thereby enabling an organization to take advantage of them.",2013,,
1660,"Vazquez Brust, Antonio and Olego, Tom\'{a}s and Rosati, Germ\'{a}n and Lang, Carolina and Bozzoli, Guillermo and Weinberg, Diego and Chuit, Roberto and Minnoni, Martin and Sarraute, Carlos",Detecting Areas of Potential High Prevalence of Chagas in Argentina,"migrations, neglected tropical diseases, epidemics, health vulnerability, Chagas disease, call detail records, social network analysis","A map of potential prevalence of Chagas disease (ChD) with high spatial disaggregation is presented. It aims to detect areas outside the Gran Chaco ecoregion (hyperendemic for the ChD), characterized by high affinity with ChD and high health vulnerability.To quantify potential prevalence, we developed several indicators: an Affinity Index which quantifies the degree of linkage between endemic areas of ChD and the rest of the country. We also studied favorable habitability conditions for Triatoma infestans, looking for areas where the predominant materials of floors, roofs and internal ceilings favor the presence of the disease vector.We studied determinants of a more general nature that can be encompassed under the concept of Health Vulnerability Index. These determinants are associated with access to health providers and the socio-economic level of different segments of the population.Finally we constructed a Chagas Potential Prevalence Index (ChPPI) which combines the affinity index, the health vulnerability index, and the population density. We show and discuss the maps obtained. These maps are intended to assist public health specialists, decision makers of public health policies and public officials in the development of cost-effective strategies to improve access to diagnosis and treatment of ChD.",2019,,10.1145/3308560.3316485
1661,"Barlaug, Nils and Gulla, Jon Atle",Neural Networks for Entity Matching: A Survey,"record linkage, entity resolution, entity matching, Deep learning, data matching","Entity matching is the problem of identifying which records refer to the same real-world entity. It has been actively researched for decades, and a variety of different approaches have been developed. Even today, it remains a challenging problem, and there is still generous room for improvement. In recent years, we have seen new methods based upon deep learning techniques for natural language processing emerge. In this survey, we present how neural networks have been used for entity matching. Specifically, we identify which steps of the entity matching process existing work have targeted using neural networks, and provide an overview of the different techniques used at each step. We also discuss contributions from deep learning in entity matching compared to traditional methods, and propose a taxonomy of deep neural networks for entity matching.",2021,,10.1145/3442200
1662,"Scheuerman, Morgan Klaus and Hanna, Alex and Denton, Emily",Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development,"datasets, work practice, machine learning, values in design, computer vision","Data is a crucial component of machine learning. The field is reliant on data to train, validate, and test models. With increased technical capabilities, machine learning research has boomed in both academic and industry settings, and one major focus has been on computer vision. Computer vision is a popular domain of machine learning increasingly pertinent to real-world applications, from facial recognition in policing to object detection for autonomous vehicles. Given computer vision's propensity to shape machine learning research and impact human life, we seek to understand disciplinary practices around dataset documentation - how data is collected, curated, annotated, and packaged into datasets for computer vision researchers and practitioners to use for model tuning and development. Specifically, we examine what dataset documentation communicates about the underlying values of vision data and the larger practices and goals of computer vision as a field. To conduct this study, we collected a corpus of about 500 computer vision datasets, from which we sampled 114 dataset publications across different vision tasks. Through both a structured and thematic content analysis, we document a number of values around accepted data practices, what makes desirable data, and the treatment of humans in the dataset construction process. We discuss how computer vision datasets authors value efficiency at the expense of care; universality at the expense of contextuality; impartiality at the expense of positionality; and model work at the expense of data work. Many of the silenced values we identify sit in opposition with social computing practices. We conclude with suggestions on how to better incorporate silenced values into the dataset creation and curation process.",2021,,10.1145/3476058
1663,"Dong, Roy and Ratliff, Lillian J. and C\'{a}rdenas, Alvaro A. and Ohlsson, Henrik and Sastry, S. Shankar",Quantifying the Utility--Privacy Tradeoff in the Internet of Things,"Privacy, smart grid","The Internet of Things (IoT) promises many advantages in the control and monitoring of physical systems from both efficacy and efficiency perspectives. However, in the wrong hands, the data might pose a privacy threat. In this article, we consider the tradeoff between the operational value of data collected in the IoT and the privacy of consumers. We present a general framework for quantifying this tradeoff in the IoT, and focus on a smart grid application for a proof of concept. In particular, we analyze the tradeoff between smart grid operations and how often data are collected by considering a realistic direct-load control example using thermostatically controlled loads, and we give simulation results to show how its performance degrades as the sampling frequency decreases. Additionally, we introduce a new privacy metric, which we call inferential privacy. This privacy metric assumes a strong adversary model and provides an upper bound on the adversary’s ability to infer a private parameter, independent of the algorithm he uses. Combining these two results allows us to directly consider the tradeoff between better operational performance and consumer privacy.",2018,,10.1145/3185511
1664,"Salminen, Joni and Jung, Soon-Gyo and Jansen, Bernard",Developing Persona Analytics Towards Persona Science,,"Much of the reported work on personas suffers from the lack of empirical evidence. To address this issue, we introduce Persona Analytics (PA), a system that tracks how users interact with data-driven personas. PA captures users’ mouse and gaze behavior to measure users’ interaction with algorithmically generated personas and use of system features for an interactive persona system. Measuring these activities grants an understanding of the behaviors of a persona user, required for quantitative measurement of persona use to obtain scientifically valid evidence. Conducting a study with 144 participants, we demonstrate how PA can be deployed for remote user studies during exceptional times when physical user studies are difficult, if not impossible.",2022,,10.1145/3490099.3511144
1665,"Pellungrini, Roberto and Pappalardo, Luca and Pratesi, Francesca and Monreale, Anna",A Data Mining Approach to Assess Privacy Risk in Human Mobility Data,"data mining, privacy, Human mobility","Human mobility data are an important proxy to understand human mobility dynamics, develop analytical services, and design mathematical models for simulation and what-if analysis. Unfortunately mobility data are very sensitive since they may enable the re-identification of individuals in a database. Existing frameworks for privacy risk assessment provide data providers with tools to control and mitigate privacy risks, but they suffer two main shortcomings: (i) they have a high computational complexity; (ii) the privacy risk must be recomputed every time new data records become available and for every selection of individuals, geographic areas, or time windows. In this article, we propose a fast and flexible approach to estimate privacy risk in human mobility data. The idea is to train classifiers to capture the relation between individual mobility patterns and the level of privacy risk of individuals. We show the effectiveness of our approach by an extensive experiment on real-world GPS data in two urban areas and investigate the relations between human mobility patterns and the privacy risk of individuals.",2017,,10.1145/3106774
1666,"Lin, Qingwei and Hsieh, Ken and Dang, Yingnong and Zhang, Hongyu and Sui, Kaixin and Xu, Yong and Lou, Jian-Guang and Li, Chenggang and Wu, Youjiang and Yao, Randolph and Chintalapati, Murali and Zhang, Dongmei",Predicting Node Failure in Cloud Service Systems,"maintenance, node failure, service availability, Failure prediction, cloud service systems","In recent years, many traditional software systems have migrated to cloud computing platforms and are provided as online services. The service quality matters because system failures could seriously affect business and user experience. A cloud service system typically contains a large number of computing nodes. In reality, nodes may fail and affect service availability. In this paper, we propose a failure prediction technique, which can predict the failure-proneness of a node in a cloud service system based on historical data, before node failure actually happens. The ability to predict faulty nodes enables the allocation and migration of virtual machines to the healthy nodes, therefore improving service availability. Predicting node failure in cloud service systems is challenging, because a node failure could be caused by a variety of reasons and reflected by many temporal and spatial signals. Furthermore, the failure data is highly imbalanced. To tackle these challenges, we propose MING, a novel technique that combines: 1) a LSTM model to incorporate the temporal data, 2) a Random Forest model to incorporate spatial data; 3) a ranking model that embeds the intermediate results of the two models as feature inputs and ranks the nodes by their failure-proneness, 4) a cost-sensitive function to identify the optimal threshold for selecting the faulty nodes. We evaluate our approach using real-world data collected from a cloud service system. The results confirm the effectiveness of the proposed approach. We have also successfully applied the proposed approach in real industrial practice.",2018,,10.1145/3236024.3236060
1667,"Ruhrl\""{a}nder, Rui Paulo and Boissier, Martin and Uflacker, Matthias",Improving Box Office Result Predictions for Movies Using Consumer-Centric Models,"motion picture industry, user ratings, box office predictions, logistic regression, recommender systems, gradient-boosted trees","Recent progress in machine learning and related fields like recommender systems open up new possibilities for data-driven approaches. One example is the prediction of a movie's box office revenue, which is highly relevant for optimizing production and marketing. We use individual recommendations and user-based forecast models in a system that forecasts revenue and additionally provides actionable insights for industry professionals. In contrast to most existing models that completely neglect user preferences, our approach allows us to model the most important source for movie success: moviegoer taste and behavior. We divide the problem into three distinct stages: (i) we use matrix factorization recommenders to model each user's taste, (ii) we then predict the individual consumption behavior, and (iii) eventually aggregate users to predict the box office result. We compare our approach to the current industry standard and show that the inclusion of user rating data reduces the error by a factor of 2x and outperforms recently published research.",2018,,10.1145/3219819.3219840
1668,"K\""{o}hler, Henning and Leck, Uwe and Link, Sebastian and Zhou, Xiaofang",Possible and Certain Keys for SQL,"Extremal combinatorics, SQL, Key, Axiomatization, Discovery, Index, Armstrong database, Implication problem, Null marker, Data profiling","Driven by the dominance of the relational model and the requirements of modern applications, we revisit the fundamental notion of a key in relational databases with NULL. In SQL, primary key columns are NOT NULL, and UNIQUE constraints guarantee uniqueness only for tuples without NULL. We investigate the notions of possible and certain keys, which are keys that hold in some or all possible worlds that originate from an SQL table, respectively. Possible keys coincide with UNIQUE, thus providing a semantics for their syntactic definition in the SQL standard. Certain keys extend primary keys to include NULL columns and can uniquely identify entities whenever feasible, while primary keys may not. In addition to basic characterization, axiomatization, discovery, and extremal combinatorics problems, we investigate the existence and construction of Armstrong tables, and describe an indexing scheme for enforcing certain keys. Our experiments show that certain keys with NULLs occur in real-world data, and related computational problems can be solved efficiently. Certain keys are therefore semantically well founded and able to meet Codd's entity integrity rule while handling high volumes of incomplete data from different formats.",2016,,10.1007/s00778-016-0430-9
1669,"Holubova, Irena and Contos, Pavel and Svoboda, Martin",Multi-Model Data Modeling and Representation: State of the Art and Research Challenges,"Inter-model relationships, Multi-model data, Conceptual modeling, Logical models, Category theory"," Following the current trend, most of the well-known database systems, being relational, NoSQL, or NewSQL, denote themselves as multi-model. This industry-driven approach, however, lacks plenty of important features of the traditional DBMSs. The primary problem is a design of an optimal multi-model schema and its sufficiently general and efficient representation. In this paper, we provide an overview and discussion of the promising approaches that could potentially be capable of solving these issues, along with a summary of the remaining open problems.",2021,,10.1145/3472163.3472267
1670,"Yang, Dejun and Xue, Guoliang and Fang, Xi and Tang, Jian",Incentive Mechanisms for Crowdsensing: Crowdsourcing with Smartphones,"incentive mechanism, Stackelberg game, crowdsensing, crowdsourcing","Smartphones are programmable and equipped with a set of cheap but powerful embedded sensors, such as accelerometer, digital compass, gyroscope, GPS, microphone, and camera. These sensors can collectively monitor a diverse range of human activities and the surrounding environment. Crowdsensing is a new paradigm which takes advantage of the pervasive smartphones to sense, collect, and analyze data beyond the scale of what was previously possible. With the crowdsensing system, a crowdsourcer can recruit smartphone users to provide sensing service. Existing crowdsensing applications and systems lack good incentive mechanisms that can attract more user participation. To address this issue, we design incentive mechanisms for crowdsensing. We consider two system models: the crowdsourcer-centric model where the crowdsourcer provides a reward shared by participating users, and the user-centric model where users have more control over the payment they will receive. For the crowdsourcer-centric model, we design an incentive mechanism using a Stackelberg game, where the crowdsourcer is the leader while the users are the followers. We show how to compute the unique Stackelberg Equilibrium, at which the utility of the crowdsourcer is maximized, and none of the users can improve its utility by unilaterally deviating from its current strategy. For the user-centric model, we design an auction-based incentive mechanism, which is computationally efficient, individually rational, profitable, and truthful. Through extensive simulations, we evaluate the performance and validate the theoretical properties of our incentive mechanisms.",2016,,10.1109/TNET.2015.2421897
1671,"Saleheen, Nazir and Ali, Amin Ahsan and Hossain, Syed Monowar and Sarker, Hillol and Chatterjee, Soujanya and Marlin, Benjamin and Ertin, Emre and al'Absi, Mustafa and Kumar, Santosh",PuffMarker: A Multi-Sensor Approach for Pinpointing the Timing of First Lapse in Smoking Cessation,"smoking cessation, wearable sensors, mobile health (mHealth), smoking detection, smartwatch","Recent researches have demonstrated the feasibility of detecting smoking from wearable sensors, but their performance on real-life smoking lapse detection is unknown. In this paper, we propose a new model and evaluate its performance on 61 newly abstinent smokers for detecting a first lapse. We use two wearable sensors --- breathing pattern from respiration and arm movements from 6-axis inertial sensors worn on wrists. In 10-fold cross-validation on 40 hours of training data from 6 daily smokers, our model achieves a recall rate of 96.9%, for a false positive rate of 1.1%. When our model is applied to 3 days of post-quit data from 32 lapsers, it correctly pinpoints the timing of first lapse in 28 participants. Only 2 false episodes are detected on 20 abstinent days of these participants. When tested on 84 abstinent days from 28 abstainers, the false episode per day is limited to 1/6.",2015,,10.1145/2750858.2806897
1672,"Zhou, Yujia and Dou, Zhicheng and Wei, Bingzheng and Xie, Ruobing and Wen, Ji-Rong",Group Based Personalized Search by Integrating Search Behaviour and Friend Network,,"The key to personalized search is to build the user profile based on historical behaviour. To deal with the users who lack historical data, group based personalized models were proposed to incorporate the profiles of similar users when re-ranking the results. However, similar users are mostly found based on simple lexical or topical similarity in search behaviours. In this paper, we propose a neural network enhanced method to highlight similar users in semantic space. Furthermore, we argue that the behaviour-based similar users are still insufficient to understand a new query when user's historical activities are limited. To tackle this issue, we introduce the friend network into personalized search to determine the closeness between users in another way. Since the friendship is often formed based on similar background or interest, there are plenty of personalized signals hidden in the friend network naturally. Specifically, we propose a friend network enhanced personalized search model, which groups the user into multiple friend circles based on search behaviours and friend relations respectively. These two types of friend circles are complementary to construct a more comprehensive group profile for refining the personalization. Experimental results show the significant improvement of our model over existing personalized search models.",2021,,
1673,"Yang, Tong and Gong, Junzhi and Zhang, Haowei and Zou, Lei and Shi, Lei and Li, Xiaoming",HeavyGuardian: Separate and Guard Hot Items in Data Streams,"data sturcture, data stream processing, probabilistic and approximate data","Data stream processing is a fundamental issue in many fields, such as data mining, databases, network traffic measurement. There are five typical tasks in data stream processing: frequency estimation, heavy hitter detection, heavy change detection, frequency distribution estimation, and entropy estimation. Different algorithms are proposed for different tasks, but they seldom achieve high accuracy and high speed at the same time. To address this issue, we propose a novel data structure named HeavyGuardian. The key idea is to intelligently separate and guard the information of hot items while approximately record the frequencies of cold items. We deploy HeavyGuardian on the above five typical tasks. Extensive experimental results show that HeavyGuardian achieves both much higher accuracy and higher speed than the state-of-the-art solutions for each of the five typical tasks. The source codes of HeavyGuardian and other related algorithms are available at GitHub.",2018,,10.1145/3219819.3219978
1674,"Gupchup, Jayant and Hosseinkashi, Yasaman and Dmitriev, Pavel and Schneider, Daniel and Cutler, Ross and Jefremov, Andrei and Ellis, Martin",Trustworthy Experimentation Under Telemetry Loss,"client experimentation, data loss, ab testing, telemetry loss, experimentation trustworthiness, online controlled experiments","Failure to accurately measure the outcomes of an experiment can lead to bias and incorrect conclusions. Online controlled experiments (aka AB tests) are increasingly being used to make decisions to improve websites as well as mobile and desktop applications. We argue that loss of telemetry data (during upload or post-processing) can skew the results of experiments, leading to loss of statistical power and inaccurate or erroneous conclusions. By systematically investigating the causes of telemetry loss, we argue that it is not practical to entirely eliminate it. Consequently, experimentation systems need to be robust to its effects. Furthermore, we note that it is nontrivial to measure the absolute level of telemetry loss in an experimentation system. In this paper, we take a top-down approach towards solving this problem. We motivate the impact of loss qualitatively using experiments in real applications deployed at scale, and formalize the problem by presenting a theoretical breakdown of the bias introduced by loss. Based on this foundation, we present a general framework for quantitatively evaluating the impact of telemetry loss, and present two solutions to measure the absolute levels of loss. This framework is used by well-known applications at Microsoft, with millions of users and billions of sessions. These general principles can be adopted by any application to improve the overall trustworthiness of experimentation and data-driven decision making.",2018,,10.1145/3269206.3271747
1675,"Zhang, Yuhao and Chen, Yifan and Cheung, Shing-Chi and Xiong, Yingfei and Zhang, Lu",An Empirical Study on TensorFlow Program Bugs,"Empirical Study, Deep Learning, TensorFlow Program Bug","Deep learning applications become increasingly popular in important domains such as self-driving systems and facial identity systems. Defective deep learning applications may lead to catastrophic consequences. Although recent research efforts were made on testing and debugging deep learning applications, the characteristics of deep learning defects have never been studied. To fill this gap, we studied deep learning applications built on top of TensorFlow and collected program bugs related to TensorFlow from StackOverflow QA pages and Github projects. We extracted information from QA pages, commit messages, pull request messages, and issue discussions to examine the root causes and symptoms of these bugs. We also studied the strategies deployed by TensorFlow users for bug detection and localization. These findings help researchers and TensorFlow users to gain a better understanding of coding defects in TensorFlow programs and point out a new direction for future research.",2018,,10.1145/3213846.3213866
1676,"Dong, Xin Luna and Saha, Barna and Srivastava, Divesh",Less is More: Selecting Sources Wisely for Integration,,"We are often thrilled by the abundance of information surrounding us and wish to integrate data from as many sources as possible. However, understanding, analyzing, and using these data are often hard. Too much data can introduce a huge integration cost, such as expenses for purchasing data and resources for integration and cleaning. Furthermore, including low-quality data can even deteriorate the quality of integration results instead of bringing the desired quality gain. Thus, ""the more the better"" does not always hold for data integration and often ""less is more"".In this paper, we study how to select a subset of sources before integration such that we can balance the quality of integrated data and integration cost. Inspired by the Marginalism principle in economic theory, we wish to integrate a new source only if its marginal gain, often a function of improved integration quality, is higher than the marginal cost, associated with data-purchase expense and integration resources. As a first step towards this goal, we focus on data fusion tasks, where the goal is to resolve conflicts from different sources. We propose a randomized solution for selecting sources for fusion and show empirically its effectiveness and scalability on both real-world data and synthetic data.",2012,,10.14778/2535568.2448938
1677,"Zhou, Yaqin and Siow, Jing Kai and Wang, Chenyu and Liu, Shangqing and Liu, Yang",SPI: Automated Identification of Security Patches via Commits,"deep learning, Machine learning, software security","Security patches in open source software, providing security fixes to identified vulnerabilities, are crucial in protecting against cyber attacks. Security advisories and announcements are often publicly released to inform the users about potential security vulnerability. Despite the National Vulnerability Database (NVD) publishes identified vulnerabilities, a vast majority of vulnerabilities and their corresponding security patches remain beyond public exposure, e.g., in the open source libraries that are heavily relied on by developers. As many of these patches exist in open sourced projects, the problem of curating and gathering security patches can be difficult due to their hidden nature. An extensive and complete security patches dataset could help end-users such as security companies, e.g., building a security knowledge base, or researcher, e.g., aiding in vulnerability research.To efficiently curate security patches including undisclosed patches at large scale and low cost, we propose a deep neural-network-based approach built upon commits of open source repositories. First, we design and build security patch datasets that include 38,291 security-related commits and 1,045 Common Vulnerabilities and Exposures (CVE) patches from four large-scale C programming language libraries. We manually verify each commit, among the 38,291 security-related commits, to determine if they are security related.We devise and implement a deep learning-based security patch identification system that consists of two composite neural networks: one commit-message neural network that utilizes pretrained word representations learned from our commits dataset and one code-revision neural network that takes code before revision and after revision and learns the distinction on the statement level. Our system leverages the power of the two networks for Security Patch Identification. Evaluation results show that our system significantly outperforms SVM and K-fold stacking algorithms. The result on the combined dataset achieves as high as 87.93% F1-score and precision of 86.24%.We deployed our pipeline and learned model in an industrial production environment to evaluate the generalization ability of our approach. The industrial dataset consists of 298,917 commits from 410 new libraries that range from a wide functionalities. Our experiment results and observation on the industrial dataset proved that our approach can identify security patches effectively among open sourced projects.",2021,,10.1145/3468854
1678,"Li, Chao and Xue, Yushu and Wang, Jing and Zhang, Weigong and Li, Tao",Edge-Oriented Computing Paradigms: A Survey on Architecture Design and System Management,"resource management, fog computing, ubiquitous data processing, Distributed cloud, edge computing, architecture design","While cloud computing has brought paradigm shifts to computing services, researchers and developers have also found some problems inherent to its nature such as bandwidth bottleneck, communication overhead, and location blindness. The concept of fog/edge computing is therefore coined to extend the services from the core in cloud data centers to the edge of the network. In recent years, many systems are proposed to better serve ubiquitous smart devices closer to the user. This article provides a complete and up-to-date review of edge-oriented computing systems by encapsulating relevant proposals on their architecture features, management approaches, and design objectives.",2018,,10.1145/3154815
1679,"Chatzigiannakis, Ioannis and Tselios, Christos",Internet of Everything,,,2021,,
1680,"Yan, Yan and Meyles, Stephen and Haghighi, Aria and Suciu, Dan",Entity Matching in the Wild: A Consistent and Versatile Framework to Unify Data in Industrial Applications,"cluster id assignment, conflict resolution in clustering, multi-level entity matching","Entity matching -- the task of clustering duplicated database records to underlying entities -- has become an increasingly critical component in modern data integration management. Amperity provides a platform for businesses to manage customer data that utilizes a machine-learning approach to entity matching, resolving billions of customer records on a daily basis. We face several challenges in deploying entity matching to industrial applications at scale, and they are less prominent in the literature. These challenges include: (1) Providing not just a single entity clustering, but supporting clusterings at multiple confidence levels to enable downstream applications with varying precision/recall trade-off needs. (2) Many customer record attributes may be systematically missing from different sources of data, creating many pairs of records in a cluster that appear to not match due to incomplete, rather than conflicting information. Allowing these records to connect transitively without introducing conflicts is invaluable to businesses because they can acquire a more comprehensive profile of their customers without incorrect entity merges. (3) How to cluster records over time and assign persistent cluster IDs that can be used for downstream use cases such as A/B tests or predictive model training; this is made more challenging by the fact that we receive new customer data every day and clusters naturally evolving over time still require persistent IDs that refer to the same entity. In this work, we describe Amperity's entity matching framework, Fusion, and how its design provides solutions to these challenges. In particular, we describe our pairwise matching model based on ordinal regression that permits a well-defined way to produce entity clusterings at different confidence levels, a novel clustering algorithm that separates conflicting record pairs in clusters while allowing for pairs that may appear dissimilar due to missing data, and a persistent ID generation algorithm which balances stability of the identifier with ever-evolving entities.",2020,,10.1145/3318464.3386143
1681,"Horv\'{a}th, G\'{a}bor and Kov\'{a}cs, Edith and Molontay, Roland and Nov\'{a}czki, Szabolcs","Copula-Based Anomaly Scoring and Localization for Large-Scale, High-Dimensional Continuous Data","Anomaly scoring, unsupervised learning, copula fitting","The anomaly detection method presented by this article has a special feature: it not only indicates whether or not an observation is anomalous but also tells what exactly makes an anomalous observation unusual. Hence, it provides support to localize the reason of the anomaly.The proposed approach is model based; it relies on the multivariate probability distribution associated with the observations. Since the rare events are present in the tails of the probability distributions, we use copula functions, which are able to model the fat-tailed distributions well. The presented procedure scales well; it can cope with a large number of high-dimensional samples. Furthermore, our procedure can cope with missing values as well, which occur frequently in high-dimensional datasets.In the second part of the article, we demonstrate the usability of the method through a case study, where we analyze a large dataset consisting of the performance counters of a real mobile telecommunication network. Since such networks are complex systems, the signs of sub-optimal operation can remain hidden for a potentially long time. With the proposed procedure, many such hidden issues can be isolated and indicated to the network operator.",2020,,10.1145/3372274
1682,"Pereira, Gabriela Viale and Testa, Maur\'{\i}cio Gregianin and Macadar, Marie Anne and Parycek, Peter and de Azambuja, Luiza Schuch",Building Understanding of Municipal Operations Centers as Smart City' Initiatives: Insights from a Cross-Case Analysis,"smart cities governance, smart cities, municipal operations center, multiple cases study","Cities around the world have been facing complex challenges from the growing urbanization. The increase of urban problems is a consequence of this phenomenon, added to the lack of policies focusing in citizens' well-being and safety. Municipal operations centers have played an important role in response of social events and natural disasters as a way to address the urgency and dynamism of urban problems. This research aims at analyzing the main dimensions and factors for implementing municipal operations centers as smart city initiatives. In order to explore this phenomenon it was conducted an exploratory study, based on multiple case studies. The empirical setting of this research is determined by municipal operations centers in Rio de Janeiro, Porto Alegre and Belo Horizonte. The research findings evidenced that the implementation of the centers comprises technological, organizational and managerial factors, in addition to political and institutional factors. Increasing smart cities governance is the main result from the initiatives.",2016,,10.1145/3014087.3014110
1683,"Liu, Can and Lindqvist, Janne",Secure Gestures—Case Study 4,,,2021,,
1684,"Traub, Jonas and Bre\ss{}, Sebastian and Rabl, Tilmann and Katsifodimos, Asterios and Markl, Volker",Optimized On-Demand Data Streaming from Sensor Nodes,"sensor data, real-time analysis, oversampling, on-demand streaming, user-defined sampling, adaptive sampling, sensor sharing","Real-time sensor data enables diverse applications such as smart metering, traffic monitoring, and sport analysis. In the Internet of Things, billions of sensor nodes form a sensor cloud and offer data streams to analysis systems. However, it is impossible to transfer all available data with maximal frequencies to all applications. Therefore, we need to tailor data streams to the demand of applications.We contribute a technique that optimizes communication costs while maintaining the desired accuracy. Our technique schedules reads across huge amounts of sensors based on the data-demands of a huge amount of concurrent queries. We introduce user-defined sampling functions that define the data-demand of queries and facilitate various adaptive sampling techniques, which decrease the amount of transferred data. Moreover, we share sensor reads and data transfers among queries. Our experiments with real-world data show that our approach saves up to 87% in data transmissions.",2017,,10.1145/3127479.3131621
1685,"Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Pu, Yanjun and Liu, Xudong",Learning to Handle Exceptions,"code generation, neural network, exception handling, deep learning","Exception handling is an important built-in feature of many modern programming languages such as Java. It allows developers to deal with abnormal or unexpected conditions that may occur at runtime in advance by using try-catch blocks. Missing or improper implementation of exception handling can cause catastrophic consequences such as system crash. However, previous studies reveal that developers are unwilling or feel it hard to adopt exception handling mechanism, and tend to ignore it until a system failure forces them to do so. To help developers with exception handling, existing work produces recommendations such as code examples and exception types, which still requires developers to localize the try blocks and modify the catch block code to fit the context. In this paper, we propose a novel neural approach to automated exception handling, which can predict locations of try blocks and automatically generate the complete catch blocks. We collect a large number of Java methods from GitHub and conduct experiments to evaluate our approach. The evaluation results, including quantitative measurement and human evaluation, show that our approach is highly effective and outperforms all baselines. Our work makes one step further towards automated exception handling.",2020,,10.1145/3324884.3416568
1686,"TeBlunthuis, Nathan and Bayer, Tilman and Vasileva, Olga",Dwelling on Wikipedia: Investigating Time Spent by Global Encyclopedia Readers,"quantitative methods, peer production, readership, Wikipedia, web analytics, dwell time, digital divides","Much existing knowledge about global consumption of peer-produced information goods is supported by data on Wikipedia page view counts and surveys. In 2017, the Wikimedia Foundation began measuring the time readers spend on a given page view (dwell time), enabling a more detailed understanding of such reading patterns. In this paper, we validate and model this new data source and, building on existing findings, use regression analysis to test hypotheses about how patterns in reading time vary between global contexts. Consistent with prior findings from self-report data, our complementary analysis of behavioral data provides evidence that Global South readers are more likely to use Wikipedia to gain in-depth understanding of a topic. We find that Global South readers spend more time per page view and that this difference is amplified on desktop devices, which are thought to be better suited for in-depth information seeking tasks.",2019,,10.1145/3306446.3340829
1687,"Vaughan, Jennifer Wortman",Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research,"incentives, behavioral experiments, mechanical turk, data generation, crowdsourcing, model evaluation, hybrid intelligence","This survey provides a comprehensive overview of the landscape of crowdsourcing research, targeted at the machine learning community. We begin with an overview of the ways in which crowdsourcing can be used to advance machine learning research, focusing on four application areas: 1) data generation, 2) evaluation and debugging of models, 3) hybrid intelligence systems that leverage the complementary strengths of humans and machines to expand the capabilities of AI, and 4) crowdsourced behavioral experiments that improve our understanding of how humans interact with machine learning systems and technology more broadly. We next review the extensive literature on the behavior of crowdworkers themselves. This research, which explores the prevalence of dishonesty among crowdworkers, how workers respond to both monetary incentives and intrinsic forms of motivation, and how crowdworkers interact with each other, has immediate implications that we distill into best practices that researchers should follow when using crowdsourcing in their own research. We conclude with a discussion of additional tips and best practices that are crucial to the success of any project that uses crowdsourcing, but rarely mentioned in the literature.",2017,,
1688,"Chung, Yeounoh and Mortensen, Michael Lind and Binnig, Carsten and Kraska, Tim",Estimating the Impact of Unknown Unknowns on Aggregate Query Results,"crowdsourcing, unknown unknowns, Aggregate query processing, species estimation","It is common practice for data scientists to acquire and integrate disparate data sources to achieve higher quality results. But even with a perfectly cleaned and merged data set, two fundamental questions remain: (1) Is the integrated data set complete? and (2) What is the impact of any unknown (i.e., unobserved) data on query results?In this work, we develop and analyze techniques to estimate the impact of the unknown data (a.k.a., unknown unknowns) on simple aggregate queries. The key idea is that the overlap between different data sources enables us to estimate the number and values of the missing data items. Our main techniques are parameter-free and do not assume prior knowledge about the distribution; we also propose a parametric model that can be used instead when the data sources are imbalanced. Through a series of experiments, we show that estimating the impact of unknown unknowns is invaluable to better assess the results of aggregate queries over integrated data sources.",2018,,10.1145/3167970
1689,"Fan, Wenfei and Lu, Ping",Dependencies for Graphs,"conditional functional dependencies, validation, implication, disjunction, axiom system, graph dependencies, keys, built-in predicates, tgds, egds, satisfiability","This paper proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs express graph functional dependencies with constant literals to catch inconsistencies, and keys carrying id literals to identify entities in a graph.We revise the chase for GEDs and prove its Church-Rosser property. We characterize GED satisfiability and implication, and establish the complexity of these problems and the validation problem for GEDs, in the presence and absence of constant literals and id literals. We also develop a sound and complete axiom system for finite implication of GEDs. In addition, we extend GEDs with built-in predicates or disjunctions, to strike a balance between the expressive power and complexity. We settle the complexity of the satisfiability, implication and validation problems for the extensions.",2017,,10.1145/3034786.3056114
1690,"McDaniel, Melinda and Storey, Veda C.","Evaluating Domain Ontologies: Clarification, Classification, and Challenges","domain ontology, ontology application, evaluation, ontology development, applied ontology, Ontology, task-ontology fit, assessment, metrics","The number of applications being developed that require access to knowledge about the real world has increased rapidly over the past two decades. Domain ontologies, which formalize the terms being used in a discipline, have become essential for research in areas such as Machine Learning, the Internet of Things, Robotics, and Natural Language Processing, because they enable separate systems to exchange information. The quality of these domain ontologies, however, must be ensured for meaningful communication. Assessing the quality of domain ontologies for their suitability to potential applications remains difficult, even though a variety of frameworks and metrics have been developed for doing so. This article reviews domain ontology assessment efforts to highlight the work that has been carried out and to clarify the important issues that remain. These assessment efforts are classified into five distinct evaluation approaches and the state of the art of each described. Challenges associated with domain ontology assessment are outlined and recommendations are made for future research and applications.",2019,,10.1145/3329124
1691,"Xie, Yiqun and Shekhar, Shashi and Li, Yan",Statistically-Robust Clustering Techniques for Mapping Spatial Hotspots: A Survey,"statistical rigor, Hotspot, scan statistics, clustering, mapping","Mapping of spatial hotspots, i.e., regions with significantly higher rates of generating cases of certain events (e.g., disease or crime cases), is an important task in diverse societal domains, including public health, public safety, transportation, agriculture, environmental science, and so on. Clustering techniques required by these domains differ from traditional clustering methods due to the high economic and social costs of spurious results (e.g., false alarms of crime clusters). As a result, statistical rigor is needed explicitly to control the rate of spurious detections. To address this challenge, techniques for statistically-robust clustering (e.g., scan statistics) have been extensively studied by the data mining and statistics communities. In this survey, we present an up-to-date and detailed review of the models and algorithms developed by this field. We first present a general taxonomy for statistically-robust clustering, covering key steps of data and statistical modeling, region enumeration and maximization, and significance testing. We further discuss different paradigms and methods within each of the key steps. Finally, we highlight research gaps and potential future directions, which may serve as a stepping stone in generating new ideas and thoughts in this growing field and beyond.",2022,,10.1145/3487893
1692,"Zafar, Farkhanda and Khattak, Hasan Ali and Aloqaily, Moayad and Hussain, Rasheed",Carpooling in Connected and Autonomous Vehicles: Current Solutions and Future Directions,"Intelligent Transportation Systems, Carpooling, Ride-sharing, Connected Autonomous Vehicles, Vehicular Networks","Owing to the advancements in communication and computation technologies, the dream of commercialized connected and autonomous cars is becoming a reality. However, among other challenges such as environmental pollution, cost, maintenance, security, and privacy, the ownership of vehicles (especially for Autonomous Vehicles (AV)) is the major obstacle in the realization of this technology at the commercial level. Furthermore, the business model of pay-as-you-go type services further attracts the consumer because there is no need for upfront investment. In this vein, the idea of car-sharing (aka carpooling) is getting ground due to, at least in part, its simplicity, cost-effectiveness, and affordable choice of transportation. Carpooling systems are still in their infancy and face challenges such as scheduling, matching passengers interests, business model, security, privacy, and communication. To date, a plethora of research work has already been done covering different aspects of carpooling services (ranging from applications to communication and technologies); however, there is still a lack of a holistic, comprehensive survey that can be a one-stop-shop for the researchers in this area to, i) find all the relevant information, and ii) identify the future research directions. To fill these research challenges, this paper provides a comprehensive survey on carpooling in autonomous and connected vehicles and covers architecture, components, and solutions, including scheduling, matching, mobility, pricing models of carpooling. We also discuss the current challenges in carpooling and identify future research directions. This survey is aimed to spur further discussion among the research community for the effective realization of carpooling.",2021,,10.1145/3501295
1693,"Brandt, Tobias and Grawunder, Marco",GeoStreams: A Survey,"data stream management systems, GeoStreams, data stream engines","Positional data from small and mobile Global Positioning Systems has become ubiquitous and allows for many new applications such as road traffic or vessel monitoring as well as location-based services. To make these applications possible, for which information on location is more important than ever, streaming spatial data needs to be managed, mined, and used intelligently. This article provides an overview of previous work in this evolving research field and discusses different applications as well as common problems and solutions. The conclusion indicates promising directions for future research.",2018,,10.1145/3177848
1694,"Mahdavi, Mohammad and Abedjan, Ziawasch and Castro Fernandez, Raul and Madden, Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan",Raha: A Configuration-Free Error Detection System,"label propagation, data cleaning, semi-supervised learning, error detection, machine learning, classification, clustering, historical data","Detecting erroneous values is a key step in data cleaning. Error detection algorithms usually require a user to provide input configurations in the form of rules or statistical parameters. However, providing a complete, yet correct, set of configurations for each new dataset is not trivial, as the user has to know about both the dataset and the error detection algorithms upfront. In this paper, we present Raha, a new configuration-free error detection system. By generating a limited number of configurations for error detection algorithms that cover various types of data errors, we can generate an expressive feature vector for each tuple value. Leveraging these feature vectors, we propose a novel sampling and classification scheme that effectively chooses the most representative values for training. Furthermore, our system can exploit historical data to filter out irrelevant error detection algorithms and configurations. In our experiments, Raha outperforms the state-of-the-art error detection techniques with no more than 20 labeled tuples on each dataset.",2019,,10.1145/3299869.3324956
1695,"Fan, Wenfei and Liu, Xueli and Lu, Ping and Tian, Chao",Catching Numeric Inconsistencies in Graphs,"incremental validation, Numeric errors, graph dependencies","Numeric inconsistencies are common in real-life knowledge bases and social networks. To catch such errors, we extend graph functional dependencies with linear arithmetic expressions and built-in comparison predicates, referred to as numeric graph dependencies (NGDs). We study fundamental problems for NGDs. We show that their satisfiability, implication, and validation problems are Σp2-complete, Πp2-complete, and coNP-complete, respectively. However, if we allow non-linear arithmetic expressions, even of degree at most 2, the satisfiability and implication problems become undecidable. In other words, NGDs strike a balance between expressivity and complexity. To make practical use of NGDs, we develop an incremental algorithm IncDect to detect errors in a graph G using NGDs in response to updates ΔG to G. We show that the incremental validation problem is coNP-complete. Nonetheless, algorithm IncDect is localizable, i.e., its cost is determined by small neighbors of nodes in ΔG instead of the entire G. Moreover, we parallelize IncDect such that it guarantees to reduce running time with the increase of processors. In addition, to strike a balance between the efficiency and accuracy, we also develop polynomial-time parallel algorithms for detection and incremental detection of top-ranked inconsistencies. Using real-life and synthetic graphs, we experimentally verify the scalability and efficiency of the algorithms.",2020,,10.1145/3385031
1696,"Yu, Jianwei and Zhang, Shi-Xiong and Wu, Bo and Liu, Shansong and Hu, Shoukang and Geng, Mengzhe and Liu, Xunying and Meng, Helen and Yu, Dong",Audio-Visual Multi-Channel Integration and Recognition of Overlapped Speech,,"Automatic speech recognition (ASR) technologies have been significantly advanced in the past few decades. However, recognition of overlapped speech remains a highly challenging task to date. To this end, multi-channel microphone array data are widely used in current ASR systems. Motivated by the invariance of visual modality to acoustic signal corruption and the additional cues they provide to separate the target speaker from the interfering sound sources, this paper presents an audio-visual multi-channel based recognition system for overlapped speech. It benefits from a tight integration between a speech separation front-end and recognition back-end, both of which incorporate additional video input. A series of audio-visual multi-channel speech separation front-end components based on &lt;italic&gt;TF masking&lt;/italic&gt;, &lt;italic&gt;Filter&amp;Sum&lt;/italic&gt; and &lt;italic&gt;mask-based MVDR&lt;/italic&gt; neural channel integration approaches are developed. To reduce the error cost mismatch between the separation and the recognition components, the entire system is jointly fine-tuned using a multi-task criterion interpolation of the scale-invariant signal to noise ratio (Si-SNR) with either the connectionist temporal classification (CTC), or lattice-free maximum mutual information (LF-MMI) loss function. Experiments suggest that: the proposed audio-visual multi-channel recognition system outperforms the baseline audio-only multi-channel ASR system by up to 8.04% (31.68% relative) and 22.86% (58.51% relative) absolute WER reduction on overlapped speech constructed using either simulation or replaying of the LRS2 dataset respectively. Consistent performance improvements are also obtained using the proposed audio-visual multi-channel recognition system when using occluded video input with the lip region randomly covered up to 60%.",2021,,10.1109/TASLP.2021.3078883
1697,,"The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions",,"The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces---user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces.This three-volume handbook is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This third volume focuses on state-of-the-art multimodal language and dialogue processing, including semantic integration of modalities. The development of increasingly expressive embodied agents and robots has become an active test-bed for coordinating multimodal dialogue input and output, including processing of language and nonverbal communication. In addition, major application areas are featured for commercializing multimodal-multisensor systems, including automotive, robotic, manufacturing, machine translation, banking, communications, and others. These systems rely heavily on software tools, data resources, and international standards to facilitate their development. For insights into the future, emerging multimodal-multisensor technology trends are highlighted for medicine, robotics, interaction with smart spaces, and similar topics. Finally, this volume discusses the societal impact of more widespread adoption of these systems, such as privacy risks and how to mitigate them. The handbook chapters provide a number of walk-through examples of system design and processing, information on practical resources for developing and evaluating new systems, and terminology and tutorial support for mastering this emerging field. In the final section of this volume, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces need to be equipped to most effectively advance human performance during the next decade.",2019,,
1698,,ICMI '16: Proceedings of the 18th ACM International Conference on Multimodal Interaction,,,2016,,
1699,"Sun, Xiyan and Xiao, Yu and Ji, Yuanfa and Huang, Jianhua and Bai, Yang",Multi Scale UNet Encoder-Decoder Network for Building Extraction,,"Buildings in remote sensing images have large scale differences and complex shapes. And there are often distractors with visual features similar to buildings in complex scenes. The traditional methods used to extract buildings are limited by the ability of feature representation, resulting in low accuracy and low universality. The semantic segmentation network based on the Encoder-Decoder structure can automatically learn multi-level building feature representation from the data set, and achieve end-to-end building extraction. UNet is a typical semantic segmentation Encoder-Decoder network, but UNet cannot explore enough building information. Small buildings are easy to be missed, large buildings with complex colors and shapes are incompletely extracted, boundary segmentation is inaccurate. And the network is easily affected by roads, trees, shadows and other distractors. Therefore, this article improves UNet and proposes a multi-scale Encoder-Decoder network to learn multi-scale and distinguishable features to better identify buildings and backgrounds. We experiment with the improved network and the classic U-Net on two data sets, and show that the multi-scale Encoder-Decoder network can effectively improve the accuracy of building extraction.",2021,,10.1145/3473465.3473478
1700,"Bhargav, Samarth and Sidiropoulos, Georgios and Kanoulas, Evangelos", 'It's on the Tip of My Tongue': A New Dataset for Known-Item Retrieval,"tip of the tongue known item retrieval, known item retrieval","The tip of the tongue known-item retrieval (TOT-KIR) task involves the 'one-off' retrieval of an item for which a user cannot recall a precise identifier. The emergence of several online communities where users pose known-item queries to other users indicates the inability of existing search systems to answer such queries. Research in this domain is hampered by the lack of large, open or realistic datasets. Prior datasets relied on either annotation by crowd workers, which can be expensive and time-consuming, or generating synthetic queries, which can be unrealistic. Additionally, small datasets make the application of modern (neural) retrieval methods unviable, since they require a large number of data-points. In this paper, we collect the largest dataset yet with 15K query-item pairs in two domains, namely, Movies and Books, from an online community using heuristics, rendering expensive annotation unnecessary while ensuring that queries are realistic. We show that our data collection method is accurate by conducting a data study. We further demonstrate that methods like BM25 fall short of answering such queries, corroborating prior research. The size of the dataset makes neural methods feasible, which we show outperforms lexical baselines, indicating that neural/dense retrieval is superior for the TOT-KIR task.",2022,,10.1145/3488560.3498421
1701,"S. Bargh, Mortaza and Meijer, Ronald and Vink, Marco and van Den Braak, Susan and Schirm, Walter and Choenni, Sunil",Opening Privacy Sensitive Microdata Sets in Light of GDPR,"Open data, Microdata, GDPR, Data protection, Criminal justice data, Privacy, Justice domain data","To enhance the transparency, accountability and efficiency of the Dutch Ministry of Justice and Security, the ministry has set up an open data program to proactively stimulate sharing its (publicly funded) data sets with the public. Disclosure of personal data is considered as one of the main threats for data opening. In this contribution we argue that, according to Dutch laws, the criminal data within the Dutch justice domain are sensitive data in GDPR terms and that the criminal data can only be opened if these sensitive data are transformed to have no personal information. Subsequently, having no personal information in data sets is related to two GDPR concepts: the data being anonymous in its GDPR sense or the data being pseudonymized in its GDPR sense. These two GDPR concepts, i.e., being anonymous data or pseudonymized data in a GDPR sense, can be distinguished in our setting based on whether the data controller cannot or can revert the data protection process, respectively. (Note that the terms anonymous and pseudonymized are interpreted differently in the technical domain.) We examine realizing these GDPR concepts with the Statistical Disclosure Control (SDC) technology and subsequently argue that pseudonymized data in a GDPR sense delivers a better data utility than the other. At the end, we present a number of the consequences of adopting either of these concepts, which can inform legislators and policymakers to define their strategy for opening privacy sensitive microdata sets, like those pertaining to the Dutch criminal justice domain.",2019,,10.1145/3325112.3325222
1702,"Lin, Yijun and Chiang, Yao-Yi and Pan, Fan and Stripelis, Dimitrios and Ambite, Jose Luis and Eckel, Sandrah P. and Habre, Rima",Mining Public Datasets for Modeling Intra-City PM2.5 Concentrations at a Fine Spatial Resolution,"Air Quality Modeling, PM2.5 Concentration Prediction, Geospatial Data Mining","Air quality models are important for studying the impact of air pollutant on health conditions at a fine spatiotemporal scale. Existing work typically relies on area-specific, expert-selected attributes of pollution emissions (e,g., transportation) and dispersion (e.g., meteorology) for building the model for each combination of study areas, pollutant types, and spatiotemporal scales. In this paper, we present a data mining approach that utilizes publicly available OpenStreetMap (OSM) data to automatically generate an air quality model for the concentrations of fine particulate matter less than 2.5 μm in aerodynamic diameter at various temporal scales. Our experiment shows that our (domain-) expert-free model could generate accurate PM2.5 concentration predictions, which can be used to improve air quality models that traditionally rely on expert-selected input. Our approach also quantifies the impact on air quality from a variety of geographic features (i.e., how various types of geographic features such as parking lots and commercial buildings affect air quality and from what distance) representing mobile, stationary and area natural and anthropogenic air pollution sources. This approach is particularly important for enabling the construction of context-specific spatiotemporal models of air pollution, allowing investigations of the impact of air pollution exposures on sensitive populations such as children with asthma at scale.",2017,,10.1145/3139958.3140013
1703,"Kenneally, Erin and Bailey, Michael",Cyber-Security Research Ethics Dialogue &amp; Strategy Workshop,"ethics, law, network measurement, trust, cyber security","The inaugural Cyber-security Research Ethics Dialogue &amp; Strategy Workshop was held on May 23, 2013, in conjunction with the IEEE Security Privacy Symposium in San Francisco, California. CREDS embraced the theme of ""ethics-by-design"" in the context of cyber security research, and aimed to: Educate participants about underlying ethics principles and applications;Discuss ethical frameworks and how they are applied across the various stakeholders and respective communities who are involved;Impart recommendations about how ethical frameworks can be used to inform policymakers in evaluating the ethical underpinning of critical policy decisions;Explore cyber security research ethics techniques,tools,standards and practices so researchers can apply ethical principles within their research methodologies; andDiscuss specific case vignettes and explore the ethical implications of common research acts and omissions.",2014,,10.1145/2602204.2602217
1704,"Zhang, Kunpeng and Bhattacharyya, Siddhartha and Ram, Sudha",Empirical Analysis of Implicit Brand Networks on Social Media,"sentiment identification, social media, mapreduce, marketing intelligence, network analysis","This paper investigates characteristics of implicit brand networks extracted from a large dataset of user historical activities on a social media platform. To our knowledge, this is one of the first studies to comprehensively examine brands by incorporating user-generated social content and information about user interactions. This paper makes several important contributions. We build and normalize a weighted, undirected network representing interactions among users and brands. We then explore the structure of this network using modified network measures to understand its characteristics and implications. As a part of this exploration, we address three important research questions: (1) What is the structure of a brand-brand network? (2) Does an influential brand have a large number of fans? (3) Does an influential brand receive more positive or more negative comments from social users? Experiments conducted with Facebook data show that the influence of a brand has (a) high positive correlation with the size of a brand, meaning that an influential brand can attract more fans, and, (b) low negative correlation with the sentiment of comments made by users on that brand, which means that negative comments have a more powerful ability to generate awareness of a brand than positive comments. To process the large-scale datasets and networks, we implement MapReduce-based algorithms.",2014,,10.1145/2631775.2631806
1705,"Shipman, Frank M. and Marshall, Catherine C.","Ownership, Privacy, and Control in the Wake of Cambridge Analytica: The Relationship between Attitudes and Awareness",,"Has widespread news of abuse changed the public's perceptions of how user-contributed content from social networking sites like Facebook and LinkedIn can be used? We collected two datasets that reflect participants' attitudes about content ownership, privacy, and control, one in April 2018, while Cambridge Analytica was still in the news, and another in February 2019, after the event had faded from the headlines, and aggregated the data according to participants' awareness of the story, contrasting the attitudes of those who reported the greatest awareness with those who reported the least. Participants with the greatest awareness of the news story's details have more polarized attitudes about reuse, especially the reuse of content as data. They express a heightened desire for data mobility, greater concern about networked privacy rights, increased skepticism of algorithmically targeted advertising and news, and more willingness for social media platforms to demand corrections of inaccurate or deceptive content.",2020,,
1706,"Shen, Cong and Qian, Zhaozhi and Huyuk, Alihan and Van Der Schaar, Mihaela",MARS: Assisting Human with Information Processing Tasks Using Machine Learning,"online learning, Data entry, human-in-the-loop decision support system","This article studies the problem of automated information processing from large volumes of unstructured, heterogeneous, and sometimes untrustworthy data sources. The main contribution is a novel framework called Machine Assisted Record Selection (MARS). Instead of today’s standard practice of relying on human experts to manually decide the order of records for processing, MARS learns the optimal record selection via an online learning algorithm. It further integrates algorithm-based record selection and processing with human-based error resolution to achieve a balanced task allocation between machine and human. Both fixed and adaptive MARS algorithms are proposed, leveraging different statistical knowledge about the existence, quality, and cost associated with the records. Experiments using semi-synthetic data that are generated from real-world patients record processing in the UK national cancer registry are carried out, which demonstrate significant (3 to 4 fold) performance gain over the fixed-order processing. MARS represents one of the few examples demonstrating that machine learning can assist humans with complex jobs by automating complex triaging tasks.",2022,,10.1145/3494582
1707,"Kriechbaumer, Thomas and Jorde, Daniel and Jacobsen, Hans-Arno",Waveform Signal Entropy and Compression Study of Whole-Building Energy Datasets,"file format, waveform compression, high sampling rate, non-intrusive load monitoring, Energy dataset, electricity aggregate","Electrical energy consumption has been an ongoing research area since the coming of smart homes and Internet of Things. Consumption characteristics and usages profiles are directly influenced by building occupants and their interaction with electrical appliances. Data analysis together with machine learning models can be utilized to extract valuable information for the benefit of occupants themselves (conserve energy and increase comfort levels), power plants (maintenance), and grid operators (stability). Public energy datasets provide a scientific foundation to develop and benchmark these algorithms and techniques. With datasets exceeding tens of terabytes, we present a novel study of five whole-building energy datasets with high sampling rates, their signal entropy, and how a well-calibrated measurement can have a significant effect on the overall storage requirements. We show that some datasets do not fully utilize the available measurement precision, therefore leaving potential accuracy and space savings untapped. We benchmark a comprehensive list of 365 file formats, transparent data transformations, and lossless compression algorithms. The primary goal is to reduce the overall dataset size while maintaining an easy-to-use file format and access API. We show that with careful selection of file format and encoding scheme, we can reduce the size of some datasets by up to 73%.",2019,,10.1145/3307772.3328285
1708,"Zhang, Lin and Fan, Lixin and Luo, Yong and Duan, Ling-Yu",Intrinsic Performance Influence Based Participant Contribution Estimation for Horizontal Federated Learning,"federated learning, neural network, participant contribution estimation","The rapid development of modern artificial intelligence technique is mainly attributed to sufficient and high-quality data. However, in the data collection, personal privacy is at risk of being leaked. This issue can be addressed by federated learning, which is proposed to achieve efficient model training among multiple data providers without direct data access and aggregation. To encourage more parties owning high-quality data to participate in the federated learning, it is important to evaluate and reward the participant contribution in a reasonable, robust and efficient manner. To achieve this goal, we propose a novel contribution estimation method - Intrinsic Performance Influence based Contribution Estimation (IPICE). In particular, the class-level intrinsic performance influence is adopted as the contribution estimation criteria in IPICE, and a neural network is employed to exploit the non-linear relationship between the performance change and estimated contribution. Extensive experiments are conducted on various datasets and the results demonstrate that IPICE is more accurate and stable than the counterpart in various data distribution settings. The computational complexity is significantly reduced in our IPICE, especially when a new party joins the federation. IPICE assigns small contributions to bad/garbage data, and thus prevent them from participating and deteriorating the learning ecosystem.",2022,,10.1145/3523059
1709,"Kumar, Devender and Jeuris, Steven and Bardram, Jakob E. and Dragoni, Nicola",Mobile and Wearable Sensing Frameworks for MHealth Studies and Applications: A Systematic Review,"mobile sensing frameworks, mobile sensing, wearable sensing, mHealth sensing, mHealth frameworks","With the widespread use of smartphones and wearable health sensors, a plethora of mobile health (mHealth) applications to track well-being, run human behavioral studies, and clinical trials have emerged in recent years. However, the design, development, and deployment of mHealth applications is challenging in many ways. To address these challenges, several generic mobile sensing frameworks have been researched in the past decade. Such frameworks assist developers and researchers in reducing the complexity, time, and cost required to build and deploy health-sensing applications. The main goal of this article is to provide the reader with an overview of the state-of-the-art of health-focused generic mobile and wearable sensing frameworks. This review gives a detailed analysis of functional and non-functional features of existing frameworks, the health studies they were used in, and the stakeholders they support. Additionally, we also analyze the historical evolution, uptake, and maintenance after the initial release. Based on this analysis, we suggest new features and opportunities for future generic mHealth sensing frameworks.",2021,,10.1145/3422158
1710,"Wei, Ziheng and Link, Sebastian",Embedded Functional Dependencies and Data-Completeness Tailored Database Design,"functional dependency, synthesis, Boyce-Codd normal form, missing value, updates, decomposition, redundancy, third normal form, key, database design, normal form","We establish a principled schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing redundant data value occurrences that may cause problems with processing data that meets the requirements. We establish axiomatic, algorithmic, and logical foundations for reasoning about embedded functional dependencies. These foundations enable us to introduce generalizations of Boyce-Codd and Third normal forms that avoid processing difficulties of any application data, or minimize these difficulties across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements, and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate the effectiveness of our framework for the acquisition of the constraints, the schema design process, and the performance of the schema designs in terms of updates and join queries.",2021,,10.1145/3450518
1711,"Osorio-Sanabria, Mariutsi Alexandra and Amaya-Fern\'{a}ndez, Ferney and Gonz\'{a}lez-Zabala, Mayda Patricia",Developing a Model to Readiness Assessment of Open Government Data in Public Institutions in Colombia,"open government data, Open data, e-government, readiness assessment, digital government","Open data is a movement that has gained worldwide political relevance as a strategy that supports active transparency, access to public information, and the generation of public, social, and economic value. To know the progress and results of open data initiatives, governments, working groups, international organizations, and researchers have proposed indexes and evaluation models. These measurements focus on the evaluation of aspects of the preparation, implementation, and impact of open data initiatives. In Colombia, the national government within the framework of its digital government policy defined the open data project. The progress in data openings is monitored through international indexes and the open government index, which focuses solely on the publication and use of open government data. This research deals with the evaluation of the preparation for the opening of data, in public entities that have not implemented an open data initiative. The study gives a general description of the evaluation of open data at the international and national level, identifies aspects to be considered to measure the preparation, and proposes a conceptual model of evaluation to measure the preparation in open data of a public sector entity. This proposal can be considered as a tool that generates information that supports the design and implementation of an effective open data initiative.",2020,,10.1145/3428502.3428548
1712,"Longo, Antonella and Zappatore, Marco and Bochicchio, Mario and Navathe, Shamkant B.",Crowd-Sourced Data Collection for Urban Monitoring via Mobile Sensors,"social sensing, Mobile crowed sensing, smart cities","A considerable amount of research has addressed Internet of Things and connected communities. It is possible to exploit the sensing capabilities of connected communities, by leveraging the continuously growing use of cloud computing solutions and mobile devices. The pervasiveness of mobile sensors also enables the Mobile Crowd Sensing (MCS) paradigm, which aims at using mobile-embedded sensors to extend monitoring of multiple (environmental) phenomena in expansive urban areas. In this article, we discuss our approach with a cloud-based platform to pave the way for applying crowd sensing in urban scenarios. We have implemented a complete solution for environmental monitoring of several pollutants, like noise, air, electromagnetic fields, and so on in an urban area based on this paradigm. Through extensive experimentation, specifically on noise pollution, we show how the proposed infrastructure exhibits the ability to collect data from connected communities, and enables a seamless support of services needed for improving citizens’ quality of life and eventually helps city decision makers in urban planning.",2017,,10.1145/3093895
1713,"Kenneally, Erin",How to Throw the Race to the Bottom: Revisiting Signals for Ethical and Legal Research Using Online Data,"law, security research ethics","With research using data available online, researcher conduct is not fully prescribed or proscribed by formal ethical codes of conduct or law because of ill-fitting ""expectations signals"" -- indicators of legal and ethical risk. This article describes where these ordering forces breakdown in the context of online research and suggests how to identify and respond to these grey areas by applying common legal and ethical tenets that run across evolving models. It is intended to advance the collective dialogue work-in-progress toward a path that revisits and harmonizes more appropriate ethical and legal signals for research using online data between and among researchers, oversight entities, policymakers and society.",2015,,10.1145/2738210.2738211
1714,"Pogorelov, Konstantin and Eskeland, Sigrun Losada and de Lange, Thomas and Griwodz, Carsten and Randel, Kristin Ranheim and Stensland, H\r{a}kon Kvale and Dang-Nguyen, Duc-Tien and Spampinato, Concetto and Johansen, Dag and Riegler, Michael and Halvorsen, P\r{a}l",A Holistic Multimedia System for Gastrointestinal Tract Disease Detection,"Performance, Interactive, Evaluation, Medical Multimedia System, Gastrointestinal Tract, Medicine","Analysis of medical videos for detection of abnormalities and diseases requires both high precision and recall, but also real-time processing for live feedback and scalability for massive screening of entire populations. Existing work on this field does not provide the necessary combination of retrieval accuracy and performance.; AB@In this paper, a multimedia system is presented where the aim is to tackle automatic analysis of videos from the human gastrointestinal (GI) tract. The system includes the whole pipeline from data collection, processing and analysis, to visualization. The system combines filters using machine learning, image recognition and extraction of global and local image features. Furthermore, it is built in a modular way so that it can easily be extended. At the same time, it is developed for efficient processing in order to provide real-time feedback to the doctors. Our experimental evaluation proves that our system has detection and localisation accuracy at least as good as existing systems for polyp detection, it is capable of detecting a wider range of diseases, it can analyze video in real-time, and it has a low resource consumption for scalability.",2017,,10.1145/3083187.3083189
1715,"Luo, Jingtang and Yao, Shiying and Gou, Jijun and Shuai, Lisha and Cao, Yu",A Secure Transmission Scheme of Sensitive Power Information in Ubiquitous Power IoT,"Ubiquitous Power IoT, Data Splitting, Information Security","As one of the national key infrastructures, the ubiquitous power Internet of Things (IoT) provides a convenient method for large-scale power information collection. The widespread transmission of massive power information using data mining techniques for large amounts of data can yield valuable information. Therefore, hacker attacks are endless, posing a threat to the security of the state, society, collectives and individuals. In this paper, we propose a secure transmission scheme of power information, named ""SSD"" (Split &amp; Signature &amp; Disturbing). In the scheme, after the data is split, it will be anonymized and selected for different paths to be transferred to the destination node. After recombination, the data will be restored. The SSD ensures the indistinguishability and security of the sensitive data by data splitting and disturbing method, and protects the anonymity of individual identities by group signature. The experimental results show that the individual prediction/actual data similarity approaches 0%, and the similarity ratio of the category data (three types in the experiment) is 37.32%, which can be judged to be basically non-correlated.",2019,,10.1145/3374587.3374631
1716,"Dong, Xin Luna and Rekatsinas, Theodoros",Data Integration and Machine Learning: A Natural Synergy,"data integration, machine learning, data enrichment","There is now more data to analyze than ever before. As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.",2018,,10.1145/3183713.3197387
1717,"Meng, Linhao and Wei, Yating and Pan, Rusheng and Zhou, Shuyue and Zhang, Jianwei and Chen, Wei",VADAF: Visualization for Abnormal Client Detection and Analysis in Federated Learning,"anomaly detection, visual analytics, Federated learning","Federated Learning (FL) provides a powerful solution to distributed machine learning on a large corpus of decentralized data. It ensures privacy and security by performing computation on devices (which we refer to as clients) based on local data to improve the shared global model. However, the inaccessibility of the data and the invisibility of the computation make it challenging to interpret and analyze the training process, especially to distinguish potential client anomalies. Identifying these anomalies can help experts diagnose and improve FL models. For this reason, we propose a visual analytics system, VADAF, to depict the training dynamics and facilitate analyzing potential client anomalies. Specifically, we design a visualization scheme that supports massive training dynamics in the FL environment. Moreover, we introduce an anomaly detection method to detect potential client anomalies, which are further analyzed based on both the client model’s visual and objective estimation. Three case studies have demonstrated the effectiveness of our system in understanding the FL training process and supporting abnormal client detection and analysis.",2021,,10.1145/3426866
1718,"Bazzi, Alessandro and Campolo, Claudia and Masini, Barbara M. and Molinaro, Antonella",How to Deal with Data Hungry V2X Applications?,"connected and automated vehicles, 5G, vehicle-to-everything, cooperative sensing","Current vehicular communication technologies were designed for a so-called phase 1, where cars needed to advise of their presence. Several projects, research activities and field tests have proved their effectiveness to this scope. But entering the phase 2, where awareness needs to be improved with non-connected objects and vulnerable road users, and even more with phases 3 and 4, where also coordination is foreseen, the spectrum scarcity becomes a critical issue. In this work, we provide an overview of various 5G and beyond solutions currently under investigation that will be needed to tackle the challenge. We first recall the undergoing activities at the access layer aimed to satisfy capacity and bandwidth demands. We then discuss the role that emerging networking paradigms can play to improve vehicular data dissemination, while preventing congestion and better exploiting resources. Finally, we give a look into edge computing and machine learning techniques that will be determinant to efficiently process and mine the massive amounts of sensor data.",2020,,10.1145/3397166.3413465
1719,"Gosh, Saptarshi and EL Boudani, Brahim and Dagiuklas, Tasos and Iqbal, Muddesar",SO-KDN: A Self-Organised Knowledge Defined Networks Architecture for Reliable Routing,"Routing, SDN, Deep Learning, SON","“When you are destined for an important appoint-ment, you would obviously opt for the most reliable route instead of the shortest in order to be well prepared”. Modern networking is presently undergoing through a quantum leap. To cope up with ambitious demands and user expectations, it is becoming more complex both structurally and functionally. Software Defined Networking (SDN) happens to be an instance of such advancements. It has significantly leveraged the network programmability, abstraction, and automation. Eventually, with acceptance form all major network infrastructure such as 5G and Cloud, SDN is becoming the standard of future networking. Likewise, Machine Learning (ML) has become the trendiest skill-in-demand recently. With its superiority of analyzing data, makes it applicable for almost every possible domain. The attempt to applying the power of ML in networking has not been too long, it allows the network to be more intelligent and capable enough to take optimal decisions to address some of its native problems. This gives rise to Self- Organized Networking (SON). In this article, Routing using Deep Neural Network (DNN) on top of SDN is addressed. We proposed a Self-organized Knowledge Defined Network (SO-KDN) architecture and an intelligent routing algorithm, that reactively finds the most reliable route, i.e., a route having least probability of fluctuation. This reduces network overhead due to re-routing and optimizes traffic congestion. Experimental data show a mean 90% accurate forecast in reliability prediction.",2021,,10.1145/3459955.3460617
1720,"Maiolo, Sof\'{\i}a and Etcheverry, Lorena and Marotta, Adriana",Data Profiling in Property Graph Databases,"Property Graph, data profiling","Property Graph databases are being increasingly used within the industry as a powerful and flexible way to model real-world scenarios. With this flexibility, a great challenge appears regarding profiling tasks due to the need of adapting them to these new models while taking advantage of the Property Graphs’ particularities. This article proposes a set of data profiling tasks by integrating existing methods and techniques and an taxonomy to classify them. In addition, an application pipeline is provided while a formal specification of some tasks is defined.",2020,,10.1145/3409473
1721,"Sun, Yan and Wang, Qing and Li, Mingshu",Understanding the Contribution of Non-Source Documents in Improving Missing Link Recovery: An Empirical Study,"Software Maintenance, Mining Software Repositories, Non-Source Documents, Missing Link Recovery","Background: Links between issue reports and their fixing commits play an important role in software maintenance. Such link data are often missing in practice and many approaches have been proposed in order to recover them automatically. Most of existing approaches focus on comparing log messages and source code files in commits with issues reports. Besides the two kinds of data in commits, non-source documents (NSDs) such as change logs usually record the fixing activities and sometimes share similar texts as those in issue reports. However, few discussions have been made on the role of NSDs in designing link recovery approaches.Aims: This paper aims at understanding whether and how NSDs affect the performance of link recovery approaches.Method: An empirical study is conducted to evaluate the role of NSDs in link recovery approaches in 18 open source projects with 6370 issues and 22761 commits.Results: With the inclusion of NSDs, link recovery approaches can get an average increase in F-Measure ranging from 2.76% - 25.63%. Further examinations show NSDs contribute to the performance improvement in 15 projects and have exceptions in 3 projects. The performance improvement in the 15 projects is mainly from the filtering of noisy links. On average, 23.59% - 76.30% false links can be excluded by exploiting NSDs in the link recovery approach. We also analyze the 3 projects in which NSDs cannot improve the performance. Our finding shows sophisticated data selection in NSDs is necessary.Conclusions: Our preliminary findings demonstrate that involving NSDs can improve the performance of link recovery approaches in most cases.",2016,,10.1145/2961111.2962605
1722,"Qu, Youyang and Uddin, Md Palash and Gan, Chenquan and Xiang, Yong and Gao, Longxiang and Yearwood, John",Blockchain-Enabled Federated Learning: A Survey,"Attacks, Countermeasures., Federated Learning, Blockchain","Federated learning (FL) is experiencing fast booming in recent years, which is jointly promoted by the prosperity of machine learning and Artificial Intelligence along with the emerging privacy issues. In the FL paradigm, a central server and local end devices maintain the same model by exchanging model updates instead of raw data, with which the privacy of data stored on end devices is not directly revealed. In this way, the privacy violation caused by the growing collection of sensitive data can be mitigated. However, the performance of FL with a central server is reaching a bottleneck while new threats are emerging simultaneously. There are various reasons, among which the most significant ones are centralized processing, data falsification, and lack of incentives. To accelerate the proliferation of FL, blockchain-enabled FL has attracted substantial attention from both academia and industry. A considerable number of novel solutions are devised to meet the emerging demands of diverse scenarios. Blockchain-enabled FL provides both theories and techniques to improve the performances of FL from various perspectives. In this survey, we will comprehensively summarize and evaluate existing variants of blockchain-enabled FL, identify the emerging challenges, and propose potentially promising research directions in this under-explored domain.",2022,,10.1145/3524104
1723,"Stupka, V\'{a}clav and Hor\'{a}k, Martin and Hus\'{a}k, Martin",Protection of Personal Data in Security Alert Sharing Platforms,"Information sharing, Intrusion detection, Personal data, Privacy, Alert sharing platform, Cyber security","In order to ensure confidentiality, integrity and availability (so called CIA triad) of data within network infrastructure, it is necessary to be able to detect and handle cyber security incidents. For this purpose, it is vital for Computer Security Incident Response Teams (CSIRT) to have enough data on relevant security events and threats. That is why CSIRTs share security alerts and incidents data using various sharing platforms. Even though they do so primarily to protect data and privacy of users, their use also lead to additional processing of personal data, which may cause new privacy risks. European data protection law, especially with the adoption of the new General data protection regulation, sets out very strict rules on processing of personal data which on one hand leads to greater protection of individual's rights, but on the other creates great obstacles for those who need to share any personal data. This paper analyses the General Data Protection Regulation (GDPR), relevant case-law and analyses by the Article 29 Working Party to propose optimal methods and level of personal data processing necessary for effective use of security alert sharing platforms, which would be legally compliant and lead to appropriate balance between risks.",2017,,10.1145/3098954.3105822
1724,"Oliver, Dev and Hoel, Erik G.",A Trace Framework for Analyzing Utility Networks: A Summary of Results (Industrial Paper),"graph algorithms, GIS, spatial databases, graphs and networks, utility networks","Given a utility network and one or more starting points that define where analysis should begin, the problem of analyzing utility networks entails assembling a subset of network elements that meet some specified criteria. Analyzing utility network data has several applications and provides tremendous business value to utilities. For example, analysis may answer questions about the current state of the network (e.g., what valves need to be closed to shut off water flow to a location of a pipe leak), help to design future facilities (e.g., how many houses are fed by a transformer and can the transformer supply another house without overloading its capacity?), and help to organize business practices (e.g., create circuit maps for work crews to facilitate damage assessment after an ice storm). Analyzing utility networks is a challenging problem due to 1) the size of the data, which could have many tens of millions of network elements per utility, and billions of elements at the nationwide or continental scale, 2) modeling and analyzing utility assets at high fidelity (level of detail), and 3) the different analysis requirements across utility domains (e.g., water, wastewater, sewer, district heating, gas, electric, fiber, and telecom). This paper describes the trace framework for utility network analysis that has been implemented in ArcGIS Pro 2.1/ArcGIS Enterprise 10.6. The trace framework features algorithms in a services-based architecture for addressing analysis tasks across a wide array of utility domains. Previous approaches have focused on solving specific problems in specific domains whereas the trace framework provides a more general, scalable solution. We present experiments that demonstrate the scalability of the trace framework and a case study that highlights its value in performing a wide variety of analytics on utility networks.",2018,,10.1145/3274895.3274899
1725,"Bajpai, Vaibhav and Berger, Arthur W. and Eardley, Philip and Ott, J\""{o}rg and Sch\""{o}nw\""{a}lder, J\""{u}rgen",Global Measurements: Practice and Experience (Report on Dagstuhl Seminar #16012),"traffic engineering, quality of experience, network management, internet measurements","This article summarises a 2.5 day long Dagstuhl seminar on Global Measurements: Practice and Experience held in January 2016. This seminar was a followup of the seminar on Global Measurement Frameworks held in 2013, which focused on the development of global Internet measurement platforms and associated metrics. The second seminar aimed at discussing the practical experience gained with building these global Internet measurement platforms. It brought together people who are actively involved in the design and maintenance of global Internet measurement platforms and who do research on the data delivered by such platforms. Researchers in this seminar have used data derived from global Internet measurement platforms in order to manage networks or services or as input for regulatory decisions. The entire set of presentations delivered during the seminar is made publicly available at [1].",2016,,10.1145/2935634.2935641
1726,"Hovsepian, Karen and al'Absi, Mustafa and Ertin, Emre and Kamarck, Thomas and Nakajima, Motohiro and Kumar, Santosh",CStress: Towards a Gold Standard for Continuous Stress Assessment in the Mobile Environment,"modeling, stress, wearable sensors, mobile health (mHealth)","Recent advances in mobile health have produced several new models for inferring stress from wearable sensors. But, the lack of a gold standard is a major hurdle in making clinical use of continuous stress measurements derived from wearable sensors. In this paper, we present a stress model (called cStress) that has been carefully developed with attention to every step of computational modeling including data collection, screening, cleaning, filtering, feature computation, normalization, and model training. More importantly, cStress was trained using data collected from a rigorous lab study with 21 participants and validated on two independently collected data sets --- in a lab study on 26 participants and in a week-long field study with 20 participants. In testing, the model obtains a recall of 89% and a false positive rate of 5% on lab data. On field data, the model is able to predict each instantaneous self-report with an accuracy of 72%.",2015,,10.1145/2750858.2807526
1727,"Wei, Ziheng and Link, Sebastian",Embedded Functional Dependencies and Data-Completeness Tailored Database Design,,"We establish a robust schema design framework for data with missing values. The framework is based on the new notion of an embedded functional dependency, which is independent of the interpretation of missing values, able to express completeness and integrity requirements on application data, and capable of capturing many redundant data value occurrences. We establish axiomatic and algorithmic foundations for reasoning about embedded functional dependencies. These foundations allow us to establish generalizations of Boyce-Codd and Third normal forms that do not permit any redundancy in any future application data, or minimize their redundancy across dependency-preserving decompositions, respectively. We show how to transform any given schema into application schemata that meet given completeness and integrity requirements and the conditions of the generalized normal forms. Data over those application schemata are therefore fit for purpose by design. Extensive experiments with benchmark schemata and data illustrate our framework, and the effectiveness and efficiency of our algorithms, but also provide quantified insight into database schema design trade-offs.",2019,,10.14778/3342263.3342626
1728,"Dodge, Jonathan and Anderson, Andrew A. and Olson, Matthew and Dikkala, Rupika and Burnett, Margaret",How Do People Rank Multiple Mutant Agents?,"Explainable AI, After-Action Review"," Faced with several AI-powered sequential decision-making systems, how might someone choose on which to rely? For example, imagine car buyer Blair shopping for a self-driving car, or developer Dillon trying to choose an appropriate ML model to use in their application. Their first choice might be infeasible (i.e., too expensive in money or execution time), so they may need to select their second or third choice. To address this question, this paper presents: 1) Explanation Resolution, a quantifiable direct measurement concept; 2) a new XAI empirical task to measure explanations: “the Ranking Task”; and 3) a new strategy for inducing controllable agent variations—Mutant Agent Generation. In support of those main contributions, it also presents 4) novel explanations for sequential decision-making agents; 5) an adaptation to the AAR/AI assessment process; and 6) a qualitative study around these devices with 10 participants to investigate how they performed the Ranking Task on our mutant agents, using our explanations, and structured by AAR/AI. From an XAI researcher perspective, just as mutation testing can be applied to any code, mutant agent generation can be applied to essentially any neural network for which one wants to evaluate an assessment process or explanation type. As to an XAI user’s perspective, the participants ranked the agents well overall, but showed the importance of high explanation resolution for close differences between agents. The participants also revealed the importance of supporting a wide diversity of explanation diets and agent “test selection” strategies.",2022,,10.1145/3490099.3511115
1729,"Dautaras, Justas and Matskin, Mihhail",Mobile Crowdsensing with Imagery Tasks,"mobile crowdsensing, mobile sensing devices, crowdsourcing"," The amount of gadgets connected to the internet has grown rapidly in the recent years. These human owned devices can potentially be used to gather sensor data without active involvement of their owners. One of the types of platforms that contribute to the utilisation of these devices are mobile crowdsensing systems. These systems can be used for different tasks including different types of community support. While these systems are quite widely used, yet little research has been done for integration of imagery data into them which require also human involvement. This paper considers a mobile crowdsensing system where gathering data from sensors is supported by crowdsourcing human intelligence for providing both textual and visual information. We also explore the best settings for such a system. Imagery processing is integrated into an already existing mobile crowdsensing platform CrowdS. The solution was evaluated both by a limited number of real life users and by conducting simulations. The simulations represent complex scenarios with multi-level variables. The results of simulation allow suggest an efficient configuration for the parameters and characteristics of the environment used in imagery integration.",2021,,10.1145/3498851.3498929
1730,"Wang, Bing and Mei, Changqing and Wang, Yuanyuan and Zhou, Yuming and Cheng, Mu-Tian and Zheng, Chun-Hou and Wang, Lei and Zhang, Jun and Chen, Peng and Xiong, Yan",Imbalance Data Processing Strategy for Protein Interaction Sites Prediction,,"Protein-protein interactions play essential roles in various biological progresses. Identifying protein interaction sites can facilitate researchers to understand life activities and therefore will be helpful for drug design. However, the number of experimental determined protein interaction sites is far less than that of protein sites in protein-protein interaction or protein complexes. Therefore, the negative and positive samples are usually imbalanced, which is common but bring result bias on the prediction of protein interaction sites by computational approaches. In this work, we presented three imbalance data processing strategies to reconstruct the original dataset, and then extracted protein features from the evolutionary conservation of amino acids to build a predictor for identification of protein interaction sites. On a dataset with 10,430 surface residues but only 2,299 interface residues, the imbalance dataset processing strategies can obviously reduce the prediction bias, and therefore improve the prediction performance of protein interaction sites. The experimental results show that our prediction models can achieve a better prediction performance, such as a prediction accuracy of 0.758, or a high F-measure of 0.737, which demonstrated the effectiveness of our method.",2021,,10.1109/TCBB.2019.2953908
1731,"B\""{a}r, Arian and Golab, Lukasz",Towards Benchmarking Stream Data Warehouses,"stream data warehousing, materialized view maintenance, data warehouse benchmarking","Data management systems are facing two challenges driven by the requirements of emerging data-intensive applications: more data and less time to process the data. Data volumes continue to increase as new sources and data collecting mechanisms appear. At the same time, these sources tend to be highly dynamic and generate data in the form of a stream, which requires quick reaction to newly arrived data. Traditional data warehouses enable scalable data storage and analytics, including the ability to define nested levels of materialized views. However, views are typically refreshed during downtimes---e.g., every night---which does not meet the latency requirements of many applications. Stream data warehousing is a new data management technology that allows nearly-continuous view refresh as new data arrive, which enables seamless integration of real-time monitoring and business intelligence with long-term data mining. In this paper, we argue that a new benchmark is required for stream warehouses, which should focus on measuring the property that determines the utility of these systems, namely how well they can keep up with the incoming data and guarantee the ""freshness"" of materialized views.",2012,,10.1145/2390045.2390062
1732,"McMenemy, David",Ethics and Statistics,,,2021,,
1733,,Data Transformation,,"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.",2019,,
1734,"Jansen, Christoph and Beier, Maximilian and Witt, Michael and Frey, Sonja and Krefting, Dagmar",Towards Reproducible Research in a Biomedical Collaboration Platform Following the FAIR Guiding Principles,"xnat, cloud computing, repeatability, docker, reproducibility, medical data","Replication of computational experiments is essential for verifiable research. However, it requires a comprehensive and unambiguous description of all employed digital artifacts, in particular data, code and the computational environment. Recently, the FAIR Guiding Principles have been published to support reproducible research. In this paper, a cloud-based biomedical collaboration platform has been evaluated regarding FAIR principles and has been extended to support reproducibility. The FAICE suite is presented, encompassing tools to thoroughly describe and reproduce a computational experiment within the original execution environment as well as within a dynamically configured VM.",2017,,10.1145/3147234.3148104
1735,"Kelley, Patrick Gage and Yang, Yongwei and Heldreth, Courtney and Moessner, Christopher and Sedley, Aaron and Kramm, Andreas and Newman, David T. and Woodruff, Allison","Exciting, Useful, Worrying, Futuristic: Public Perception of Artificial Intelligence in 8 Countries",,"As the influence and use of artificial intelligence (AI) have grown and its transformative potential has become more apparent, many questions have been raised regarding the economic, political, social, and ethical implications of its use. Public opinion plays an important role in these discussions, influencing product adoption, commercial development, research funding, and regulation. In this paper we present results of an in-depth survey of public opinion of artificial intelligence conducted with 10,005 respondents spanning eight countries and six continents. We report widespread perception that AI will have significant impact on society, accompanied by strong support for the responsible development and use of AI, and also characterize the public's sentiment towards AI with four key themes (exciting, useful, worrying, and futuristic) whose prevalence distinguishes response to AI in different countries.",2021,,
1736,"Kienzle, J\""{o}rg and Mussbacher, Gunter and Combemale, Benoit and Bastin, Lucy and Bencomo, Nelly and Bruel, Jean-Michel and Becker, Christoph and Betz, Stefanie and Chitchyan, Ruzanna and Cheng, Betty H. C. and Klingert, Sonja and Paige, Richard F. and Penzenstadler, Birgit and Seyff, Norbert and Syriani, Eugene and Venters, Colin C.",Toward Model-Driven Sustainability Evaluation,,Exploring the vision of a model-based framework that may enable broader engagement with and informed decision making about sustainability issues.,2020,,10.1145/3371906
1737,"Brailsford, Sally C and Carter, Michael W and Jacobson, Sheldon H",Five Decades of Healthcare Simulation,,"In this paper we have not attempted to produce any kind of systematic review of simulation in healthcare to compete with the dozen (at least) excellent and comprehensive survey papers on this topic that already exist. We begin with a glance back at the early days of Wintersim, but then proceed, in line with the theme of this special track, to reflect on general developments in healthcare simulation over the years from our own personal perspectives. We include some memories and reflections by several pioneers in this area, both academics and healthcare practitioners, on both sides of the Atlantic. We also asked four current simulation modelers, who all specialize in healthcare applications but from very diverse perspectives, to reflect on their experiences. We endeavor to identify some common or recurring themes across the years, and end with a glimpse into the future.",2017,,
1738,"Xue, Hongfei and Jiang, Wenjun and Miao, Chenglin and Yuan, Ye and Ma, Fenglong and Ma, Xin and Wang, Yijiang and Yao, Shuochao and Xu, Wenyao and Zhang, Aidong and Su, Lu",DeepFusion: A Deep Learning Framework for the Fusion of Heterogeneous Sensory Data,"Sensor Fusion, Deep Learning, Internet of Things","In recent years, significant research efforts have been spent towards building intelligent and user-friendly IoT systems to enable a new generation of applications capable of performing complex sensing and recognition tasks. In many of such applications, there are usually multiple different sensors monitoring the same object. Each of these sensors can be regarded as an information source and provides us a unique ""view"" of the observed object. Intuitively, if we can combine the complementary information carried by multiple sensors, we will be able to improve the sensing performance. Towards this end, we propose DeepFusion, a unified multi-sensor deep learning framework, to learn informative representations of heterogeneous sensory data. DeepFusion can combine different sensors' information weighted by the quality of their data and incorporate cross-sensor correlations, and thus can benefit a wide spectrum of IoT applications. To evaluate the proposed DeepFusion model, we set up two real-world human activity recognition testbeds using commercialized wearable and wireless sensing devices. Experiment results show that DeepFusion can outperform the state-of-the-art human activity recognition methods.",2019,,10.1145/3323679.3326513
1739,"Vilela, Jessyka and Goncalves, Enyo and Holanda, Ana Carla and Castro, Jaelson and Figueiredo, Bruno",A Retrospective Analysis of SAC Requirements: Engineering Track,"relevance, scoping study, SAC, systematic mapping study, symposium on applied computing, trends, retrospective, requirements engineering","Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: In a previous paper, we investigated how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. In this paper, we extended the analysis including the papers of the last edition (2016) and a brief resume of all papers published in the nine editions of SAC-RE track. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 86 papers over the 9 previous SAC RETrack editions, which were analyzed and discussed.",2016,,10.1145/2993231.2993234
1740,"Cheng, Russell",History of Input Modeling,,"In stochastic simulation, input modeling refers to the process of identifying and selecting the probability distributions, called input models, from which are generated the random variates that are the source of the stochastic variation in the simulation model when it is run. This article reviews the history of the development and use of such models with the main focus on discrete-event simulation (DES).",2017,,
1741,"Du, Fan and Plaisant, Catherine and Spring, Neil and Shneiderman, Ben","Finding Similar People to Guide Life Choices: Challenge, Design, and Evaluation","temporal event analytics, temporal visualization, visual analytics, similarity, decision making","People often seek examples of similar individuals to guide their own life choices. For example, students making academic plans refer to friends; patients refer to acquaintances with similar conditions, physicians mention past cases seen in their practice. How would they want to search for similar people in databases? We discuss the challenge of finding similar people to guide life choices and report on a need analysis based on 13 interviews. Our PeerFinder prototype enables users to find records that are similar to a seed record, using both record attributes and temporal events found in the records. A user study with 18 participants and four experts shows that users are more engaged and more confident about the value of the results to provide useful evidence to guide life choices when provided with more control over the search process and more context for the results, even at the cost of added complexity.",2017,,10.1145/3025453.3025777
1742,"Van Kleunen, Lucy and Muller, Brian and Voida, Stephen","""Wiring a City"": A Sociotechnical Perspective on Deploying Urban Sensor Networks","urban sensor networks, sociotechnical system, civic data, smart cities","We use a sociotechnical perspective to expand upon prior characterizations of deploying end-to-end urban sensor networks that focus primarily on the technical aspects of such systems. Via exploratory, semi-structured interviews with those deploying a number of urban sensor networks in a single American city, we identify ways that human decision-making and collaborative processes influence how these infrastructures are built. We synthesize these findings into a framework in which sociotechnical factors show up across the phases of data collection, management, analysis, and impacts within smart city projects. Each phase can display variability in immediacy, automation, geographic scope, and ownership. Finally, we use our situated work to discuss a generalizable tension within smart city projects between cross-domain data integration and fragmentation and provide implications for CSCW research, the design of smart city data platforms, and municipal policy.",2021,,10.1145/3449252
1743,"Alarabiat, Ayman and Soares, Delfina and Ferreira, Luis and de S\'{a}-Soares, Filipe",Analyzing E-Governance Assessment Initiatives: An Exploratory Study,"evaluation, e-government, e-governance, assessment","This paper presents an exploratory study aimed at identifying, exploring, and analyzing current EGOV assessment initiatives. We do so based on data obtained from a desktop research and from a worldwide questionnaire directed to the 193 countries that are part of the list used by the Statistics Division of the United Nations Department of Economic and Social Affairs (UNDESA). The study analyses 12 EGOV assessment initiatives: a) seven of them are international/regional EGOV assessment initiatives performed by the United Nations (UN), European Union (EU), Waseda-IAC, Organisation for Economic Co-operation and Development (OECD), World Bank (WB), WWW Foundation, and Open Knowledge Network (OKN); b) five of them are country-level EGOV assessment initiatives performed by Norway, Germany, India, Saudi Arabia, and the United Arab Emirates. Further, the study provides general results obtained from a questionnaire with participation from 18 countries: Afghanistan, Angola, Brazil, Cabo Verde, Denmark, Estonia, Finland, Germany, Ghana, Ireland, Latvia, the Netherlands, Norway, Oman, Pakistan, the Philippines, Portugal, and Slovenia. The findings show that there is no shortage of interest in assessing EGOV initiatives. However, the supply side of EGOV initiatives is the dominant perspective being assessed, particularly by regional and international organizations. While there is an increasing interest in assessing the users' perspective (demand side) by individual countries, such attempts still seem to be at an early stage. Additionally, the actual use and impact of various EGOV services and activities are rarely well identified and measured. This study represents a stepping stone for developing instruments for assessing EGOV initiatives in future works. For the current stage, the study presents several general suggestions to be considered during the assessment process.",2018,,10.1145/3209281.3209309
1744,"Fu, Yupeng and Soman, Chinmay",Real-Time Data Infrastructure at Uber,,"Uber's business is highly real-time in nature. PBs of data is continuously being collected from the end users such as Uber drivers, riders, restaurants, eaters and so on everyday. There is a lot of valuable information to be processed and many decisions must be made in seconds for a variety of use cases such as customer incentives, fraud detection, machine learning model prediction. In addition, there is an increasing need to expose this ability to different user categories, including engineers, data scientists, executives and operations personnel which adds to the complexity. In this paper, we present the overall architecture of the real-time data infrastructure and identify three scaling challenges that we need to continuously address for each component in the architecture. At Uber, we heavily rely on open source technologies for the key areas of the infrastructure. On top of those open-source software, we add significant improvements and customizations to make the open-source solutions fit in Uber's environment and bridge the gaps to meet Uber's unique scale and requirements. We then highlight several important use cases and show their real-time solutions and tradeoffs. Finally, we reflect on the lessons we learned as we built, operated and scaled these systems.",2021,,
1745,"Masseroli, Marco and Canakoglu, Arif and Ceri, Stefano",Integration and Querying of Genomic and Proteomic Semantic Annotations for Biomedical Knowledge Extraction,,"Understanding complex biological phenomena involves answering complex biomedical questions on multiple biomolecular information simultaneously, which are expressed through multiple genomic and proteomic semantic annotations scattered in many distributed and heterogeneous data sources; such heterogeneity and dispersion hamper the biologists’ ability of asking global queries and performing global evaluations. To overcome this problem, we developed a software architecture to create and maintain a Genomic and Proteomic Knowledge Base (GPKB), which integrates several of the most relevant sources of such dispersed information (including Entrez Gene, UniProt, IntAct, Expasy Enzyme, GO, GOA, BioCyc, KEGG, Reactome, and OMIM). Our solution is general, as it uses a flexible, modular, and multilevel global data schema based on abstraction and generalization of integrated data features, and a set of automatic procedures for easing data integration and maintenance, also when the integrated data sources evolve in data content, structure, and number. These procedures also assure consistency, quality, and provenance tracking of all integrated data, and perform the semantic closure of the hierarchical relationships of the integrated biomedical ontologies. At  http://www.bioinformatics.deib.polimi.it/GPKB/, a Web interface allows graphical easy composition of queries, although complex, on the knowledge base, supporting also semantic query expansion and comprehensive explorative search of the integrated data to better sustain biomedical knowledge extraction.",2016,,10.1109/TCBB.2015.2453944
1746,"Usman, Muhammad and Jan, Mian Ahmad and He, Xiangjian and Chen, Jinjun",A Survey on Big Multimedia Data Processing and Management in Smart Cities,"IoMT, smart cities, management, machine learning, multimedia","Integration of embedded multimedia devices with powerful computing platforms, e.g., machine learning platforms, helps to build smart cities and transforms the concept of Internet of Things into Internet of Multimedia Things (IoMT). To provide different services to the residents of smart cities, the IoMT technology generates big multimedia data. The management of big multimedia data is a challenging task for IoMT technology. Without proper management, it is hard to maintain consistency, reusability, and reconcilability of generated big multimedia data in smart cities. Various machine learning techniques can be used for automatic classification of raw multimedia data and to allow machines to learn features and perform specific tasks. In this survey, we focus on various machine learning platforms that can be used to process and manage big multimedia data generated by different applications in smart cities. We also highlight various limitations and research challenges that need to be considered when processing big multimedia data in real-time.",2019,,10.1145/3323334
1747,"Chen, Junjie and Mowlaei, Mohammad Erfan and Shi, Xinghua",Population-Scale Genomic Data Augmentation Based on Conditional Generative Adversarial Networks,"generative adversarial networks, data augmentation, deep learning, machine learning, genomics","Although next generation sequencing technologies have made it possible to quickly generate a large collection of sequences, current genomic data still suffer from small data sizes, imbalances, and biases due to various factors including disease rareness, test affordability, and concerns about privacy and security. In order to address these limitations of genomic data, we develop a Population-scale Genomic Data Augmentation based on Conditional Generative Adversarial Networks (PG-cGAN) to enhance the amount and diversity of genomic data by transforming samples already in the data rather than collecting new samples. Both the generator and discriminator in the PG-CGAN are stacked with convolutional layers to capture the underlying population structure. Our results for augmenting genotypes in human leukocyte antigen (HLA) regions showed that PC-cGAN can generate new genotypes with similar population structure, variant frequency distributions and LD patterns. Since the input for PC-cGAN is the original genomic data without assumptions about prior knowledge, it can be extended to enrich many other types of biomedical data and beyond.",2020,,10.1145/3388440.3412475
1748,"Skluzacek, Tyler J. and Chard, Ryan and Wong, Ryan and Li, Zhuozhao and Babuji, Yadu N. and Ward, Logan and Blaiszik, Ben and Chard, Kyle and Foster, Ian",Serverless Workflows for Indexing Large Scientific Data,"materials science, serverless, file systems, metadata extraction, data lakes","The use and reuse of scientific data is ultimately dependent on the ability to understand what those data represent, how they were captured, and how they can be used. In many ways, data are only as useful as the metadata available to describe them. Unfortunately, due to growing data volumes, large and distributed collaborations, and a desire to store data for long periods of time, scientific ""data lakes"" quickly become disorganized and lack the metadata necessary to be useful to researchers. New automated approaches are needed to derive metadata from scientific files and to use these metadata for organization and discovery. Here we describe one such system, Xtract, a service capable of processing vast collections of scientific files and automatically extracting metadata from diverse file types. Xtract relies on function as a service models to enable scalable metadata extraction by orchestrating the execution of many, short-running extractor functions. To reduce data transfer costs, Xtract can be configured to deploy extractors centrally or near to the data (i.e., at the edge). We present a prototype implementation of Xtract and demonstrate that it can derive metadata from a 7 TB scientific data repository.",2019,,10.1145/3366623.3368140
1749,"Gannon, Dennis and Fay, Dan and Green, Daron and Takeda, Kenji and Yi, Wenming",Science in the Cloud: Lessons from Three Years of Research Projects on Microsoft Azure,"infrastructure as a service, platform as a service, cloud programming models, map reduce, scalable systems, cloud computing","Microsoft Research is now in its fourth year of awarding Windows Azure cloud resources to the academic community. As of April 2014, over 200 research projects have started. In this paper we review the results of this effort to date. We also characterize the computational paradigms that work well in public cloud environments and those that are usually disappointing. We also discuss many of the barriers to successfully using commercial cloud platforms in research and ways these problems can be overcome.",2014,,10.1145/2608029.2608030
1750,"Wang, Weina and Ying, Lei and Zhang, Junshan","The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms, and Fundamental Limits","Data collection, randomized response, differential privacy","We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. One primary goal of the data collector is to learn some desired information from the elicited data. Specifically, this information is modeled by an underlying state, and the private data of each individual represents his of her knowledge about the state. Departing from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Further, an individual takes full control of his or her own data privacy and reports only a privacy-preserving version of his or her data.In this article, the value of ϵ units of privacy is measured by the minimum payment among all nonnegative payment mechanisms, under which an individual’s best response at a Nash equilibrium is to report his or her data in an ϵ-locally differentially private manner. The higher ϵ is, the less private the reported data is. We derive lower and upper bounds on the value of privacy that are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use a lower payment to buy ϵ units of privacy, and the upper bound is given by an achievable payment mechanism that we design. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given accuracy target for learning the underlying state and show that the total payment of the designed mechanism is at most one individual’s payment away from the minimum.",2018,,10.1145/3232863
1751,"Soufan, Ayah",Deep Learning for Sentiment Analysis of Arabic Text,,"Deep learning has been very successful in the past decades, especially in Computer Vision and Speech Recognition fields. It has been also used successfully in the Natural Language Processing field because of the availability of an enormous amount of online text data, such as social networks and reviews websites, which have gained a lot of popularity and success in the past years. Sentiment Analysis is one of the hottest applications of Natural Language Processing (NLP). Many researchers have done excellent work on Sentiment Analysis for English language. However, the amount of work on Sentiment Analysis for Arabic language is, in comparison, very limited due to the complexity of the Arabic language's morphology and orthography. Unlike the English language, Arabic has many different dialects which makes Sentiment Analysis for Arabic more difficult and challenging, especially when working on data collected from social networks, which is known to be unstructured and noisy. Most of the work that has been done on Sentiment Analysis of Arabic language, focused on using lexicons and basic machine learning models. In addition, most of the work has been done on small datasets because of the limited number of the available annotated datasets for Arabic language. This paper proposes state-of-the-art research for Sentiment Analysis of Arabic microblogging using new techniques, and a sophisticated Arabic text data preprocessing.",2019,,10.1145/3333165.3333185
1752,"Seong, Younho and Nuamah, Joseph and Yi, Sun",Guidelines for Cybersecurity Visualization Design,"cybersecurity, cognition, visualization, ecological interface design, affordance",Cyber security visualization designers can benefit from human factors engineering concepts and principles to resolve key human factors challenges in visual interface design. We survey human factors concepts and principles that have been applied in the past decade of human factors research. We highlight these concepts and relate them to cybersecurity visualization design. We provide guidelines to help cybersecurity visualization designers address some human factors challenges in the context of interface design. We use ecological interface design approach to present human factors-based principles of interface design for visualization. Cyber security visualization designers will benefit from human factors engineering concepts and principles to resolve key human factors challenges in visual interface design.,2020,,10.1145/3410566.3410606
1753,"Shivaprabhu, Vivek R. and Balasubramani, Booma Sowkarthiga and Cruz, Isabel F.",Ontology-Based Instance Matching for Geospatial Urban Data Integration,"Geospatial Data, Instance Matching, Record Linkage, Ontology, Data Integration","To run a smart city, data is collected from disparate sources such as IoT devices, social media, private and public organizations, and government agencies. In the US, the City of Chicago has been a pioneer in the collection of data and in the development of a framework, called OpenGrid, to curate and analyze the collected data. OpenGrid is a geospatial situational awareness platform that allows policy makers, service providers, and the general public to explore city data and to perform advanced data analytics to enable planning of services, prediction of events and patterns, and identification of incidents across the city. This paper presents the instance matching module of GIVA, a Geospatial data Integration, Visualization, and Analytics platform, as applied to the integration of information related to businesses, which is spread across several datasets. In particular, we describe the integration of two datasets, Business Licenses and Food Inspections, so as to enable predictive analytics to determine which food establishments the city should inspect first. The paper describes semantic web-based instance matching mechanisms to compare the Business Names and Address fields.",2017,,10.1145/3152178.3152186
1754,"Zhang, Han and Hill, Shawndra and Rothschild, David",Addressing Selection Bias in Event Studies with General-Purpose Social Media Panels,"selection bias, social media, coverage bias, panels, survey, non-response bias, geolocation, Twitter","Data from Twitter have been employed in prior research to study the impacts of events. Conventionally, researchers use keyword-based samples of tweets to create a panel of Twitter users who mention event-related keywords during and after an event. However, the keyword-based sampling is limited in its objectivity dimension of data and information quality. First, the technique suffers from selection bias since users who discuss an event are already more likely to discuss event-related topics beforehand. Second, there are no viable control groups for comparison to a keyword-based sample of Twitter users. We propose an alternative sampling approach to construct panels of users defined by their geolocation. Geolocated panels are exogenous to the keywords in users’ tweets, resulting in less selection bias than the keyword panel method. Geolocated panels allow us to follow within-person changes over time and enable the creation of comparison groups. We compare different panels in two real-world settings: response to mass shootings and TV advertising. We first show the strength of the selection biases of keyword panels. Then, we empirically illustrate how geolocated panels reduce selection biases and allow meaningful comparison groups regarding the impact of the studied events. We are the first to provide a clear, empirical example of how a better panel selection design, based on an exogenous variable such as geography, both reduces selection bias compared to the current state of the art and increases the value of Twitter research for studying events. While we advocate for the use of a geolocated panel, we also discuss its weaknesses and application scenario seriously. This article also calls attention to the importance of selection bias in impacting the objectivity of social media data.",2018,,10.1145/3185048
1755,,Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker,,"At the ACM Awards banquet in June 2017, during the 50th anniversary celebration of the A.M. Turing Award, ACM announced the launch of the ACM A.M. Turing Book Series, a sub-series of ACM Books, to celebrate the winners of the A.M. Turing Award, computing's highest honor, the ""Nobel Prize"" for computing. This series aims to highlight the accomplishments of awardees, explaining their major contributions of lasting importance in computing.""Making Databases Work: The Pragmatic Wisdom of Michael Stonebraker,"" the first book in the series, celebrates Mike's contributions and impact. What accomplishments warranted computing's highest honor? How did Stonebraker do it? Who is Mike Stonebraker---researcher, professor, CTO, lecturer, innovative product developer, serial entrepreneur, and decades-long leader, and research evangelist for the database community. This book describes Mike's many contributions and evaluates them in light of the Turing Award.The book describes, in 36 chapters, the unique nature, significance, and impact of Mike's achievements in advancing modern database systems over more than 40 years. The stories involve technical concepts, projects, people, prototype systems, failures, lucky accidents, crazy risks, startups, products, venture capital, and lots of applications that drove Mike Stonebraker's achievements and career. Even if you have no interest in databases at all, you'll gain insights into the birth and evolution of Turing Award-worthy achievements from the perspectives of 39 remarkable computer scientists and professionals.Today, data is considered the world's most valuable resource (""The Economist,"" May 6, 2017), whether it is in the tens of millions of databases used to manage the world's businesses and governments, in the billions of databases in our smartphones and watches, or residing elsewhere, as yet unmanaged, awaiting the elusive next generation of database systems. Every one of the millions or billions of databases includes features that are celebrated by the 2014 A.M. Turing Award and are described in this book.",2018,,
1756,"Attard, Judie and Orlandi, Fabrizio and Auer, S\""{o}ren",Exploiting the Value of Data through Data Value Networks,"value creation, open data, innovation, impacts, data value network, exploitation, data value chain, data supply, data demand","Open data is increasingly permeating into all dimensions of our society and has become an indispensable commodity that serves as a basis for many products and services. Governments are generating a huge amount of data spanning different dimensions. This dataification shows the paramount need to identify the means and methods in which the value of data and knowledge can be exploited. While not restricted to the government domain, this dataification is certainly relevant in a government context, particularly due to the large volume of data generated by public institutions. In this paper we identify the various activities and roles within a data value chain, and hence proceed to provide our own definition of a Data Value Network. We specifically cater for non-tangible data products and characterise three dimensions that play a vital role within the Data Value Network. We also propose a Demand and Supply Distribution Model with the aim of providing insight on how an entity can participate in the global data market by producing a data product, as well as a concrete implementation through the Demand and Supply as a Service. Through our contributions we therefore project our vision of enhancing the process of open (government) data exploitation and innovation, with the aim of achieving the highest possible impact.",2017,,10.1145/3047273.3047299
1757,"Netten, Niels and Bargh, Mortaza S. and Choenni, Sunil",Exploiting Data Analytics for Social Services: On Searching for Profiles of Unlawful Use of Social Benefits,"Data analytics, profiling, Genetic Algorithm, social benefits","In this paper we present a data-driven profiling approach that we have adopted and implemented for a municipality. Our aim was to make profiles transparent and meaningful for citizens, policymakers and authorities so that they can validate, scrutinize and challenge the profiles. Our approach relies on a Genetic Algorithm (GA) that searches for useful and human understandable group profiles. Furthermore, we discuss some of the challenges encountered, show a selection of the profiles that were found by the GA, and discuss the necessity and a number of ways of validating these profiles in accordance with, e.g., privacy and non-discrimination laws and guidelines before using them in practice.",2018,,10.1145/3209415.3209481
1758,"Barik, Titus and DeLine, Robert and Drucker, Steven and Fisher, Danyel",The Bones of the System: A Case Study of Logging and Telemetry at Microsoft,"practices, logging, boundary object, developer tools, telemetry, collaboration","Large software organizations are transitioning to event data platforms as they culturally shift to better support data-driven decision making. This paper offers a case study at Microsoft during such a transition. Through qualitative interviews of 28 participants, and a quantitative survey of 1,823 respondents, we catalog a diverse set of activities that leverage event data sources, identify challenges in conducting these activities, and describe tensions that emerge in data-driven cultures as event data flow through these activities within the organization. We find that the use of event data span every job role in our interviews and survey, that different perspectives on event data create tensions between roles or teams, and that professionals report social and technical challenges across activities.",2016,,10.1145/2889160.2889231
1759,"Cambo, Scott Allen and Gergle, Darren",Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science,"critical data studies, human-centered data science, annotator fingerprinting, model positionality, data science, human-centered machine learning, position mining, Computational reflexivity"," Data science and machine learning provide indispensable techniques for understanding phenomena at scale, but the discretionary choices made when doing this work are often not recognized. Drawing from qualitative research practices, we describe how the concepts of positionality and reflexivity can be adapted to provide a framework for understanding, discussing, and disclosing the discretionary choices and subjectivity inherent to data science work. We first introduce the concepts of model positionality and computational reflexivity that can help data scientists to reflect on and communicate the social and cultural context of a model’s development and use, the data annotators and their annotations, and the data scientists themselves. We then describe the unique challenges of adapting these concepts for data science work and offer annotator fingerprinting and position mining as promising solutions. Finally, we demonstrate these techniques in a case study of the development of classifiers for toxic commenting in online communities.",2022,,10.1145/3491102.3501998
1760,"Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh",TabReformer: Unsupervised Representation Learning for Erroneous Data Detection,"Error detection, bidirectional encoder, transformers, data augmentation","Error detection is a crucial preliminary phase in any data analytics pipeline. Existing error detection techniques typically target specific types of errors. Moreover, most of these detection models either require user-defined rules or ample hand-labeled training examples. Therefore, in this article, we present TabReformer, a model that learns bidirectional encoder representations for tabular data. The proposed model consists of two main phases. In the first phase, TabReformer follows encoder architecture with multiple self-attention layers to model the dependencies between cells and capture tuple-level representations. Also, the model utilizes a Gaussian Error Linear Unit activation function with the Masked Data Model objective to achieve deeper probabilistic understanding. In the second phase, the model parameters are fine-tuned for the task of erroneous data detection. The model applies a data augmentation module to generate more erroneous examples to represent the minority class. The experimental evaluation considers a wide range of databases with different types of errors and distributions. The empirical results show that our solution can enhance the recall values by 32.95% on average compared with state-of-the-art techniques while reducing the manual effort by up to 48.86%.",2021,,10.1145/3447541
1761,"Guo, Bin and Wang, Zhu and Yu, Zhiwen and Wang, Yu and Yen, Neil Y. and Huang, Runhe and Zhou, Xingshe",Mobile Crowd Sensing and Computing: The Review of an Emerging Human-Powered Sensing Paradigm,"Mobile phone sensing, cross-space sensing and mining, urban/community dynamics, crowd intelligence, human-machine systems","With the surging of smartphone sensing, wireless networking, and mobile social networking techniques, Mobile Crowd Sensing and Computing (MCSC) has become a promising paradigm for cross-space and large-scale sensing. MCSC extends the vision of participatory sensing by leveraging both participatory sensory data from mobile devices (offline) and user-contributed data from mobile social networking services (online). Further, it explores the complementary roles and presents the fusion/collaboration of machine and human intelligence in the crowd sensing and computing processes. This article characterizes the unique features and novel application areas of MCSC and proposes a reference framework for building human-in-the-loop MCSC systems. We further clarify the complementary nature of human and machine intelligence and envision the potential of deep-fused human--machine systems. We conclude by discussing the limitations, open issues, and research opportunities of MCSC.",2015,,10.1145/2794400
1762,"Chen, Wenqiang and Chen, Lin and Ma, Meiyi and Parizi, Farshid Salemi and Patel, Shwetak and Stankovic, John",ViFin: Harness Passive Vibration to Continuous Micro Finger Writing with a Commodity Smartwatch,"wearable devices, vibration intelligence, micro finger writing, text input","Wearable devices, such as smartwatches and head-mounted devices (HMD), demand new input devices for a natural, subtle, and easy-to-use way to input commands and text. In this paper, we propose and investigate ViFin, a new technique for input commands and text entry, which harness finger movement induced vibration to track continuous micro finger-level writing with a commodity smartwatch. Inspired by the recurrent neural aligner and transfer learning, ViFin recognizes continuous finger writing, works across different users, and achieves an accuracy of 90% and 91% for recognizing numbers and letters, respectively. We quantify our approach's accuracy through real-time system experiments in different arm positions, writing speeds, and smartwatch position displacements. Finally, a real-time writing system and two user studies on real-world tasks are implemented and assessed.",2021,,10.1145/3448119
1763,"Altwaijry, Hotham and Mehrotra, Sharad and Kalashnikov, Dmitri V.",QuERy: A Framework for Integrating Entity Resolution with Query Processing,,"This paper explores an analysis-aware data cleaning architecture for a large class of SPJ SQL queries. In particular, we propose QuERy, a novel framework for integrating entity resolution (ER) with query processing. The aim of QuERy is to correctly and efficiently answer complex queries issued on top of dirty data. The comprehensive empirical evaluation of the proposed solution demonstrates its significant advantage in terms of efficiency over the traditional techniques for the given problem settings.",2015,,10.14778/2850583.2850587
1764,"Tsai, Yi-Shan and Moreno-Marcos, Pedro Manuel and Tammets, Kairit and Kollom, Kaire and Ga\v{s}evi\'{c}, Dragan",SHEILA Policy Framework: Informing Institutional Strategies and Policy Processes of Learning Analytics,"policy, learning analytics, higher education, strategy, ROMA model","This paper introduces a learning analytics policy development framework developed by a cross-European research project team - SHEILA (Supporting Higher Education to Integrate Learning Analytics), based on interviews with 78 senior managers from 51 European higher education institutions across 16 countries. The framework was developed using the RAPID Outcome Mapping Approach (ROMA), which is designed to develop effective strategies and evidence-based policy in complex environments. This paper presents three case studies to illustrate the development process of the SHEILA policy framework, which can be used to inform strategic planning and policy processes in real world environments, particularly for large-scale implementation in higher education contexts.",2018,,10.1145/3170358.3170367
1765,"Li, Yonghan and Lv, Hongjiang",The Dilemma of Digital Transformation of China's Hotel Industry and the Construction of Technology Platform: A Survey of Hotels Industry in China,,"The digital economy has been a hot spot in social development in recent years, and all walks of life are facing the opportunities and challenges of digital transformation. Successful digital transformation can enable traditional industries to gain dynamic capabilities in a changing environment, thereby gaining a leading competitive advantage. The previous literature paid more attention to the digital transformation of traditional industries, but lacked enough attention to the hotel industry. Through the case analysis of several major hotel groups in China, this article has gained profound insights in the digital transformation, enriched the influence of digital technology on the organization and management changes of the hotel industry, and has enlightening significance for guiding the hotel industry's practice.",2021,,
1766,"Suni-Lopez, Franci and Condori-Fernandez, Nelly and Catala, Alejandro",Understanding Implicit User Feedback from Multisensorial and Physiological Data: A Case Study,,"Ensuring the quality of user experience is very important for increasing the acceptance likelihood of software applications, which can be affected by several contextual factors that continuously change over time (e.g., emotional state of end-user). Due to these changes in the context, software continually needs to adapt for delivering software services that can satisfy user needs. However, to achieve this adaptation, it is important to gather and understand the user feedback. In this paper, we mainly investigate whether physiological data can be considered and used as a form of implicit user feedback. To this end, we conducted a case study involving a tourist traveling abroad, who used a wearable device for monitoring his physiological data, and a smartphone with a mobile app for reminding him to take his medication on time during four days. Through the case study, we were able to identify some factors and activities as emotional triggers, which were used for understanding the user context. Our results highlight the importance of having a context analyzer, which can help the system to determine whether the detected stress could be considered as actionable and consequently as implicit user feedback.",2020,,
1767,"Baltes, Sebastian and Diehl, Stephan",Worse Than Spam: Issues In Sampling Software Developers,"Sampling, Empirical Research, Ethics, Software Developers","Background: Reaching out to professional software developers is a crucial part of empirical software engineering research. One important method to investigate the state of practice is survey research. As drawing a random sample of professional software developers for a survey is rarely possible, researchers rely on various sampling strategies. Objective: In this paper, we report on our experience with different sampling strategies we employed, highlight ethical issues, and motivate the need to maintain a collection of key demographics about software developers to ease the assessment of the external validity of studies. Method: Our report is based on data from two studies we conducted in the past. Results: Contacting developers over public media proved to be the most effective and efficient sampling strategy. However, we not only describe the perspective of researchers who are interested in reaching goals like a large number of participants or a high response rate, but we also shed light onto ethical implications of different sampling strategies. We present one specific ethical guideline and point to debates in other research communities to start a discussion in the software engineering research community about which sampling strategies should be considered ethical.",2016,,10.1145/2961111.2962628
1768,"Chen, Yueyue and Guo, Deke and Bhuiyan, MD Zakirul Alam and Xu, Ming and Wang, Guojun and Lv, Pin",Towards Profit Optimization During Online Participant Selection in Compressive Mobile Crowdsensing,"Compressive mobile crowdsensing, data reconstruction, online participant selection","A mobile crowdsensing (MCS) platform motivates employing participants from the crowd to complete sensing tasks. A crucial problem is to maximize the profit of the platform, i.e., the charge of a sensing task minus the payments to participants that execute the task. In this article, we improve the profit via the data reconstruction method, which brings new challenges, because it is hard to predict the reconstruction quality due to the dynamic features and mobility of participants. In particular, two Profit-driven Online Participant Selection (POPS) problems under different situations are studied in our work: (1) for S-POPS, the sensing cost of the different parts within the target area is the Same. Two mechanisms are designed to tackle this problem, including the ProSC and ProSC+. An exponential-based quality estimation method and a repetitive cross-validation algorithm are combined in the former mechanism, and the spatial distribution of selected participants are further discussed in the latter mechanism; (2) for V-POPS, the sensing cost of different parts within the target area is Various, which makes it the NP-hard problem. A heuristic mechanism called ProSCx is proposed to solve this problem, where the searching space is narrowed and both the participant quantity and distribution are optimized in each slot. Finally, we conduct comprehensive evaluations based on the real-world datasets. The experimental results demonstrate that our proposed mechanisms are more effective and efficient than baselines, selecting the participants with a larger profit for the platform.",2019,,10.1145/3342515
1769,"Hossain, Md. Ekramul and Khan, Arif and Moni, Mohammad Ali and Uddin, Shahadat",Use of Electronic Health Data for Disease Prediction: A Comprehensive Literature Review,,"Disease prediction has the potential to benefit stakeholders such as the government and health insurance companies. It can identify patients at risk of disease or health conditions. Clinicians can then take appropriate measures to avoid or minimize the risk and in turn, improve quality of care and avoid potential hospital admissions. Due to the recent advancement of tools and techniques for data analytics, disease risk prediction can leverage large amounts of semantic information, such as demographics, clinical diagnosis and measurements, health behaviours, laboratory results, prescriptions and care utilisation. In this regard, electronic health data can be a potential choice for developing disease prediction models. A significant number of such disease prediction models have been proposed in the literature over time utilizing large-scale electronic health databases, different methods, and healthcare variables. The goal of this comprehensive literature review was to discuss different risk prediction models that have been proposed based on electronic health data. Search terms were designed to find relevant research articles that utilized electronic health data to predict disease risks. Online scholarly databases were searched to retrieve results, which were then reviewed and compared in terms of the method used, disease type, and prediction accuracy. This paper provides a comprehensive review of the use of electronic health data for risk prediction models. A comparison of the results from different techniques for three frequently modelled diseases using electronic health data was also discussed in this study. In addition, the advantages and disadvantages of different risk prediction models, as well as their performance, were presented. Electronic health data have been widely used for disease prediction. A few modelling approaches show very high accuracy in predicting different diseases using such data. These modelling approaches have been used to inform the clinical decision process to achieve better outcomes.",2021,,10.1109/TCBB.2019.2937862
1770,"Zimmermann, Olaf and L\""{u}bke, Daniel and Zdun, Uwe and Pautasso, Cesare and Stocker, Mirko",Interface Responsibility Patterns: Processing Resources and Operation Responsibilities,,"Remote Application Programming Interfaces (APIs), as for instance offered in microservices architectures, are used in almost any distributed system today and are thus enablers for many digitalization efforts. It is hard to design such APIs so that they are easy and effective to use; maintaining their runtime qualities while preserving backward compatibility is equally challenging. Finding well suited granularities in terms of the architectural capabilities of endpoints and the read-write semantics of their operations are particularly important design concerns. Existing pattern languages have dealt with local APIs in object-oriented programming, with remote objects, with queue-based messaging and with service-oriented computing platforms. However, patterns or equivalent guidances for the architectural design of API endpoints, operations and their request and response message structures are still missing. In this paper, we extend our microservice API pattern language (MAP) and introduce endpoint role and operation responsibility patterns, namely Processing Resource, Computation Function, State Creation Operation, Retrieval Operation, and State Transition Operation. Known uses and examples of the patterns are drawn from public Web APIs, as well as application development and system integration projects the authors have been involved in.",2020,,10.1145/3424771.3424822
1771,"Antunes, Rodolfo S. and Seewald, Lucas A. and Rodrigues, Vinicius F. and Costa, Cristiano A. Da and Jr., Luiz Gonzaga and Righi, Rodrigo R. and Maier, Andreas and Eskofier, Bj\""{o}rn and Ollenschl\""{a}ger, Malte and Naderi, Farzad and Fahrig, Rebecca and Bauer, Sebastian and Klein, Sigrun and Campanatti, Gelson",A Survey of Sensors in Healthcare Workflow Monitoring,"healthcare, Real-time Location Systems, Computer Vision, workflow monitoring","Activities of a clinical staff in healthcare environments must regularly be adapted to new treatment methods, medications, and technologies. This constant evolution requires the monitoring of the workflow, or the sequence of actions from actors involved in a procedure, to ensure quality of medical services. In this context, recent advances in sensing technologies, including Real-time Location Systems and Computer Vision, enable high-precision tracking of actors and equipment. The current state-of-the-art about healthcare workflow monitoring typically focuses on a single technology and does not discuss its integration with others. Such an integration can lead to better solutions to evaluate medical workflows. This study aims to fill the gap regarding the analysis of monitoring technologies with a systematic literature review about sensors for capturing the workflow of healthcare environments. Its main scientific contribution is to identify both current technologies used to track activities in a clinical environment and gaps on their combination to achieve better results. It also proposes a taxonomy to classify work regarding sensing technologies and methods. The literature review does not present proposals that combine data obtained from Real-time Location Systems and Computer Vision sensors. Further analysis shows that a multimodal analysis is more flexible and could yield better results.",2018,,10.1145/3177852
1772,"Hafen, Ryan and Gibson, Tara D. and van Dam, Kerstin Kleese and Critchlow, Terence","Large-Scale Exploratory Analysis, Cleaning, and Modeling for Event Detection in Real-World Power Systems Data","power systems, phasor measurement unit, exploratory data analysis, divide and recombine, R, Hadoop","In this paper, we present an approach to large-scale data analysis, Divide and Recombine (D&amp;R), and describe a hardware and software implementation that supports this approach. We then illustrate the use of D&amp;R on large-scale power systems sensor data to perform initial exploration, discover multiple data integrity issues, build and validate algorithms to filter bad data, and construct statistical event detection algorithms. This paper also reports on experiences using a non-traditional Hadoop distributed computing setup on top of a HPC computing cluster.",2013,,10.1145/2536780.2536783
1773,"Nagaraja, Arun and Kumar, T. Satish","An Extensive Survey on Intrusion Detection- Past, Present, Future","Intrusion Detection, Datasets, Feature Selection, Anomaly Detection, Measures, Classification, Clustering","Intrusion Detection is the most eminent fields in the network which can also be called as anomaly detection. Various methods used by early research tells that, the kind of measures used to detect the intrusion is not specified. Research has grown extensively in Anomaly intrusion detection by using different data mining techniques. Most researchers have not briefed on the kinds of distances measures used, the classification and feature selection techniques used in identifying intrusion detection. Intrusion detection is classified with problems as Outlier problems, Sparseness problem and Data Distribution. One of the important observations made is, High Dimensional Data Reduction is not performed, and conventional dataset is not used or maintained by any researchers. A survey is performed to identify the type of distance measures used and the type of datasets used in the early research. In this extended survey, the measures like Distance measure, pattern behaviors are used in identifying the Network Intrusion Detection. In this paper, we present the various methods used by authors to obtain feature selection methods. Also, the discussion is towards, Computation of High Dimensional Data, how to decide the Choice of Learning algorithm, Efficient Distance and similarity measures to identify the intrusion detection from different datasets.",2018,,10.1145/3234698.3234743
1774,"Zhang, Zhiqiang and Huang, Xiangbing and Iqbal, Muhammad Faisal Buland and Ye, Songtao",Better Weather Forecasting through Truth Discovery Analysis,"weather, truth discovery, heterogeneous data","In many real world applications, the same object or event may be described by multiple sources. As a result, conflicts among these sources are inevitable and these conflicts cause confusion as we have more than one value or outcome for each object. One significant problem is to resolve the confusion and to identify a piece of information which is trustworthy. This process of finding the truth from conflicting values of an object provided by multiple sources is called truth discovery or fact-finding. The main purpose of the truth discovery is to find more and more trustworthy information and reliable sources. Because the major assumption of truth discovery is on this intuitive principle, the source that provides trustworthy information is considered more reliable, and moreover, if the piece of information is from a reliable source, then it is more trustworthy. However, previously proposed truth discovery methods either do not conduct source reliability estimation at all (Voting Method), or even if they do, they do not model multiple properties of the object separately. This is the motivation for researchers to develop new techniques to tackle the problem of truth discovery in data with multiple properties. We present a method using an optimization framework which minimizes the overall weighted deviation between the truths and the multi-source observations. In this framework, different types of distance functions can be plugged in to capture the characteristics of different data types. We use weather datasets collected by four different platforms for extensive experiments and the results verify both the efficiency and precision of our methods for truth discovery.",2017,,10.1145/3144789.3144797
1775,"Qiao, Lin and Ran, Ran and Wu, He and Zhou, Qiaoni and Liu, Sai and Liu, Yunfei",Imputation Method of Missing Values for Dissolved Gas Analysis Data Based on Iterative KNN and XGBoost,"Iterative KNN, Dissolved Gas Analysis, Interpolation Priority, Missing Values","Power transformers are an important part of the power system. Accurate monitoring of its operating status is particularly important for the normal and stable operation of the entire power system and the timely diagnosis of potential faults. Dissolved Gas Analysis (DGA) can detect and judge the oil-immersed power transformer failure by comparing the dissolved gas content of the power transformer in the normal operating state and the oil in the fault state. However, in the operation process of the grid transformer, the detection data is often missing. This paper proposes an effective method based on iterative KNN and XGBoost method for missing values. Firstly, according to the XGBoost integration tree, there are missing values. Information such as the number of attribute divisions obtained by data set training calculates the importance scores of different attributes to determine the priority of the attributes, and then performs interpolation on the missing values ?in an iterative manner. The experimental results in the case of DGA dataset and different missing rate show that the proposed method is superior to the existing similar methods in accuracy, and the dataset after interpolation has a significant improvement on the classification effect of the classifier.",2018,,10.1145/3302425.3302447
1776,"Bento, Fernando and Costa, Carlos J.",ERP Measure Success Model; a New Perspective,"success measuring models, ERP's, ERP's life cycle, information systems, performance","This paper addresses the problem of defining and evaluating the success of ERP throughout the life cycle of the information system. In order to solve this problem, many of the theoretical and empirical contributions on the success of the information system are analysed and discussed.This approach allows the development of a new model; especially in Delone &amp; Mclean supported research.This work will try to establish a different perspective on the success of the ERP and can be an encouragement to some organizations or the many researchers that will be engaging in these areas, in order to help achieve more clearly the expected performance in the acquisition phase of ERPs. Many times that performance does not always happen [1].",2013,,10.1145/2503859.2503863
1777,"Sulser, Fabio and Giangreco, Ivan and Schuldt, Heiko",Crowd-Based Semantic Event Detection and Video Annotation for Sports Videos,"sports, video annotation, crowdsourcing, multimedia retrieval","Recent developments in sport analytics have heightened the interest in collecting data on the behavior of individuals and of the entire team in sports events. Rather than using dedicated sensors for recording the data, the detection of semantic events reflecting a team's behavior and the subsequent annotation of video data is nowadays mostly performed by paid experts. In this paper, we present an approach to generating such annotations by leveraging the wisdom of the crowd. We present the CrowdSport application that allows to collect data for soccer games. It presents crowd workers short video snippets of soccer matches and allows them to annotate these snippets with event information. Finally, the various annotations collected from the crowd are automatically disambiguated and integrated into a coherent data set. To improve the quality of the data entered, we have implemented a rating system that assigns each worker a trustworthiness score denoting the confidence towards newly entered data. Using the DBSCAN clustering algorithm and the confidence score, the integration ensures that the generated event labels are of high quality, despite of the heterogeneity of the participating workers. These annotations finally serve as a basis for a video retrieval system that allows users to search for video sequences on the basis of a graphical specification of team behavior or motion of the individual player. Our evaluations of the crowd-based semantic event detection and video annotation using the Microworkers platform have shown the effectiveness of the approach and have led to results that are in most cases close to the ground truth and can successfully be used for various retrieval tasks.",2014,,10.1145/2660114.2660119
1778,"Tian, Luogeng and Yang, Bailong and Yin, Xinli and Su, Yang",A Survey of Personalized Recommendation Based on Machine Learning Algorithms,"Graph Neural Networks, Sparse matrix, Personalized recommendation, Machine learning","Personalized recommendation is a key technology to effectively solve the overload of online information and eliminate information islands. It is widely known as an important way to improve the quality of information services. However, cold start, data sparseness, algorithm performance, recommendation accuracy and surprise are still the key issues that restrict users' personalized recommendations. Firstly, we review the development trend of personalized information recommendation algorithms in the past 15 years. And then we propose a new classification method for users' personalized recommendation based on machine learning algorithms with cold start, data sparseness, and the performance of the algorithm as the main goals. On this basis, we summarize and compare the ideas, practices and conclusions of related machine learning algorithms. Finally, we further summarize the main advantages and disadvantages of the 10 kinds of personalized recommendation algorithms from the perspective of classification proposed, and look forward to the development directions, difficulties, focus and methods of personalized recommendation algorithms.",2020,,10.1145/3443467.3444711
1779,"Rixen, Jan Ole and Colley, Mark and Askari, Ali and Gugenheimer, Jan and Rukzio, Enrico",Consent in the Age of AR: Investigating The Comfort With Displaying Personal Information in Augmented Reality,"Personal Information, Disclosure, Augmented Reality, Public Experiences, Mixed Reality, Comfort, Data Glasses, User Acceptance, Consent, Social Acceptability"," Social Media (SM) has shown that we adapt our communication and disclosure behaviors to available technological opportunities. Head-mounted Augmented Reality (AR) will soon allow to effortlessly display the information we disclosed not isolated from our physical presence (e.g., on a smartphone) but visually attached to the human body. In this work, we explore how the medium (AR vs. Smartphone), our role (being augmented vs. augmenting), and characteristics of information types (e.g., level of intimacy, self-disclosed vs. non-self-disclosed) impact the users’ comfort when displaying personal information. Conducting an online survey (N=148), we found that AR technology and being augmented negatively impacted this comfort. Additionally, we report that AR mitigated the effects of information characteristics compared to those they had on smartphones. In light of our results, we discuss that information augmentation should be built on consent and openness, focusing more on the comfort of the augmented rather than the technological possibilities.",2022,,10.1145/3491102.3502140
1780,"Sima, Ana Claudia and Mendes de Farias, Tarcisio and Anisimova, Maria and Dessimoz, Christophe and Robinson-Rechavi, Marc and Zbinden, Erich and Stockinger, Kurt",Bio-SODA: Enabling Natural Language Question Answering over Knowledge Graphs without Training Data,"Ranking, Question Answering, Knowledge Graphs"," The problem of natural language processing over structured data has become a growing research field, both within the relational database and the Semantic Web community, with significant efforts involved in question answering over knowledge graphs (KGQA). However, many of these approaches are either specifically targeted at open-domain question answering using DBpedia, or require large training datasets to translate a natural language question to SPARQL in order to query the knowledge graph. Hence, these approaches often cannot be applied directly to complex scientific datasets where no prior training data is available. In this paper, we focus on the challenges of natural language processing over knowledge graphs of scientific datasets. In particular, we introduce Bio-SODA, a natural language processing engine that does not require training data in the form of question-answer pairs for generating SPARQL queries. Bio-SODA uses a generic graph-based approach for translating user questions to a ranked list of SPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm that includes node centrality as a measure of relevance for selecting the best SPARQL candidate query. Our experiments with real-world datasets across several scientific domains, including the official bioinformatics Question Answering over Linked Data (QALD) challenge, as well as the CORDIS dataset of European projects, show that Bio-SODA outperforms publicly available KGQA systems by an F1-score of least 20% and by an even higher factor on more complex bioinformatics datasets. ",2021,,10.1145/3468791.3469119
1781,"Elmeleegy, Hazem and Li, Yinan and Qi, Yan and Wilmot, Peter and Wu, Mingxi and Kolay, Santanu and Dasdan, Ali and Chen, Songting",Overview of Turn Data Management Platform for Digital Advertising,,"This paper gives an overview of Turn Data Management Platform (DMP). We explain the purpose of this type of platforms, and show how it is positioned in the current digital advertising ecosystem. We also provide a detailed description of the key components in Turn DMP. These components cover the functions of (1) data ingestion and integration, (2) data warehousing and analytics, and (3) real-time data activation. For all components, we discuss the main technical and research challenges, as well as the alternative design choices. One of the main goals of this paper is to highlight the central role that data management is playing in shaping this fast growing multi-billion dollars industry.",2013,,10.14778/2536222.2536238
1782,"Herbert, Franziska and Schmidbauer-Wolf, Gina Maria and Reuter, Christian",Who Should Get My Private Data in Which Case? Evidence in the Wild,"privacy, survey, awareness, data sharing"," As a result of the ongoing digitalization of our everyday lives, the amount of data produced by everyone is steadily increasing. This happens through personal decisions and items, such as the use of social media or smartphones, but also through more and more data acquisition in public spaces, such as e.g., Closed Circuit Television. Are people aware of the data they are sharing? What kind of data do people want to share with whom? Are people aware if they have Wi-Fi, GPS, or Bluetooth activated as potential data sharing functionalities on their phone? To answer these questions, we conducted a representative online survey as well as face-to-face interviews with users in Germany. We found that most users wanted to share private data on premise with most entities, indicating that willingness to share data depends on who has access to the data. Almost half of the participants would be more willing to share data with specific entities (state bodies &amp; rescue forces) in the event that an acquaintance is endangered. For Wi-Fi and GPS the frequencies of self-reported and actual activation on the smartphone are almost equal, but 17% of participants were unaware of the Bluetooth status on their smartphone. Our research is therefore in line with other studies suggesting relatively low privacy awareness of users.",2021,,10.1145/3473856.3473879
1783,,Rule-Based Data Cleaning,,"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.",2019,,
1784,"Bellio, Maura and Furniss, Dominic and Oxtoby, Neil P. and Garbarino, Sara and Firth, Nicholas C. and Ribbens, Annemie and Alexander, Daniel C. and Blandford, Ann",Opportunities and Barriers for Adoption of a Decision-Support Tool for Alzheimer’s Disease,"Diffusion of innovation, user-centred design, technology adoption, healthcare, design-reality gap","Clinical decision-support tools (DSTs) represent a valuable resource in healthcare. However, lack of Human Factors considerations and early design research has often limited their successful adoption. To complement previous technically focused work, we studied adoption opportunities of a future DST built on a predictive model of Alzheimer’s Disease (AD) progression. Our aim is two-fold: exploring adoption opportunities for DSTs in AD clinical care, and testing a novel combination of methods to support this process. We focused on understanding current clinical needs and practices, and the potential for such a tool to be integrated into the setting, prior to its development. Our user-centred approach was based on field observations and semi-structured interviews, analysed through workflow analysis, user profiles, and a design-reality gap model. The first two are common practice, whilst the latter provided added value in highlighting specific adoption needs. We identified the likely early adopters of the tool as being both psychiatrists and neurologists based in research-oriented clinical settings. We defined ten key requirements for the translation and adoption of DSTs for AD around IT, user, and contextual factors. Future works can use and build on these requirements to stand a greater chance to get adopted in the clinical setting.",2021,,10.1145/3462764
1785,"Zhang, Yun C. and Zhang, Shibo and Liu, Miao and Daly, Elyse and Battalio, Samuel and Kumar, Santosh and Spring, Bonnie and Rehg, James M. and Alshurafa, Nabil",SyncWISE: Window Induced Shift Estimation for Synchronization of Video and Accelerometry from Wearable Sensors,"Accelerometry, Video, Time Synchronization, Wearable Sensor, Temporal Drift, Automatic Synchronization, Wearable Camera","The development and validation of computational models to detect daily human behaviors (e.g., eating, smoking, brushing) using wearable devices requires labeled data collected from the natural field environment, with tight time synchronization of the micro-behaviors (e.g., start/end times of hand-to-mouth gestures during a smoking puff or an eating gesture) and the associated labels. Video data is increasingly being used for such label collection. Unfortunately, wearable devices and video cameras with independent (and drifting) clocks make tight time synchronization challenging. To address this issue, we present the Window Induced Shift Estimation method for Synchronization (SyncWISE) approach. We demonstrate the feasibility and effectiveness of our method by synchronizing the timestamps of a wearable camera and wearable accelerometer from 163 videos representing 45.2 hours of data from 21 participants enrolled in a real-world smoking cessation study. Our approach shows significant improvement over the state-of-the-art, even in the presence of high data loss, achieving 90% synchronization accuracy given a synchronization tolerance of 700 milliseconds. Our method also achieves state-of-the-art synchronization performance on the CMU-MMAC dataset.",2020,,10.1145/3411824
1786,"Rodrigues, Jo\~{a}o G. P. and Aguiar, Ana",Extracting 3D Maps from Crowdsourced GNSS Skyview Data,"crowdsensing, 3d mapping, gnss snr measurements","3D maps of urban environments are useful in various fields ranging from cellular network planning to urban planning and climatology. These models are typically constructed using expensive techniques such as manual annotation with 3D modeling tools, extrapolated from satellite or aerial photography, or using specialized hardware with depth sensing devices. In this work, we show that 3D urban maps can be extracted from standard GNSS data, by analyzing the received satellite signals that are attenuated by obstacles, such as buildings. Furthermore, we show that these models can be extracted from low-accuracy GNSS data, crowdsourced opportunistically from standard smartphones during their user's uncontrolled daily commute trips, unleashing the potential of applying the principle to wide areas. Our proposal incorporates position inaccuracies in the calculations, and accommodates different sources of variability of the satellite signals' SNR. The diversity of collection conditions of crowdsourced GNSS positions is used to mitigate bias and noise from the data. A binary classification model is trained and evaluated on multiple urban scenarios using data crowdsourced from over 900 users. Our results show that the generalization accuracy for a Random Forest classifier in typical urban environments lies between 79% and 91% on 4 m wide voxels, demonstrating the potential of the proposed method for building 3D maps for wide urban areas.",2019,,10.1145/3300061.3345456
1787,"West, Peter and Giordano, Richard and Van Kleek, Max and Shadbolt, Nigel",The Quantified Patient in the Doctor's Office: Challenges &amp; Opportunities,"clinical decision making, quantified self, self-tracking","While the Quantified Self and personal informatics fields have focused on the individual's use of self-logged data about themselves, the same kinds of data could, in theory, be used to improve diagnosis and care planning. In this paper, we seek to understand both the opportunities and bottlenecks in the use of self-logged data for differential diagnosis and care planning during patient visits to both primary and secondary care. We first conducted a literature review to identify potential factors influencing the use of self-logged data in clinical settings. This informed the design of our experiment, in which we applied a vignette-based role-play approach with general practitioners and hospital specialists in the US and UK, to elicit reflections on and insights about using patient self-logged data. Our analysis reveals multiple opportunities for the use of self-logged data in the differential diagnosis workflow, identifying capture, representational, and interpretational challenges that are potentially preventing self-logged data from being effectively interpreted and applied by clinicians to derive a patient's prognosis and plan of care.",2016,,10.1145/2858036.2858445
1788,"Peng, Jinglin and Wu, Weiyuan and Lockhart, Brandon and Bian, Song and Yan, Jing Nathan and Xu, Linghao and Chi, Zhixuan and Rzeszotarski, Jeffrey M. and Wang, Jiannan",DataPrep.EDA: Task-Centric Exploratory Data Analysis for Statistical Modeling in Python,,"Exploratory Data Analysis (EDA) is a crucial step in any data science project. However, existing Python libraries fall short in supporting data scientists to complete common EDA tasks for statistical modeling. Their API design is either too low level, which is optimized for plotting rather than EDA, or too high level, which is hard to specify more fine-grained EDA tasks. In response, we propose DataPrep.EDA, a novel task-centric EDA system in Python. DataPrep.EDA allows data scientists to declaratively specify a wide range of EDA tasks in different granularity with a single function call. We identify a number of challenges to implement DataPrep.EDA, and propose effective solutions to improve the scalability, usability, customizability of the system. In particular, we discuss some lessons learned from using Dask to build the data processing pipelines for EDA tasks and describe our approaches to accelerate the pipelines. We conduct extensive experiments to compare DataPrep.EDA with Pandas-profiling, the state-of-the-art EDA system in Python. The experiments show that DataPrep.EDA significantly outperforms Pandas-profiling in terms of both speed and user experience. DataPrep.EDA is open-sourced as an EDA component of DataPrep: https://github.com/sfu-db/dataprep.",2021,,
1789,,Machine Learning and Probabilistic Data Cleaning,,"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.",2019,,
1790,"Kohavi, Ron and Deng, Alex and Frasca, Brian and Walker, Toby and Xu, Ya and Pohlmann, Nils",Online Controlled Experiments at Large Scale,"a/b testing, randomized experiments, controlled experiments","Web-facing companies, including Amazon, eBay, Etsy, Facebook, Google, Groupon, Intuit, LinkedIn, Microsoft, Netflix, Shop Direct, StumbleUpon, Yahoo, and Zynga use online controlled experiments to guide product development and accelerate innovation. At Microsoft's Bing, the use of controlled experiments has grown exponentially over time, with over 200 concurrent experiments now running on any given day. Running experiments at large scale requires addressing multiple challenges in three areas: cultural/organizational, engineering, and trustworthiness. On the cultural and organizational front, the larger organization needs to learn the reasons for running controlled experiments and the tradeoffs between controlled experiments and other methods of evaluating ideas. We discuss why negative experiments, which degrade the user experience short term, should be run, given the learning value and long-term benefits. On the engineering side, we architected a highly scalable system, able to handle data at massive scale: hundreds of concurrent experiments, each containing millions of users. Classical testing and debugging techniques no longer apply when there are billions of live variants of the site, so alerts are used to identify issues rather than relying on heavy up-front testing. On the trustworthiness front, we have a high occurrence of false positives that we address, and we alert experimenters to statistical interactions between experiments. The Bing Experimentation System is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars, by allowing us to find and focus on key ideas evaluated through thousands of controlled experiments. A 1% improvement to revenue equals more than $10M annually in the US, yet many ideas impact key metrics by 1% and are not well estimated a-priori. The system has also identified many negative features that we avoided deploying, despite key stakeholders' early excitement, saving us similar large amounts.",2013,,10.1145/2487575.2488217
1791,"Amyrotos, Christos",Adaptive Visualizations for Enhanced Data Understanding and Interpretation,," In a data driven economy where data volume and dimensions are explosively increasing, businesses rely on business intelligence and analytics (BI&amp;A) platforms for analysing their data and coming to beneficial decisions. With the ever-growing generation of data, the process of data analysis is becoming more complicated for the business users, as the exploration of more demanding use cases increases. While the existing BI&amp;A platforms provide myriads of data visualizations that support data exploration, none of those account for the user’s individual differences, needs or requirements, and thus may hinder the user’s understanding of visual data and consequently their decision-making processes. This work embarks on an interdisciplinary endeavour to introduce a human-centred adaptive data visualizations framework in the context of business, as the core of an adaptive data analytics platform, that aims to enhance the business user’s decision making by increasing her understanding of data. The framework is built using a multi-dimensional human-centred user model that goes beyond traditional user characteristics and accounts for cognitive factors, domain expertise and experience and factors related to the business context i.e., data, visualizations and tasks; a data visualization engine that will recommend to the unique-user the best-fit data visualizations based on the abovementioned user model; and an intelligent data analytics component that enhances the efficiency and effectiveness of the data exploration process by leveraging user interactions during the explorations to further inform the user model on the user’s expertise and experience.",2021,,
1792,"McMenemy, David",The Internet of Everything—Introducing Privacy,,,2021,,
1793,"De-Arteaga, Maria and Herlands, William and Neill, Daniel B. and Dubrawski, Artur",Machine Learning for the Developing World,"developing countries, Global development","Researchers from across the social and computer sciences are increasingly using machine learning to study and address global development challenges. This article examines the burgeoning field of machine learning for the developing world (ML4D). First, we present a review of prominent literature. Next, we suggest best practices drawn from the literature for ensuring that ML4D projects are relevant to the advancement of development objectives. Finally, we discuss how developing world challenges can motivate the design of novel machine learning methodologies. This article provides insights into systematic differences between ML4D and more traditional machine learning applications. It also discusses how technical complications of ML4D can be treated as novel research questions, how ML4D can motivate new research directions, and where machine learning can be most useful.",2018,,10.1145/3210548
1794,,Authors’ Biographies/Index,,,2021,,
1795,"Chowdhury, S. M. Habibul Mursaleen and Jahan, Ferdous and Sara, Sarawat Murtaza and Nandi, Dip",Secured Blockchain Based Decentralised Internet: A Proposed New Internet,"Whisper, Bitcoin, Blockchain, Data Privacy, DApp, Decentralised Web, Cryptography, Server vulnerabilities, Peer-To-Peer Network, Smart Contracts, Encryption, Ethereum, Web 3.0","Throughout this paper, we try to describe with blockchain technology the decentralization of the internet. A decentralized network that encourages the internet to operate from the smartphone or tablet of anybody instead of centralized servers. A decentralized implementation would be based on a peer-to-peer network that is dependent on a user community. Their machines connected to the internet will host the network, not a community of more powerful servers. Each site would be distributed across thousands of nodes on various devices. The data is therefore not contained, owned by private storage facilities. There is therefore no central point to hack, and no way for an oligarchy of entities to take control of it. A proposed alternative was formed based on a systematic literature review that demonstrates that Internet decentralization is what this modern technology needs in order to address not only the weaknesses of current servers including server down issue, hacking and data manipulation or single point of failure, but also to prevent companies from monetizing the data of citizens through their server and to market them to the advertisers.",2020,,10.1145/3377049.3377083
1796,"Leznik, Mark and Grohmann, Johannes and Kliche, Nina and Bauer, Andr\'{e} and Seybold, Daniel and Eismann, Simon and Kounev, Samuel and Domaschka, J\""{o}rg","Same, Same, but Dissimilar: Exploring Measurements for Workload Time-Series Similarity","data sets, time series, distance metrics, time series similarity, workload analysis","Benchmarking is a core element in the toolbox of most systems researchers and is used for analyzing, comparing, and validating complex systems. In the quest for reliable benchmark results, a consensus has formed that a significant experiment must be based on multiple runs. To interpret these runs, mean and standard deviation are often used. In case of experiments where each run produces a time series, applying and comparing the mean is not easily applicable and not necessarily statistically sound. Such an approach ignores the possibility of significant differences between runs with a similar average. In order to verify this hypothesis, we conducted a survey of 1,112 publications of selected performance engineering and systems conferences canvassing open data sets from performance experiments. The identified 3 data sets purely rely on average and standard deviation. Therefore, we propose a novel analysis approach based on similarity analysis to enhance the reliability of performance evaluations. Our approach evaluates 12 (dis-)similarity measures with respect to their applicability in analysing performance measurements and identifies four suitable similarity measures. We validate our approach by demonstrating the increase in reliability for the data sets found in the survey.",2022,,10.1145/3489525.3511699
1797,"McMenemy, David",Ethics and Secure Gestures,,,2021,,
1798,,Preface,,"Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice provides a comprehensive resource on what has become the dominant paradigm for novel interaction design methods involving gesture, speech, text, and touch embedded in novel and emerging interfaces. These interfaces support smartphones, wearables, in-vehicle devices, virtual reality, robotic, the Internet of Things (IoT), brain–computer interaction, and many other applications that are now highly competitive commercially.This edited collection is written by international experts and pioneers in the field of digital signal processing (DSP) and machine learning (ML) for interactive systems. It provides a textbook for students, and a reference and technology roadmap for developers and professionals working in interaction design on emerging platforms. This introductory textbook presents theory chapters on statistical grounding, signal processing, and ML foundations for guiding the development of novel interactive systems. Additional chapters discuss case studies on smart cities, brain–computer interfaces (BCI), probabilistic text entry, secure gestures, personal context from mobile phones, building adaptive touch interfaces, and automotive user interfaces (UIs). The chapters on case studies also highlight an in-depth look at domain-specific language (DSL) and ML methods used, for example, in touch, gesture, electroencephalography (EEG), electrocardiography (ECG), and galvanic skin response (GSR) signals, or embedded sensor inputs. A common theme throughout is the ubiquitous support for humans as they go about their daily professional or personal activities.This introductory book provides walk-through examples of different DSP and ML techniques and their use in interactive systems. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal–multisensor systems. After each chapter an expert on the legal and ethical issues explores the wider ethical issues on how DSP and ML should be adopted and used in socially appropriate ways, to most effectively advance human performance during interaction with novel platforms.Parisa Eslambolchilar, Andreas Komninos, and Mark D. Dunlop, March 2020AcknowledgmentsWe would like to thank our external reviewers for their valuable feedback throughout the writing process.",2021,,
1799,"Liu, Xingchen and Zhao, Boyu and Qian, Haotian and Liu, Yuhang",Multidimensional Data Mining on the Early Scientific Talents of China,"Data-mining, Education, Cultivation of high-level talents","The cultivation of high-level talents in either scientific or engineering domain is of significant importance to the development of a country. From the perspective of data science, this paper takes the group of academicians of the Chinese Academy of Sciences and the Chinese Academy of Engineering as an example to explore the method of cultivating high-end talents. Through multidimensional data analysis, it is of great significance to explore the spatial pattern and time evolution characteristics of the first generation of top natural science talents in China, to deepen the understanding of the growth and education laws of talents, optimize top-level design, and implement targeted scientific policies. It is found that Chinese culture and Chinese native universities have made significant contributions for the early scientific talents of China, and the analysis method of data science can be used to facilitate the education innovation.",2020,,10.1145/3414274.3414509
1800,"Seabolt, Ed and Kandogan, Eser and Roth, Mary",Contextual Intelligence for Unified Data Governance,"Graph, Analytics, Data Governance, Context","Current data governance techniques are very labor-intensive, as teams of data stewards typically rely on best practices to transform business policies into governance rules. As data plays an increasingly key role in today's data-driven enterprises, current approaches do not scale to the complexity and variety present in the data ecosystem of an enterprise as an increasing number of data requirements, use cases, applications, tools and systems come into play. We believe techniques from artificial intelligence and machine learning have potential to improve discoverability, quality and compliance in data governance. In this paper, we propose a framework for 'contextual intelligence', where we argue for (1) collecting and integrating contextual metadata from variety of sources to establish a trusted unified repository of contextual data use across users and applications, and (2) applying machine learning and artificial intelligence techniques over this rich contextual metadata to improve discoverability, quality and compliance in governance practices. We propose an architecture that unifies governance across several systems, with a graph serving as a core repository of contextual metadata, accurately representing data usage across the enterprise and facilitating machine learning, We demonstrate how our approach can enable ML-based recommendations in support of governance best practices.",2018,,10.1145/3211954.3211955
1801,"Hu, Yanhua and Bai, Xianyang and Sun, Shuyang",Readiness Assessment of Open Government Data Programs: A Case of Shenzhen,"Readiness assessment, Open government data, Open data","More and more cities in China are implementing various open government data initiatives for improving their governance. Little research, however, has been done in evaluating the readiness of individual governments in pursuing such initiatives. This paper presents a case study of the readiness assessment on the adoption of open data programs in Shenzhen based on the open data readiness assessment framework of the World Bank. The result shows that there are several issues including developing an action plan, providing privacy and ownership solutions, designating a unified administration, and implementing consistent data management policies and standards that need to be adequately addressed for the effective adoption of the open data program in the city.",2016,,10.1145/2912160.2912179
1802,"Vilela, J\'{e}ssyka and Gon\c{c}alves, Enyo and Holanda, Ana and Figueiredo, Bruno and Castro, Jaelson","Retrospective, Relevance, and Trends of SAC Requirements Engineering Track","systematic mapping study, retrospective, scoping study, relevance, requirements engineering, trends, SAC, symposium on applied computing","Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: This study aims to investigate how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 79 papers over the 8 previous SAC RE-Track editions, which were analyzed and discussed.",2016,,10.1145/2851613.2851757
1803,"Wang, Rongxiao and Chen, Bin and Wang, Yiduo and Zhu, Zhengqiu and Ma, Liang and Qiu, Xiaogang",The Air Contaminant Dispersion Prediction by the Integration of the Neural Network and AermodSystem,"Particle Swarm Optimization, Artificial neural network, Atmospheric dispersion prediction, AermodSystem, Source estimation","Air pollution caused by industrial production has become a serious problem for public health. This challenging problem promotes the development of the research in the air contaminant dispersion (ADS) prediction, for the management of the emission and leak accident. However, conventional ADS models can hardly meet the requirement of both accuracy and efficiency. The data model, like the artificial neural network (ANN) provides a feasible way of forecasting the dispersion with high accuracy and efficiency. However, the construction of the ANN for prediction needs plenty of data, which is impractical to obtain in most emission cases. To address this problem, an ADS simulation software AermodSystem is applied to build the simulated dispersion scenarios and provide synthetic dataset for the model training and test. Based on the synthetic data set, the ANN prediction model is established, and evaluated on the test set, as well as the Gaussian model. Further, these two models are served as the forward dispersion model and combined with the Particle Swarm Optimization (PSO) for source estimation. The results verify the effectiveness of the proposed model and indicate that the ANN together with the AermodSystem as the data generator is feasible in the air contaminant dispersion forecast and the source estimation of a particular case.",2018,,10.1145/3284103.3284114
1804,"Agrawal, Rakesh and Ailamaki, Anastasia and Bernstein, Philip A. and Brewer, Eric A. and Carey, Michael J. and Chaudhuri, Surajit and Doan, AnHai and Florescu, Daniela and Franklin, Michael J. and Garcia-Molina, Hector and Gehrke, Johannes and Gruenwald, Le and Haas, Laura M. and Halevy, Alon Y. and Hellerstein, Joseph M. and Ioannidis, Yannis E. and Korth, Hank F. and Kossmann, Donald and Madden, Samuel and Magoulas, Roger and Ooi, Beng Chin and O'Reilly, Tim and Ramakrishnan, Raghu and Sarawagi, Sunita and Stonebraker, Michael and Szalay, Alexander S. and Weikum, Gerhard",The Claremont Report on Database Research,,"In late May, 2008, a group of database researchers, architects, users and pundits met at the Claremont Resort in Berkeley, California to discuss the state of the research field and its impacts on practice. This was the seventh meeting of this sort in twenty years, and was distinguished by a broad consensus that we are at a turning point in the history of the field, due both to an explosion of data and usage scenarios, and to major shifts in computing hardware and platforms. Given these forces, we are at a time of opportunity for research impact, with an unusually large potential for influential results across computing, the sciences and society. This report details that discussion, and highlights the group's consensus view of new focus areas, including new database engine architectures, declarative programming languages, the interplay of structured and unstructured data, cloud data services, and mobile and virtual worlds. We also report on discussions of the community's growth, including suggestions for changes in community processes to move the research agenda forward, and to enhance impact on a broader audience.",2008,,10.1145/1462571.1462573
1805,"Lukyanenko, Roman and Samuel, Binny M.",Are All Classes Created Equal? Increasing Precision of Conceptual Modeling Grammars,"Conceptual modeling, database design","Recent decade has seen a dramatic change in the information systems landscape that alters the ways we design and interact with information technologies, including such developments as the rise of business analytics, user-generated content, and NoSQL databases, to name just a few. These changes challenge conceptual modeling research to offer innovative solutions tailored to these environments. Conceptual models typically represent classes (categories, kinds) of objects rather than concrete specific objects, making the class construct a critical medium for capturing domain semantics. While representation of classes may differ between grammars, a common design assumption is what we term different semantics same syntax (D3S). Under D3S, all classes are depicted using the same syntactic symbols. Following recent findings in psychology, we introduce a novel assumption semantics-contingent syntax (SCS) whereby syntactic representations of classes in conceptual models may differ based on their semantic meaning. We propose a core SCS design principle and five guidelines pertinent for conceptual modeling. We believe SCS carries profound implications for theory and practice of conceptual modeling as it seeks to better support modern information environments.",2017,,10.1145/3131780
1806,"Mei, Songzhu and Liu, Cong and Wang, Qinglin and Su, Huayou",Model Provenance Management in MLOps Pipeline,"Machine learning engineering, Artificial Intelligence, Model provenance, MLOps",,2022,,10.1145/3512850.3512861
1807,"Celes, Clayson and Boukerche, Azzedine and Loureiro, Antonio A. F.","Mobility Trace Analysis for Intelligent Vehicular Networks: Methods, Models, and Applications","vanet, topology, mobility, data analysis, routing, Vehicular networks, survey, data mining","Intelligent vehicular networks emerge as a promising technology to provide efficient data communication in transportation systems and smart cities. At the same time, the popularization of devices with attached sensors has allowed the obtaining of a large volume of data with spatiotemporal information from different entities. In this sense, we are faced with a large volume of vehicular mobility traces being recorded. Those traces provide unprecedented opportunities to understand the dynamics of vehicular mobility and provide data-driven solutions. In this article, we give an overview of the main publicly available vehicular mobility traces; then, we present the main issues for preprocessing these traces. Also, we present the methods used to characterize and model mobility data. Finally, we review existing proposals that apply the hidden knowledge extracted from the mobility trace for vehicular networks. This article provides a survey on studies that use vehicular mobility traces and provides a guideline for the proposition of data-driven solutions in the domain of vehicular networks. Moreover, we discuss open research problems and give some directions to undertake them.",2021,,10.1145/3446679
1808,"Ma, Yuxing and Bogart, Chris and Amreen, Sadika and Zaretzki, Russell and Mockus, Audris",World of Code: An Infrastructure for Mining the Universe of Open Source VCS Data,"software supply chain, software ecosystem, software mining","Open source software (OSS) is essential for modern society and, while substantial research has been done on individual (typically central) projects, only a limited understanding of the periphery of the entire OSS ecosystem exists. For example, how are tens of millions of projects in the periphery interconnected through technical dependencies, code sharing, or knowledge flows? To answer such questions we a) create a very large and frequently updated collection of version control data for FLOSS projects named World of Code (WoC) and b) provide basic tools for conducting research that depends on measuring interdependencies among all FLOSS projects. Our current WoC implementation is capable of being updated on a monthly basis and contains over 12B git objects. To evaluate its research potential and to create vignettes for its usage, we employ WoC in conducting several research tasks. In particular, we find that it is capable of supporting trend evaluation, ecosystem measurement, and the determination of package usage. We expect WoC to spur investigation into global properties of OSS development leading to increased resiliency of the entire OSS ecosystem. Our infrastructure facilitates the discovery of key technical dependencies, code flow, and social networks that provide the basis to determine the structure and evolution of the relationships that drive FLOSS activities and innovation.",2019,,10.1109/MSR.2019.00031
1809,"Nguyen-Duc, Anh and Abrahamsson, Pekka",Continuous Experimentation on Artificial Intelligence Software: A Research Agenda,,"Moving from experiments to industrial level AI software development requires a shift from understanding AI/ ML model attributes as a standalone experiment to know-how integrating and operating AI models in a large-scale software system. It is a growing demand for adopting state-of-the-art software engineering paradigms into AI development, so that the development efforts can be aligned with business strategies in a lean and fast-paced manner. We describe AI development as an “unknown unknown” problem where both business needs and AI models evolve over time. We describe a holistic view of an iterative, continuous approach to develop industrial AI software basing on business goals, requirements and Minimum Viable Products. From this, five areas of challenges are presented with the focus on experimentation. In the end, we propose a research agenda with seven questions for future studies.",2020,,
1810,"Bromander, Siri and Swimmer, Morton and Muller, Lilly Pijnenburg and J\o{}sang, Audun and Eian, Martin and Skj\o{}tskift, Geir and Borg, Fredrik",Investigating Sharing of Cyber Threat Intelligence and Proposing A New Data Model for Enabling Automation in Knowledge Representation and Exchange,"security, knowledge graph, ontology, Cyber threat intelligence","For a strong, collective defense in the digital domain, we need to produce, consume, analyze, and share cyber threat intelligence. With an increasing amount of available information, we need automation to ensure adequate efficiency. We present the results from a questionnaire investigating the use of standards and standardization and how practitioners share and use cyber threat intelligence (CTI). We propose a strict data model for CTI that enables consumption of all relevant data, data validation, and analysis of consumed content. The main contribution of this article is insight into how CTI is shared and used by practitioners, and the strictness of the data model that enforces input of information and enables automation and deduction of new knowledge.",2021,,10.1145/3458027
1811,"Mehta, R. Vasanth Kumar and Sankarasubramaniam, B. and Rajalakshmi, S.",An Algorithm for Fuzzy-Based Sentence-Level Document Clustering for Micro-Level Contradiction Analysis,"contradiction analysis, information-retrieval, document clustering","Contradiction Analysis is one of the popular text-mining operations in which a document whose content is contradictory to the theme of a set of documents is identified. It is a means to identifying Outlier documents that do not confirm to the overall sense conveyed by other documents. Most of the existing techniques perform document-level comparisons, ignoring the sentence-level semantics, often leading to loss of vital information. Applications in domains like Defence and Healthcare require high levels of accuracy and identification of micro-level contradictions are vital. In this paper, we propose an algorithm for identifying contradictory documents using sentence-level clustering technique along with an optimization feature. A novel visualization scheme is also suggested to present the results to an end-user.",2012,,10.1145/2345396.2345413
1812,"Li, Pei and Dai, Chaofan and Wang, Wenqian",Inconsistent Data Detection Based on Maximum Dependency Set,"inconsistent data, dynamic domain adjustment, Conditional functional dependency (CFDs), maximum dependency set (MDS)","For1 the incomplete detection of inconsistent data by conditional functional dependency(CFDs), this paper proposes a dependency lifting algorithm (DLA) based on maximum dependency set (MDS), which detects inconsistent data in data set by acquiring recessive conditional functional dependencies (RCFDs) in CFDs. Presenting the dynamic domain adjustment, setting forward and backward pointers of numerical change to improve the enumeration process in original algorithm, the applicability of the algorithm to the continuous attributes is raised too. Then, this paper provides the algorithm flow and pseudo code of dynamic domain adjustment and the DLA, analyses the convergence and time complexity of them. Finally, the validity of the DLA is verified by comparing the detection accuracy and time-cost.",2018,,10.1145/3207677.3277983
1813,"Rezig, El Kindi and Ouzzani, Mourad and Elmagarmid, Ahmed K. and Aref, Walid G. and Stonebraker, Michael",Towards an End-to-End Human-Centric Data Cleaning Framework,,"Data Cleaning refers to the process of detecting and fixing errors in the data. Human involvement is instrumental at several stages of this process such as providing rules or validating computed repairs. There is a plethora of data cleaning algorithms addressing a wide range of data errors (e.g., detecting duplicates, violations of integrity constraints, and missing values). Many of these algorithms involve a human in the loop, however, this latter is usually coupled to the underlying cleaning algorithms. In a real data cleaning pipeline, several data cleaning operations are performed using different tools. A high-level reasoning on these tools, when combined to repair the data, has the potential to unlock useful use cases to involve humans in the cleaning process. Additionally, we believe there is an opportunity to benefit from recent advances in active learning methods to minimize the effort humans have to spend to verify data items produced by tools or humans. There is currently no end-to-end data cleaning framework that systematically involves humans in the cleaning pipeline regardless of the underlying cleaning algorithms. In this paper, we present opportunities that this framework could offer, and highlight key challenges that need to be addressed to realize this vision. We present a design vision and discuss scenarios that motivate the need for this framework to judiciously assist humans in the cleaning process.",2019,,10.1145/3328519.3329133
1814,"Thelisson, Eva and Sharma, Kshitij and Salam, Hanan and Dignum, Virginia",The General Data Protection Regulation: An Opportunity for the HCI Community?,"quality standards, privacy, labeling, codes of conduct, soft law, design, responsible innovation","With HCI, researchers conduct studies in interdisciplinary projects involving massive volume of data, artificial intelligence and machine learning capabilities. Awareness of the responsibility is emerging as a key concern for the HCI community.This Community will be impacted by the General Data Protection Regulation (GDPR) [5], that will enter into force on the 25th of May 2018. From that date, each data controller and data processor will face an increase of its legal obligations (in particular its accountability) under certain conditions.The GDPR encourages the adoption of Soft Law mechanisms, approved by the national competent authority on data protection, to demonstrate the compliance to the Regulation. Approved Guidelines, Codes of Conducts, Labeling, Marks and Seals dedicated to data protection, as well as certification mechanisms are some of the options proposed by the GDPR.There may be discrepancies between the realities of HCI fieldwork and the formal process of obtaining Soft Law approval by Competent Authorities dedicated to data protection. Given these issues, it is important for researchers to reflect on legal and ethical encounters in HCI research as a community.This workshop will provide a forum for researchers to share experiences about Soft Law they have put in place to increase Trust, Transparency and Accountability among the shareholders. These discussions will be used to develop a white paper of practical Soft Law mechanisms (certification, labeling, marks, seals...) emerging in HCI research with the aim to demonstrate that the GDPR may be an opportunity for the HCI community.",2018,,10.1145/3170427.3170632
1815,"Wang, BingQiang and See, Simon",Accelerate High Throughput Analysis for Genome Sequencing with GPU,,,2012,,
1816,"Momen, Nurul and Bock, Sven and Fritsch, Lothar",Accept - Maybe - Decline: Introducing Partial Consent for the Permission-Based Access Control Model of Android,"partial consent, data protection, access control, privacy","The consent to personal data sharing is an integral part of modern access control models on smart devices. This paper examines the possibility of registering conditional consent which could potentially increase trust in data sharing. We introduce an indecisive state of consenting to policies that will enable consumers to evaluate data services before fully committing to their data sharing policies. We address technical, regulatory, social, individual and economic perspectives for inclusion of partial consent within an access control mechanism. Then, we look into the possibilities to integrate it within the access control model of Android by introducing an additional button in the interface--Maybe. This article also presents a design for such implementation and demonstrates feasibility by showcasing a prototype built on Android platform. Our effort is exploratory and aims to shed light on the probable research direction.",2020,,10.1145/3381991.3395603
1817,"Rao, Jinmeng and Gao, Song and Zhu, Xiaojin",VTSV: A Privacy-Preserving Vehicle Trajectory Simulation and Visualization Platform Using Deep Reinforcement Learning,,"Trajectory data is among the most sensitive data and the society increasingly raises privacy concerns. In this demo paper, we present a privacy-preserving Vehicle Trajectory Simulation and Visualization (VTSV) web platform (demo video: https://youtu.be/NY5L4bu2kTU), which automatically generates navigation routes between given pairs of origins and destinations and employs a deep reinforcement learning model to simulate vehicle trajectories with customized driving behaviors such as normal driving, overspeed, aggressive acceleration, and aggressive turning. The simulated vehicle trajectory data contain high-sample-rate of attributes including GPS location, speed, acceleration, and steering angle, and such data are visualized in VTSV using streetscape.gl, an autonomous driving data visualization framework. Location privacy protection methods such as origin-destination geomasking and trajectory k-anonymity are integrated into the platform to support privacy-preserving trajectory data generation and publication. We design two application scenarios to demonstrate how VTSV performs location privacy protection and customize driving behavior, respectively. The demonstration shows that VTSV is able to mitigate data privacy, sparsity, and imbalance sampling issues, which offers new insights into driving trajectory simulation and GeoAI-powered privacy-preserving data publication.",2021,,
1818,"Wu, Haiqin and Wang, Liangmin and Xue, Guoliang and Tang, Jian and Yang, Dejun",Enabling Data Trustworthiness and User Privacy in Mobile Crowdsensing,,"Ubiquitous mobile devices with rich sensors and advanced communication capabilities have given rise to mobile crowdsensing systems. The diverse reliabilities of mobile users and the openness of sensing paradigms raise concerns for data trustworthiness, user privacy, and incentive provision. Instead of considering these issues as isolated modules in most existing researches, we comprehensively capture both conflict and inner-relationship among them. In this paper, we propose a holistic solution for trustworthy and privacy-aware mobile crowdsensing with no need of a trusted third party. Specifically, leveraging cryptographic technologies, we devise a series of protocols to enable benign users to request tasks, contribute their data, and earn rewards anonymously without any data linkability. Meanwhile, an anonymous trust/reputation model is seamlessly integrated into our scheme, which acts as reference for our fair incentive design, and provides evidence to detect malicious users who degrade the data trustworthiness. Particularly, we first propose the idea of limiting the number of issued pseudonyms which serves to efficiently tackle the anonymity abuse issue. Security analysis demonstrates that our proposed scheme achieves stronger security with resilience against possible collusion attacks. Extensive simulations are presented which demonstrate the efficiency and practicality of our scheme.",2019,,10.1109/TNET.2019.2944984
1819,"Naumann, Felix",Data Profiling Revisited,,"Data profiling comprises a broad range of methods to efficiently analyze a given data set. In a typical scenario, which mirrors the capabilities of commercial data profiling tools, tables of a relational database are scanned to derive metadata, such as data types and value patterns, completeness and uniqueness of columns, keys and foreign keys, and occasionally functional dependencies and association rules. Individual research projects have proposed several additional profiling tasks, such as the discovery of inclusion dependencies or conditional functional dependencies.Data profiling deserves a fresh look for two reasons: First, the area itself is neither established nor defined in any principled way, despite significant research activity on individual parts in the past. Second, more and more data beyond the traditional relational databases are being created and beg to be profiled. The article proposes new research directions and challenges, including interactive and incremental profiling and profiling heterogeneous and non-relational data.",2014,,10.1145/2590989.2590995
1820,"Qian, Bin and Su, Jie and Wen, Zhenyu and Jha, Devki Nandan and Li, Yinhao and Guan, Yu and Puthal, Deepak and James, Philip and Yang, Renyu and Zomaya, Albert Y. and Rana, Omer and Wang, Lizhe and Koutny, Maciej and Ranjan, Rajiv",Orchestrating the Development Lifecycle of Machine Learning-Based IoT Applications: A Taxonomy and Survey,"orchestration, IoT, machine learning, deep learning","Machine Learning (ML) and Internet of Things (IoT) are complementary advances: ML techniques unlock the potential of IoT with intelligence, and IoT applications increasingly feed data collected by sensors into ML models, thereby employing results to improve their business processes and services. Hence, orchestrating ML pipelines that encompass model training and implication involved in the holistic development lifecycle of an IoT application often leads to complex system integration. This article provides a comprehensive and systematic survey of the development lifecycle of ML-based IoT applications. We outline the core roadmap and taxonomy and subsequently assess and compare existing standard techniques used at individual stages.",2020,,10.1145/3398020
1821,"Lipuntsov, Yuri P.",Three Types of Data Exchange in the Open Government Information Projects,"Core Component, Linked Data, Data Standardization, Open Data, Open Government, Shared Environment","Open government as a new system of public administration requires a qualitatively new level of information support for digital interactions of agencies, as well as between government and citizens, experts, and businesses. This article examines three categories of government counterparties, interactions with which are based on different principles: community of interest; subject areas; a loosely coupled environment. The public sector has now many information projects, and most of them deal with the data exchange, data delivery from and to the environment. The understanding of an environment is various for different projects. The categorization of projects depends on nature and the level of intellectual interaction with the external environment needed for the correct definition of project objectives, assessment of effectiveness. With the growing number of users and the transition to an open world, semantic principles are becoming more significant. Given the shift from systems integration to the semantic method, the role of subject matter expert is growing substantially.",2014,,10.1145/2729104.2729129
1822,"Altenburger, Kristen M. and Ho, Daniel E.",Is Yelp Actually Cleaning Up the Restaurant Industry? A Re-Analysis on the Relative Usefulness of Consumer Reviews,"food safety, regulation, replication, Yelp, consumer reviews","Social media provides the government with novel methods to improve regulation. One leading case has been the use of Yelp reviews to target food safety inspections. While previous research on data from Seattle finds that Yelp reviews can predict unhygienic establishments, we provide a more cautionary perspective. First, we show that prior results are sensitive to what we call “Extreme Imbalanced Sampling”: extreme because the dataset was restricted from roughly 13k inspections to a sample of only 612 inspections with only extremely high or low inspection scores, and imbalanced by not accounting for class imbalance in the population. We show that extreme imbalanced sampling is responsible for claims about the power of Yelp information in the original classification setup. Second, a re-analysis that utilizes the full dataset of 13k inspections and models the full inspection score (regression instead of classification) shows that (a) Yelp information has lower predictive power than prior inspection history and (b) Yelp reviews do not significantly improve predictions, given existing information about restaurants and inspection history. Contrary to prior claims, Yelp reviews do not appear to aid regulatory targeting. Third, this case study highlights critical issues when using social media for predictive models in governance and corroborates recent calls for greater transparency and reproducibility in machine learning.",2019,,10.1145/3308558.3313683
1823,"Zimmermann, Olaf and Pautasso, Cesare and L\""{u}bke, Daniel and Zdun, Uwe and Stocker, Mirko",Data-Oriented Interface Responsibility Patterns: Types of Information Holder Resources,,"Remote Application Programming Interfaces (APIs) are used in almost any distributed system today, for instance in microservices-based systems, and are thus enablers for many digitalization efforts. API design not only impacts whether software provided as a service is easy and efficient to develop applications with, but also affects the long term evolution of the software system. In general, APIs are responsible for providing remote and controlled access to the functionality provided as services; however, APIs often are also used to expose and share information. We focus on such data-related aspects of microservice APIs in this paper. Depending on the life cycle of the information published through the API, its mutability and the endpoint role, data-oriented APIs can be designed following patterns such as Operational Data Holder, Master Data Holder, Reference Data Holder, Data Transfer Holder, and Link Lookup Resource. Known uses and examples of the patterns are drawn from public Web APIs as well as application development and integration projects we have been involved in.",2020,,10.1145/3424771.3424821
1824,"Russell, Daniel M. and Convertino, Gregorio and Kittur, Aniket and Pirolli, Peter and Watkins, Elizabeth Anne",Sensemaking in a Senseless World: 2018 Workshop Abstract,"sensemaking, information understanding, collaborative work","Sensemaking is a common activity in the analysis of a large or complex amount of information. This active area of HCI research asks how DO people come to understand such difficult sets of information? The information workplace is increasing dominated by high velocity, high volume, complex information streams. At the same time, understanding how sensemaking operates has become an urgent need in an era of increasingly unreliable news and information sources. While there has been a huge amount of work in this space, the research involved is scattered over a number of different domains with differing approaches. This workshop will focus on the most recent work in sensemaking, the activities, technologies and behaviors that people do when making sense of their complex information spaces. In the second part of the workshop we will work to synthesize a cross-disciplinary view of how sensemaking works in people, along with the human behaviors, biases, proclivities, and technologies required to support it.",2018,,10.1145/3170427.3170636
1825,"Gotz, David and Sun, Shun and Cao, Nan and Kundu, Rita and Meyer, Anne-Marie",Adaptive Contextualization Methods for Combating Selection Bias during High-Dimensional Visualization,"selection bias, visual analytics, Visualization, intelligent visual interfaces, exploratory analysis","Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. This article describes Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. The AC approach (1) monitors and models a user’s visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. This article expands on an earlier article presented at ACM IUI 2016 [16] by providing a more detailed review of the AC methodology and additional evaluation results.",2017,,10.1145/3009973
1826,"Draheim, Dirk",On Architecture of E-Government Ecosystems: From e-Services to e-Participation: [IiWAS'2020 Keynote],"public key infrastructures, data exchange layers, data governance, digital transformation, e-governance, Fiware, X-Road, GAIA-X, consent management, e-government, persistent messaging","The ""digital transformation"" is perceived as the key enabler for increasing wealth and well-being by many in politics, media and among the citizens alike. In the same vein, e-Government steadily received and receives more and more attention. e-Government gives rise to complex, large-scale system landscapes consisting of many players and technological systems - and we call such system landscapes e-Government ecosystems. In this talk, we are interested in the architecture of e-Government ecosystems. ""Form ever follows function."" Now, what is the function that determines e-Government? And what is the form in which it manifests? After briefly reviewing the purpose of e-Government from a democratic as well as a technocratic viewpoint, we will discover the primacy of the state's institutional design in the architecture of e-Government ecosystems. From there, we will arrive at the notion of data governance architecture, which provides the core of all system design efforts in e-Government. A data governance architecture maps data assets to accountable legal entities and represents the essence of co-designing institutions and technological systems. Against the background of what has been achieved, we review a series of established and emerging technologies that have been explicitly designed for or are otherwise relevant for building e-Government systems.",2020,,10.1145/3428757.3429972
1827,"Wang, Weina and Ying, Lei and Zhang, Junshan","The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms and Fundamental Limits","mechanism design, differential privacy, incentive mechanism, game theory, strategic data subjects","We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. The private data of each individual represents her knowledge about an underlying state, which is the information that the data collector desires to learn. Different from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Then, an individual takes full control of its own data privacy and reports only a privacy-preserving version of her data. In this paper, the value of ε units of privacy is measured by the minimum payment of all nonnegative payment mechanisms, under which an individual's best response at a Nash equilibrium is to report the data with a privacy level of ε. The higher ε is, the less private the reported data is. We derive lower and upper bounds on the value of privacy which are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use less amount of payment to buy ε units of privacy, and the upper bound is given by an achievable payment mechanism that we designed. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given learning accuracy target, and show that the total payment of the designed mechanism is at most one individual's payment away from the minimum.",2016,,10.1145/2896377.2901461
1828,"Wang, Weina and Ying, Lei and Zhang, Junshan","The Value of Privacy: Strategic Data Subjects, Incentive Mechanisms and Fundamental Limits","differential privacy, incentive mechanism, strategic data subjects, game theory, mechanism design","We study the value of data privacy in a game-theoretic model of trading private data, where a data collector purchases private data from strategic data subjects (individuals) through an incentive mechanism. The private data of each individual represents her knowledge about an underlying state, which is the information that the data collector desires to learn. Different from most of the existing work on privacy-aware surveys, our model does not assume the data collector to be trustworthy. Then, an individual takes full control of its own data privacy and reports only a privacy-preserving version of her data. In this paper, the value of ε units of privacy is measured by the minimum payment of all nonnegative payment mechanisms, under which an individual's best response at a Nash equilibrium is to report the data with a privacy level of ε. The higher ε is, the less private the reported data is. We derive lower and upper bounds on the value of privacy which are asymptotically tight as the number of data subjects becomes large. Specifically, the lower bound assures that it is impossible to use less amount of payment to buy ε units of privacy, and the upper bound is given by an achievable payment mechanism that we designed. Based on these fundamental limits, we further derive lower and upper bounds on the minimum total payment for the data collector to achieve a given learning accuracy target, and show that the total payment of the designed mechanism is at most one individual's payment away from the minimum.",2016,,10.1145/2964791.2901461
1829,"Zhicheng, Dai and Feng, Liu",Evaluation of the Smart Campus Information Portal,"Smart campus, Campus information portal, Comprehensive evaluation, Index system","As the internet wave swept the world, ""Internet plus education"" came into being. Smart campus design and construction has since become a research hotspot. The Campus Information Portal (CIP) plays an increasingly important role in the management of smart campuses. That is why, conducting a comprehensive evaluation study on the construction level of campus information portals is necessary. By combining CIP's own characteristics and incorporating intelligent needs, a comprehensive evaluation index system for CIP was developed. An Analytic Hierarchy Process (AHP) was used to determine index weights, while a Fuzzy Comprehensive Evaluation (FCE) was used to calculate the quantitative scores of the evaluation objects. We selected 10 representative Chinese universities for a comprehensive CIP evaluation and experimental analysis. We analyze the final results of the study, evaluate the validity of our process and methods and finally provide guidance for the construction of a smart campus information portal.",2018,,10.1145/3291078.3291083
1830,"Cai, Hongxia and Tan, Qiqi",Blockchain-Based Data Control for Complex Product Assembly Collaboration Process,"Blockchain, Complex product assembly, Collaborative Production","The collaborative development model of complex products brings the challenge to the data interaction management. There are many manufacturers and suppliers involved in the whole life cycle of the assembly process, which makes it difficult to ensure the data security and traceability. The Hyperledger-Fabric architecture in blockchain technology has modular design, pluggable architecture, complete authority control and security, which can well solve the data security and traceability management in the collaborative development of complex products. Therefore, this paper proposes a framework based on the Hyperledger-Fabric architecture of blockchain for the whole life cycle data management of complex products. We also demonstrate the effectiveness of our proposed new framework integrating blockchain technology through the case of quality data control during the aircraft final assembly collaborative process.",2021,,10.1145/3488838.3488873
1831,"Maurino, Andrea and Rula, Anisa and von, Bj\o{}rn Marius and Gomez, Mauricio Soto and Elves\ae{}ter, Brian and Roman, Dumitru",Modelling and Linking Company Data in the EuBusinessGraph Platform,"Record Linkage, RDF, Entity Matching, Company data","In the business environment, knowledge of company data is essential for a variety of tasks. The European funded project euBusinessGraph enables the establishment of a company data platform where data providers and consumers can publish and access company data. The core of the platform is the semantic data model that is the conceptual representation of company data in a common way so that it is easier to share and interlink company data. In this paper we show how the unified model and Grafterizer, a tool for manipulating and transforming raw data into Linked Data, support the linking challenge proposed in FEIII 2019. Results show that geographical enrichment of RDF data supports the interlinking process between company entities in different datasets.",2019,,10.1145/3336499.3338012
1832,"Esnaola-Gonzalez, Iker and Berm\'{u}dez, Jes\'{u}s and Fern\'{a}ndez, Izaskun and Fern\'{a}ndez, Santiago and Arnaiz, Aitor",Towards a Semantic Outlier Detection Framework in Wireless Sensor Networks,"Wireless Sensor Network, Semantic Technologies, Outlier Detection, Knowledge Discovery in Databases","Outlier detection in the preprocessing phase of Knowledge Discovery in Databases (KDD) processes has been a widely researched topic for many years. However, identifying the potential outlier cause still remains an unsolved challenge even though it could be very helpful for determining what actions to take after detecting it. Furthermore, conventional outlier detection methods might still overlook outliers in certain complex contexts. In this article, Semantic Technologies are used to contribute overcoming these problems by proposing the SemOD (Semantic Outlier Detection) Framework. This framework guides the data-scientist towards the detection of certain types of outliers in WSNs (Wireless Sensor Network). Feasibility of the approach has been tested in outdoor temperature sensors and results show that the proposed approach is generic enough to apply it to different sensors, even improving the accuracy, specificity and sensitivity of outlier detection as well as spotting their potential cause.",2017,,10.1145/3132218.3132226
1833,"Gotz, David and Sun, Shun and Cao, Nan",Adaptive Contextualization: Combating Bias During High-Dimensional Visualization and Data Selection,"intelligent visual interfaces, exploratory analysis, visual analytics, visualization","Large and high-dimensional real-world datasets are being gathered across a wide range of application disciplines to enable data-driven decision making. Interactive data visualization can play a critical role in allowing domain experts to select and analyze data from these large collections. However, there is a critical mismatch between the very large number of dimensions in complex real-world datasets and the much smaller number of dimensions that can be concurrently visualized using modern techniques. This gap in dimensionality can result in high levels of selection bias that go unnoticed by users. The bias can in turn threaten the very validity of any subsequent insights. In this paper, we present Adaptive Contextualization (AC), a novel approach to interactive visual data selection that is specifically designed to combat the invisible introduction of selection bias. Our approach (1) monitors and models a user's visual data selection activity, (2) computes metrics over that model to quantify the amount of selection bias after each step, (3) visualizes the metric results, and (4) provides interactive tools that help users assess and avoid bias-related problems. We also share results from a user study which demonstrate the effectiveness of our technique.",2016,,10.1145/2856767.2856779
1834,"Seto, Toshikazu and Sekimoto, Yoshihide and Asahi, Kosuke and Endo, Takahiro",Constructing a Digital City on a Web-3D Platform: Simultaneous and Consistent Generation of Metadata and Tile Data from a Multi-Source Raw Dataset,"infrastructure 3D tiles, digital city, Mapbox GL JS, Deck.GL","In this study, we develop a platform that can display approximately 20 types of data via a web browser to realize a digital twin of a wider area, including a detailed reading display of block units and individual three-dimensional point cloud data (point cloud) of a city. Using actual data, we examine if the data model and visualization design correspond with the zoom level. Owing to the comparative examination of the wide-area display performance and the map representation design in a JavaScript-based open-source library, we were able to develop a platform with light architecture and an easily customizable display. Furthermore, prototyping, based on Mapbox GL JS and Deck.GL, and the display of spatiotemporal flow layers, such as background maps, point cloud data in many places, dozens of layer display types, and the General Transit Feed Specification (GTFS) allowed for the seamless transition from the local government to the wide-area display in the prefecture unit in approximately 10-20 s.It is recommended that this digital smart city platform should be standardized by other local governments, especially in areas where higher-order data visualization is yet to advance. To display this digital city in a lightweight environment, we consider the digital data situation of local governments in Japan. It is necessary to define the visualized design for each zoom level according to the characteristics of the data. We then arranged the display model of each zoom level for 20 types of urban infrastructure data related to the digital smart city by referring to the style schema of the tile form. Through these tasks, we organized the commonality and optimization of data models and formats.",2020,,10.1145/3423455.3430316
1835,"Schmeck, Hartmut and Monti, Antonello and Hagenmeyer, Veit",Energy Informatics: Key Elements for Tomorrow's Energy System,,,2022,,10.1145/3511666
1836,"Zhang, Jiamin",Potential Energy Saving Estimation for Retrofit Building with ASHRAE-Great Energy Predictor III Using Machine Learning,"energy saving, machine learning, green architecture, retrofit building","Energy is material basis for social development and conflicting issue of economic development around the world. With the continuous urbanization, the energy consumption of buildings will be further increased, accounting for about 40% of the total energy consumption eventually. Thus, enhancement of energy efficiency of the buildings has become an essential issue to reduce the amount of gas emission as well as fossil fuel consumption. Delivering a high-quality built environment in an energy efficient way is the crucial key to energy conservation. Energy efficiency retrofit for buildings is considered to be a promising way to achieve energy savings. Machine learning provides the ability to learn from data using multiple computer algorithms. This paper introduces several algorithms, including random forest and Light GBM, to analyze building energy consumption based on the data from Kaggle competition, providing discussion of improvement in model efficiency and economic analysis by simulating different scenarios. In addition, sensitivity analysis is conducted to show the influence of different parameters in models and metrics to quantify the accuracy of prediction are proposed. The results of this paper can help people understand quantitative influence of different variables on energy use and energy baseline models. Future works will incorporate more data type in order to enhance the performance of prediction.",2021,,10.1145/3473714.3473788
1837,"Li, Guoliang and Zhou, Xuanhe and Cao, Lei",AI Meets Database: AI4DB and DB4AI,,"Database and Artificial Intelligence (AI) can benefit from each other. On one hand, AI can make database more intelligent (AI4DB). For example, traditional empirical database optimization techniques (e.g., cost estimation, join order selection, knob tuning, index and view advisor) cannot meet the high-performance requirement for large-scale database instances, various applications and diversified users, especially on the cloud. Fortunately, learning-based techniques can alleviate this problem. On the other hand, database techniques can optimize AI models (DB4AI). For example, AI is hard to deploy, because it requires developers to write complex codes and train complicated models. Database techniques can be used to reduce the complexity of using AI models, accelerate AI algorithms and provide AI capability inside databases. DB4AI and AI4DB have been extensively studied recently. In this tutorial, we review existing studies on AI4DB and DB4AI. For AI4DB, we review the techniques on learning-based database configuration, optimization, design, monitoring, and security. For DB4AI, we review AI-oriented declarative language, data governance, training acceleration, and inference acceleration. Finally, we provide research challenges and future directions in AI4DB and DB4AI.",2021,,
1838,"Feger, Sebastian S. and Wozniak, Pawe\l{} W. and Lischke, Lars and Schmidt, Albrecht"," 'Yes, I Comply!': Motivations and Practices around Research Data Management and Reuse across Scientific Fields","motivation, reuse, human data interventions, research data management, data-processing science, reproducibility","As science becomes increasingly data-intensive, the requirements for comprehensive Research Data Management (RDM) grow. This often overwhelms scientists, requiring more workload and training. The failure to conduct effective RDM leads to producing research artefacts that cannot be reproduced or reused. Past research placed high value on supporting data science workers, but focused mainly on data production, collection, processing, and sensemaking. In order to understand practices and needs of data science workers in relation to documentation, preservation, sharing, and reuse, we conducted a cross-domain study with 15 scientists and data managers from diverse scientific domains. We identified five core concepts which describe requirements, drivers, and boundaries in the development of commitment for RDM, essential for generating reproducible research artefacts: Practice, Adoption, Barriers, Education, and Impact. Based on those concepts, we introduce a stage-based model of personal RDM commitment evolution. The model can be used to drive the design of future systems that support a transition to open science. We discuss infrastructure, policies, and motivations involved at the stages and transitions in the model. Our work supports designers in understanding the constraints and challenges involved in designing for reproducibility in an age of data-driven science.",2020,,10.1145/3415212
1839,"Perera, Charith and Qin, Yongrui and Estrella, Julio C. and Reiff-Marganiec, Stephan and Vasilakos, Athanasios V.",Fog Computing for Sustainable Smart Cities: A Survey,"Internet of things, fog computing, smart cities, sustainability","The Internet of Things (IoT) aims to connect billions of smart objects to the Internet, which can bring a promising future to smart cities. These objects are expected to generate large amounts of data and send the data to the cloud for further processing, especially for knowledge discovery, in order that appropriate actions can be taken. However, in reality sensing all possible data items captured by a smart object and then sending the complete captured data to the cloud is less useful. Further, such an approach would also lead to resource wastage (e.g., network, storage, etc.). The Fog (Edge) computing paradigm has been proposed to counterpart the weakness by pushing processes of knowledge discovery using data analytics to the edges. However, edge devices have limited computational capabilities. Due to inherited strengths and weaknesses, neither Cloud computing nor Fog computing paradigm addresses these challenges alone. Therefore, both paradigms need to work together in order to build a sustainable IoT infrastructure for smart cities. In this article, we review existing approaches that have been proposed to tackle the challenges in the Fog computing domain. Specifically, we describe several inspiring use case scenarios of Fog computing, identify ten key characteristics and common features of Fog computing, and compare more than 30 existing research efforts in this domain. Based on our review, we further identify several major functionalities that ideal Fog computing platforms should support and a number of open challenges toward implementing them, to shed light on future research directions on realizing Fog computing for building sustainable smart cities.",2017,,10.1145/3057266
1840,"Jin, Yong and Qian, Zhenjiang and Chen, Shunjiang",Data Collection Scheme with Minimum Cost and Location of Emotional Recognition Edge Devices,"Data acquisition, Location, Edge devices, Emotional recognition, Data collection, Collection cost","This paper develops a real-time and reliable data collection system for big scale emotional recognition systems. Based on the data sample set collected in the initialization stage and by considering the dynamic migration of emotional recognition data, we design an adaptive Kth average device clustering algorithm for migration perception. We define a sub-modulus weight function, which minimizes the sum of the weights of the subsets covered by a cover to achieve high-precision device positioning. Combining the energy of the data collection devices and the energy of the wireless emotional device, we balance the data collection efficiency and energy consumption, and define a minimum access number problem based on energy and storage space constraints. By designing an approximate algorithm to solve the approximate minimum Steiner point problem, the continuous collection of emotional recognition data and the connectivity of data acquisition devices are guaranteed under the energy constraint of wireless devices. We validate the proposed algorithms through simulation experiments using different emotional recognition systems and different data scale. Furthermore, we analyze the proposed algorithms in terms of topology for devices classification, location accuracy, and data collection efficiency by comparing with the Bayesian classifier-based expectation maximization algorithm, the background difference-based moving target detection arithmetic averaging algorithm, and the Hungarian algorithm for solving the assignment problem.",2019,,10.1007/s00779-019-01217-0
1841,"Diakopoulos, Nicholas",Accountability in Algorithmic Decision-Making: A View from Computational Journalism,,"Every fiscal quarter automated writing algorithms churn out thousands of corporate earnings articles for the AP (Associated Press) based on little more than structured data. Companies such as Automated Insights, which produces the articles for AP, and Narrative Science can now write straight news articles in almost any domain that has clean and well-structured data: finance, sure, but also sports, weather, and education, among others. The articles aren’t cardboard either; they have variability, tone, and style, and in some cases readers even have difficulty distinguishing the machine-produced articles from human-written ones.",2015,,10.1145/2857274.2886105
1842,"Mauriello, Matthew Louis and Norooz, Leyla and Froehlich, Jon E.",Understanding the Role of Thermography in Energy Auditing: Current Practices and the Potential for Automated Solutions,"sustainable hci, formative inquiry, robotics, energy audits, human-robotic interaction, design probes, thermography","The building sector accounts for 41% of primary energy consumption in the US, contributing an increasing portion of the country's carbon dioxide emissions. With recent sensor improvements and falling costs, auditors are increasingly using thermography-infrared (IR) cameras-to detect thermal defects and analyze building efficiency. Research in automated thermography has grown commensurately, aimed at reducing manual labor and improving thermal models. Though promising, we could find no prior work exploring the professional auditor's perspectives of thermography or reactions to emerging automation. To address this gap, we present results from two studies: a semi-structured interview with 10 professional energy auditors, which includes design probes of five automated thermography scenarios, and an observational case study of a residential audit. We report on common perspectives, concerns, and benefits related to thermography and summarize reactions to our automated scenarios. Our findings have implications for thermography tool designers as well as researchers working on automated solutions in robotics, computer science, and engineering.",2015,,10.1145/2702123.2702528
1843,"Deng, Alex",Objective Bayesian Two Sample Hypothesis Testing for Online Controlled Experiments,"empirical bayes, controlled experiments, objective bayes, bayesian statistics, multiple testing, a/b testing, optional stopping, prior","As A/B testing gains wider adoption in the industry, more people begin to realize the limitations of the traditional frequentist null hypothesis statistical testing (NHST). The large number of search results for the query ``Bayesian A/B testing'' shows just how much the interest in the Bayesian perspective is growing. In recent years there are also voices arguing that Bayesian A/B testing should replace frequentist NHST and is strictly superior in all aspects. Our goal here is to clarify the myth by looking at both advantages and issues of Bayesian methods. In particular, we propose an objective Bayesian A/B testing framework for which we hope to bring the best from Bayesian and frequentist methods together. Unlike traditional methods, this method requires the existence of historical A/B test data to objectively learn a prior. We have successfully applied this method to Bing, using thousands of experiments to establish the priors.",2015,,10.1145/2740908.2742563
1844,"Welbourne, Evan and Wu, Pang and Bao, Xuan and Munguia-Tapia, Emmanuel",Crowdsourced Mobile Data Collection: Lessons Learned from a New Study Methodology,,"In this paper we explore a scalable data collection methodology that simultaneously achieves low cost and a high degree of control. We use popular online crowdsourcing platforms to recruit 63 subjects for a 90-day data collection that resulted in over 75K hours of data. The total cost of data collection was dramatically lower than for alternative methodologies, with total subject compensation under $3.5K US, and a total of less than 10 hours/week spent by researchers managing the study. At the same time, our methodology enhances control and enables richer study protocols by allowing direct contact with subjects. We were able to conduct surveys, exchange messages, and debug remotely with feedback from subjects. In addition to reporting on study details, we also discuss interesting findings and offer lessons learned.",2014,,10.1145/2565585.2565608
1845,"McMenemy, David",Ethical Issues in Machine Learning,,,2021,,
1846,"Diakopoulos, Nicholas",Accountability in Algorithmic Decision Making,,A view from computational journalism.,2016,,10.1145/2844110
1847,"Chen, Tao and Ran, Longya and Gao, Xian",AI Innovation for Advancing Public Service: The Case of China's First Administrative Approval Bureau,,"The adoption of artificial intelligence (AI) is becoming increasingly popular in the public sector, but there is a severe lack of relevant theoretical research. The government of China also has high expectations for AI innovation. This paper proposes a four-stage model for AI development in public sectors to help public administrators think about the impact of AI on their organizations. We empirically investigate a case of AI adoption for delivering public services in local government in China. The findings improve our understanding of not only the status of AI innovation but also the factors motivating and challenging public sectors that are intending to adopt AI. Given that AI application in public sectors is still in its infancy, this study provides us with an opportunity to conduct longitudinal tracking of AI innovation in local government in China. This could help public administrators to think more comprehensively about the changes and transformations that AI may bring to the public sector.",2019,,10.1145/3325112.3325243
1848,"Shekhar, Shashi and Feiner, Steven K. and Aref, Walid G.",Spatial Computing,,"Knowing where you are in space and time promises a deeper understanding of neighbors, ecosystems, and the environment.",2015,,10.1145/2756547
1849,"Li, Xiang and Kardes, Hakan and Wang, Xin and Sun, Ang",HMM-Based Address Parsing: Efficiently Parsing Billions of Addresses on MapReduce,"address parsing, large-scale data, record linkage","Record linkage is the task of identifying which records in one or more data collections refer to the same entity, and address is one of the most commonly used fields in databases. Hence, segmentation of the raw addresses into a set of semantic fields is the primary step in this task. In this paper, we present a probabilistic address parsing system based on the Hidden Markov Model. We also introduce several novel approaches to build models for noisy real-world addresses, obtaining 95.6% F-measure. Furthermore, we demonstrate the viability and efficiency of this system for large-scale data by scaling it up to parse billions of addresses with Hadoop.",2014,,10.1145/2666310.2666471
1850,"Ahlers, Dirk and Wilde, Erik and Spaniol, Marc and Baeza-Yates, Ricardo and Alonso, Omar",Report on the 11th International Workshop on Location and the Web (LocWeb 2021) and the 11th Temporal Web Analytics Workshop (TempWeb2021) at WWW2021,,"LocWeb and TempWeb 2021 were the eleventh events in their workshop series and took place co-located on 12th April 2021 in conjunction with The Web Conference WWW 2021. They were intended to be held in Ljubljana, Slovenia as a potentially hybrid event, but due to the pandemic, were fully moved online.LocWeb and TempWeb were held as one colocated session with a merged programme and shared topics to explore similarities and introduce attendees to the two related and complementary areas. LocWeb 2021 explored the intersection of location-based analytics and Web architecture with a focus on on Web-scale services and location-aware information access. TempWeb 2021 discussed temporal analytics at a Web scale with experts from science and industry.Date: 12 April, 2021.Websites: https://dhere.de/locweb/locweb2021 and http://temporalweb.net/.",2022,,10.1145/3527546.3527555
1851,"Xu, Jiejun and Xie, Daniel and Lu, Tsai-Ching and Cafeo, John",EDSV: Emerging Defect Surveillance for Vehicles,"user generated content, measurement, online social media, business intelligence, quality management","We present early findings on building a proof of concept for an automated system to identify emerging trends regarding vehicle defects. The proposed system functions by continuously collecting and monitoring publicly available data from several heterogeneous channels ranging from online social media to vehicle enthusiast forums and consumer reporting sites. By mining the collected data, the system would provide real-time detection of ongoing consumer issues with vehicles. In addition, our system has special emphasis on detecting early signals prior to the widespread knowledge of the general public. One of the system components involves estimating a baseline statistical distribution governing the frequency of observing specific types of vehicle defective complaints from our data sources and subsequently identifying irregular deviations from this distribution. A web interface is made available to visualize descriptive statistics derived from various channels, with the intent to provide timely insights for human analysts.",2017,,10.1145/3091478.3091521
1852,"Li, Xiang and Kardes, Hakan and Wang, Xin and Sun, Ang",HMM-Based Address Parsing with Massive Synthetic Training Data Generation,"record linkage, large-scale data, address parsing","Record linkage is the task of identifying which records in one or more data collections refer to the same entity, and address is one of the most commonly used fields in databases. Hence, segmentation of the raw addresses into a set of semantic fields is the primary step in this task. In this paper, we present a probabilistic address parsing system based on the Hidden Markov Model. We also introduce several novel approaches of synthetic training data generation to build robust models for noisy real-world addresses, obtaining 95.6% F-measure. Furthermore, we demonstrate the viability and efficiency of this system for large-scale data by scaling it up to parse billions of addresses.",2014,,10.1145/2663713.2664430
1853,"Du, Yifan and Issarny, Val\'{e}rie and Sailhan, Fran\c{c}oise",When the Power of the Crowd Meets the Intelligence of the Middleware: The Mobile Phone Sensing Case,,"The data gluttony of AI is well known: Data fuels the artificial intelligence. Technologies that help to gather the needed data are then essential, among which the IoT. However, the deployment of IoT solutions raises significant challenges, especially regarding the resource and financial costs at stake. It is our view that mobile crowdsensing, aka phone sensing, has a major role to play because it potentially contributes massive data at a relatively low cost. Still, crowdsensing is useless, and even harmful, if the contributed data are not properly analyzed. This paper surveys our work on the development of systems facing this challenge, which also illustrates the virtuous circles of AI. We specifically focus on how intelligent crowdsensing middleware leverages on-device machine learning to enhance the reported physical observations. Keywords: Crowdsensing, Middleware, Online learning.",2019,,10.1145/3352020.3352033
1854,"Alonso, Omar and Kamps, Jaap and Karlgren, Jussi",Report on the Seventh Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR'14),,"There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, emerging robust NLP tools, and an ever growing volume of linked data. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. The goal of the ESAIR'14 workshop remained to advance the general research agenda on this core problem, with an explicit focus on one of the most challenging aspects to address in the coming years. The main remaining challenge is on the user's side---the potential of rich document annotations can only be realized if matched by more articulate queries exploiting these powerful retrieval cues---and a more dynamic approach is emerging by exploiting new forms of query autosuggest. How can the query suggestion paradigm be used to encourage searcher to articulate longer queries, with concepts and relations linking their statement of request to existing semantic models? How do entity results and social network data in ""graph search"" change the classic division between searchers and information and lead to extreme personalization---are you the query? How to leverage transaction logs and recommendation, and how adaptive should we make the system? What are the privacy ramifications and the UX aspects---how to not creep out users.There was a strong feeling that we made substantial progress. Specifically, the discussion contributed to our understanding of the way forward. First, for notable (head, shoulder, but not tail) entities in semantic search we have reached the level of quality at minimal costs allowing for deployment in major web search engines---the dream has become a reality. Second, entity detection is moving fast into domain specific, personal, and business domains, and has become a vital component for a range of applications. Third, semantic web has exchanged logic for machine learning approaches, and machine learning is the natural unification of semantic web and information retrieval approaches.",2015,,10.1145/2795403.2795412
1855,"Soldatos, John and Draief, Moez and Macdonald, Craig and Ounis, Iadh",Multimedia Search over Integrated Social and Sensor Networks,"search engine, sensors, multimedia","This paper presents work in progress within the FP7 EU-funded project SMART to develop a multimedia search engine over content and information stemming from the physical world, as derived through visual, acoustic and other sensors. Among the unique features of the search engine is its ability to respond to social queries, through integrating social networks with sensor networks. Motivated by this innovation, the paper presents and discusses the state-of-the-art in participatory sensing and other technologies blending social and sensor networks.",2012,,10.1145/2187980.2188029
1856,"Lee, Rich C. and Cuzzocrea, Alfredo and Lee, Wookey and Leung, Carson K.",An Innovative Majority Voting Mechanism in Interactive Social Network Clustering,"majority, vote, graph clustering, social networks","We describe a new method of voting system in social networks environment1. We suggest a sequence of continuous support via a social network after electing representatives or exemplars in the network that is different from the typical majority voting. In other words, this paper suggests the method of elected representatives using network clustering approach to counts voting. On the network structure, sending messages from each node reflects the influence or importance to the representative and that can be readjusted and send back to each node. Where the representatives can be clustered within which the selectivity can be decided through the graph edges. In the experiment our algorithm outperformed conventional approaches in social network synthetic dataset as well as real dataset.",2017,,10.1145/3102254.3102268
1857,"Tahir, Madiha and Halim, Zahid and Rahman, Atta Ur and Waqas, Muhammad and Tu, Shanshan and Chen, Sheng and Han, Zhu",Non-Acted Text and Keystrokes Database and Learning Methods to Recognize Emotions,"data-driven decision-making, pattern recognition, Affective computing, affective states, machine learning","The modern computing applications are presently adapting to the convenient availability of huge and diverse data for making their pattern recognition methods smarter. Identification of dominant emotion solely based on the text data generated by humans is essential for the modern human–computer interaction. This work presents a multimodal text-keystrokes dataset and associated learning methods for the identification of human emotions hidden in small text. For this, a text-keystrokes data of 69 participants is collected in multiple scenarios. Stimuli are induced through videos in a controlled environment. After the stimuli induction, participants write their reviews about the given scenario in an unguided manner. Afterward, keystroke and in-text features are extracted from the dataset. These are used with an assortment of learning methods to identify emotion hidden in the short text. An accuracy of 86.95% is achieved by fusing text and keystroke features. Whereas, 100% accuracy is obtained for pleasure-displeasure classes of emotions using the fusion of keystroke/text features, tree-based feature selection method, and support vector machine classifier. The present work is also compared with four state-of-the-art techniques for the same task, where the results suggest that the present proposal performs better in terms of accuracy.",2022,,10.1145/3480968
1858,"Nikiforov, Alexander and Singireja, Anastasija",Open Data and Crowdsourcing Perspectives for Smart City in the United States and Russia,"civic issue tracker, crowdsourcing, e-government, government 2.0, open data, open innovations, smart city",In this research paper we describe the transformation of open data strategy and implementation of crowdsourcing technologies for the city E-government services. Analysis of smart city projects provides the role of open data and crowdsourcing for smart city vision in United States and Russia. We define challenges and perspectives for collaboration of open data and crowdsourcing in smart city projects.,2016,,10.1145/3014087.3014112
1859,"Fan, Liju and Flood, Mark D.",An Ontology of Form PF,"supervisory data, knowledge representation, data integrity, Financial regulation, hedge funds, ontologies, data integration, investment advisers","Form PF, mandated by the 2010 Dodd-Frank Act, is financial regulators' primary source for supervisory data on the risk exposures of hedge funds. Investment advisers use the Securities and Exchange Commission's (SEC) Form ADV to register, and then submit Form PF to report on each of the funds that they advise. These forms embed significant internal structure that is amenable to knowledge representation via formal ontologies, which would facilitate key tasks, such as data integration and the assurance of data integrity. We argue that ontologies should be a core and integral component of information management for financial regulatory reporting. We have tested the approach by designing, developing, and integrating ontologies in OWL/RDF in prototype to consistently describe Form ADV and Form PF with precise semantics. Preliminary results indicate that this technique is feasible in practice for data search and analysis, and will yield useful functionality. We also outline directions for future research.",2016,,10.1145/2951894.2951901
1860,"Sun, Fangyu",Manufacturing Audit Quality Analysis Model Based on Data Mining Technology,,"As our country's economic development enters a new normal, the original manufacturing development model can no longer meet the needs of current economic development, and it is urgent to accelerate the transformation of the manufacturing industry. At present, the country's supply-side structural reforms are deepening, and listed manufacturing companies are the most important backbone in terms of scale and innovation opportunities. Data mining technology is used to study the impact of quality control on corporate performance. Listed companies have a positive impact on the further realization of transformation and upgrading and the improvement of corporate performance. This article aims to study the manufacturing audit quality analysis model based on data mining technology, and adopts the analysis method of the combination of supervisory research and empirical analysis, from the perspective of supervisory research, summarizes the theory of internal audit quality and company performance in the research process, and summarizes predecessors' research results and research ideas. The experimental data in this article shows that the average quality of internal audit information disclosure is 3.1156, indicating that the audit disclosure status of listed companies selected by the Shenzhen Stock Exchange is good, and to a certain extent reflects the quality level of some internal controls.",2021,,10.1145/3510858.3510863
1861,"Kumar Chandrasekaran, Muthu and Jaidka, Kokil and Mayr, Philipp",Joint Workshop on Bibliometric-Enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2018),"information retrieval, digital libraries, information extraction, citation analysis, text mining, natural language processing, bibliometrics","The large scale of scholarly publications poses a challenge for scholars in information seeking and sensemaking. Information retrieval~(IR), bibliometric and natural language processing (NLP) techniques could enhance scholarly search, retrieval and user experience but are not yet widely used. To this purpose, we propose the third iteration of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL). The workshop is intended to stimulate IR, NLP researchers and Digital Library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometrics, text mining and recommendation techniques that can advance the state-of-the-art in scholarly document understanding, analysis, and retrieval at scale. The BIRNDL workshop will incorporate multiple invited talks, paper sessions, a poster session and the 4th edition of the Computational Linguistics (CL) Scientific Summarization Shared Task.",2018,,10.1145/3209978.3210194
1862,"Truong, Hong-Linh and Berardinelli, Luca",Testing Uncertainty of Cyber-Physical Systems in IoT Cloud Infrastructures: Combining Model-Driven Engineering and Elastic Execution,"Cloud, testing, IoT, uncertainty, elasticity, MDE, MBT"," Today's cyber-physical systems (CPS) span IoT and cloud-based datacenter infrastructures, which are highly heterogeneous with various types of uncertainty. Thus, testing uncertainties in these CPS is a challenging and multidisciplinary activity. We need several tools for modeling, deployment, control, and analytics to test and evaluate uncertainties for different configurations of the same CPS. In this paper, we explain why using state-of-the art model-driven engineering (MDE) and model-based testing (MBT) tools is not adequate for testing uncertainties of CPS in IoT Cloud infrastructures. We discus how to combine them with techniques for elastic execution to dynamically provision both CPS under test and testing utilities to perform tests in various IoT Cloud infrastructures. ",2017,,10.1145/3107091.3107093
1863,"Kuo, Yu-Hsuan and Li, Zhenhui and Kifer, Daniel",Detecting Outliers in Data with Correlated Measures,"robust regression, contextual outlier detection","Advances in sensor technology have enabled the collection of large-scale datasets. Such datasets can be extremely noisy and often contain a significant amount of outliers that result from sensor malfunction or human operation faults. In order to utilize such data for real-world applications, it is critical to detect outliers so that models built from these datasets will not be skewed by outliers. In this paper, we propose a new outlier detection method that utilizes the correlations in the data (e.g., taxi trip distance vs. trip time). Different from existing outlier detection methods, we build a robust regression model that explicitly models the outliers and detects outliers simultaneously with the model fitting. We validate our approach on real-world datasets against methods specifically designed for each dataset as well as the state of the art outlier detectors. Our outlier detection method achieves better performances, demonstrating the robustness and generality of our method. Last, we report interesting case studies on some outliers that result from atypical events.",2018,,10.1145/3269206.3271798
1864,"Zhao, Yan and Li, Yang and Wang, Yu and Su, Han and Zheng, Kai",Destination-Aware Task Assignment in Spatial Crowdsourcing,"spatial crowdsourcing, spatial task assignment, user mobility","With the proliferation of GPS-enabled smart devices and increased availability of wireless network, spatial crowdsourcing (SC) has been recently proposed as a framework to automatically request workers (i.e., smart device carriers) to perform location-sensitive tasks (e.g., taking scenic photos, reporting events). In this paper we study a destination-aware task assignment problem that concerns the optimal strategy of assigning each task to proper worker such that the total number of completed tasks can be maximized whilst all workers can reach their destinations before deadlines after performing assigned tasks. Finding the global optimal assignment turns out to be an intractable problem since it does not imply optimal assignment for individual worker. Observing that the task assignment dependency only exists amongst subsets of workers, we utilize tree-decomposition technique to separate workers into independent clusters and develop an efficient depth-first search algorithm with progressive bounds to prune non-promising assignments. Our empirical studies demonstrate that our proposed technique is quite effective and settle the problem nicely.",2017,,10.1145/3132847.3132894
1865,"Lin, Zheng and Jin, Xiaolong and Xu, Xueke and Wang, Yuanzhuo and Cheng, Xueqi and Wang, Weiping and Meng, Dan",An Unsupervised Cross-Lingual Topic Model Framework for Sentiment Classification,"topic model, cross-language, sentiment classification","Sentiment classification aims to determine the sentiment polarity expressed in a text. In online customer reviews, the sentiment polarities of words are usually dependent on the corresponding aspects. For instance, in mobile phone reviews, we may expect the long battery time but not enjoy the long response time of the operating system. Therefore, it is necessary and appealing to consider aspects when conducting sentiment classification. Probabilistic topic models that jointly detect aspects and sentiments have gained much success recently. However, most of the existing models are designed to work well in a language with rich resources. Directly applying those models on poor-quality corpora often leads to poor results. Consequently, a potential solution is to use the cross-lingual topic model to improve the sentiment classification for a target language by leveraging data and knowledge from a source language. However, the existing cross-lingual topic models are not suitable for sentiment classification because sentiment factors are not considered therein. To solve these problems, we propose for the first time a novel cross-lingual topic model framework which can be easily combined with the state-of-the-art aspect/sentiment models. Extensive experiments in different domains and multiple languages demonstrate that our model can significantly improve the accuracy of sentiment classification in the target language.",2016,,10.1109/TASLP.2015.2512041
1866,"Liu, Zhicheng and Zhang, Yang and Huang, Ruihong and Chen, Zhiwei and Song, Shaoxu and Wang, Jianmin",EXPERIENCE: Algorithms and Case Study for Explaining Repairs with Uniform Profiles over IoT Data,"outlier repairs, Outlier explanation, time series, data profiling","IoT data with timestamps are often found with outliers, such as GPS trajectories or sensor readings. While existing systems mostly focus on detecting temporal outliers without explanations and repairs, a decision maker may be more interested in the cause of the outlier appearance such that subsequent actions would be taken, e.g., cleaning unreliable readings or repairing broken devices or adopting a strategy for data repairs. Such outlier detection, explanation, and repairs are expected to be performed in either offline (batch) or online modes (over streaming IoT data with timestamps). In this work, we present TsClean, a new prototype system for detecting and repairing outliers with explanations over IoT data. The framework defines uniform profiles to explain the outliers detected by various algorithms, including the outliers with variant time intervals, and take approaches to repair outliers. Both batch and streaming processing are supported in a uniform framework. In particular, by varying the block size, it provides a tradeoff between computing the accurate results and approximating with efficient incremental computation. In this article, we present several case studies of applying TsClean in industry, e.g., how this framework works in detecting and repairing outliers over excavator water temperature data, and how to get reasonable explanations and repairs for the detected outliers in tracking excavators.",2021,,10.1145/3436239
1867,"Fan, Wenfei",Dependencies for Graphs: Challenges and Opportunities,"dependency discovery, implication, error detection, certain fixes, satisfiability, Dependencies, validation, graphs",What are graph dependencies? What do we need them for? What new challenges do they introduce? This article tackles these questions. It aims to incite curiosity and interest in this emerging area of research.,2019,,10.1145/3310230
1868,"Liu, Jing and Cho, Sungchol and Han, Sunyoung and Kim, Keecheon and Ha, YoungGuk and Choe, Jongwon and Kamolphiwong, Sinchai and Choo, Hyunseung and Shin, Yongtae and Kim, Chinchol","Establishment and Traffic Measurement of Overlay Multicast Testbed in KOREN, THaiREN and TEIN2","overlay multicast, KOREN, TEIN2, measurement, UniNet, multicast, overlay","Nowadays not only many of research works with various international networks are increasing more and more but also commercial works are increasing with different international networks. In this paper, we have constructed the overlay multicast testbed with KOREN and TEIN2 network, and then we also analyze and research many works with the data got from the testbed experiments, and research works for speed of transmission and transmission security when the data is forwarded to several various international network. We work out the process of problem based on several data of experiments. We analyze these problems and propose the research way to other researchers in overlay multicast area, and we also provide these useful results to other researchers in this area.",2009,,10.1145/1710035.1710077
1869,"Millard, Jeremy and Thomasen, Louise and Pastrovic, Goran and Cvetkovic, Bojan",A Roadmap for E-Participation and Open Government: Empirical Evidence from the Western Balkans,"Transparency, E-government, Policy, Participation, E-Participation, Collaboration, Open government","This paper describes why and how a conceptual framework for e-participation and open government has been developed and applied to six aspirant EU countries in the Western Balkans. It provides a rationale and background, and then examines the main academic and other relevant sources used. This is followed by an overview of the conceptual framework and a description of its main elements. Finally, the paper examines international data on e-participation covering the Western Balkan countries, uses this to examine the results of applying the conceptual framework in each country, and then provides conclusions and recommendations.",2018,,10.1145/3209415.3209459
1870,"Gavrilova, Marina L.",Machine Learning for Social Behavior Understanding,"virtual worlds, decision-making, machine learning, Human behavior recognition, online networks, social behavioral biometrics","Human brain has an ability to perform a massive processing of auxiliary information such as visual cues, cognitive and social interactions, contextual and spatio-temporal data. Similarly to a human brain, social behavioral cues can aid the reliable decision-making of a biometric security system. Being an integral part of human behavior, social interactions are likely to possess unique behavioral patterns. This state-of-the-art review paper discusses an emerging person recognition approach based on the in-depth analysis of individuals' social behavior in order to enhance the performance of a traditional biometric system. The social behavioral information can be mined from their offline or online interactions, and can be identified as a set of Social Behavioral Biometric (SBB) features. These features could be used on their own or further combined with other behavioral and physiological patters, and classification can be enhanced by the use of machine learning approaches. An overview of open problems and challenges as well as applications of studying social behavior in various domains concludes this paper.",2018,,10.1145/3208159.3208187
1871,"Costa, Daniel G.",On the Development of Visual Sensors with Raspberry Pi,"Raspberry Pi, Visual sensors, Camera, Internet of things, Wireless sensor networks","The increasing interest for Internet of Things (IoT) technologies has brought a lot of attention to microelectronics and sensors development. With the availability of affordable embedded platforms for countless applications, it is possible to develop low-cost programmable sensors to provide different types of data, benefiting applications in the IoT world. When cameras can be integrated to such development platforms, visual sensors can be easily created, supporting monitoring and controls functions based on the processing of images and videos. In this context, some of the most relevant details concerning the development of visual sensors with the Raspberry Pi platform are described herein, bringing fundamentals for the creation of highly programmable visual sensors.",2018,,10.1145/3243082.3264607
1872,"Jin, Ying and Gao, Ming and Yu, Jixiang",A Transformer Based Sales Prediction of Smart Container in New Retail Era,,"With the advent of the new retail era, the value of unmanned smart container is increasingly prominent. Fast and flexible self-service is favored by consumers. How to use accumulated historical sales data to predict sales in the future is an important part of smart container operation management. Reasonable sales prediction can not only reduce the inventory cost, but also reduce the shortage loss of the container. Based on the smart container sales data of Dalian Xiaode New Retail Co., Ltd., through detailed exploratory analysis in many aspects, this paper carries out the feature selection of sales prediction, and uses random forest, XGBoost, Transformer and other algorithms to predict sales. The experimental results show that the prediction accuracy of Transformer is better than traditional algorithms, whose MAPE is 14.67% lower than that of the worst one. Transformer can be well applied in the field of sales prediction of smart container. And in this experiment, compared with Transformer using sine and cosine functions for positional encoding, Transformer encoded by position index has better prediction performance and stronger stability.",2021,,
1873,"Bazai, Sibghat Ullah and Jang-Jaccard, Julian and Alavizadeh, Hooman",A Novel Hybrid Approach for Multi-Dimensional Data Anonymization for Apache Spark,"Mondrian, resilient distributed dataset (RDD), data anonymization, multi-dimensional data, Spark","Multi-dimensional data anonymization approaches (e.g., Mondrian) ensure more fine-grained data privacy by providing a different anonymization strategy applied for each attribute. Many variations of multi-dimensional anonymization have been implemented on different distributed processing platforms (e.g., MapReduce, Spark) to take advantage of their scalability and parallelism supports. According to our critical analysis on overheads, either existing iteration-based or recursion-based approaches do not provide effective mechanisms for creating the optimal number of and relative size of resilient distributed datasets (RDDs), thus heavily suffer from performance overheads. To solve this issue, we propose a novel hybrid approach for effectively implementing a multi-dimensional data anonymization strategy (e.g., Mondrian) that is scalable and provides high-performance. Our hybrid approach provides a mechanism to create far fewer RDDs and smaller size partitions attached to each RDD than existing approaches. This optimal RDD creation and operations approach is critical for many multi-dimensional data anonymization applications that create tremendous execution complexity. The new mechanism in our proposed hybrid approach can dramatically reduce the critical overheads involved in re-computation cost, shuffle operations, message exchange, and cache management.",2021,,10.1145/3484945
1874,"Blunck, Henrik and Bouvin, Niels Olof and Franke, Tobias and Gr\o{}nb\ae{}k, Kaj and Kjaergaard, Mikkel B. and Lukowicz, Paul and W\""{u}stenberg, Markus",On Heterogeneity in Mobile Sensing Applications Aiming at Representative Data Collection,"data collection, mobile sensing, heterogeneity, representativeness","Gathering representative data using mobile sensing to answer research questions is becoming increasingly popular, driven by growing ubiquity and sensing capabilities of mobile devices. However, there are pitfalls along this path, which introduce heterogeneity in the gathered data, and which are rooted in the diversity of the involved device platforms, hardware, software versions and participants. Thus, we, as a research community, need to establish good practices and methodologies for addressing this issue in order to help ensure that, e.g., scientific results and policy changes based on collective, mobile sensed data are valid. In this paper, we aim to inform researchers and developers about mobile sensing data heterogeneity and ways to combat it. We do so via distilling a vocabulary of underlying causes, and via describing their effects on mobile sensing---building on experiences from three projects within citizen science, crowd awareness and trajectory tracking.",2013,,10.1145/2494091.2499576
1875,,Outlier Detection,,"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.",2019,,
1876,"Ma, Tianle and Zhang, Aidong",Omics Informatics: From Scattered Individual Software Tools to Integrated Workflow Management Systems,,"Omic data analyses pose great informatics challenges. As an emerging subfield of bioinformatics, omics informatics focuses on analyzing multi-omic data efficiently and effectively, and is gaining momentum. There are two underlying trends in the expansion of omics informatics landscape: the explosion of scattered individual omics informatics tools with each of which focuses on a specific task in both single- and multi- omic settings, and the fast-evolving integrated software platforms such as workflow management systems that can assemble multiple tools into pipelines and streamline integrative analysis for complicated tasks. In this survey, we give a holistic view of omics informatics, from scattered individual informatics tools to integrated workflow management systems. We not only outline the landscape and challenges of omics informatics, but also sample a number of widely used and cutting-edge algorithms in omics data analysis to give readers a fine-grained view. We survey various workflow management systems WMSs, classify them into three levels of WMSs from simple software toolkits to integrated multi-omic analytical platforms, and point out the emerging needs for developing intelligent workflow management systems. We also discuss the challenges, strategies and some existing work in systematic evaluation of omics informatics tools. We conclude by providing future perspectives of emerging fields and new frontiers in omics informatics.",2017,,10.1109/TCBB.2016.2535251
1877,"Gorenflo, Christian and Golab, Lukasz and Keshav, Srinivasan",Managing Sensor Data Streams: Lessons Learned from the WeBike Project,"Time series data management, Data feed management, Data management for the Internet of Things (IoT)","We present insights on data management resulting from a field deployment of approximately 30 sensor-equipped electric bicycles (e-bikes) at the University of Waterloo. The trial has been in operation for the last two-and-a-half years, and we have collected and analyzed more than 150 gigabytes of data. We discuss best practices for the entire data management process, spanning data collection, extract-transform-load, data cleaning, and choosing a suitable data management ecosystem. We also comment on how our experiences will inform the design of a future large-scale field trial involving several thousand fully-instrumented e-bikes.",2017,,10.1145/3085504.3085505
1878,"Cai, Jianghui and Yang, Yuqing and Yang, Haifeng and Zhao, Xujun and Hao, Jing",ARIS: A Noise Insensitive Data Pre-Processing Scheme for Data Reduction Using Influence Space,"data representation, ranking factor, data pre-processing scheme, influence space, noise identification","The extensive growth of data quantity has posed many challenges to data analysis and retrieval. Noise and redundancy are typical representatives of the above-mentioned challenges, which may reduce the reliability of analysis and retrieval results and increase storage and computing overhead. To solve the above problems, A two-stage data pre-processing framework for noise identification and data reduction, called ARIS, is proposed in this paper. The first stage identifies and removes noises by the following steps: First, the influence space (IS) is introduced to elaborate data distribution. Second, a ranking factor (RF) is defined to describe the possibility that the points are regarded as noises, then, the definition of noise is given based on RF. Third, a clean dataset (CD) is obtained by removing noise from the original dataset. The second stage learns representative data and realizes data reduction. In this process, CD is divided into multiple small regions by IS. Then the reduced dataset is formed by collecting the representations of each region. The performance of ARIS are verified by experiments on artificial and real datasets. Experimental results show that ARIS effectively weakens the impact of noise and reduces the amount of data and significantly improves the accuracy of data analysis within a reasonable time cost range.",2022,,10.1145/3522592
1879,"Hu, Yerong and He, Xiangzhen and Zhang, Yihao and Zeng, Jia and Yang, Huaiyuan and Zhou, Shuaihang",Research and Application of Digital Collection Method of Human Movement,,,2021,,10.1145/3469213.3470272
1880,"Gatziolis, Kleanthis and Boucouvalas, Anthony C.",User Profile Extraction Engine,"e-Shopping, User Profiling, Mobile shopping, Retailing, E-Commerce","The Internet is overwhelmed by a huge amount of information every day and every user has different interests from another. It is therefore important that this information is filtered and sorted according to their preferences. Thus, the profiling systems exploit particularities and preferences of each user and finally they can be studied or used by other applications or humans. This paper analyzes the methods of collecting data (data gathering), and the ways in which this information can be used - filtered so as to create knowledge. A user profile extraction engine is presented and analyzed.",2016,,10.1145/3003733.3003761
1881,"Varde, Aparna S.",Computational Estimation by Scientific Data Mining with Classical Methods to Automate Learning Strategies of Scientists,"classification, scientific applications, machine learning, clustering, estimation, graphical data mining, domain knowledge, predictive analytics, Applied research","Experimental results are often plotted as 2-dimensional graphical plots (aka graphs) in scientific domains depicting dependent versus independent variables to aid visual analysis of processes. Repeatedly performing laboratory experiments consumes significant time and resources, motivating the need for computational estimation. The goals are to estimate the graph obtained in an experiment given its input conditions, and to estimate the conditions that would lead to a desired graph. Existing estimation approaches often do not meet accuracy and efficiency needs of targeted applications. We develop a computational estimation approach called AutoDomainMine that integrates clustering and classification over complex scientific data in a framework so as to automate classical learning methods of scientists. Knowledge discovered thereby from a database of existing experiments serves as the basis for estimation. Challenges include preserving domain semantics in clustering, finding matching strategies in classification, striking a good balance between elaboration and conciseness while displaying estimation results based on needs of targeted users, and deriving objective measures to capture subjective user interests. These and other challenges are addressed in this work. The AutoDomainMine approach is used to build a computational estimation system, rigorously evaluated with real data in Materials Science. Our evaluation confirms that AutoDomainMine provides desired accuracy and efficiency in computational estimation. It is extendable to other science and engineering domains as proved by adaptation of its sub-processes within fields such as Bioinformatics and Nanotechnology.",2022,,10.1145/3502736
1882,"Chen, Yiyuan and Wang, Yufeng and Cao, Liang and Jin, Qun",CCFS: A Confidence-Based Cost-Effective Feature Selection Scheme for Healthcare Data Classification,,"Feature selection (FS) is one of the fundamental data processing techniques in various machine learning algorithms, especially for classification of healthcare data. However, it is a challenging issue due to the large search space. Binary Particle Swarm Optimization (BPSO) is an efficient evolutionary computation technique, and has been widely used in FS. In this paper, we proposed a Confidence-based and Cost-effective feature selection (CCFS) method using BPSO to improve the performance of healthcare data classification. Specifically, first, CCFS improves search effectiveness by developing a new updating mechanism that designs the feature confidence to explicitly take into account the fine-grained impact of each dimension in the particle on the classification performance. The feature confidence is composed of two measurements: the correlation between feature and categories, and historically selected frequency of each feature. Second, considering the fact that the acquisition costs of different features are naturally different, especially for medical data, and should be fully taken into account in practical applications, besides the classification performance, the feature cost and the feature reduction ratio are comprehensively incorporated into the design of fitness function. The proposed method has been verified in various UCI public datasets and compared with various benchmark schemes. The thoroughly experimental results show the effectiveness of the proposed method, in terms of accuracy and feature selection cost.",2021,,10.1109/TCBB.2019.2903804
1883,"Hockenhull, Michael and Cohn, Marisa Leavitt",Speculative Data Work &amp; Dashboards: Designing Alternative Data Visions,"data work, business intelligence, speculative design, data visualization, ethnography","This paper studies data work in an organizational context, and suggests speculative data work as a useful concept and the speculative dashboard as a design concept, to better understand and support cooperative work. Drawing on fieldwork in a Danish public sector organisation, the paper identifies and conceptualizes the speculative data work performed around processes of digitalization and the push to become data-driven. The speculative dashboard is proposed as a design concept and opportunity for design, using practices from speculative design and research to facilitate speculation about data?its sources, visualizations, practices and infrastructures. It does so by hacking the 'genre' of the business intelligence data dashboard, and using it as a framework for the juxtaposition of different kinds of data, facilitating and encouraging speculation on alternative visions for data types and use. The paper contributes an empirical study of organizational use of and attitudes towards data, informing a novel design method and concept for co-speculating on alternative visions of and for organizational data.",2021,,10.1145/3434173
1884,"Yang, Qiang",Toward Responsible AI: An Overview of Federated Learning for User-Centered Privacy-Preserving Computing,"Federated learning, privacy-preserving computing, decentralized AI, data security, machine learning, blockchain, user privacy, responsible AI","With the rapid advances of Artificial Intelligence (AI) technologies and applications, an increasing concern is on the development and application of responsible AI technologies. Building AI technologies or machine-learning models often requires massive amounts of data, which may include sensitive, user private information to be collected from different sites or countries. Privacy, security, and data governance constraints rule out a brute force process in the acquisition and integration of these data. It is thus a serious challenge to protect user privacy while achieving high-performance models. This article reviews recent progress of federated learning in addressing this challenge in the context of privacy-preserving computing. Federated learning allows global AI models to be trained and used among multiple decentralized data sources with high security and privacy guarantees, as well as sound incentive mechanisms. This article presents the background, motivations, definitions, architectures, and applications of federated learning as a new paradigm for building privacy-preserving, responsible AI ecosystems.",2021,,10.1145/3485875
1885,"Gathani, Sneha and Lim, Peter and Battle, Leilani","Debugging Database Queries: A Survey of Tools, Techniques, and Users",,"Database management systems (or DBMSs) have been around for decades, and yet are still difficult to use, particularly when trying to identify and fix errors in user programs (or queries). We seek to understand what methods have been proposed to help people debug database queries, and whether these techniques have ultimately been adopted by DBMSs (and users). We conducted an interdisciplinary review of 112 papers and tools from the database, visualisation and HCI communities. To better understand whether academic and industry approaches are meeting the needs of users, we interviewed 20 database users (and some designers), and found surprising results. In particular, there seems to be a wide gulf between users' debugging strategies and the functionality implemented in existing DBMSs, as well as proposed in the literature. In response, we propose new design guidelines to help system designers to build features that more closely match users debugging strategies.",2020,,
1886,"Eslambolchilar, Parisa and Komninos, Andreas and Dunlop, Mark D.",Introduction,,,2021,,
1887,"Hartner, Raphael and Mezhuyev, Vitaliy and Tschandl, Martin and Bischof, Christian",Digital Shop Floor Management: A Practical Framework For Implementation,"Middleware, Fog Computing, Lean Production, Cloud Computing, Mist Computing, Retrofitting, Industry 4.0, Shop Floor Management, Smart Production","In the context of manufacturing, shop floor management (SFM) is employed to ensure efficient production operations and workflows. Advanced technologies and methods can be used to improve the SFM and achieve close to real-time responsiveness. Even though there is a number of research available for the digitalized SFM (DSFM), a supportive framework for implementation purposes was not considered yet. Consequently, this paper utilizes concepts from related disciplines and research areas to derive an architectural framework for a DSFM. This particular architecture is then implemented to ensure its practicability and foster the understanding of challenges and opportunities. The proposed multi-layer framework and supportive methods can be employed by manufacturing companies to implement a DSFM focused on interoperability, security and low-latency.",2020,,10.1145/3384544.3384611
1888,"Schuler, Robert and Czajkowski, Karl and D'Arcy, Mike and Tangmunarunkit, Hongsuda and Kesselman, Carl",Towards Co-Evolution of Data-Centric Ecosystems,"model management, schema evolution, software ecosystems, application-database co-evolution","Database evolution is a notoriously difficult task, and it is exacerbated by the necessity to evolve database-dependent applications. As science becomes increasingly dependent on sophisticated data management, the need to evolve an array of database-driven systems will only intensify. In this paper, we present an architecture for data-centric ecosystems that allows the components to seamlessly co-evolve by centralizing the models and mappings at the data service and pushing model-adaptive interactions to the database clients. Boundary objects fill the gap where applications are unable to adapt and need a stable interface to interact with the components of the ecosystem. Finally, evolution of the ecosystem is enabled via integrated schema modification and model management operations. We present use cases from actual experiences that demonstrate the utility of our approach.",2020,,10.1145/3400903.3400908
1889,"Chen, Zhuangbin and Kang, Yu and Li, Liqun and Zhang, Xu and Zhang, Hongyu and Xu, Hui and Zhou, Yangfan and Yang, Li and Sun, Jeffrey and Xu, Zhangwei and Dang, Yingnong and Gao, Feng and Zhao, Pu and Qiao, Bo and Lin, Qingwei and Zhang, Dongmei and Lyu, Michael R.",Towards Intelligent Incident Management: Why We Need It and How We Make It,,"The management of cloud service incidents (unplanned interruptions or outages of a service/product) greatly affects customer satisfaction and business revenue. After years of efforts, cloud enterprises are able to solve most incidents automatically and timely. However, in practice, we still observe critical service incidents that occurred in an unexpected manner and orchestrated diagnosis workflow failed to mitigate them. In order to accelerate the understanding of unprecedented incidents and provide actionable recommendations, modern incident management system employs the strategy of AIOps (Artificial Intelligence for IT Operations). In this paper, to provide a broad view of industrial incident management and understand the modern incident management system, we conduct a comprehensive empirical study spanning over two years of incident management practices at Microsoft. Particularly, we identify two critical challenges (namely, incomplete service/resource dependencies and imprecise resource health assessment) and investigate the underlying reasons from the perspective of cloud system design and operations. We also present IcM BRAIN, our AIOps framework towards intelligent incident management, and show its practical benefits conveyed to the cloud services of Microsoft.",2020,,
1890,"Raj, Rajendra K. and Romanowski, Carol J. and Impagliazzo, John and Aly, Sherif G. and Becker, Brett A. and Chen, Juan and Ghafoor, Sheikh and Giacaman, Nasser and Gordon, Steven I. and Izu, Cruz and Rahimi, Shahram and Robson, Michael P. and Thota, Neena",High Performance Computing Education: Current Challenges and Future Directions,"high-performance computing curricula, iticse working group, computer science education, hpc education, high performance computing, contemporary computing education","High Performance Computing (HPC) is the ability to process data and perform complex calculations at extremely high speeds. Current HPC platforms can achieve calculations on the order of quadrillions of calculations per second, with quintillions on the horizon. The past three decades witnessed a vast increase in the use of HPC across different scientific, engineering, and business communities on problems such as sequencing the genome, predicting climate changes, designing modern aerodynamics, or establishing customer preferences. Although HPC has been well incorporated into science curricula such as bioinformatics, the same cannot be said for most computing programs. Computing educators are only now beginning to recognize the need for HPC Education (HPCEd).  Building on earlier work, this working group explored how HPCEd can make inroads into computing education, focusing on the undergraduate level. This paper presents the background of HPC and HPCEd, identifies several of the needed core HPC competencies for students, identifies the support needed by educators for HPCEd, and explores the symbiosis between HPCEd and computing education in contemporary areas such as artificial intelligence and data science, as well as how HPCEd can be applied to benefit diverse non-computing domains such as atmospheric science, biological sciences and critical infrastructure protection. Finally, the report makes several recommendations to improve and facilitate HPC education in the future.",2020,,10.1145/3437800.3439203
1891,"Dominguez, Hector and Mowry, Judith and Perez, Elisabeth and Kendrick, Christine and Martin, Kevin",Privacy and Information Protection for a New Generation of City Services,"Digital equity, government services, Automatic decision systems, Privacy, Digital Inclusion","This paper will showcase the work that the City of Portland has done around developing Privacy and Information Protection Principles considering the current state of technology, the social digital age, and advance inference algorithms like machine learning or other Artificial Intelligence tools. By creating more responsible data stewardship in the public sector, municipalities are set to build trusted information networks involving communities and complex social issues. Particularly, the promotion of data privacy can lead to the emergence of anti-poverty and economic development strategies.The City of Portland has developed seven Privacy and Information Protection Principles: Transparency and accountability, full lifecycle stewardship, equitable data management, ethical and non-discriminatory use of data, data openness, automated decision systems, and data utility. These principles have implications in social equity and the future of technology management in smart cities projects. Principle implementation involves the collaboration of different agencies, particularly focused on ethics and human rights supporting sustainable development.This work is part of emergent strategies for a new generation of city services based on data and information, which aim to improve civic engagement, social benefits to communities in city neighborhoods and better collaboration with partners and other government agencies.",2019,,10.1145/3357492.3358628
1892,"Ranbaduge, Thilina and Schnell, Rainer",Securing Bloom Filters for Privacy-Preserving Record Linkage,"sliding window, xor, random sampling, perturbation, hardening","Privacy-preserving record linkage (PPRL) facilitates the matching of records that correspond to the same real-world entities across different databases while preserving the privacy of the individuals in these databases. A Bloom filter (BF) is a space efficient probabilistic data structure that is becoming popular in PPRL as an efficient privacy technique to encode sensitive information in records while still enabling approximate similarity computations between attribute values. However, BF encoding is susceptible to privacy attacks which can re-identify the values that are being encoded. In this paper we propose two novel techniques that can be applied on BF encoding to improve privacy against attacks. Our techniques use neighbouring bits in a BF to generate new bit values. An empirical study on large real databases shows that our techniques provide high security against privacy attacks, and achieve better similarity computation accuracy and linkage quality compared to other privacy improvements that can be applied on BF encoding.",2020,,10.1145/3340531.3412105
1893,"Hagemann, Simon and Stark, Rainer",Automated Body-in-White Production System Design: Data-Based Generation of Production System Configurations,"process automatization, knowledge-engineering, optimization, ruled-based algorithms, automotive, production system design, data mining, body-in-white, artificial intelligence","Design processes for production systems (PS) in the automotive body-in-white (BIW) sector tie up tremendous resources. Current challenges like the continuous increase of product variants and product complexity have direct impact on the required planning effort in production system design (PSD), which is currently increasing significantly. Analysis of these design processes have revealed a high potential for process automatization. In order to achieve this, suitable methods are required as well as a data basis of reasonable quality. Both methods and data basis are deeply investigated in this paper. The investigations' results create a solid basis for further research in the young field of automated BIW PSD.",2018,,10.1145/3233347.3233373
1894,"Barth, Susanne and Ionita, Dan and Hartel, Pieter",Understanding Online Privacy—A Systematic Review of Privacy Visualizations and Privacy by Design Guidelines,"privacy factors, Privacy attributes, privacy icons, privacy labels, privacy by design","Privacy visualizations help users understand the privacy implications of using an online service. Privacy by Design guidelines provide generally accepted privacy standards for developers of online services. To obtain a comprehensive understanding of online privacy, we review established approaches, distill a unified list of 15 privacy attributes and rank them based on perceived importance by users and privacy experts. We then discuss similarities, explain notable differences, and examine trends in terms of the attributes covered. Finally, we show how our results provide a foundation for user-centric privacy visualizations, inspire best practices for developers, and give structure to privacy policies.",2022,,10.1145/3502288
1895,"Haak, Elise and Ubacht, Jolien and Van den Homberg, Marc and Cunningham, Scott and Van den Walle, Bartel",A Framework for Strengthening Data Ecosystems to Serve Humanitarian Purposes,"data preparedness, governance, humanitarian sector, framework, data ecosystem","The incidence of natural disasters worldwide is increasing. As a result, a growing number of people is in need of humanitarian support, for which limited resources are available. This requires an effective and efficient prioritization of the most vulnerable people in the preparedness phase, and the most affected people in the response phase of humanitarian action. Data-driven models have the potential to support this prioritization process. However, the applications of these models in a country requires a certain level of data preparedness. To achieve this level of data preparedness on a large scale we need to know how to facilitate, stimulate and coordinate data-sharing between humanitarian actors. We use a data ecosystem perspective to develop success criteria for establishing a ""humanitarian data ecosystem"". We first present the development of a general framework with data ecosystem governance success criteria based on a systematic literature review. Subsequently, the applicability of this framework in the humanitarian sector is assessed through a case study on the ""Community Risk Assessment and Prioritization toolbox"" developed by the Netherlands Red Cross. The empirical evidence led to the adaption the framework to the specific criteria that need to be addressed when aiming to establish a successful humanitarian data ecosystem.",2018,,10.1145/3209281.3209326
1896,"Bhargav, Samarth and Sidiropoulos, Georgios and Kanoulas, Evangelos", 'It's on the Tip of My Tongue': A New Dataset for Known-Item Retrieval,"tip of the tongue known item retrieval, known item retrieval","The tip of the tongue known-item retrieval (TOT-KIR) task involves the 'one-off' retrieval of an item for which a user cannot recall a precise identifier. The emergence of several online communities where users pose known-item queries to other users indicates the inability of existing search systems to answer such queries. Research in this domain is hampered by the lack of large, open or realistic datasets. Prior datasets relied on either annotation by crowd workers, which can be expensive and time-consuming, or generating synthetic queries, which can be unrealistic. Additionally, small datasets make the application of modern (neural) retrieval methods unviable, since they require a large number of data-points. In this paper, we collect the largest dataset yet with 15K query-item pairs in two domains, namely, Movies and Books, from an online community using heuristics, rendering expensive annotation unnecessary while ensuring that queries are realistic. We show that our data collection method is accurate by conducting a data study. We further demonstrate that methods like BM25 fall short of answering such queries, corroborating prior research. The size of the dataset makes neural methods feasible, which we show outperforms lexical baselines, indicating that neural/dense retrieval is superior for the TOT-KIR task.",2022,,10.1145/3488560.3498421
1897,"S. Bargh, Mortaza and Meijer, Ronald and Vink, Marco and van Den Braak, Susan and Schirm, Walter and Choenni, Sunil",Opening Privacy Sensitive Microdata Sets in Light of GDPR,"Microdata, Open data, Justice domain data, GDPR, Data protection, Privacy, Criminal justice data","To enhance the transparency, accountability and efficiency of the Dutch Ministry of Justice and Security, the ministry has set up an open data program to proactively stimulate sharing its (publicly funded) data sets with the public. Disclosure of personal data is considered as one of the main threats for data opening. In this contribution we argue that, according to Dutch laws, the criminal data within the Dutch justice domain are sensitive data in GDPR terms and that the criminal data can only be opened if these sensitive data are transformed to have no personal information. Subsequently, having no personal information in data sets is related to two GDPR concepts: the data being anonymous in its GDPR sense or the data being pseudonymized in its GDPR sense. These two GDPR concepts, i.e., being anonymous data or pseudonymized data in a GDPR sense, can be distinguished in our setting based on whether the data controller cannot or can revert the data protection process, respectively. (Note that the terms anonymous and pseudonymized are interpreted differently in the technical domain.) We examine realizing these GDPR concepts with the Statistical Disclosure Control (SDC) technology and subsequently argue that pseudonymized data in a GDPR sense delivers a better data utility than the other. At the end, we present a number of the consequences of adopting either of these concepts, which can inform legislators and policymakers to define their strategy for opening privacy sensitive microdata sets, like those pertaining to the Dutch criminal justice domain.",2019,,10.1145/3325112.3325222
1898,"Papadopoulos, Theodoros and Charalabidis, Yannis",What Do Governments Plan in the Field of Artificial Intelligence? Analysing National AI Strategies Using NLP,"machine learning, Automated Text Analysis, document similarity, AI strategies, topic modelling, NLP","The primary goal of this paper is to explore how Natural Language Processing techniques (NLP) can assist in reviewing, understanding, and drawing conclusions from text datasets. We explore NLP techniques for the analysis and the extraction of useful information from the text of twelve national strategies on artificial intelligence (AI). For this purpose, we are using a set of machine learning algorithms in order to (a) extract the most significant keywords and summarize each strategy document, (b) discover and assign topics to each document, and (c) cluster the strategies based on their pair-wise similarity. Using the results of the analysis, we discuss the findings and highlight critical issues that emerge from the national strategies for artificial intelligence, such as the importance of the data ecosystem for the development of AI, the increasing considerations about ethical and safety issues, as well as the growing ambition of many countries to lead in the AI race. Utilizing the LDA topic model, we were able to reveal the distributions of thematic sub-topics among the strategic documents. The topic modelling distributions were then used along with other document similarity measures as an input for the clustering of the strategic documents into groups. The results revealed three clusters of countries with a visible differentiation between the strategies of China and Japan on the one hand and the Scandinavian strategies (plus the German and the Luxemburgish) one on the other. The former promote technology and innovation-driven development plans in order to integrate AI with the economy, while the latter share a common view regarding the role of the public sector both as a promoter and investor but also as a user and beneficiary of AI, and give a higher priority to the ethical &amp; safety issues that are connected to the development of AI.",2020,,10.1145/3428502.3428514
1899,"Moazeni, Ramin and Link, Daniel and Boehm, Barry",COCOMO II Parameters and IDPD: Bilateral Relevances,"IDPD, cost drivers, Parametric cost estimation, scale factors, incremental development"," The phenomenon called Incremental Development Productivity Decline (IDPD) is presumed to be present in all incremental soft-ware projects to some extent. COCOMO II is a popular parametric cost estimation model that has not yet been adapted to account for the challenges that IDPD poses to cost estimation. Instead, its cost driver and scale factors stay constant throughout the increments of a project. While a simple response could be to make these parameters variable per increment, questions are raised as to whether the existing parameters are enough to predict the behavior of an incrementally developed project even in that case. Individual COCOMO II parameters are evaluated with regard to their development over the course of increments and how they influence IDPD. The reverse is also done. In light of data collected in recent experimental projects, additional new variable parameters that either extend COCOMO II or could stand on their own are proposed. ",2014,,10.1145/2600821.2600847
1900,"Hanbal, Rajesh Dinesh and Prakash, Amit and Srinivasan, Janaki",Who Drives Data in Data-Driven Governance? The Politics of Data Production in India's Livelihood Program,"Data production, Open government data, data justice","The increased digitisation of government information systems, as well as emerging data analytics and visualization techniques, have led lately to a surge in interest in the role of data in governance and development. The latest buzzwords in governance now include data-driven governance, data-for-development, evidence-based policy-making, and open government data. However, not much attention has been paid to understand the process of the production of data in government information systems. Our findings are based on six months of an ethnographic study of India's livelihood program- Mahatma Gandhi National Rural Employment Guarantee Act (MGNREGA) in a rural district of Karnataka. We argue that the practice of data production is carefully managed and controlled by local power elites providing an illusion of transparency in a digital information system. Understanding and recognizing the political nature of data production can help in better evaluation of development interventions, policy-making as well as in the design of more just information systems.",2020,,10.1145/3428502.3428576
1901,"Bogers, Toine and G\""{a}de, Maria and Freund, Luanne and Hall, Mark and Koolen, Marijn and Petras, Vivien and Skov, Mette",Workshop on Barriers to Interactive IR Resources Re-Use,"evaluation, repository, intertactive information retrieval","The goal of this workshop is to serve as a starting point for a community-driven effort to design and implement a platform for the collection, organization, maintenance, and sharing of resources for IIR experimentation. As in all scientific endeavors, progress in IIR research is contingent on the ability to build on previous ideas, approaches, and resources. However, we believe there to be a number of barriers to reproducibility and re-use of resources in IIR research: the fragmentary nature of how the community»s resources are organized, the lack of awareness of their existence, documentation and organization of the resources, the nature of the typical research publication cycle, and the effort required to make such resources available. We believe that an online platform dedicated to the collection and organization of IIR resources could be a promising way of overcoming these barriers. The workshop therefore aims to serve both as a brainstorming opportunity about the shape this iRepository should take, as well as a way of building support in the community for its implementation.",2018,,10.1145/3176349.3176901
1902,"Al-Khowarizmi, Al-Khowarizmi and Lubis, Muharman and Ridho Lubis, Arif and Fauzi, Fauzi and Ramadhan Nasution, Ilham",Model of Business Intelligence Applied the Principle of Cooperative Society in the Business Forums,"Model and Simulation, Business Intelligence, Business Forum, Cooperative Society","Business forums are activities between individuals and organizations that carry out the transactions on online media or within applications, which spread across countries. Along with the development of information technology towards business intelligence (BI), the business processes carried out in the business forum are modeled specifically in order to create an effort and attempt to follow the indicator and criteria from the industrial revolution 4.0. In this paper, a model is designed to combine three type of principles, namely the business forum, BI and the cooperative principle. Actually, cooperatives have been long abandoned since the existence of conventional and Islamic banking concept but it has kinship principle to divide the profits based on the size of the contribution given. Meanwhile, BI model is designed to obtain a formula from the cooperative principle, namely the residual income from operations where the transaction process is successfully implemented through the application to allocate a portion of the profits to the members based on the specified percent.",2021,,10.1145/3457784.3457820
1903,"Sambasivan, Nithya",Seeing like a Dataset from the Global South,,"This is a forum for perspectives on designing for marginalized communities worldwide. Articles will discuss design methods, theoretical/conceptual contributions, and participatory interventions with underserved communities. --- Nithya Sambasivan, Editor",2021,,10.1145/3466160
1904,"Baeza-Yates, Ricardo and Masan\`{e}s, Julien and Spaniol, Marc",The 1st Temporal Web Analytics Workshop (TWAW),"web scale data analytics, distributed data analytics, temporal web analytics","The objective of the 1st Temporal Web Analytics Workshop (TWAW) is to provide a venue for researchers of all domains (IE/IR, Web mining etc.) where the temporal dimension opens up an entirely new range of challenges and possibilities. The workshop's ambition is to help shaping a community of interest on the research challenges and possibilities resulting from the introduction of the time dimension in Web analysis. The maturity of the Web, the emergence of large scale repositories of Web material, makes this very timely and a growing sets of research and services (recorded future1, truthy2 launched just in the last months) are emerging that have this focus in common. Having a dedicated workshop will help, we believe, to take a rich and cross-domain approach to this new research challenge with a strong focus on the temporal dimension.",2011,,10.1145/1963192.1963325
1905,"Devarakonda, Ranjeet and Guntupally, Kavya and Thornton, Michele and Wei, Yaxing and Singh, Debjani and Lunga, Dalton",FAIR Interfaces for Geospatial Scientific Data Searches,"FAIR data principle for scientific data, ORNL DAAC, Geospatial search interfaces, ARM Data Center","Several factors must be considered in designing a highly accurate, reliable, scalable, and user-friendly geospatial data search interfaces. This paper examines four critical questions that ought to be considered during design phase: (1) Is the search interface or API that provides the search capability useable by both humans and machines? (2) Are the results consistent and reliable? (3) Is the output response format free to use, community-defined, and non-propriety? (4) Does the API clearly state the usage clauses? This paper discusses how certain data repositories at the US Department of Energy's Oak Ridge National Laboratory apply FAIR data principles to enable geospatial searches and address the above-mentioned questions.",2021,,10.1145/3486640.3491391
1906,"Dahbi, Kawtar Younsi and Lamharhar, Hind and Chiadmi, Dalila",Exploring Dimensions Influencing the Usage of Open Government Data Portals,"Open Government Data, Evaluation, usage, Open Government Data portals","Governments are considered as one of the major producers of data. Opening up and publishing this Big Government Data in national portals have significant impact on fostering innovation, improving transparency, public accountability and collaboration. Thus, the expected benefits are hindered by several factors that influence the usage of Open Government Data portals, exploring and investigating these factors is the first step to propose an evaluation approach for OGD portals and promote their usage. In this work, we identified a set of evaluation dimensions that affect OGD portal's usage and fulfillment of users' needs and requirements. According to the identified dimensions, we propose an evaluation of two national OGD portals",2018,,10.1145/3289402.3289526
1907,"Weller, Katrin and Kinder-Kurlanda, Katharina E.",A Manifesto for Data Sharing in Social Media Research,"data archives, social media, methodology, data protection, privacy, archiving, data sharing, reproducibility, legal issues","More and more researchers want to share research data collected from social media to allow for reproducibility and comparability of results. With this paper we want to encourage them to pursue this aim -- despite initial obstacles that they may face. Sharing can occur in various, more or less formal ways. We provide background information that allows researchers to make a decision about whether, how and where to share depending on their specific situation (data, platform, targeted user group, research topic etc.). Ethical, legal and methodological considerations are important for making this decision. Based on these three dimensions we develop a framework for social media sharing that can act as a first set of guidelines to help social media researchers make practical decisions for their own projects. In the long run, different stakeholders should join forces to enable better practices for data sharing for social media researchers. This paper is intended as our call to action for the broader research community to advance current practices of data sharing in the future.",2016,,10.1145/2908131.2908172
1908,"Li, Ying and Zhang, AiMin and Zhang, Xinman and Wu, Zhihui",A Data Lake Architecture for Monitoring and Diagnosis System of Power Grid,"Data Lake, Power Grid, Data Pond, Monitoring And Diagnostic","In this paper, a data lake architecture is proposed for a class of monitoring and diagnostic systems applied to power grid. The differences between data lake and data warehouse is studied to make an informed decision on how to manage a huge amount of data. To adapt to the characteristics and performances of historical data and real-time data of power grid equipment, a monitoring and diagnosis system based on data lake storage architecture is designed. The application of the framework indicates the applicability and effectiveness of data lake architecture.",2018,,10.1145/3299819.3299850
1909,"Yu, Qianqian and Zhang, Jianyong and Qian, Li and Dong, Zhipeng and Huang, Yongwen and Jianhua, Liu",Practice of Constructing Name Authority Database Based on Multi-Source Data Integration,"multi-source, name disambiguation, NSTL, name authority","Name authority is a common issue in digital library. This paper mainly summarizes the practice of constructing name authority database based on multi-source data in NSTL. Firstly, we load, integrate different source data and convert them into unified structure. Then, we extract scientific entities and relationships from these data, according to metadata model. For different entities, we use different disambiguation rules and algorithms. As a result, we have constructed author name authority database, institution authority database, journal authority database, and fund authority database. Compared with Incites, taking six institutions name authority data as a sample, the result shows that the average accuracy can reach 86.8%.1",2019,,10.1109/JCDL.2019.00088
1910,"Ramanujapuram, Arun and Malemarpuram, Charan Kumar",Enabling Sustainable Behaviors of Data Recording and Use in Low-Resource Supply Chains,,"Public services, such as public health supply chains, in low- and middle-income countries can be characterized as low-resource environments, where both infrastructure and human capacity are limited. There is no strong culture of data recording or use, with ad hoc reporting practices, poor planning and lack of coordination. All these lead to poor supply chain performance, thereby restricting access to medicines, and eventually resulting in poorer health and mortality.We describe the ground-up design of Logistimo SCM, a supply chain management software, offered as a service, that has enabled a transformative change in public health supply chains, leading to improved performance. Our approach is rooted in bottom-up empowerment of the human value chain, based on the principle that higher self-efficacy amongst health workers and managers can lead to sustained changes in data recording and use behaviors. This is achieved through a service that optimizes data collection effort, maximizes supervisory bandwidth, promotes proactive and collaborative operations, and enables frictionless performance recognition. We describe the guiding principles of inclusive software service design and four mechanisms that enable the appropriate conditions for stimulating a behavior of data recording and use. We demonstrate their effectiveness in achieving good supply chain performance through case studies in India and Africa. The principles and methods discussed here are generic and can be applied to any low-resource environment.",2020,,
1911,"Grillenberger, Andreas and Romeike, Ralf",Developing a Theoretically Founded Data Literacy Competency Model,"competency model, data management, data, data science, CS education, data literacy","Today, data is everywhere: Our digitalized world depends on enormous amounts of data that are captured by and about everyone and considered a valuable resource. Not only in everyday life, but also in science, the relevance of data has clearly increased in recent years: Nowadays, data-driven research is often considered a new research paradigm. Thus, there is general agreement that basic competencies regarding gathering, storing, processing and visualizing data, often summarized under the term data literacy, are necessary for every scientist today. Moreover, data literacy is generally important for everyone, as it is essential for understanding how the modern world works. Yet, at the moment data literacy is hardly considered in CS teaching at schools. To allow deeper insight into this field and to structure related competencies, in this work we develop a competency model of data literacy by theoretically deriving central content and process areas of data literacy from existing empirical work, keeping a school education perspective in mind. The resulting competency model is contrasted to other approaches describing data literacy competencies from different perspectives. The practical value of this work is emphasized by giving insight into an exemplary lesson sequence fostering data literacy competencies.",2018,,10.1145/3265757.3265766
1912,"Badia, Antonio and Lemire, Daniel",A Call to Arms: Revisiting Database Design,,,2011,,10.1145/2070736.2070750
1913,"Sambasivan, Nithya","All Equation, No Human: The Myopia of AI Models",,"This is a forum for perspectives on designing for marginalized communities worldwide. Articles will discuss design methods, theoretical/conceptual contributions, and participatory interventions with underserved communities. --- Nithya Sambasivan, Editor",2022,,10.1145/3516515
1914,"Fathy, Yasmin and Barnaghi, Payam and Tafazolli, Rahim","Large-Scale Indexing, Discovery, and Ranking for the Internet of Things (IoT)","indexing, wireless sensor network (WSN), large-scale data, discovery, ranking, Internet of things (IoT)","Network-enabled sensing and actuation devices are key enablers to connect real-world objects to the cyber world. The Internet of Things (IoT) consists of the network-enabled devices and communication technologies that allow connectivity and integration of physical objects (Things) into the digital world (Internet). Enormous amounts of dynamic IoT data are collected from Internet-connected devices. IoT data are usually multi-variant streams that are heterogeneous, sporadic, multi-modal, and spatio-temporal. IoT data can be disseminated with different granularities and have diverse structures, types, and qualities. Dealing with the data deluge from heterogeneous IoT resources and services imposes new challenges on indexing, discovery, and ranking mechanisms that will allow building applications that require on-line access and retrieval of ad-hoc IoT data. However, the existing IoT data indexing and discovery approaches are complex or centralised, which hinders their scalability. The primary objective of this article is to provide a holistic overview of the state-of-the-art on indexing, discovery, and ranking of IoT data. The article aims to pave the way for researchers to design, develop, implement, and evaluate techniques and approaches for on-line large-scale distributed IoT applications and services.",2018,,10.1145/3154525
1915,"Satti, Fahad Ahmed and Khan, Wajahat Ali and Lee, Ganghun and Khattak, Asad Masood and Lee, Sungyoung",Resolving Data Interoperability in Ubiquitous Health Profile Using Semi-Structured Storage and Processing,"ACM proceedings, text tagging","Advancements in the field of healthcare information management have led to the development of a plethora of software, medical devices and standards. As a consequence, the rapid growth in quantity and quality of medical data has compounded the problem of heterogeneity; thereby decreasing the effectiveness and increasing the cost of diagnostics, treatment and follow-up. However, this problem can be resolved by using a semi-structured data storage and processing engine, which can extract semantic value from a large volume of patient data, produced by a variety of data sources, at variable rates and conforming to different abstraction levels. Going beyond the traditional relational model and by re-purposing state-of-the-art tools and technologies, we present, the Ubiquitous Health Profile (UHPr), which enables a semantic solution to the data interoperability problem, in the domain of healthcare1.",2019,,10.1145/3297280.3297354
1916,"Budde, Matthias and Riedel, Till",Challenges in Capturing and Analyzing High Resolution Urban Air Quality Data,"sensing, challenges, Air quality, PM10, urban air, particulate matter, PM2.5","Classic measurement grids with their static and expensive infrastructure are unfit to realize modern air quality monitoring needs, such as source appointment, pollution tracking or the assessment of personal exposure. Fine grained air quality assessment (both in time and space) is the future. Different approaches, ranging from measurement with low-cost sensors over advanced modeling and remote sensing to combinations of these have been proposed. This position paper summarizes our previous contributions in this field and lists what we see as open challenges for future research.",2018,,10.1145/3267305.3274762
1917,"Muller, Michael and Guha, Shion and Baumer, Eric P.S. and Mimno, David and Shami, N. Sadat","Machine Learning and Grounded Theory Method: Convergence, Divergence, and Combination","supervised learning, unsupervised learning, axial coding, coding families, grounded theory, machine learning","Grounded Theory Method (GTM) and Machine Learning (ML) are often considered to be quite different. In this note, we explore unexpected convergences between these methods. We propose new research directions that can further clarify the relationships between these methods, and that can use those relationships to strengthen our ability to describe our phenomena and develop stronger hybrid theories.",2016,,10.1145/2957276.2957280
1918,"Lamghari, Zineb and Radgui, Maryam and Saidi, Rajaa and Rahmani, Moulay Driss",Passage Challenges from Data-Intensive System to Knowledge-Intensive System Related to Process Mining Field,"Business Process Management, Adaptive Case Management, Data-intensive, Knowledge-intensive, Process Mining, Process mining challenges","Process mining has emerged as a research field that focuses on the analysis of processes using event data. Process mining is a current topic that reveals several challenges, the most important of which have defined in the Process Mining Manifesto [1]. However, none of the published works have mentioned the progress of process challenges from data-intensive system to knowledge-intensive system related to process mining field. Therefore, the objective of this paper is to provide researchers with the recent challenges emerged during the passage from data-intensive system to knowledge-intensive system.",2019,,10.1145/3333165.3333168
1919,"Liu, Ying and Yu, Wei and Xiao, Suhong",Digital Protection of Nanfeng Nuo Mask Based on AR Technology,,"The sculpture of Nanfeng Nuo mask originated in the Han Dynasty, developed in the Tang and Song Dynasties, and prospered in the Ming and Qing Dynasties. The carving art has been passed down to this day. The sculptures of Nanfeng Nuo masks are famous for their simple and profound, vivid shapes and delicate techniques. This article first stated that under the background of AR technology, in terms of digital protection mode, limited by ideological understanding and technology, the current digital protection of Chinese traditional culture is still at the stage of digital information collection and preservation. How to enrich and perfect the existing digital protection mode with new digital technology is an urgent problem to be solved in the new era. Taking the Nanfeng Nuo mask as an example, this research analyzes the inheritance dilemma of the Nanfeng Nuo mask wood carving and the modern and innovative protection model through the reading of historical documents, field inspections of existing wood carvings, survey visits to the protection status, and understanding of digital technology. Through the fuzzy KNN algorithm and AR compared to the database, the various databases are related to form a complete protection system; in the ""live inheritance protection mode"", AR technology is proposed as the basic technology, and AR image acquisition technology and AR display technology are proposed. , AR human-computer interaction technology and digital protection mode are combined, and then a digital protection platform based on AR technology is designed to achieve the realization of the live inheritance mode by the way audiences participate in the use of the digital protection platform. Experimental research results show that people in their 30s to 40s may take a long time to receive and learn when facing a new interactive operating system, but now due to the popularity of social software such as WeChat, the 30 to 40 age group 24.4% of the population can also perform basic operations, which provides a prerequisite for the use of AR technology to digitize the Nanfeng Nuo mask.",2021,,10.1145/3495018.3495486
1920,"Liu, Jinwei and Shen, Haiying and Narman, Husnu S. and Chung, Wingyan and Lin, Zongfang",A Survey of Mobile Crowdsensing Techniques: A Critical Component for The Internet of Things,"redundancy elimination, quality of service, Internet of Things, cost-effectiveness, Mobile crowdsensing","Mobile crowdsensing serves as a critical building block for emerging Internet of Things (IoT) applications. However, the sensing devices continuously generate a large amount of data, which consumes much resources (e.g., bandwidth, energy, and storage) and may sacrifice the Quality-of-Service (QoS) of applications. Prior work has demonstrated that there is significant redundancy in the content of the sensed data. By judiciously reducing redundant data, data size and load can be significantly reduced, thereby reducing resource cost and facilitating the timely delivery of unique, probably critical information and enhancing QoS. This article presents a survey of existing works on mobile crowdsensing strategies with an emphasis on reducing resource cost and achieving high QoS. We start by introducing the motivation for this survey and present the necessary background of crowdsensing and IoT. We then present various mobile crowdsensing strategies and discuss their strengths and limitations. Finally, we discuss future research directions for mobile crowdsensing for IoT. The survey addresses a broad range of techniques, methods, models, systems, and applications related to mobile crowdsensing and IoT. Our goal is not only to analyze and compare the strategies proposed in prior works, but also to discuss their applicability toward the IoT and provide guidance on future research directions for mobile crowdsensing.",2018,,10.1145/3185504
1921,"Okcan, Alper and Riedewald, Mirek and Panda, Biswanath and Fink, Daniel",Scolopax: Exploratory Analysis of Scientific Data,,"The formulation of hypotheses based on patterns found in data is an essential component of scientific discovery. As larger and richer data sets become available, new scalable and user-friendly tools for scientific discovery through data analysis are needed. We demonstrate Scolopax, which explores the idea of a search engine for hypotheses. It has an intuitive user interface that supports sophisticated queries. Scolopax can explore a huge space of possible hypotheses, returning a ranked list of those that best match the user preferences. To scale to large and complex data sets, Scolopax relies on parallel data management and mining techniques. These include model training, efficient model summary generation, and novel parallel join techniques that together with traditional approaches such as clustering manipulate massive model-summary collections to find the most interesting hypotheses. This demonstration of Scolopax uses a real observational data set, provided by the Cornell Lab of Ornithology. It contains more than 3.3 million bird sightings reported by citizen scientists and has almost 2500 attributes. Conference attendees have the opportunity to make novel discoveries in this data set, ranging from identifying variables that strongly affect bird populations in specific regions to detecting more sophisticated patterns such as habitat competition and migration.",2013,,10.14778/2536274.2536300
1922,"Saddiqa, Mubashrah and Rasmussen, Lise and Magnussen, Rikke and Larsen, Birger and Pedersen, Jens Myrup",Bringing Open Data into Danish Schools and Its Potential Impact on School Pupils,"impact, educational themes, educational resource, open data, school pupils","Private and public institutions are using open and public data to provide better services, which increases the impact of open data on daily life. With the advancement of technology, it becomes also important to equip our younger generation with the essential skills for future challenges. In order to bring up a generation equipped with 21st century skills, open data could facilitate educational processes at school level as an educational resource. Open data could acts as a key resource to enhance the understanding of data through critical thinking and ethical vision among the youth and school pupils. To bring open data into schools, it is important to know the teacher's perspective on open data literacy and its possible impact on pupils. As a research contribution, we answered these questions through a Danish public school teacher's survey where we interviewed 10 Danish public school teachers of grade 5-7th and analyzed their views about the impact of open data on pupils' learning development. After analyzing Copenhagen city's open data, we identified four open data educational themes that could facilitate different subjects, e.g. geography, mathematics, basic science and social science. The survey includes interviews, open discussions, questionnaires and an experiment with the grade 7th pupils, where we test the pupils' understanding with open data. The survey concluded that open data cannot only empower pupils to understand real facts about their local areas, improve civics awareness and develop digital and data skills, but also enable them to come up with the ideas to improve their communities.",2019,,10.1145/3306446.3340821
1923,"Nakuc\c{c}i, Emona and Theodorou, Vasileios and Jovanovic, Petar and Abell\'{o}, Alberto",Bijoux: Data Generator for Evaluating ETL Process Quality,"ETL, data generator, process quality","Obtaining the right set of data for evaluating the fulfillment of different quality standards in the extract-transform-load (ETL) process design is rather challenging. First, the real data might be out of reach due to different privacy constraints, while providing a synthetic set of data is known as a labor-intensive task that needs to take various combinations of process parameters into account. Additionally, having a single dataset usually does not represent the evolution of data throughout the complete process lifespan, hence missing the plethora of possible test cases. To facilitate such demanding task, in this paper we propose an automatic data generator (i.e., Bijoux). Starting from a given ETL process model, Bijoux extracts the semantics of data transformations, analyzes the constraints they imply over data, and automatically generates testing datasets. At the same time, it considers different dataset and transformation characteristics (e.g., size, distribution, selectivity, etc.) in order to cover a variety of test scenarios. We report our experimental findings showing the effectiveness and scalability of our approach.",2014,,10.1145/2666158.2666183
1924,"Andersen, Kim Normann and Lee, Jungwoo and Henriksen, Helle Zinner",Digital Sclerosis? Wind of Change for Government and the Employees,"work, e-Government, public sector, changing nature of work, workplace, digitalization, future work, digital sclerosis","Contrasting the political ambitions on the next generation of government, the uptake of technology can lead to digital sclerosis characterized by stiffening of the governmental processes, failure to respond to changes in demand, and lowering innovation feedback from workers. In this conceptual article, we outline three early warnings of digital sclerosis: decreased bargaining and discretion power of governmental workers, enhanced agility and ability at shifting and extended proximities, and panopticonization. To respond proactively and take preventive care initiatives, policy makers and systems developers need to be sensitized about the digital sclerosis, prepare the technology, and design intelligent augmentations in a flexible and agile approach.",2020,,10.1145/3360000
1925,"Stefanidis, Dimosthenis and Christodoulou, Chrysovalantis and Symeonidis, Moysis and Pallis, George and Dikaiakos, Marios and Pouis, Loukas and Orphanou, Kalia and Lampathaki, Fenareti and Alexandrou, Dimitrios",The ICARUS Ontology: A General Aviation Ontology Developed Using a Multi-Layer Approach,"queries, aviation, datasets, services, ontology","The management of aviation data is a great challenge in the aviation industry, as they are complex and can be derived from heterogeneous data sources. To handle this challenge, ontologies can be applied to facilitate the modelling of the data across multiple data sources. This paper presents an aviation domain ontology, the ICARUS ontology, which aims at facilitating the semantic description and integration of information resources that represent the various assets of the ICARUS platform and their use. To present the functionality and usability of the proposed ontology, we present the results of querying the ontology using SPARQL queries through three use case scenarios. As shown from the evaluation, the ICARUS ontology enables the integration and reasoning over multiple sources of heterogeneous aviation-related data, the semantic description of metadata produced by ICARUS, and their storage in a knowledge-base which is dynamically updated and provides access to its contents via SPARQL queries.",2020,,10.1145/3405962.3405983
1926,"Klein, Martin and Xie, Zhiwu and Fox, Edward A.",Web Archiving and Digital Libraries (WADL),"digital preservation, web archiving, community building",The 2018 edition of the Workshop on Web Archiving and Digital Libraries (WADL) will explore the integration of Web archiving and digital libraries. The workshop aims at addressing aspects covering the entire life cycle of digital resources and will also explore areas such as community building and ethical questions around web archiving.,2018,,10.1145/3197026.3200209
1927,"Suhail, Sabah and Hussain, Rasheed and Jurdak, Raja and Oracevic, Alma and Salah, Khaled and Hong, Choong Seon and Matulevi\v{c}ius, Raimundas","Blockchain-Based Digital Twins: Research Trends, Issues, and Future Challenges","Digital Twins (DTs), Blockchain, Cyber-Physical Systems (CPSs), Industrial Control Systems (ICSs), Artificial Intelligence (AI), Internet of Things (IoT), Industry 4.0","Industrial processes rely on sensory data for decision-making processes, risk assessment, and performance evaluation. Extracting actionable insights from the collected data calls for an infrastructure that can ensure the dissemination of trustworthy data. For the physical data to be trustworthy, it needs to be cross-validated through multiple sensor sources with overlapping fields of view. Cross-validated data can then be stored on the blockchain, to maintain its integrity and trustworthiness. Once trustworthy data is recorded on the blockchain, product lifecycle events can be fed into data-driven systems for process monitoring, diagnostics, and optimized control. In this regard, Digital Twins (DTs) can be leveraged to draw intelligent conclusions from data by identifying the faults and recommending precautionary measures ahead of critical events. Empowering DTs with blockchain in industrial use-cases targets key challenges of disparate data repositories, untrustworthy data dissemination, and the need for predictive maintenance. In this survey, while highlighting the key benefits of using blockchain-based DTs, we present a comprehensive review of the state-of-the-art research results for blockchain-based DTs. Based on the current research trends, we discuss a trustworthy blockchain-based DTs framework. We also highlight the role of Artificial Intelligence (AI) in blockchain-based DTs. Furthermore, we discuss the current and future research and deployment challenges of blockchain-supported DTs that require further investigation.",2022,,10.1145/3517189
1928,"Tentori, Monica and Ziviani, Artur and Muchaluat-Saade, D\'{e}bora C. and Favela, Jesus",Digital Healthcare in Latin America: The Case of Brazil and Mexico,,,2020,,10.1145/3423923
1929,"Wang, Deli",Research on Bank Marketing Behavior Based on Machine Learning,"C5.0 algorithm, Classification algorithm, Customer segmentation, Bank Direct Sales Project","At present, under the background that data mining technology is becoming more mature and widely used in various fields, and due to the advent of the customer-oriented era and increased competition from banks, data mining technology is being widely used in the field of banking and finance to determine the target customer group And promote bank sales. Therefore, based on the Bank Marketing data in the UCI Machine Learning Repository database, this article uses the C5.0 algorithm to classify customers on the clementine experimental platform, and proposes corresponding suggestions for bank marketing based on the classification results.This article first explores and understands the Bank Marketing data set, and describes the distribution of the customer background in the data set. The quality of the data set was further explored, and the outliers and outliers were corrected by replacing them with normal data that were closest to the outliers or extreme values.This paper further selects the optimal feature variable. First, use the Filter node to filter the unimportant variables of the classification, and further select one of the more relevant variables to reduce the redundancy of the variables. The final variables are: previous, age, duration, outcome, contact, housing, job, loan, marital, education.Secondly, this paper uses sampling nodes to perform undersampling to balance the data set. On this basis, the C5.0 algorithm is used to establish a classification model and optimize parameters, and finally obtain eight classification rules. Based on this, suggestions are provided for target group determination.Finally, this article introduces the remaining four classification algorithms: C&amp;T, QUEST, CHAID, Neural Networks, and compares the C5.0 algorithm with the four classification algorithms based on the balanced data set. It is concluded that several algorithms have certain differences and the overall prediction accuracy is good.This article combines data mining theory with practical problems of banking business, and establishes a bank target customer classification model based on C5.0 algorithm. The obtained classification rules can effectively help banks to divide customer groups and take targeted measures to improve marketing efficiency.",2020,,10.1145/3421766.3421800
1930,"Alexander, Rohan and Lyons, Kelly and Alexopoulos, Michelle and Austin, Lisa",Workshop on Barriers to Data Science Adoption: Why Existing Frameworks Aren't Working,"organisational, data science adoption, legal, business practices, challenges","Data science is an interdisciplinary scientific approach that provides methods to understand and solve problems in an evidence-based manner, using data and experience. Despite the clear benefits from adoption, many firms face challenges, be that legal, organisational, or business practices, when seeking to implement and embed data science within an existing framework. In this workshop, panel and audience members drew on their experiences to elaborate on the challenges encountered when attempting to deploying data science within existing frameworks. Panel and audience members were drawn from business, academia, and think-tanks. For discussion purposes the challenges were grouped within three themes: regulatory; investment; and workforce.",2019,,
1931,"Tang, Xinzhong and Zhuang, Bing and Yao, Ying and Dong, Xuesong",Research on High-Reliability Intelligent-Sensing Health Service Support Platform and Key Technologies Based on Biometrics and Blockchain Security Technology,"biological characteristics, deep learning, High-reliability, blockchain, health services","A new type of high-reliability intelligent-sensing health service support platform and its key technologies are introduced in this article. By the technologies of automatic data collection, perceptual data removal, abnormal data detection, perceptual heterogeneous data identification, it is realizable to collect, analyze and process the health data of users. In order to solve the security problem in the process of data transmission, biometric identification and blockchain are used to realize the high-reliability transmission. At the end of the paper, a high-reliability intelligent-sensing health service support platform is built. And the implementation and service support process are expounded, indicating that the platform has high practical value.",2021,,10.1145/3482632.3487461
1932,"Yang, Rui",Statistics and Mining Analysis of Lightning Monitoring Data in Power Grid Based on Classical Metrology Model,,"Lightning, also known as lightning, is a strong catastrophic discharge phenomenon between clouds and between clouds and the ground in the process of atmospheric convection. There are two types: cloud flash (between clouds) and ground flash (between clouds and the earth). Lightning location system is a system that uses telemetry technology to monitor lightning activities in full-automatic, large-area, high-precision, continuous and real-time. By analyzing lightning location data collected for a long time, lightning accident points can be quickly located, the distribution of regional lightning activities can be counted, the development trend can be analyzed, and early warning can be carried out, which can provide reference for lightning protection research of ground buildings, thus reducing the harm to human activities. In this paper, the grid method is used to store and query lightning data based on the classical measurement model. Taking the lightning protection technology of transmission network as an example, the method and application of statistics and mining of lightning monitoring data in power grid are studied.",2021,,10.1145/3482632.3484010
1933,"Song, Zekun and Zhang, Lvyang and Liu, Tao and Chen, Ying",Ranking Learning Algorithm of Information Retrieval Based on WeChat Public Numbers,"Rank learning algorithm, Meta data model, Recommendation system, WeChatpublic number","On the basis of obtaining the data of mass WeChat public1, in order to improve the operational efficiency and quality of WeChat public number. On the basis of the retrieval technology, the quality evaluation model of WeChat public number was established. A sort learning algorithm based on model retrieval is proposed. Use the vector space technology based on the weight of the entry position to retrieve the contents of WeChat public number, and then use the WeChat public number quality evaluation model to sort. The retrieved articles sorted data to recommend to the operator, so that the operator can be faster and more efficient to find their hope to find high quality WeChat number of public articles.",2017,,10.1145/3078564.3078572
1934,"Baban, Philsy",Pre-Processing and Data Validation in IoT Data Streams,"resiliency, stream processing, data pre-processing, data validation","In the last few years, distributed stream processing engines have been on the rise due to their crucial impacts on real-time data processing with guaranteed low latency in several application domains such as financial markets, surveillance systems, manufacturing, smart cities, etc. Stream processing engines are run-time libraries to process data streams without knowing the lower level streaming mechanics. Apache Storm, Apache Flink, Apache Spark, Kafka Streams and Hazelcast Jet are some of the popular stream processing engines. Nowadays, critical systems like energy systems, are interconnected and automated. As a result, these systems are vulnerable to cyber-attacks. In real-world applications, the sensing values come from sensor devices contains missing values, redundant data, data outliers, manipulated data, data failures, etc. Therefore, our system must be resilient to these conditions. In this paper, we present an approach to check if there is any above mentioned conditions by pre-processing data streams using a stream processing engine like Apache Flink which will be updated as a library in future. Then, the pre-processed streams are forwarded to other stream processing engines like Apache Kafka for real stream processing. As a result, data validation, data consistency and integrity for a resilient system can be accomplished before initiating the actual stream processing.",2020,,10.1145/3401025.3406443
1935,"Zeleti, Fatemeh Ahmadi and Ojo, Adegboyega",An Ontology for Open Government Data Business Model,"e-Commerce ontology, e-Business ontology, open data business model, and business model ontology, open data-driven organization, formal conceptualization, Open government data","Despite the existence of number of well-known conceptualization in e-Business and e-Commerce, there have been no efforts so far to develop a detailed, comprehensive conceptualization for business model. Current business literature is replete with fragmented conceptualizations, which only partially describe aspects of a business model. In addition, the existing conceptualizations do not explicitly support the emerging phenomenon of open government data -- an increasingly valuable economic and strategic resource. Consequently, no comprehensive, formal, executable open government data business model ontology exists, that could be directly leveraged to facilitate the design, development of an operational open data business model. This paper bridges this gap by providing a parsimonious yet sufficiently detailed, conceptualization and formal ontology of open government data business model for open data-driven organizations. Following the design science approach, we developed the ontology as a 'design artefact' and validate the ontology by using it to describe an open data business model of an open data-driven organization.",2017,,10.1145/3047273.3047327
1936,"Manea, Vlad and Berrocal, Allan and De Masi, Alexandre and M\o{}ller, Naja Holten and Wac, Katarzyna and Bayer, Hannah and Lehmann, Sune and Ashley, Euan",LDC '19: International Workshop on Longitudinal Data Collection in Human Subject Studies,"panel technique, human sensing, human subject studies, mobile devices, longitudinal studies, in situ","Individuals increasingly use mobile, wearable, and ubiquitous devices capable of unobtrusive collection of vast amounts of scientifically rich personal data over long periods (months to years), and in the context of their daily life. However, numerous human and technological factors challenge longitudinal data collection, often limiting research studies to very short data collection periods (days to weeks), spawning recruitment biases, and affecting participant retention over time. This workshop is designed to bring together researchers involved in longitudinal data collection studies to foster an insightful exchange of ideas, experiences, and discoveries to improve the studies' reliability, validity, and perceived meaning of longitudinal mobile, wearable, and ubiquitous data collection for the participants.",2019,,10.1145/3341162.3347758
1937,"De Aguiar, Erikson J\'{u}lio and Fai\c{c}al, Bruno S. and Krishnamachari, Bhaskar and Ueyama, J\'{o}",A Survey of Blockchain-Based Strategies for Healthcare,"survey, medical, blockchain, healthcare, distributed ledger technology, Distributed systems","Blockchain technology has been gaining visibility owing to its ability to enhance the security, reliability, and robustness of distributed systems. Several areas have benefited from research based on this technology, such as finance, remote sensing, data analysis, and healthcare. Data immutability, privacy, transparency, decentralization, and distributed ledgers are the main features that make blockchain an attractive technology. However, healthcare records that contain confidential patient data make this system very complicated because there is a risk of a privacy breach. This study aims to address research into the applications of the blockchain healthcare area. It sets out by discussing the management of medical information, as well as the sharing of medical records, image sharing, and log management. We also discuss papers that intersect with other areas, such as the Internet of Things, the management of information, tracking of drugs along their supply chain, and aspects of security and privacy. As we are aware that there are other surveys of blockchain in healthcare, we analyze and compare both the positive and negative aspects of their papers. Finally, we seek to examine the concepts of blockchain in the medical area, by assessing their benefits and drawbacks and thus giving guidance to other researchers in the area. Additionally, we summarize the methods used in healthcare per application area and show their pros and cons.",2020,,10.1145/3376915
1938,"Beek, Wouter and Fern\'{a}ndez, Javier D. and Verborgh, Ruben",LOD-a-Lot: A Single-File Enabler for Data Science,,"Many data scientists make use of Linked Open Data (LOD) as a huge interconnected knowledge base represented in RDF. However, the distributed nature of the information and the lack of a scalable approach to manage and consume such Big Semantic Data makes it difficult and expensive to conduct large-scale studies. As a consequence, most scientists restrict their analyses to one or two datasets (often DBpedia) that contain at most hundreds of millions of triples. LOD-a-lot is a dataset that integrates a large portion (over 28 billion triples) of the LOD Cloud into a single ready-to-consume file that can be easily downloaded, shared and queried with a small memory footprint. This paper shows there exists a wide collection of Data Science use cases that can be performed over such a LOD-a-lot file. For these use cases LOD-a-lot significantly reduces the cost and complexity of conducting Data Science.",2017,,10.1145/3132218.3132241
1939,"Resch, Bernd and Blaschke, Thomas",Fusing Human and Technical Sensor Data: Concepts and Challenges,,"As geo-sensor webs have not grown as quickly as expected, new, alternative data sources have to be found for near real-time analysis in areas like emergency management, environmental monitoring, public health, or urban planning. This paper assesses the ability of human sensors, i.e., user-generated observations in a wide range of social networks, the mobile phone network, or micro-blogs, to complement geo-sensor networks. We clearly delineate the concepts of People as Sensors, Collective Sensing and Citizen Science. Furthermore, we point out current challenges in fusing data from technical and human sensors, and sketch future research areas in this field.",2015,,10.1145/2826686.2826692
1940,"Cabanac, Guillaume and Chandrasekaran, Muthu Kumar and Frommholz, Ingo and Jaidka, Kokil and Kan, Min-Yen and Mayr, Philipp and Wolfram, Dietmar",Joint Workshop on Bibliometric-Enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2016),"bibliometrics, natural language processing, information retrieval, text mining, digital libraries","The large scale of scholarly publications poses a challenge for scholars in information-seeking and sensemaking. Bibliometric, information retrieval~(IR), text mining and NLP techniques could help in these activities, but are not yet widely used in digital libraries. This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometric and recommendation techniques which can advance the state-of-the-art in scholarly document understanding, analysis and retrieval at scale.",2016,,10.1145/2910896.2926734
1941,"Herschel, Melanie and Diestelk\""{a}mper, Ralf and Ben Lahmar, Houssem",A Survey on Provenance: What for? What Form? What From?,"Survey, Provenance types, Workflow provenance, Provenance applications, Data provenance, Provenance capture, Provenance requirements","Provenance refers to any information describing the production process of an end product, which can be anything from a piece of digital data to a physical object. While this survey focuses on the former type of end product, this definition still leaves room for many different interpretations of and approaches to provenance. These are typically motivated by different application domains for provenance (e.g., accountability, reproducibility, process debugging) and varying technical requirements such as runtime, scalability, or privacy. As a result, we observe a wide variety of provenance types and provenance-generating methods. This survey provides an overview of the research field of provenance, focusing on what provenance is used for (what for?), what types of provenance have been defined and captured for the different applications (what form?), and which resources and system requirements impact the choice of deploying a particular provenance solution (what from?). For each of these three key questions, we provide a classification and review the state of the art for each class. We conclude with a summary and possible future research challenges.",2017,,10.1007/s00778-017-0486-1
1942,"Schaberreiter, Thomas and Kupfersberger, Veronika and Rantos, Konstantinos and Spyros, Arnolnt and Papanikolaou, Alexandros and Ilioudis, Christos and Quirchmayr, Gerald",A Quantitative Evaluation of Trust in the Quality of Cyber Threat Intelligence Sources,"quality parameters, cyber threat information sharing, trust indicators, Cooperative and collaborative cybersecurity, cyber threat intelligence source evaluation","Threat intelligence sharing has become a cornerstone of cooperative and collaborative cybersecurity. Sources providing such data have become more widespread in recent years, ranging from public entities (driven by legislatorial changes) to commercial companies and open communities that provide threat intelligence in order to help organisations and individuals to better understand and assess the cyber threat landscape putting their systems at risk. Tool support to automatically process this information is emerging concurrently. It has been observed that the quality of information received by the sources varies significantly and that in order to assess the quality of a threat intelligence source it is not sufficient to only consider qualitative indications of the source itself, but it is necessary to monitor the data provided by the source continuously to be able to draw conclusions about the quality of information provided by a source. In this paper, we propose a methodology for evaluating cyber threat information sources based on quantitative parameters. The methodology aims to facilitate trust establishment to threat intelligence sources, based on a weighted evaluation method that allows each entity to adapt it to its own needs and priorities. The approach facilitates automated tools utilising threat intelligence, since information to be considered can be prioritised based on which source is trusted the most at the time the intelligence arrives.",2019,,10.1145/3339252.3342112
1943,"Wang, Ying and Wen, Ming and Liu, Zhenwei and Wu, Rongxin and Wang, Rui and Yang, Bo and Yu, Hai and Zhu, Zhiliang and Cheung, Shing-Chi",Do the Dependency Conflicts in My Project Matter?,"third party library, static analysis, Empirical study","Intensive dependencies of a Java project on third-party libraries can easily lead to the presence of multiple library or class versions on its classpath. When this happens, JVM will load one version and shadows the others. Dependency conflict (DC) issues occur when the loaded version fails to cover a required feature (e.g., method) referenced by the project, thus causing runtime exceptions. However, the warnings of duplicate classes or libraries detected by existing build tools such as Maven can be benign since not all instances of duplication will induce runtime exceptions, and hence are often ignored by developers. In this paper, we conducted an empirical study on real-world DC issues collected from large open source projects. We studied the manifestation and fixing patterns of DC issues. Based on our findings, we designed Decca, an automated detection tool that assesses DC issues' severity and filters out the benign ones. Our evaluation results on 30 projects show that Decca achieves a precision of 0.923 and recall of 0.766 in detecting high-severity DC issues. Decca also detected new DC issues in these projects. Subsequently, 20 DC bug reports were filed, and 11 of them were confirmed by developers. Issues in 6 reports were fixed with our suggested patches.",2018,,10.1145/3236024.3236056
1944,"Qin, Xuedi and Luo, Yuyu and Tang, Nan and Li, Guoliang",Making Data Visualization More Efficient and Effective: A Survey,"Visualization languages, Data visualization, Data visualization recommendation, Efficient data visualization","Data visualization is crucial in today’s data-driven business world, which has been widely used for helping decision making that is closely related to major revenues of many industrial companies. However, due to the high demand of data processing w.r.t. the volume, velocity, and veracity of data, there is an emerging need for database experts to help for efficient and effective data visualization. In response to this demand, this article surveys techniques that make data visualization more efficient and effective. (1) Visualization specifications define how the users can specify their requirements for generating visualizations. (2) Efficient approaches for data visualization process the data and a given visualization specification, which then produce visualizations with the primary target to be efficient and scalable at an interactive speed. (3) Data visualization recommendation is to auto-complete an incomplete specification, or to discover more interesting visualizations based on a reference visualization.",2020,,10.1007/s00778-019-00588-3
1945,"Tian, Bing and Lv, Shuqing and Yin, Qilin and Li, Ning and Zhang, Yue and Liu, Ziyan",Real-Time Dynamic Data Desensitization Method Based on Data Stream,"data desensitization, stream data, dynamic desensitization","With the rapid development of the data mining industry, the value hidden in the massive data has been discovered, but at the same time it has also raised concerns about privacy leakage, leakage of sensitive data and other issues. These problems have also become numerous studies. Among the methods for solving these problems, data desensitization technology has been widely adopted for its outstanding performance. However, with the increasing scale of data and the increasing dimension of data, the traditional desensitization method for static data can no longer meet the requirements of various industries in today's environment to protect sensitive data. In the face of ever-changing data sets of scale and dimension, static desensitization technology relies on artificially designated desensitization rules to grasp the massive data, and it is difficult to control the loss of data connotation. In response to these problems, this paper proposes a real-time dynamic desensitization method based on data flow, and combines the data anonymization mechanism to optimize the data desensitization strategy. Experiments show that this method can efficiently and stably perform real-time desensitization of stream data, and can save more information to support data mining in the next steps.",2019,,10.1145/3373477.3373499
1946,,Preface,,"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.",2019,,
1947,"Chaudhuri, Surajit and Dayal, Umeshwar and Narasayya, Vivek",An Overview of Business Intelligence Technology,,BI technologies are essential to running today's businesses and this technology is going through sea changes.,2011,,10.1145/1978542.1978562
1948,"Koesten, Laura and Simperl, Elena",UX of Data: Making Data Available Doesn't Make It Usable,,"This forum provides a space to engage with the challenges of designing for intelligent algorithmic experiences. We invite articles that tackle the tensions between research and practice when integrating AI and UX design. We welcome interdisciplinary debate, artful critique, forward-looking research, case studies of AI in practice, and speculative design explorations. --- Juho Kim and Henriette Cramer, Editors",2021,,10.1145/3448888
1949,"de Jager, Tiaan and Brown, Irwin",A Descriptive Categorized Typology of Requisite Skills for Business Intelligence Professionals,"Analytics, IT Skills, Typology, IS Profession, Business Intelligence","Business Intelligence (BI) is regarded by executives as a critical practice to adopt and invest in. The purpose of this research is to develop a categorized typology of skills required by BI professionals. A review of extant literature resulted in the identification of twenty three skills. The research aimed to validate these skills, and add additional skills to this typology based on the experiences of BI professionals within industry. These experiences were captured through interviews. Skills were then categorized by identifying commonalities across them. No additional skills were identified by the interviewed participants. A categorized typology of skills was developed which grouped the initial twenty three skills into seven higher order categories. The seven categories of skills were identified as: (1) Prepare data for subject matter expert (SME), analyst or other external party for further analysis; (2) Apply simulation modelling, statistical techniques and provide business insight; (3) Manage stakeholders and project and operational tasks; (4) Develop strategic long term BI roadmap that links to corporate strategy; (5) Understand business processes in order to effectively extract user requirements; (6) Design and code sustainable solutions; (7) Absorb and distribute knowledge.",2016,,10.1145/2987491.2987521
1950,"Siqueira, Sean W. M. and Carvalho, Sergio T.","Session Details: Main Track - Management, Governance, and Government",,,2015,,
1951,"Al-Jaroodi, Jameela and Mohamed, Nader and Jawhar, Imad",A Service-Oriented Middleware Framework for Manufacturing Industry 4.0,"smart manufacturing, fog computing, middleware, IoT, cyber-physical systems, cloud computing, industry 4.0","The advantages of the Internet of things (IoT) initiated the vision of Industry 4.0 in Europe and smart manufacturing in USA. Both visions aim to implement the smart factory to achieve similar objectives by utilizing new technologies. These technologies include cloud computing, fog computing, cyber-physical systems (CPS), and data analytics. Together they help automate and autonomize the manufacturing processes and controls to optimize the productivity, reliability, quality, cost-effeteness, and safety of these processes. While both visions are promising, developing and operating Industry 4.0 applications are extremely challenging. This is due to the complexity of the manufacturing processes as well as their management, controls, and integration dynamics. This paper introduces Man4Ware, a service-oriented middleware for Industry 4.0. Man4Ware can help facilitate the development and operations of cloud and fog-integrated smart manufacturing applications. Man4Ware offers many advantages through service level interfaces to enable easy utilization of new technologies and integration of different services to relax many of the challenges facing the development and operations of such applications1.",2018,,10.1145/3292384.3292389
1952,"Valachamy, Mageshwari and Sahibuddin, Shamsul and Ahmad, Noor Azurati and Bakar, Nur Azaliah Abu",Geospatial Data Sharing: Preliminary Studies on Issues and Challenges in Natural Disaster Management,"Geospatial data, Spatial Data Infrastructure, Sharing, Spatial data, Issues and Challenges","The rapid development of information technology has led to the demand for the latest, precise and easy to understand data. Data especially geospatial data is becoming increasingly crucial in all types of planning and decision making. Geospatial data sharing can be categorized into different disciplines such as public safety, disaster management, transportation, traffic control, tracking, health, environment, natural resources, mining, agriculture, utilities and many more. Whether as a way of distribution or retrieval of data, geospatial data has become an essential component of government GIS operations. Despite the prominence of this activity and its centrality to the day-to-day function of many government systems, the geospatial data sharing is still given less attention in the field of natural disaster management. Preliminary information is gathered from Literature Reviews (LR) and unstructured interviews with experts to seek information in depth. Thirteen (13) issues and challenges of geospatial data sharing in Malaysia Public Sector (MPS) for natural disaster management have been identified.",2020,,10.1145/3384544.3384596
1953,"H. Gyldenkaerne, Christopher and From, Gustav and M\o{}nsted, Troels and Simonsen, Jesper",PD and The Challenge of AI in Health-Care,"Participatory Design, Electronic Health Record data, precision medicine, Primary- and Secondary Use data, Artificial Intelligence","In its promise to contribute to considerable cost savings and improved patient care through efficient analysis of the tremendous amount of data stored in electronic health records (EHR), there is currently a strong push for the proliferation of artificial intelligence (AI) in health-care. We identify, through a study of AI being used to predict patient no-show’s, that for the AI to gain full potential there lies a need to balance the introduction of AI with a proper focus on the patients and the clinicians’ interests. We call for a Participatory Design (PD) approach to understand and reconfigure the socio-technical setup in health-care, especially where AI is being used on EHR data that are manually being submitted by health-care personnel.",2020,,10.1145/3384772.3385138
1954,,References,,"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.",2019,,
1955,"Morstatter, Fred and Liu, Huan",Replacing Mechanical Turkers? Challenges in the Evaluation of Models with Semantic Properties,"crowdsourcing, data mining, automation, evaluation, Artificial intelligence",,2016,,10.1145/2935752
1956,"Maamar, Zakaria and Baker, Thar and Faci, Noura and Ugljanin, Emir and Khafajiy, Mohammed Al and Bur\'{e}gio, Vanilson",Towards a Seamless Coordination of Cloud and Fog: Illustration through the Internet-of-Things,"healthcare, cloud, coordination, internet-of-things, fog","With the increasing popularity of the Internet-of-Things (IoT), organizations are revisiting their practices as well as adopting new ones so they can deal with an ever-growing amount of sensed and actuated data that IoT-compliant things generate. Some of these practices are about the use of cloud and/or fog computing. The former promotes ""anything-as-a-service"" and the latter promotes ""process data next to where it is located"". Generally presented as competing models, this paper discusses how cloud and fog could work hand-in-hand through a seamless coordination of their respective ""duties"". This coordination stresses out the importance of defining where the data of things should be sent (either cloud, fog, or cloud&amp;fog concurrently) and in what order (either cloud then fog, fog then cloud, or fog&amp;cloud concurrently). Applications' concerns with data such as latency, sensitivity, and freshness dictate both the appropriate recipients and the appropriate orders. For validation purposes, a healthcare-driven IoT application along with an in-house testbed, that features real sensors and fog and cloud platforms, have permitted to carry out different experiments that demonstrate the technical feasibility of the coordination model.",2019,,10.1145/3297280.3297477
1957,"Baeza-Yates, Ricardo",Bias on the Web,,"Bias in Web data and use taints the algorithms behind Web-based applications, delivering equally biased results.",2018,,10.1145/3209581
1958,"Truong, Hong-Linh","Dynamic IoT Data, Protocol, and Middleware Interoperability with Resource Slice Concepts and Tools: Tutorial","cloud computing, resource slice, IoT interoperability","Dealing with interoperability in the IoT domain is a complex matter that requires various techniques for tackling data, protocol and middleware interoperability. We cannot solve IoT interoperability problems by just developing (new) software components and (semantic) data models. In this tutorial, we will present interoperability techniques for complex IoT Cloud applications by leveraging dynamic solutions of provisioning and reconfiguring of IoT data processing pipelines, protocol bridges, IoT middleware and cloud services. First, the tutorial will examine cross-layered, cross-system inter-operability issues and present a DevOps IoT Interoperability approach for defining metadata, selecting resources and software artifacts, and provisioning and connecting resources to create various potential solutions for IoT Cloud interoperability using resource slice concepts. Second, the tutorial will present techniques for dynamically provisioning data pipelines, middleware services, protocol adapters and custom solutions to address cross-layered, cross-system interoperability for IoT Cloud applications. Such solutions also allow dynamic reconfiguration of resources to add/remove interoperability support. We will present the concepts and techniques with hands-on examples using our research tools rsiHub and IoTCloudSamples.",2018,,10.1145/3277593.3277642
1959,"Alonso, Omar and Kamps, Jaap and Karlgren, Jussi",Seventh Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR'14): CIKM 2014 Workshop,"semantic annotation, query suggest, graph search","There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, emerging robust NLP tools, and an ever growing volume of linked data. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. The goal of the ESAIR'14 workshop remains to advance the general research agenda on this core problem, with an explicit focus on one of the most challenging aspects to address in the coming years. The main remaining challenge is on the user's side - the potential of rich document annotations can only be realized if matched by more articulate queries exploiting these powerful retrieval cues - and a more dynamic approach is emerging by exploiting new forms of query autosuggest. How can the query suggestion paradigm be used to encourage searcher to articulate longer queries, with concepts and relations linking their statement of request to existing semantic models? How do entity results and social network data in ""graph search"" change the classic division between searchers and information and lead to extreme personalization - are you the query? How to leverage transaction logs and recommendation, and how adaptive should we make the system? What are the privacy ramifications and the UX aspects - how to not creep out users?",2014,,10.1145/2661829.2663539
1960,"Rizzo, Giuseppe and Corcho, Oscar and Troncy, Rapha\""{e}l and Plu, Julien and Hermida, Juan Carlos Ballesteros and Assaf, Ahmad",The 3cixty Knowledge Base for Expo Milano 2015: Enabling Visitors to Explore the City,"Smart City, Data Integration, Expo 2015, 3cixty, Data Reconciliation, Knowledge base","In this paper, we present the 3cixty Knowledge Base, which collects and harmonizes descriptions of events, places, transportation facilities and user-generated data such as reviews of the city and Expo site of Milan. This knowledge base is used by a set of web and mobile applications to guide Expo Milano 2015 visitors in the city and in the exhibit, allowing them to find places, satellite events and transportation facilities around Milan. As of July 24th, 2015 the knowledge base contains 18665 unique events, 225821 unique places, 94789 reviews, and 9343 transportation facilities, collected from several static, near- and real time local and global data providers, including Expo Milano 2015 official services and numerous social media platforms. The ontologies used as a backbone for structuring the knowledge base follow a rigorous development method where the design principle has generally been to re-use existing ontologies when they exist. We think that the lessons learned from this development will be useful for similar endeavors in other cities or large events around the world with a similar ecosystem of data provisioning services.",2015,,10.1145/2815833.2816944
1961,"Tan, Wenan and Jiang, Zihui",A Novel Experience-Based Incentive Mechanism for Mobile Crowdsensing System,"mobile crowdsensing, sensor network, fairness competition, incentive mechanism","While sensor networks have been pervasively deployed in the real world, more and more mobile crowdsensing (MCS) applications have come into realization to collaboratively detect events and collect data. This paper aims to design a novel incentive mechanism to achieve good services for mobile crowdsensing applications. Responding to insufficient participants, we propose a novel Experience-Based incentive mechanism using Reverse Auction (EBRA). Additionally, it can also guarantee fair competition while maximizing the total profit of the service platform. Through strictly proving, our proposed EBRA incentive mechanism satisfies four properties: computational efficiency, individual rationality, profitability, and truthfulness. The extensive simulations show that the proposed EBRA method has a better performance over 20% than other benchmark mechanisms.",2019,,10.1145/3371425.3371459
1962,"Baasch, Gaby and Wicikowski, Adam and Faure, Ga\""{e}lle and Evins, Ralph",Comparing Gray Box Methods to Derive Building Properties from Smart Thermostat Data,"thermal characteristics, gray box models, smart thermostats, Buildings","The development of quantitative techniques for determining the amount of heat lost through the building envelope is essential for targeted retrofits. This type of evaluation is traditionally a resource intensive process that involves onsite appraisal and in-situ measurements. In order to build more efficient and scalable methods for retrofit analysis, new sources of data could be used. Smart thermostat data, for example, provide a valuable resource, however they often lack detailed information about the building characteristics and energy loads. This paper presents and compares three methods for assessing heating characteristics of households using a dataset that does not contain heating power. The three methods are based on: (1) balance point plots, (2) the extraction of indoor temperature decay curves, and (3) the classic differential equation for indoor temperature. These methods all take a gray box approach in which physics-based and machine learning models are combined. The dataset used for this study consists of over 4,000 houses in Ontario and New York. The three methods are applied to each building and the resulting data is analyzed to determine whether the results are statistically sound. It is found that there is a positive linear correlation between characteristics derived for each method, although there is uncertainty about absolute values. This result indicates that the methods can be used to ascertain relative values for the thermal characteristics of a building. The methods suggested in this paper may therefore be used to filter heating profiles to target potential retrofit measures or other stock-level decisions.",2019,,10.1145/3360322.3360836
1963,"Zouari, Firas and Kabachi, Nadia and Boukadi, Khouloud and Ghedira Guegan, Chirine",Data Management in the Data Lake: A Systematic Mapping,"Data management, Data lake, Systematic mapping"," The computer science community is paying more and more attention to data due to its crucial role in performing analysis and prediction. Researchers have proposed many data containers such as files, databases, data warehouses, cloud systems, and recently data lakes in the last decade. The latter enables holding data in its native format, making it suitable for performing massive data prediction, particularly for real-time application development. Although data lake is well adopted in the computer science industry, its acceptance by the research community is still in its infancy stage. This paper sheds light on existing works for performing analysis and predictions on data placed in data lakes. Our study reveals the necessary data management steps, which need to be followed in a decision process, and the requirements to be respected, namely curation, quality evaluation, privacy-preservation, and prediction. This study aims to categorize and analyze proposals related to each step mentioned above.",2021,,10.1145/3472163.3472173
1964,"Ali, Syed Muhammad and Wrembel, Robert",From Conceptual Design to Performance Optimization of ETL Workflows: Current State of Research and Open Problems,"ETL optimization, ETL logical design, ETL workflow, ETL conceptual design, ETL physical implementation","In this paper, we discuss the state of the art and current trends in designing and optimizing ETL workflows. We explain the existing techniques for: (1) constructing a conceptual and a logical model of an ETL workflow, (2) its corresponding physical implementation, and (3) its optimization, illustrated by examples. The discussed techniques are analyzed w.r.t. their advantages, disadvantages, and challenges in the context of metrics such as autonomous behavior, support for quality metrics, and support for ETL activities as user-defined functions. We draw conclusions on still open research and technological issues in the field of ETL. Finally, we propose a theoretical ETL framework for ETL optimization.",2017,,10.1007/s00778-017-0477-2
1965,"Li, Xi and Wang, Zehua and Leung, Victor C. M. and Ji, Hong and Liu, Yiming and Zhang, Heli",Blockchain-Empowered Data-Driven Networks: A Survey and Outlook,"blockchain-empowered data-driven networks, networking technologies, blockchain, Data-driven networks","The paths leading to future networks are pointing towards a data-driven paradigm to better cater to the explosive growth of mobile services as well as the increasing heterogeneity of mobile devices, many of which generate and consume large volumes and variety of data. These paths are also hampered by significant challenges in terms of security, privacy, services provisioning, and network management. Blockchain, which is a technology for building distributed ledgers that provide an immutable log of transactions recorded in a distributed network, has become prominent recently as the underlying technology of cryptocurrencies and is revolutionizing data storage and processing in computer network systems. For future data-driven networks (DDNs), blockchain is considered as a promising solution to enable the secure storage, sharing, and analytics of data, privacy protection for users, robust, trustworthy network control, and decentralized routing and resource managements. However, many important challenges and open issues remain to be addressed before blockchain can be deployed widely to enable future DDNs. In this article, we present a survey on the existing research works on the application of blockchain technologies in computer networks and identify challenges and potential solutions in the applications of blockchains in future DDNs. We identify application scenarios in which future blockchain-empowered DDNs could improve the efficiency and security, and generally the effectiveness of network services.",2021,,10.1145/3446373
1966,"Lin, Yuxiang and Dong, Wei and Chen, Yuan",Calibrating Low-Cost Sensors by a Two-Phase Learning Approach for Urban Air Quality Measurement,"Air quality, Low-cost sensors, Mobile sensor network, Sensor calibration","Urban air quality information, e.g., PM2.5 concentration, is of great importance to both the government and society. Recently, there is a growing interest in developing low-cost sensors, installed on moving vehicles, for fine-grained air quality measurement. However, low-cost mobile sensors typically suffer from low accuracy and thus need careful calibration to preserve a high measurement quality. In this paper, we propose a two-phase data calibration method consisting of a linear part and a nonlinear part. We use MLS (multiple least square) to train the linear part, and use RF (random forest) to train the nonlinear part. We propose an automatic feature selection algorithm based on AIC (Akaike information criterion) for the linear model, which helps avoid overfitting due to the inclusion of inappropriate features. We evaluate our method extensively. Results show that our method outperforms existing approaches, achieving an overall accuracy improvement of 16.4% in terms of PM2.5 levels compared with state-of-the-art approach.",2018,,10.1145/3191750
1967,"Deng, Alex and Dmitriev, Pavel and Gupta, Somit and Kohavi, Ron and Raff, Paul and Vermeer, Lukas",A/B Testing at Scale: Accelerating Software Innovation,"experimentation, a/b testing","The Internet provides developers of connected software, including web sites, applications, and devices, an unprecedented opportunity to accelerate innovation by evaluating ideas quickly and accurately using controlled experiments, also known as A/B tests. From front-end user-interface changes to backend algorithms, from search engines (e.g., Google, Bing, Yahoo!) to retailers (e.g., Amazon, eBay, Etsy) to social networking services (e.g., Facebook, LinkedIn, Twitter) to travel services (e.g., Expedia, Airbnb, Booking.com) to many startups, online controlled experiments are now utilized to make data-driven decisions at a wide range of companies. While the theory of a controlled experiment is simple, and dates back to Sir Ronald A. Fisher's experiments at the Rothamsted Agricultural Experimental Station in England in the 1920s, the deployment and evaluation of online controlled experiments at scale (100's of concurrently running experiments) across variety of web sites, mobile apps, and desktop applications presents many pitfalls and new research challenges. In this tutorial we will give an introduction to A/B testing, share key lessons learned from scaling experimentation at Bing to thousands of experiments per year, present real examples, and outline promising directions for future work. The tutorial will go beyond applications of A/B testing in information retrieval and will also discuss on practical and research challenges arising in experimentation on web sites and mobile and desktop apps. Our goal in this tutorial is to teach attendees how to scale experimentation for their teams, products, and companies, leading to better data-driven decisions. We also want to inspire more academic research in the relatively new and rapidly evolving field of online controlled experimentation.",2017,,10.1145/3077136.3082060
1968,,Conclusion and Future Thoughts,,"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.",2019,,
1969,"Guo, Yuanyuan",Data Protection Measures in E-Society: Policy Implications of British Data Protection Act to China,"Public policy, Data protection, E-society, Private information",,2021,,10.1145/3485768.3485770
1970,"Jilani, Musfira and Corcoran, Padraig and Bertolotto, Michela",A Multi-Layer CRF Based Methodology for Improving Crowdsourced Street Semantics,"OpenStreetMap, Semantics, Street Networks, Conditional Random Fields, Hierarchical Classification","This paper presents an intuitive and novel method for improving the semantic quality of streets in crowdsourced maps. Two factors negatively affecting the quality are incorrect and ambiguous semantics. Toward overcoming these, a multi-layer CRF based model is proposed that performs a simultaneous hierarchical classification of streets into fine-grained (crowdsourced; therefore, rich but ambiguous) and coarse-grained (familiar and standard) semantics. Inference is performed using Lazy Flipper algorithm which is fast for street network consisting of several hundred thousand streets. The model achieves a classification accuracy of 61% for fine-grained classification and 77% for coarse-grained classification respectively.",2018,,10.1145/3283207.3283210
1971,"Caruccio, Loredana and Deufemia, Vincenzo and Polese, Giuseppe",On the Discovery of Relaxed Functional Dependencies,"functional dependency, approximate match, discovery algorithm, Database integration","Functional dependencies (fds) express important relationships among data, which can be used for several goals, including schema normalization and data cleansing. However, to solve several issues in emerging application domains, such as the identification of data inconsistencies or patterns of semantically related data, it has been necessary to relax the fd definition through the introduction of approximations in data comparison and/or validity. Moreover, while fds were originally specified at design time, with the availability of massive data and computational power many algorithms have been devised to automatically discover them from data, including algorithms for discovering some types of relaxed fds. In this paper we present a technique that exploits lattice-based algorithms for the discovery of fds from data, in order to detect relaxed fds. Moreover, we introduce an algorithm to determine a proper distance threshold for a given relaxed fd holding over the entire database.",2016,,10.1145/2938503.2938519
1972,"Razzaque, M. A. and Hira, Muta Tah and Dira, Mukta",QoS in Body Area Networks: A Survey,"Body area networks, healthcare, medical care, QoS, cloud computing","Body Area Networks (BANs) are becoming increasingly popular and have shown great potential in real-time monitoring of the human body. With the promise of being cost-effective and unobtrusive and facilitating continuous monitoring, BANs have attracted a wide range of monitoring applications, including medical and healthcare, sports, and rehabilitation systems. Most of these applications are real time and life critical and require a strict guarantee of Quality of Service (QoS) in terms of timeliness, reliability, and so on. Recently, there has been a number of proposals describing diverse approaches or frameworks to achieve QoS in BANs (i.e., for different layers or tiers and different protocols). This survey put these individual efforts into perspective and presents a more holistic view of the area. In this regard, this article identifies a set of QoS requirements for BAN applications and shows how these requirements are linked in a three-tier BAN system and presents a comprehensive review of the existing proposals against those requirements. In addition, open research issues, challenges, and future research directions in achieving these QoS in BANs are highlighted.",2017,,10.1145/3085580
1973,"Nyk\""{a}nen, Ossi",Datamap Visualization Technique for Interactively Visualizing Large Datasets,"Data and knowledge visualization, Interactive data exploration and discovery, Datamap visualization, Visualization techniques and methodologies","This article describes the novel datamap visualization technique which enables visualizing large datasets interactively and fairly, inspired by geographic maps and microscopes. The main contributions include introducing the datamap metaphor and datamap visualization architecture, specifying efficient methods of approximate rendering, and illustrating the basic concepts in terms of example applications.",2013,,10.1145/2523429.2523458
1974,"Baker, Karen S. and Karasti, Helena",Data Care and Its Politics: Designing for Local Collective Data Management as a Neglected Thing,"science and technology studies, data care, information infrastructure, local collective data management, participatory design, politics, information management, infrastructuring, matters of care, partnering designer","In this paper, we think with Puig de la Bellacasa's 'matters of care' about how to support data care and its politics. We use the notion to reflect on participatory design activities in two recent case studies of local collective data management in ecological research. We ask ""How to design for data care?"" and ""How to account for the politics of data care in design?"" Articulation of data care together with ethically and politically significant data issues in design, reveals in these cases the invisible labors of care by local data advocates and a 'partnering designer'. With digital data work in the sciences increasing and data infrastructures for research under development at a variety of large scales, the local level is often considered merely a recipient of services rather than an active participant in design of data practices and infrastructures. We identify local collective data management as a 'neglected thing' in infrastructure planning and speculate on how things could be different in the data landscape.",2018,,10.1145/3210586.3210587
1975,"Leibold, Christian F. and Spies, Marcus",Towards a Pattern Language for Cognitive Systems Integration,"requirements engineering, social and ethical impact, cognitive systems, scope identification, pattern language, enterprise integration pattern","This paper discusses the influence of recent advances in cognitive computing systems on enterprise software architecture and design/development. Specifically, building on key features and capabilities of cognitive computing systems, we propose a new schema of enterprise application integration patterns in the tradition of the design pattern literature. Our schema has three groups of patterns addressing essential scoping, security and service integration issues related to cognitive components in enterprise architecture. While some patterns are modifications or refinements of known Enterprise Application Integration patterns, some of them are new and require dedicated consideration by enterprise architects and software designers.",2014,,10.1145/2721956.2721968
1976,"Grillenberger, Andreas and Romeike, Ralf",Key Concepts of Data Management: An Empirical Approach,"principles, data management, key concepts, model, CS education, core technologies, mechanics, practices","When preparing new topics for teaching, it is important to identify their central aspects. Sets of fundamental ideas, great principles or big ideas have already been described for several parts of computer science. Yet, existing catalogs of ideas, principles and concepts of computer science only consider the field data management marginally. However, we assume that several concepts of data management are fundamental to CS and, despite the significant changes in this field in recent years, have long-term relevance. In order to provide a comprehensive overview of the key concepts of data management and to bring relevant parts of this field to school, we describe and use an empirical approach to determine such central aspects systematically. This results in a model of key concepts of data management. On the basis of examples, we show how the model can be interpreted and used in different contexts and settings.",2017,,10.1145/3141880.3141886
1977,"MacLean, Diana Lynn",Gathering People to Gather Data,,"An interview with Paul Wicks, Vice President of Innovation at PatientsLikeMe, a patient network and real-time research platform.",2014,,10.1145/2676566
1978,"Diamantini, Claudia and Potena, Domenico and Storti, Emanuele",A Semantic Data Lake Model for Analytic Query-Driven Discovery,"indicator, ontology, query-driven discovery, Data Lake"," Data Lake (DL) architectures have recently emerged as an effective solution to the problem of data analytics with big, highly heterogeneous, and quickly changing data sources. However, novel challenges arise too, including how to make sense of disparate raw data and how to identify the sources that satisfy a data need. In the paper, we introduce a semantic model for a Data Lake aimed to support data discovery and integration in data analytics scenarios. By formally modeling indicators of interest, their computation formulas, and dimensions of analysis in a knowledge graph, and by seamlessly mapping them to relevant source metadata, the framework is suited for identifying the sources and the required transformation steps according to the analytical request.",2021,,10.1145/3487664.3487783
1979,"Su, Hang and He, Qian and Guo, Biao",KPI Anomaly Detection Method for Data Center AIOps Based on GRU-GAN,"GAN, Data Center, GRU, AIOps, KPI Anomaly Detection","The system architecture and application services of the data center are becoming increasingly large. To ensure the stable operation of the systems and businesses carried by the data center, the operations engineer needs to collect and monitor the generating KPIs during the operation of the systems and services. Traditional KPI anomaly detection methods are faced with the challenges of the huge amount of KPIs and constantly changing data characteristics, which are gradually no longer suitable for highly dynamic systems and services. With the popularity of artificial intelligence algorithms, machine learning and deep learning methods have also begun to be applied in operation and maintenance scenarios, that is the emergence of Artificial Intelligence for IT Operations (AIOps). KPI anomaly detection is the underlying core technology of AIOps. This paper proposes a hybrid model based on GRU-GAN (GGAN) for KPI anomaly detection in data center AIOps. The Gated Recurrent Unit (GRU) network is selected as the generator and discriminator of Generative adversarial network (GAN) in this model, which get the time correlation and data distribution of KPI through the adversarial training between the generator and the discriminator to make use of the reconstruction ability of the generator and the discriminant ability of the discriminator at the same time. At the anomaly detection stage, the anomaly score is formed by integrating reconstruction difference and discrimination loss to complete the anomaly detection task. Experimental results show that the proposed method can more accurately capture the variable data characteristics of KPI compared with the traditional KPI anomaly detection method and the general unsupervised method, as well as achieve better performance in the KPI anomaly detection task.",2021,,10.1145/3485314.3485323
1980,"Nurmikko-Fuller, Terhi and Pickering, Paul",Reductio Ad Absurdum?: From Analogue Hypertext to Digital Humanities,"linked data, hypertext, political history, information aggregation, australian history","In this paper we report on a complex and complete archive of historical primary sources that map the political landscape of the anglophone world in the mid-to late 1800s. The ruthless pragmatism applied to the construction of the initial Humanities dataset resulted in an analogue equivalent of a hypertext system, which has already resulted in published academic books and articles. Here, we describe the processes of a current project, which consists of the translation of this analogue information aggregation system into a graph database using Linked Data and semantic Web technologies.",2021,,10.1145/3465336.3475107
1981,"Brolch\'{a}in, Niall \'{O} and Porwol, Lukasz and Ojo, Adegboyega and Wagner, Tilman and Lopez, Eva Tamara and Karstens, Eric",Extending Open Data Platforms with Storytelling Features,"YDS Platform, Journalism, Data-Driven Journalism, Open Data, Usable Open Data Platform, Data-Driven Storytelling","1Research into Data-Driven Storytelling using Open Data has led to considerable discussion into many possible futures for storytelling and journalism in a Data-Driven world, in particular, into the Open Data directives framed by various governments across the globe as a means of facilitating governments, transparency enabled citizens and journalists to get more insights into government actions and enable deeper and easier monitoring of governments' work. While progress in the development of Open Data platforms (usually funded by national and local governments) has been significant, it is only now that we are beginning to see the emergence of more practical and more applied use of Open Data platforms. Previous works have highlighted the potential for storytelling using Open Data as a source of information for journalistic stories. Nevertheless, there is a paucity of studies into Open Data platform affordances to support Data-Driven Storytelling. In this paper, we elaborate on existing Open Data platforms in terms of support for storytelling and analyse feedback from stakeholder focus groups, to discover what methods and tools can introduce or facilitate the storytelling capabilities of Open Data platforms.",2017,,10.1145/3085228.3085283
1982,"Ding, Xiaoou and Wang, Hongzhi and Su, Jiaxuan and Li, Zijue and Li, Jianzhong and Gao, Hong",Cleanits: A Data Cleaning System for Industrial Time Series,,"The great amount of time series generated by machines has enormous value in intelligent industry. Knowledge can be discovered from high-quality time series, and used for production optimization and anomaly detection in industry. However, the original sensors data always contain many errors. This requires a sophisticated cleaning strategy and a well-designed system for industrial data cleaning. Motivated by this, we introduce Cleanits, a system for industrial time series cleaning. It implements an integrated cleaning strategy for detecting and repairing three kinds of errors in industrial time series. We develop reliable data cleaning algorithms, considering features of both industrial time series and domain knowledge. We demonstrate Cleanits with two real datasets from power plants. The system detects and repairs multiple dirty data precisely, and improves the quality of industrial time series effectively. Cleanits has a friendly interface for users, and result visualization along with logs are available during each cleaning process.",2019,,10.14778/3352063.3352066
1983,"Agosti, Maristella and Fuhr, Norbert and Toms, Elaine and Vakkari, Pertti",Evaluation Methodologies in Information Retrieval Dagstuhl Seminar 13441,,,2014,,10.1145/2641383.2641390
1984,"Bishop, Bradley Wade and Hank, Carolyn",Data Curation Profiling of Biocollections,"data provenance, data curation profiles, biology, data curation, biocollections","In the contexts of the data deluge and open data, scientists studying biodiversity benefit from online access to global datasets of existing vouchered biological and paleontological collections. Using biocollections collected over time across the world allows for the advancement of scientific knowledge concerning evolution in process as well as species poleward migrations, an indicator of climate change. This study's purpose was to validate and expand the Data Curation Profiles (DCP) to digital biocollections and inform a DCP framework for worldwide biota. Ten biocollection producers, curating various types of specimens affiliated with the project building the United States' national biodiversity infrastructure, were interviewed using the DCP questionnaire. Results indicate there is extreme diversity in the curation of biocollections and additional DCP questions should be added to reflect the complicated approaches to biological data curation. Although discipline specific metadata creation tools, standards, and practices enable long-term sustainability of the U.S. digitization effort, some scientists would benefit from further clarification and guidance on the information needs of consumers beyond designated communities of expert users, and the long-term preservation of biocollections.",2016,,
1985,"Pugmire, David and Bozda\u{g}, Ebru and Lefebvre, Matthieu and Tromp, Jeroen and Komatitsch, Dmitri and Peter, Daniel and Podhorszki, Norbert and Hill, Judith",Pillars of the Mantle: Imaging the Interior of the Earth with Adjoint Tomography,,"In this work, we investigate global seismic tomographic models obtained by spectral-element simulations of seismic wave propagation and adjoint methods. Global crustal and mantle models are obtained based on an iterative conjugate-gradient type of optimization scheme. Forward and adjoint seismic wave propagation simulations, which result in synthetic seismic data to make measurements and data sensitivity kernels to compute gradient for model updates, respectively, are performed by the SPECFEM3D_GLOBE package [1] [2] at the Oak Ridge Leadership Computing Facility (OLCF) to study the structure of the Earth at unprecedented levels. Using advances in solver techniques that run on the GPUs on Titan at the OLCF, scientists are able to perform large-scale seismic inverse modeling and imaging. Using seismic data from global and regional networks from global CMT earthquakes, scientists are using SPECFEM3D_GLOBE to understand the structure of the mantle layer of the Earth. Visualization of the generated data sets provide an effective way to understand the computed wave perturbations which define the structure of mantle in the Earth.",2017,,10.1145/3093338.3104170
1986,"P\""{o}hn, Daniela and Seeber, Sebastian and Hanauer, Tanja and Ziegler, Jule A. and Schmitz, David",Towards Improving Identity and Access Management with the IdMSecMan Process Framework,"Security Management, Identity Management, Server, Security"," In today’s networks, administrative access to Linux servers is commonly managed by Privileged Access Management (PAM). It is not only important to monitor these privileged accounts, but also to control segregation of duty and detect keys as well as accounts that potentially bypass PAM. Unprohibited access can become a business risk. In order to improve the security in a controlled manner, we establish IdMSecMan, a security management process tailored for identity and access management (IAM). Security management processes typically use the Deming Cycle or an adaption for continuous improvements of products, services, or processes within the network infrastructure. We adjust a security management process with visualization for IAM, which also shifts the focus from typical assets to the attacker. With the controlled cycles, the maturity of IAM is measured and can continually advance. This paper presents and applies the work in progress IdMSecMan to a motivating scenario in the field of Linux server. We evaluate our approach in a controlled test environment with first steps to roll it out in our data center. Last but not least, we discuss challenges and future work. ",2021,,10.1145/3465481.3470055
1987,"Ammerlaan, Remmelt and Antonius, Gilbert and Friedman, Marc and Hossain, H M Sajjad and Jindal, Alekh and Orenberg, Peter and Patel, Hiren and Qiao, Shi and Ramani, Vijay and Rosenblatt, Lucas and Roy, Abhishek and Shaffer, Irene and Srinivasan, Soundarajan and Weimer, Markus","PerfGuard: Deploying ML-for-Systems without Performance Regressions, Almost!",,"Modern data processing systems require optimization at massive scale, and using machine learning to optimize these systems (ML-for-systems) has shown promising results. Unfortunately, ML-for-systems is subject to over generalizations that do not capture the large variety of workload patterns, and tend to augment the performance of certain subsets in the workload while regressing performance for others. In this paper, we introduce a performance safeguard system, called PerfGuard, that designs pre-production experiments for deploying ML-for-systems. Instead of searching the entire space of query plans (a well-known, intractable problem), we focus on query plan deltas (a significantly smaller space). PerfGuard formalizes these differences, and correlates plan deltas to important feedback signals, like execution cost. We describe the deep learning architecture and the end-to-end pipeline in PerfGuard that could be used with general relational databases. We show that this architecture improves on baseline models, and that our pipeline identifies key query plan components as major contributors to plan disparity. Offline experimentation shows PerfGuard as a promising approach, with many opportunities for future improvement.",2021,,10.14778/3484224.3484233
1988,"Hunker, Joachim and Wuttke, Alexander and Scheidler, Anne Antonia and Rabe, Markus",A Farming-for-Mining-Framework to Gain Knowledge in Supply Chains,,"Gaining knowledge from a given data basis is a complex challenge. One of the frequently used methods in the context of a supply chain (SC) is knowledge discovery in databases (KDD). For a purposeful and successful knowledge discovery, valid and preprocessed input data are necessary. Besides preprocessing collected observational data, simulation can be used to generate a data basis as an input for the knowledge discovery process. The process of using a simulation model as a data generator is called data farming. This paper investigates the link between data farming and data mining. We developed a Farming-for-Mining-Framework, where we highlight requirements of knowledge discovery techniques and derive how the simulation model for data generation can be configured accordingly, e.g., to meet the required data accuracy. We suggest that this is a promising approach and is worth further research attention.",2021,,
1989,"Yoon, Sang-Pil and Joo, Moon-Ho and Kwon, Hun-Yeong",Role of Law as a Guardian of the Right to Use Public Sector Information: Case Study of Korean Government,"public data, data management, the right to know, public-private cooperation, reuse of PSI, legal right management, openness, public sector information","With data revolution, data is emerging as a new raw material. As the importance of data has increased, interest in the availability of public sector information (PSI) has also grown. PSI created in the public sector comprises public attributes and directly impacts national administration and citizen's lives.Korea has almost the highest level of Information and Communication Technology (ICT) infrastructure and considerable data through government-led policies. As a result of such policies, Korea has demonstrated excellent results in the United Nation's e-government survey, ITU's ICT development index, and OECD's public data openness index. Paradoxically, however, this history and experience is a stumbling block to a new era. PSI, which is a basic resource for realizing the value of openness, sharing, cooperation, and communication, should be actively managed and opened by government to provide support for reuse in the sense that the government is its main producer and manager. However, no matter how good the quality of PSI through data management is and how excellent policies and institutions are established, if the private sector cannot actively use it, it is useless. What is the role of government and law in the context of changing the way data is managed and blurring sectoral boundaries?This paper aims to propose core challenges by analyzing the case of Korea in order to derive a basis for discussions to coordinate public and private cooperation and legal relations in the process. To begin with, we analyze the changes in the management environment of data and PSI and identify the role of government and law in responding to changes in the legal rights. Then, we discuss how Korea responds to change, examines related policies by function and discussions on the data law, which seem to have the greatest effect on government's role, and suggests essential tasks to change its role accordingly.",2018,,10.1145/3209281.3209297
1990,"Xu, Jianqiu and Lu, Hua and G\""{u}ting, Ralf Hartmut",Understanding Human Mobility: A Multi-Modal and Intelligent Moving Objects Database,,"The research field of moving objects has been quite active in the past 20 years. The recording of position data becomes easy and huge amounts of mobile data are collected. Moving objects databases represent time-dependent objects and support queries with spatial and temporal constraints. In this paper we provide the vision of a multi-model and intelligent moving objects database. The goal is to enhance the data management of moving objects by providing extensive data models for different applications and fusing artificial intelligence techniques. Toward this goal, we propose how to develop corresponding modules and integrate them into the system to achieve the next-generation moving objects database.",2019,,10.1145/3340964.3340975
1991,"Mitra, Ritayan and Chavan, Pankaj",DEBE Feedback for Large Lecture Classroom Analytics,"Large lectures, quantified self, learning analytics, live feedback, mobile application",Learning Analytics (LA) research has demonstrated the potential of LA in detecting and monitoring cognitive-affective parameters and improving student success. But most of it has been applied to online and computerized learning environments whereas physical classrooms have largely remained outside the scope of such research. This paper attempts to bridge that gap by proposing a student feedback model in which they report on the difficult/easy and engaging/boring aspects of their lecture. We outline the pedagogical affordances of an aggregated time-series of such data and discuss it within the context of LA research.,2019,,10.1145/3303772.3303821
1992,"Li, Hongqin and Zhai, Jun",Constructing Investment Open Data of Chinese Listed Companies Based on Linked Data,"Linked Enterprise Data, Open Data, Linked Data, XBRL","Linked Data can provide the data according to user's demand, promote the availability of half structured and unstructured data on the network, improve the interoperability of open data. With the development of Linked Data, Linked Enterprise Data becomes a research hotspot. This article draw lessons from foreign investment Linked Data research of listed companies, selected the listed company information and XBRL reports from Shanghai stock exchange and Shenzhen stock exchange, industry information from the China securities regulatory commission and daily stock price from Flush as data source, built investment open data of Chinese listed companies based on Linked Data. This work could promote the internationalization of the Chinese data and the commercial use of open government data, lay the foundation for the global data ecological system, at the same time prepare for the challenge of Shanghai-Hong Kong Stock Connect and Shenzhen-Hong Kong Stock Connect even the challenge of international investment.",2016,,10.1145/2912160.2912206
1993,"Yang, Xiaohui and Guo, Chenxi and Ren, Huan and Dong, Ming",Research on Anomaly Detection Method of Online Monitoring Data of Dissolved Gas in Transformer Oil,"Dissolved gases in oil, Anomaly detection, On-line monitoring, Hierarchical agglomerative cluster, Time series","Dissolved gases in oil analysis has been a significant conventional condition detection method for condition evaluation for power transformers. But false data and wrong data do exist in DGA on-line monitoring system, which often lead to misjudgment. To handle this problem, the monitoring system often uses a threshold method based on data distribution statistics to determine the authenticity of the data. However, it is difficult to grasp the rules of the data distribution in advance, resulting in the problem of generally low detection rate of abnormal data. In this paper, according to the time series characteristics of on-line monitoring data of DGA, an abnormal data detection method based on condensed hierarchical clustering is proposed. First, the sliding time window is used to preprocess a variety of oil gas monitoring data to obtain a time series set of monitoring data, and comprehensively apply statistical indicators to classify them and establish Typical time series map; on this basis, the agglomerated hierarchical clustering model is used to perform similarity clustering on the distance between different characteristic data points and the typical abnormal map to determine the abnormal type of the monitoring data. The verification of the application of actual monitoring data shows that this method can detect data anomalies in the online monitoring data stream and determine its type in real time.",2022,,10.1145/3512826.3512840
1994,"Verma, Neeta and Dawar, Savita",Digital Transformation in the Indian Government,,,2019,,10.1145/3349629
1995,"Pedersen, Torben Bach and Lehner, Wolfgang",Report on the Second International Workshop on Energy Data Management (EnDM 2013),,,2014,,10.1145/2590989.2591002
1996,"Haescher, Marian and Matthies, Denys J. C. and Krause, Silvio and Bieber, Gerald",Presenting a Data Imputation Concept to Support the Continuous Assessment of Human Vital Data and Activities,"controlled data creation, data fusion, data imputation, sensor fusion, mobile device, accelerometer, coherent database, smartwatch","Data acquisition of mobile tracking devices often suffers from invalid and non-continuous input data streams. This issue especially occurs with current wearables tracking the user's activity and vital data. Typical reasons include the short battery life and the fact that the body-worn tracking device may be doffed. Other reasons, such as technical issues, can corrupt the data and render it unusable. In this paper, we introduce a data imputation concept which complements and thus fixes incomplete datasets by using a new merging approach that is particularly suitable for assessing activities and vital data. Our technique enables the dataset to become coherent and comprehensive so that it is ready for further analysis. In contrast to previous approaches, our technique enables the controlled creation of continuous data sets that also contain information on the level of uncertainty for possible reconversions, approximations, or later analysis.",2019,,10.1145/3316782.3322785
1997,"Kondylakis, Haridimos and Stefanidis, Kostas and Rao, Praveen",Report on the Third International Workshop on Semantic Web Meets Health Data Management (SWH 2020),,"Creating a holistic view of patient data comes with many challenges but also brings many benefits for disease prediction, prevention, diagnosis, and treatment. Especially in the COVID-19 era, this is more important than ever before. The third International Workshop on Semantic Web Meets Health Data Management (SWH) was aimed at bringing together an interdisciplinary audience who was interested in the fields of Semantic Web, data management, and health informatics. The workshop goal was to discuss the challenges in healthcare data management and to propose new solutions for the next generation of data-driven healthcare systems. In this article, we summarize the outcomes of the workshop, and we present a number of key observations and research directions that emerged from presentations.",2021,,10.1145/3503780.3503792
1998,"Zhu, Chengang and Cheng, Guang and Guo, Xiaojun and Wang, Yuxiang",RBAS: A Real-Time User Behavior Analysis System for Internet TV in Cloud Computing,"SQL-on-Hadoop, User behavior analysis, cloud computing, Internet TV","The characteristic of Internet TV user behavior is quite essential for designers to optimize resource schedule and improve user experience. With the rapid development of Internet, both Internet TV users and STB (set top boxes) models are booming. This brings a large amount of behavior data which requires matching computing and storage resource to process. Therefore, scalable Internet TV user behavior analysis becomes more difficult. As a solution, cloud computing framework such as Hive is emerged. But limited by performance, it's not an appropriate choice for interactive analysis or real-time data exploration. In this paper, we present a real-time Internet TV user behavior analysis system with advantages of high concurrency, low latency and good transportability. Firstly, we design an event capture scheme, consisted of agents embedded in STBs and capture server clusters, to capture every manipulation performed by users. Secondly, we develop a SQL-on-Hadoop engine with distributed transactional management to decrease the response time. The engine has excellent query performance and ability to interactively query various data sources in different Hadoop formats. Lastly, we evaluate RBAS in a commercial Internet TV platform of 16 million registered users. The results show that, with a 32-node cluster, the system can effectively process 10.2 TB of behavior data every day, which is about 40x faster than original Hive-based system.",2016,,10.1145/2935663.2935664
1999,"Zhang, Jiapeng and Zhuang, Cunbo and Liu, Jianhua and Yuan, Kun and Zhang, Jin and Liu, Juan",Digital Twin-Based Three-Dimensional Visual and Global Monitoring of Assembly Shop-Floor,"Global monitoring, Assembly shop-floor, Digital twin, 3D visual monitoring","Aiming at the requirements of rapid response and production efficiency improvement in the assembly shop-floor, a three-dimensional (3D) visual and global monitoring method for the assembly shop-floor based on the digital twin is proposed. The paper analyzes the monitoring objectives, objects, and methods of assembly shop-floor, and constructs a global monitoring framework of digital twin-based assembly shop-floor. Then, three key technologies of realizing global monitoring shop-floor are described: current-time data perception and collection, current-time information-driven digital twin generation, and state monitoring and optimization based on digital twin. Finally, a global monitoring prototype system is designed and developed to verify the effectiveness of the proposed method.",2021,,10.1145/3487075.3487147
2000,"Qiao, Lin and Li, Yinan and Takiar, Sahil and Liu, Ziyang and Veeramreddy, Narasimha and Tu, Min and Dai, Ying and Buenrostro, Issac and Surlaker, Kapil and Das, Shirshanka and Botev, Chavdar",Gobblin: Unifying Data Ingestion for Hadoop,,"Data ingestion is an essential part of companies and organizations that collect and analyze large volumes of data. This paper describes Gobblin, a generic data ingestion framework for Hadoop and one of LinkedIn's latest open source products. At LinkedIn we need to ingest data from various sources such as relational stores, NoSQL stores, streaming systems, REST endpoints, filesystems, etc. into our Hadoop clusters. Maintaining independent pipelines for each source can lead to various operational problems. Gobblin aims to solve this issue by providing a centralized data ingestion framework that makes it easy to support ingesting data from a variety of sources.Gobblin distinguishes itself from similar frameworks by focusing on three core principles: generality, extensibility, and operability. Gobblin supports a mixture of data sources out-of-the-box and can be easily extended for more. This enables an organization to use a single framework to handle different data ingestion needs, making it easy and inexpensive to operate. Moreover, with an end-to-end metrics collection and reporting module, Gobblin makes it simple and efficient to identify issues in production.",2015,,10.14778/2824032.2824073
2001,"Rozi, Muhamad Fahru and Sucahyo, Yudho Giri and Gandhi, Arfive and Ruldeviyani, Yova",Appraising Personal Data Protection in Startup Companies in Financial Technology: A Case Study of ABC Corp,"Financial technology, personal data, data privacy, data protection, digital economy","Financial Technology (fintech) has been immerged extensively in the last decade. In the realm of disruptive world, there are many areas in which startup companies are developing their business. There is always contradiction when dealing with innovation as core of digital disruption and how privacy remains as hot issues at the edge of everybody's talks. Internet plays important roles to sustain the trends. As rapidly growing country, 68% of Indonesian has access to the Internet. It drives startup companies on financial technology to innovate more and besides that they must comply to regulation in regard with personal data protection. This research aims to appraise how startup company on financial technology protect users' personal data. Personal data protection principles from international organization and Indonesian regulation regarding personal data protection are used to appraise how ABC Corp as a startup company that deliver financial technology service in Indonesian society. To ensure that its service is qualified and trustable, ABC Corp should be appraised using relevant criteria and qualitative approach. The results showed that most of regulations from sectorial supervising agency have been adhered by ABC Corp. The results bring meaningful insight to improve performance on personal data protection. They can became lessons for similar emerging startup companies in financial technology when acquiring their qualifications to protect users' personal data and keep their sustainability.",2020,,10.1145/3379310.3379322
2002,"Dennis, Louise A.",Conference Reports,,This section is compiled from reports of recent events sponsored or run in cooperation with ACM SIGAI. In general these reports were written and submitted by the conference organisers.,2022,,10.1145/3511322.3511326
2003,"Kumar, Sathish Alampalayam",Designing a Graduate Program in Information Security and Analytics: Masters Program in Information Security and Analytics (MISA),"analytics, information technology education, cybersecurity","This paper introduces the concept of the Master of Information Security and Analytics (MISA) program for the graduate students with a background in CS, IS and IT. The 10-course graduate level program is benchmarked against existing masters programs in the areas of Information Security and Data Analytics, and an assessment was done on the estimated demand for MISA graduates in the nation. The program outcomes were then mapped against the course objectives to insure the correct mix of courses and topics. The program's admission requirement is also being discussed. This paper discusses the design process and possible ways to reduce risk in the start-up of a new degree program. How a program is marketed to prospective students and what program graduates will do after program completion is just as important as the initial design of the program. Planning for the administration of the program and the assessment process is an important phase of the initial design.",2014,,10.1145/2656450.2656453
2004,"Baolong, Yang and Hong, Wu and Haodong, Zhang",Research and Application of Data Management Based on Data Management Maturity Model (DMM),"Unstructured data, Data management, maturity model, Measurement and evaluation","Through the analysis and contrast of the different Data Management Maturity Model, such as DCAM, DMM, DCMM and the model of IBM, we try to make empirical research under the framework of data management maturity model. This article take a project whose main research object is about the academic career of scientists and with massive unstructured data for example, through analysis of the goal, management processes and influence factors of this project in detail, we built up an evaluation system for data management for such projects under the framework of DCMM. It is expected to have a positive significance to the evaluation of similar data management capability.",2018,,10.1145/3195106.3195177
2005,"An, Zhenpeng and Zhang, Di and Liang, Yunjie",Research on Data Governance Framework for Fire Department,,"This paper analyzes data governance elements, models and frameworks, provides a clear plan for data governance for fire department. Using the method of literature research, network investigation and conclude data system of fire departments, the china domestic and foreign research status of data governance is reviewed. We build the framework of data governance for fire department, including Data resource directory system, Data technology support system and Data standardization system. This paper preliminarily forms the framework of data governance for fire department. This framework was applied to the fire information planning work. The results indicate that based on the status and characteristics of fire industry, the implementation of this framework is effective and feasible, and it is also the basis of standard fire control data governance in future.",2021,,
2006,"Hui, Pan and Ou, Zhonghong and Zhang, Yanyong and Striegel, Aaron D",The 7th International Workshop on Hot Topics in Planet-Scale Measurement (HotPlanet '16),"social computing, crowdsourcing, cloud computing, planet-scale measurement, deployment experiences, data analytics, crowd sensing","The recent advances of mobile devices, online social networks, and the emergence of the Internet of Things have driven the corresponding data collection and analytics to planetary scale. It is, thus, essential to provide a forum to discuss the technical advances, share the lessons, experiences, and challenges associated with real-world large-scale deployment. The 7th International Workshop on Hot Topics in Planet-Scale Measurement (HotPlanet '16) is to provide such a forum for the researchers and practitioners in the fields mentioned above. By bringing together the experts in these fields, and through thoughtful discussions and valuable sharing, HotPlanet '16 aims to advance the work in these fields forward.",2016,,10.1145/2968219.2985840
2007,"Duan, Rong and Xiao, Yanghua",Enterprise Knowledge Graph From Specific Business Task to Enterprise Knowledge Management,"enterprise knowledge management, entity recognition, ontology, relation extraction, knowledge graph","Data driven Knowledge Graph is rapidly adapted by different societies. Many open domain and specific domain knowledge graphs have been constructed, and many industries have benefited from knowledge graph. Currently, enterprise related knowledge graph is classified as specific domain, but the applications span from solving a narrow specific problem to Enterprise Knowledge Management system. With the digital transform of traditional industry, Enterprise knowledge becomes more and more complicated, it involves knowledge from common domain, multiple specific domains, and corporate-specific in general. This tutorial provides an overview of current Enterprise Knowledge Graph(EKG). It distinguishes the EKG from specific domain according to the knowledge it covers, and provides the examples to illustrate the difference between EKG and specific domain KG. The tutorial further summarizes EKG into three types: Specific Business Task Enterprise KG, Specific Business Unit Enterprise KG and Cross Business Unit Enterprise KG, and illustrates the characteristics, steps, challenges, and future research in constructing and consuming of each of these three types of EKG .",2019,,10.1145/3357384.3360314
2008,"Nabipourshiri, Rouzbeh and Abu-Salih, Bilal and Wongthongtham, Pornpit",Tree-Based Classification to Users' Trustworthiness in OSNs,"data mining, machine learning, Trust, users' trustworthiness, Twitter, social media","In the light of the information revolution, and the propagation of big social data, the dissemination of misleading information is certainly difficult to control. This is due to the rapid and intensive flow of information through unconfirmed sources under the propaganda and tendentious rumors. This causes confusion, loss of trust between individuals and groups and even between governments and their citizens. This necessitates a consolidation of efforts to stop penetrating of false information through developing theoretical and practical methodologies aim to measure the credibility of users of these virtual platforms. This paper presents an approach to domain-based prediction to user's trustworthiness of Online Social Networks (OSNs). Through incorporating three machine learning algorithms, the experimental results verify the applicability of the proposed approach to classify and predict domain-based trustworthy users of OSNs.",2018,,10.1145/3192975.3193004
2009,"Lin, Yuting",Government Management Model of Non-Profit Organizations Based on E-Government,"E-government, management model, non-profit organization","With the development and popularization of Internet technology, our country is increasingly aware of the importance of e-government, and continuously expands the channels and means of e-government development in policy, such as the application of e-government to the management of non-profit organizations. However, in practice, ""e-government + NPO (non-profit organization) management"" still has problems such as digital divide, information sharing and insufficient disclosure, and information security. Therefore, this paper proposes a more complete non-profit organization management model based on e-government. From the perspectives of optimization services, information sharing, network supervision and information security, it is explained how to effectively realize the efficient management of non-profit organizations based on e-government.",2019,,10.1145/3348445.3348464
2010,"Grillenberger, Andreas and Romeike, Ralf",A Comparison of the Field Data Management and Its Representation in Secondary CS Curricula,"curricula, secondary school, characterization, databases, analysis, data management, standards","In the last few years, the focus of data management has changed from handling relatively small amounts of data, often in relational databases, to managing large amounts of data using various different database types. In many secondary school curricula, data management is mainly considered from a ""database"" perspective. However, in contrast to the developments in computer science research and practice, the new and changing aspects of data management have hardly been discussed with respect to CS education. We suggest re-evaluating the focus and relevance of the established database syllabi, to discuss the educational value of the newly arising developments and to prevent the teaching of outdated concepts. In this paper, we will contrast current educational standards and curricula with an up-to-date characterization of data management in order to identify gaps between the principles and concepts of data management that are considered as important today from a professional point of view on the one side, and the emphasis in current CS education on the other side.The findings of this analysis will provide a basis for aligning the concepts taught in CS education with the developments in data management research and practice, as well as for re-evaluating the educational value of these concepts.",2014,,10.1145/2670757.2670779
2011,"Zeleti, Fatemeh Ahmadi and Ojo, Adegboyega",Competitive Capability Framework for Open Government Data Organizations,"competitive strategies, open data capabilities, Competitiveness in open data businesses, organizational capabilities, competitive advantage, open data organization","Open data-driven organizations compete in a complex and uncertain environment with growing global competition, changing and emerging demand and market, and increasing levels of analytical tools and technology. For these organizations to exploit open data for competitive advantage, they need to develop the requisite competitive capabilities. This article presents an open data competitive capability framework grounded in theory and practice of open data. Based on extant literature and insights from domain experts, we identify and describe four dimensions of competitive capabilities required for open data driven organizations. We argue that by implementing the proposed framework, organizations can increase their chances to favorably compete in their respective markets. We further argue that by understanding open government data as a strategic resource for enterprises, government as producers or suppliers of this resource become key partners to data-driven organizations.",2017,,10.1145/3085228.3085280
2012,"Ren, Kang and Liu, Fan and Zhuang, Haimei and Ling, Yun",AI-Based Multimodal Data Management and Intelligent Analysis System for Parkinson's Disease: GYENNO PD CIS,,"The GYENNO PD CIS is an AI-based multimodal data management and intelligent analysis system for Parkinson's disease (PD). The main purpose is to solve the problems in traditional diagnosis of PD such as lack of objective evaluation data, lack of reproducible diagnosis system, and lack of closed-loop treatment tracking, and then to construct a multimodal data management and intelligent analysis platform for PD, which can achieve the goals - standardization of data, objectification of evaluation, standardization of diagnosis, individualization of treatment, continuousness of management. It also helps Parkinson's experts in patient management, clinical data management, analysis and data mining, and supports multi-center projects, and finally lets patients benefit a lot from innovative technology.",2020,,
2013,"Li, Ting and Zhang, Bo",Development Dilemma and Countermeasures of Data Journalism,"Data journalism, visualization, information cocoons, data opening","In the field of news, with the operation of data journalism, the traditional press is facing great innovation and shock in the production, circulation, distribution and consumption of information. As McLuhan said, the birth of new media has opened up new possibilities in this era. Data, as a medium of the new era, is creating a new way for people to understand the world.This paper mainly discusses that it is still facing the problem of low degree of data opening in the current development, and the negative impact of disclosing users' personal privacy and information cocoon room. In view of these problems, relevant departments need to further strengthen the policy of data opening, improve the legal system, and optimize the link mode of information content dissemination, so as to promote the better development of data journalism and make data benefit people truly.",2020,,10.1145/3433996.3434019
2014,"Best, Daniel M. and Endert, Alex and Kidwell, Daniel",7 Key Challenges for Visualization in Cyber Network Defense,"cyber security, defense, visualization","What does it take to be a successful visualization in cyber security? This question has been explored for some time, resulting in many potential solutions being developed and offered to the cyber security community. However, when one reflects upon the successful visualizations in this space they are left wondering where all those offerings have gone. Excel and Grep are still the kings of cyber security defense tools; there is a great opportunity to help in this domain, yet many visualizations fall short and are not utilized.In this paper we present seven challenges, informed by two user studies, to be considered when developing a visualization for cyber security purposes. Cyber security visualizations must go beyond isolated solutions and ""pretty picture"" visualizations in order to impact users. We provide an example prototype that addresses the challenges with a description of how they are met. Our aim is to assist in increasing utility and adoption rates for visualization capabilities in cyber security.",2014,,10.1145/2671491.2671497
2015,"Beskales, George and Das, Gautam and Elmagarmid, Ahmed K. and Ilyas, Ihab F. and Naumann, Felix and Ouzzani, Mourad and Papotti, Paolo and Quiane-Ruiz, Jorge and Tang, Nan",The Data Analytics Group at the Qatar Computing Research Institute,,,2013,,10.1145/2430456.2430466
2016,"Liu, Shuhua Monica and Pan, Liting and Lei, Yupei",What is the Role of New Generation of ICTs in Transforming Government Operation and Redefining State-Citizen Relationship in the Last Decade?,"Information and communication technology (ICT), Transformative governance, E-governance","This article first introduce a new government initiative emerging after the US presidential election in 2008. Comparing to the more descriptive definitions of e-government, supporters of these new government initiatives emphasize the transformative and normative aspect of the newest generation of Information and Communication Technology (ICTs). They argue that the new initiative redefines how government should operate and transform state-citizen relationships. To understand the core of this initiative and whether it offers new opportunities to solve public problems, we collected and analyzed research papers published in the e-governance area between 2008 and 2017. Our analysis demonstrates that the use of new generation of ICTs has promoted the government information infrastructure. In other words, the application of new ICTs enables the government to accumulate and use a large amount of data, so that the government makes better decisions. The advancement of open data, the wide use of social media, and the potential of data analytics have also generated pressure to address challenging questions and issues in e-democracy. However, the analysis leads us to deliberate on whether the use of new generation of ICTs worldwide have actually achieved their goal. In the conclusion, we present challenges to be addressed before new innovative ICTs realize their potential towards better public governance.",2019,,10.1145/3326365.3326374
2017,"Wang, Deqiang and Guo, Danhuai and Zhang, Hui",Spatial Temporal Data Visualization In Emergency Management: A View from Data-Driven Decision,"emergency management, spatio-temporal visualization, review","Recent years, extreme events caused a great loss of human society. Emergency management is playing a more and more important role in handling disaster events. With the raising of data-intensive decision making, how to visualize large, multi-dimension data become an important challenge. Spatial temporal data visualization, a powerful tool, could transform data in to visual structure and make core information easily be captured by human. It could support spatial analysis, decision making and be used in all phase of emergency management. In this paper, we reviewed the general method of spatial temporal data visualization and the methods in data-intensive environment. Summarized the problems of each phase of emergency management and presented how spatial temporal visualization tools applied in each phase of emergency management. Finally, we conduct a short conclusion and outlook the future of spatial temporal visualization applied in data-driven emergency management environment.",2017,,10.1145/3152465.3152473
2018,"Barata, Andre Montoia and Prado, Edmir Parada Vasques",Data Governance in Brazilian Organizations,"System Information, Data Governance, Management Frameworks","Organizations are increasingly looking for data integrity and quality to assist in strategic making decision and value creation. In this context Data Governance (DG) provide processes and practices that assist in the management and maintenance data. There are many frameworks to implementation DG process and benefits they may provide, however there are few implementation reported in the literature. This study aims to identify the DG process and frameworks implemented in Brazilian organizations and compare the benefits in implementation with those proposed by literature. For this will be carried out case studies in Brazilian organizations that implemented or are implementing DG frameworks.",2015,,
2019,"Elmegreen, Bruce G. and Sanchez, Susan M. and Szalay, Alexander S.",The Future of Computerized Decision Making,,"Computerized decision making is becoming a reality with exponentially growing data and machine capabilities. Some decision making is extremely complex, historically reserved for governing bodies or market places where the collective human experience and intelligence come to play. Other decision making can be trusted to computers that are on a path now into the future through novel software development and technological improvements in data access. In all cases, we should think about this carefully first: what data are really important for our goals and what data should be ignored or not even stored? The answer to these questions involves human intelligence and understanding before the data-to-decision process begins.",2014,,
2020,"Cassel, Lillian and Hongzhi, Wang",Panel: The Computing in Data Science,"computing curriculum, computing for data science, data science","This panel brings the workings and results of the ACM Education Council Task Force on Data Science Education. The task force has gathered information on existing programs and has reviewed documents such as the result of the National Academies deliberations on data science. The task force is charged with exploring the role of computer science in data science education, understanding that data science is an inherently interdisciplinary field and not exclusively a computer science field. The panel will present a summary of the task force findings by two members of the task force and perspectives from leaders in data-intensive applications from China. The goal of the panel is to present the findings, but also to obtain perspectives from the attendees in order to enrich the task force's work.",2019,,10.1145/3300115.3312508
2021,"Raschid, Louiqa",Editor-in-Chief (January 2014-May 2017) Farewell Report,,,2017,,10.1145/3143313
2022,"Raschid, Louiqa",Editorial,,,2014,,10.1145/2579167
2023,"Chen, Yumei and Dawes, Sharon S. and Chen, Shanshan",E-Government Support for Administrative Reform in China,"Chinese government and reform, Administrative reform, E-government","This1 paper summarizes the history of Chinese administrative modernization and reform and discusses the ways in which China's e-government development agenda supports reform in the areas of transforming functions, streamlining processes, and enhancing transparency and citizen engagement. It offers a conceptual model of how e-government supports reform through policies, technologies, management improvements, and data designed to overcome the barriers of technical capability, staff resistance, and lack of cross-boundary collaboration. The analysis also shows how this interaction has generated new issues regarding official corruption and public engagement. We conclude with a future research agenda.",2017,,10.1145/3085228.3085269
2024,"Philipp, Robert and Mladenow, Andreas and Strauss, Christine and V\""{o}lz, Alexander",Machine Learning as a Service: Challenges in Research and Applications,"Machine Learning Platform, Machine Learning, Machine Learning as a Service, MLaaS, Machine Learning Services","This study aims to evaluate the current state of research with regards to Machine Learning as a Service (MLaaS) and to identify challenges and research fields of this novel topic. First, a literature review on a basket of eight leading journals was performed. We motivate this study by identifying a lack of studies in the field of MLaaS. The structured literature review was further extended to established scientific databases relevant in this field. We found 30 contributions on MLaaS. As a result of the analysis we grouped them into four key concepts: Platform, Applications; Performance Enhancements and Challenges. Three of the derived concepts are discussed in detail to identify future research areas and to reveal challenges in research as well as in applications.",2020,,10.1145/3428757.3429152
2025,"Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix",Profiling Relational Data: A Survey,,"Profiling data to determine metadata about a given dataset is an important and frequent activity of any IT professional and researcher and is necessary for various use-cases. It encompasses a vast array of methods to examine datasets and produce metadata. Among the simpler results are statistics, such as the number of null values and distinct values in a column, its data type, or the most frequent patterns of its data values. Metadata that are more difficult to compute involve multiple columns, namely correlations, unique column combinations, functional dependencies, and inclusion dependencies. Further techniques detect conditional properties of the dataset at hand. This survey provides a classification of data profiling tasks and comprehensively reviews the state of the art for each class. In addition, we review data profiling tools and systems from research and industry. We conclude with an outlook on the future of data profiling beyond traditional profiling tasks and beyond relational databases.",2015,,10.1007/s00778-015-0389-y
2026,"Tufi\c{s}, Mihnea and Boratto, Ludovico",Toward a Complete Data Valuation Process. Challenges of Personal Data,"data valuation, Datasets, data markets",,2021,,10.1145/3447269
2027,"Laube, Patrick",The Low Hanging Fruit is Gone: Achievements and Challenges of Computational Movement Analysis,,"This position paper reviews the achievements and open challenges of movement analysis within Geographical Information Science. The paper argues that the simple problems of movement analysis have mostly been addressed to a sufficient level (""the low hanging fruit""), leaving the research community with the much more challenging problems for the years ahead (""the high hanging fruit""). Whereas the community has made good progress in structuring trajectory data (segmentation, similarity, clustering) and conceptualizing and detecting movement patterns, the much harder task of semantic annotation of structures and patterns remains difficult. The position paper summarizes both achievements and challenges with two sets assertions and calls for the establishment of a unifying theory of Computational Movement Analysis.",2015,,10.1145/2782759.2782762
2028,"Madnick, Stuart E. and Lee, Yang W.",Editorial Notes Classification and Assessment of Large Amounts of Data: Examples in the Healthcare Industry and Collaborative Digital Libraries,,,2011,,10.1145/2063504.2063505
2029,"Sulistyowati, Ira and Fransisca, Dyna and Ruldeviyani, Yova",Data Analytics Readiness Model in Indonesian Government,,"The development of information technology encourages the government to digitize business processes. It is generated with a large and varied volume from various data sources so that advanced data analytics (DA) is required to overcome this to support organization's data driven decision making. It's necessary to prepare DA based on DA readiness model so that the implementation of DA can run successfully. Whereas currently, there is limited study and no standard model for DA readiness. The focus of this study is to propose model readiness of implementing data analytics that is suitable in Indonesian government. The model refers to DA readiness model based on literature review on 15 papers relevant to DA readiness. Then it's verified by 7 experts. Furthermore, online survey was conducted to test the model that affects the readiness of implementing data analytics in Indonesian government. The survey results were analyzed using factor analysis. As a result, DA readiness model contains 4 dimensions, 11 factors, and 78 indicators where its dimensions consist of information system, organizational and cultural, organization structure and resource readiness. This model can describe 85% of the data analysis readiness requirements in the Indonesian government. In order to implement data analytics successfully, the government needs to improve the readiness of information systems, organizational and cultural, organizational structures, and resources.",2021,,
2030,"Abiteboul, Serge and Arenas, Marcelo and Barcel\'{o}, Pablo and Bienvenu, Meghyn and Calvanese, Diego and David, Claire and Hull, Richard and H\""{u}llermeier, Eyke and Kimelfeld, Benny and Libkin, Leonid and Martens, Wim and Milo, Tova and Murlak, Filip and Neven, Frank and Ortiz, Magdalena and Schwentick, Thomas and Stoyanovich, Julia and Su, Jianwen and Suciu, Dan and Vianu, Victor and Yi, Ke",Research Directions for Principles of Data Management (Abridged),,,2017,,10.1145/3092931.3092933
2031,"Umejiaku, Afamefuna and Dang, Tommy","Visualising Developing Nations Health Records: Opportunities, Challenges and Research Agenda","Health records, Visualisation, Developing Nations"," The benefits of effectively visualizing health records in huge volumes has resulted in health organizations, insurance companies, policy and decision makers, governments and drug manufactures’ transformation in the way research is conducted. This has also played a key role in determining investment of resources. Health records contain highly valuable information; processing these records in large volumes is now possible due to technological advancement which allows for the extraction of highly valuable knowledge that has resulted in breakthroughs in scientific communities. To visualize health records in large volumes, the records need to be stored in electronic forms, properly documented, processed, and analyzed. A good visualization technique is used to present the analyzed information, allowing for effective knowledge extraction which is done in a secured manner protecting the privacy of the patients whose health records were used. As research and technological advancement have improved, the quality of knowledge extracted from health records have also improved; unfortunately, the numerous benefits of visualizing health records have only been felt in developed nations, unlike other sectors where technological advancement in developed nations have had similar impact in developing nations. This paper identifies the characteristics of health records and the challenges involved in processing large volumes of health records. This is to identify possible steps that could be taken for developing nations to benefit from visualizing health records in huge volumes. ",2021,,10.1145/3468784.3471607
2032,"Munappy, Aiswarya Raj and Mattos, David Issa and Bosch, Jan and Olsson, Helena Holmstr\""{o}m and Dakkak, Anas",From Ad-Hoc Data Analytics to DataOps,"DataOps, Continuous Monitoring, Data Pipelines, Agile Methodology, DevOps, Data technologies","The collection of high-quality data provides a key competitive advantage to companies in their decision-making process. It helps to understand customer behavior and enables the usage and deployment of new technologies based on machine learning. However, the process from collecting the data, to clean and process it to be used by data scientists and applications is often manual, non-optimized and error-prone. This increases the time that the data takes to deliver value for the business. To reduce this time companies are looking into automation and validation of the data processes. Data processes are the operational side of data analytic workflow.DataOps, a recently coined term by data scientists, data analysts and data engineers refer to a general process aimed to shorten the end-to-end data analytic life-cycle time by introducing automation in the data collection, validation, and verification process. Despite its increasing popularity among practitioners, research on this topic has been limited and does not provide a clear definition for the term or how a data analytic process evolves from ad-hoc data collection to fully automated data analytics as envisioned by DataOps.This research provides three main contributions. First, utilizing multi-vocal literature we provide a definition and a scope for the general process referred to as DataOps. Second, based on a case study with a large mobile telecommunication organization, we analyze how multiple data analytic teams evolve their infrastructure and processes towards DataOps. Also, we provide a stairway showing the different stages of the evolution process. With this evolution model, companies can identify the stage which they belong to and also, can try to move to the next stage by overcoming the challenges they encounter in the current stage.",2020,,10.1145/3379177.3388909
2033,"Hattingh, Mari\'{e} and Marshall, Linda and Holmner, Marlene and Naidoo, Rennie",Data Science Competency in Organisations: A Systematic Review and Unified Model,"Data Science, Systematic Literature Review, Competency, Skills","The paper presents a systematic literature review of the literature on the competencies that are essential to develop a globally competitive workforce in the field of data science. The systematic review covers a wide range of literature but focuses primarily, but not exclusively, on the computing, information systems, management, and organisation science literature. The paper uses a broad research search strategy covering four separate electronic databases. The search strategy led the researchers to scan 139 titles, abstracts and keywords. Sixty potentially relevant articles were identified, of which 42 met the quality criteria and contributed to the analysis. A critical appraisal checklist assessed the validity of each empirical study. The researchers grouped the findings under six broad competency themes: organisational, technical, analytical, ethical and regulatory, cognitive and social. Thematic analysis was used to develop a unified model of data science competency based on the evidence of the findings. This model will be applied to case studies and survey research in future studies. A unified data science competency model, supported by empirical evidence, is crucial in closing the skills gap, thereby improving the quality and competitiveness of the South Africa's data science workforce. Researchers are encouraged to contribute to the further conceptual development of data science competency.",2019,,10.1145/3351108.3351110
2034,"Torresen, Jim",Addressing Ethical Challenges within Evolutionary Computation Applications: GECCO 2020 Tutorial,,,2020,,
2035,"Millard, Jeremy",ICT-Enabled Public Sector Innovation: Trends and Prospects,"public value, open engagement, open services, open assets, government as a platform, open governance","This experience paper is a personal thinkpiece which outlines many of the main issues and discussions taking place in Europe and elsewhere about the future of the public sector and how it can respond positively to some of the acute challenges it faces in light of the financial crisis and other global challenges. The paper examines how ICT-enabled public sector innovation highlights concepts like open governance, public value, government as a platform, open assets, open services and open engagement. It develops a vision of an 'open governance framework', moving beyond 'new public management', based on ICT-enabled societal-wide collaboration. It recognises that although the public sector can in principle create public value on its own, its potential to do so is greatly enhanced and extended by direct cooperation with other actors, or by facilitating public value creation by other actors on their own. It also examines the role of bottom-up innovation and public policy experimentation, as well as the need to focus on empowering civil servants and changing public sector working practices and mindsets.",2013,,10.1145/2591888.2591901
2036,"Abadi, Daniel and Ailamaki, Anastasia and Andersen, David and Bailis, Peter and Balazinska, Magdalena and Bernstein, Philip and Boncz, Peter and Chaudhuri, Surajit and Cheung, Alvin and Doan, AnHai and Dong, Luna and Franklin, Michael J. and Freire, Juliana and Halevy, Alon and Hellerstein, Joseph M. and Idreos, Stratos and Kossmann, Donald and Kraska, Tim and Krishnamurthy, Sailesh and Markl, Volker and Melnik, Sergey and Milo, Tova and Mohan, C. and Neumann, Thomas and Chin Ooi, Beng and Ozcan, Fatma and Patel, Jignesh and Pavlo, Andrew and Popa, Raluca and Ramakrishnan, Raghu and R\'{e}, Christopher and Stonebraker, Michael and Suciu, Dan",The Seattle Report on Database Research,,"Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle.",2020,,10.1145/3385658.3385668
2037,"Auer, S\""{o}ren and Heath, Tom and Bizer, Christian and Berners-Lee, Tim",LDOW2016: 9th Workshop on Linked Data on the Web,"rdf, linked data, semantic web","The ninth workshop on Linked Data (LDOW2016) on the Web is held in Montreal, Quebec, Canada on April 12, 2016 and co-located with the 25rd International World Wide Web Conference (WWW2016). The Web is developing from a medium for publishing textual documents into a medium for sharing structured data. This trend is fueled on the one hand by the adoption of the Linked Data principles by a growing number of data providers. On the other hand, large numbers of websites have started to semantically mark up the content of their HTML pages and thus also contribute to the wealth of structured data available on the Web. The 9th Workshop on Linked Data on the Web aims to stimulate discussion and further research into the challenges of publishing, consuming, and integrating structured data from the Web as well as mining knowledge from the global Web of Data.",2016,,10.1145/2872518.2890599
2038,"Alonso, Omar",Challenges with Label Quality for Supervised Learning,"Label quality, crowdsourcing, human computation, machine learning","Organizations that develop and use technologies around information retrieval, machine learning, recommender systems, and natural language processing depend on labels for engineering and experimentation. These labels, usually gathered via human computation, are used in machine-learned models for prediction and evaluation purposes. In such scenarios, collecting high-quality labels is a very important part of the overall process. We elaborate on these challenges and discuss research directions.",2015,,10.1145/2724721
2039,"Ge, Juan and Han, Wenli and Zhang, Xunhu and Zhou, Jin",Research on Construction of Quality Service Platform of Survey and Mapping,"Quality, Data management, Service platform","Quality data of Surveying and mapping is an intuitive reflection of the industry's quality situation and technical development situation. The construction of quality service platform of Surveying and mapping is discussed for the problems existing in the management of surveying and mapping quality data and for the demand for the use of quality data. It discusses the contents, framework and techniques used by the platform. The platform can be used to assist scientific decision-making and improve the service level.",2020,,10.1145/3397056.3397078
2040,"Aljawarneh, Shadi and Radhakrishna, Vangipuram and Kumar, Gunupudi Rajesh",A Recent Survey on Challenges in Security and Privacy in Internet of Things,"challenges in IoT, research issues, S/W weakness, IoT classification, IoT services, IoT architecture, security and privacy, vulnerability","Computing environment in IoT (Internet of Things) is surrounded with huge amounts of heterogeneous data fulfilling many services in everyone's daily life. Since, communication process in IoT takes place using different devices such as smart phones, sensors, mobile devices, household devices, embedded equipment etc. With the use of these variety of devices, the exchange of data in open internet environment is prone to vulnerabilities. The main cause for these vulnerabilities is the weaknesses in the design of software components and hardware components. Bridging communications gaps in the IoT is a complex process as the data is from heterogeneous sources. An effort is made in this paper to discuss various challenges that are being faced in security and privacy of data. This will be very much helpful for researchers who want to pursue research.",2019,,10.1145/3330431.3330457
2041,"Wu, Xueqiong and Chen, Lei and Ji, Kun and Wang, Huidong and Qian, Hao and Ma, Lidong",Design and Application of Virtual Production Command Service in Power Distribution Network Based on Artificial Intelligence,"AI, Dispatch Professional Decision, Power Distribution Network Virtual Production Command Engine, Power Knowledge Graph, Power Distribution Network Regulations","Abstract: As the distribution network business hub, the distribution network production command center faces the need to improve the efficiency of the distribution network production command business. This article draws on international mainstream artificial intelligence (such as Google AlphaGo) and other independent learning models to explore the integration of artificial intelligence and power grid professional business. This paper analyzes the development trend of artificial intelligence technology in the fields of power grid distribution and power knowledge map, and proposes a distribution network virtual production commander engine with dispatch operation, remote monitoring, and intelligent screen monitoring capabilities based on the distribution network knowledge map to realize power grid dispatch Intelligent applications in the fields of operation command, emergency repair, and smart services, and some functions have been verified by the State Grid Hangzhou Electric Power Company.",2022,,10.1145/3512826.3512836
2042,"Tanaka, Yasuhiro and Kodate, Akihisa and Bolt, Timothy",Data Sharing System Based on Legal Risk Assessment,"Privacy Protection, Data Sharing, Personal Data, Legal Risk Assessment, Information system","Regulations on protection of personal information vary from country to country. Therefore, when conducting international surveys for research, it is required to collect, manage and operate personal data properly complying with the laws and regulations of each country.We design a support system to fulfill conditions in terms of compliance for the proper and efficient management of data collection and utilization especially universities by making compliance management related to data cooperation a common foundation.This study aims to discuss requirements for the compliance management base system for data alliance and shared use of data.",2018,,10.1145/3227696.3227715
2043,"Lucic, Ana and Blake, Catherine",Preparing a Workforce to Effectively Reuse Data,"program development and evaluation, data analytics and evaluation, survey results, data science","For centuries, library and information science professionals have been responsible for curating and preserving access to information resources. The last few decades have seen an unprecedented change in how new knowledge is created, disseminated and reused both within academe and industry, which provides new opportunities to intervene within the data lifecycle. This paper documents efforts to create a graduate educational program that produces alum who understand both the social and technical aspects of data analytics and who can effectively employ data to address questions in academe and industry. We share perspectives gained from initial interviews with project partners who have data needs, and report on how those needs directly informed curricula development of the Socio-technical Data Analytics (SODA) program at the School of Information Sciences at the University of Illinois. We also provide a formative student evaluation of the program that was conducted to identify aspects that are successful, and those where further work is needed in order to help other schools who are developing similar programs that prepare a workforce who can effectively reuse data.",2016,,
2044,"Churchill, Elizabeth F.",From Data Divination to Data-Aware Design,,,2012,,10.1145/2334184.2334188
2045,"Nargesian, Fatemeh and Zhu, Erkang and Miller, Ren\'{e}e J. and Pu, Ken Q. and Arocena, Patricia C.",Data Lake Management: Challenges and Opportunities,,"The ubiquity of data lakes has created fascinating new challenges for data management research. In this tutorial, we review the state-of-the-art in data management for data lakes. We consider how data lakes are introducing new problems including dataset discovery and how they are changing the requirements for classic problems including data extraction, data cleaning, data integration, data versioning, and metadata management.",2019,,10.14778/3352063.3352116
2046,"Zhou, Ke and Song, Jingkuan",Introduction to the Special Issue on Learning-Based Support for Data Science Applications,,,2021,,10.1145/3450751
2047,"Wan, Xinxin",A Study on the Current Development of Artificial Intelligence in Education Industry in China,"AI Education, Smart Classroom, Education Informatization, Oral Assessment, Adaptive Learning","This article first explained the definition of AI in education (AIEd) and reported findings regarding the current development of AIEd industry in the Chinese context. The research design is a context-specific case study using the supply and demand theoretical framework. From a demand-side perspective, the author made an in-depth analysis of the specific AI applications employed in different educational scenarios, including the automated speaking assessment system, the content-based image retrieval system, adaptive learning system, AI-supported classrooms, and AI-assisted campus safety system. For the supply analysis of the AIEd industry, this article summarized key AIEd industry chains and technologies currently widely used in China, obtaining the industry market scale through data collected from different sources. In addition, the iFLYTEK company, as a typical enterprise in the AIEd industry, was taken as a medium to conduct a case analysis. The employment of various AI applications in smart classrooms, smart exams, and smart terminals were comprehensively discussed. In a nutshell, this article discussed the development status and future trends of Chinese AIEd industry, with an aim to offer suggestions and implications for education practitioners.",2021,,10.1145/3463531.3463536
2048,"Bentalha, Badr and Hmioui, Aziz and Alla, Lhoussaine",The Digitalization of the Supply Chain Management of Service Companies: A Prospective Approach,"prospective approach, service company, supply chain, SCM, digital","Supply Chain Management (SCM) was born and developed first in an industrial context. In the field of services, little research has addressed the issue of the company's SCM. According to [1] ""service logistics is an approach that stabilizes and guarantees the continuity of flows: it is then oriented more towards the service provided than towards reducing traffic costs"". The SCM of services is of increasing interest to companies facing strong competition, market globalization and rapid changes in information and communication technologies. This evolution has led to a rapid integration of new digital practices in this field.So, how is the digitalization of the SCM of service companies looking today and what will be the future trends? On the one hand, with the help of the literature review, we seek to identify the concept of the SCM in services and its specificities, then that of digitization of the SCM and its organizational dimension. On the other hand, we are attempting a prospective approach to the current practices and digitalization prospects of the service company's SCM.",2019,,10.1145/3368756.3369005
2049,"Zhou, Zirui and Chu, Lingyang and Liu, Changxin and Wang, Lanjun and Pei, Jian and Zhang, Yong",Towards Fair Federated Learning,"distributed learning, collaborative fairness, data leakage, federated learning, data privacy, model fairness","Federated learning has become increasingly popular as it facilitates collaborative training of machine learning models among multiple clients while preserving their data privacy. In practice, one major challenge for federated learning is to achieve fairness in collaboration among the participating clients, because different clients' contributions to a model are usually far from equal due to various reasons. Besides, as machine learning models are deployed in more and more important applications, how to achieve model fairness, that is, to ensure that a trained model has no discrimination against sensitive attributes, has become another critical desiderata for federated learning. In this tutorial, we discuss formulations and methods such that collaborative fairness, model fairness, and privacy can be fully respected in federated learning. We review the existing efforts and the latest progress, and discuss a series of potential directions.",2021,,10.1145/3447548.3470814
2050,"Matheus, Ricardo and Janssen, Marijn",How to Become a Smart City? Balancing Ambidexterity in Smart Cities,"innovation, ambidexterity, exploration, transformation, exploitation, smart cities, e-government","Most cities have limited resources to become a smart city. Yet some cities have been more successful than others in becoming a smart city. This raises the questions why were some cities able to become smart, whereas other were not able to do so? This research is aimed at identifying factors influencing the shift towards becoming a smart city. In this way insight is gained into factors that governments can influence to become a smart city. First, Literature was reviewed to identify dimensions and factors enabling or impeding the process of becoming a smart city. These factors were used to compare two similar type of case studies. The cases took different paths to become a smart city and had different levels of success. This enabled us to identify factors influencing the move towards smart cities. The results reveal that existing infrastructures should be used and extended in such a way that they can facilitate a variety of different applications. Synergy from legacy systems can avoid extra expenditures. Having such an infrastructure in place facilitates the development of new organizational models. These models are developed outside the existing organization structure to avoid hinder from existing practices and organizational structures. This finding suggests that smart cities focussed on structural ambidexterity innovate quicker.",2017,,10.1145/3047273.3047386
2051,"Lee, Jae-Gil and Roh, Yuji and Song, Hwanjun and Whang, Steven Euijong","Machine Learning Robustness, Fairness, and Their Convergence","fairness, convergence, robustness, machine learning","Responsible AI becomes critical where robustness and fairness must be satisfied together. Traditionally, the two topics have been studied by different communities for different applications. Robust training is designed for noisy or poisoned data where image data is typically considered. In comparison, fair training primarily deals with biased data where structured data is typically considered. Nevertheless, robust training and fair training are fundamentally similar in considering that both of them aim at fixing the inherent flaws of real-world data. In this tutorial, we first cover state-of-the-art robust training techniques where most of the research is on combating various label noises. In particular, we cover label noise modeling, robust training approaches, and real-world noisy data sets. Then, proceeding to the related fairness literature, we discuss pre-processing, in-processing, and post-processing unfairness mitigation techniques, depending on whether the mitigation occurs before, during, or after the model training. Finally, we cover the recent trend emerged to combine robust and fair training in two flavors: the former is to make the fair training more robust (i.e., robust fair training), and the latter is to consider robustness and fairness as two equals to incorporate them into a holistic framework. This tutorial is indeed timely and novel because the convergence of the two topics is increasingly common, but yet to be addressed in tutorials. The tutors have extensive experience publishing papers in top-tier machine learning and data mining venues and developing machine learning platforms.",2021,,10.1145/3447548.3470799
2052,"Kokkinakos, Panagiotis and Koutras, Costas and Markaki, Ourania and Koussouris, Sotirios and Trutnev, Dmitrii and Glikman, Yuri",Assessing Governmental Policies' Impact through Prosperity Indicators and Open Data,"Fuzzy Cognitive Maps, Policy Impact Evaluation, Prosperity Indicators, Open Data, Policy Making","The aim of this paper is to provide an overview of (the theory and practice of) prosperity indicators for assessing the impact of governmental policies and the data sources associated to their calculation, touching also on the broad theme of Open Data which opens up new horizons for the calculation and exploitation of Social Indicators. Following a quick overview of the basics of prosperity indicators, their basic methodological principles and their typology, a presentation of the Policy Compass project approach and the description of its pilot application in St. Petersburg are provided, which are tackling the above mentioned issue with the provision of a powerful ICT platform.",2014,,10.1145/2729104.2729134
2053,"Wu, Xinghao and Qiao, Fei and Poon, Kwok",Cloud Manufacturing Application in Semiconductor Industry,,"This paper aims to shed some light on how the concept of cloud manufacturing has been applied to the semiconductor manufacturing operations. It starts with describing the challenges to the semiconductor manufacturing due to evolving of outsourcing business model in global context, then discusses the different forms of cloud manufacturing and proposes the semiconductor industry oriented architecture for cloud manufacturing. Serus is used as a case study to share how the cloud manufacturing has created the values for the customer and its outsourced suppliers in the semiconductor industry.",2014,,
2054,"Kim, Miryung and Zimmermann, Thomas and DeLine, Robert and Begel, Andrew",The Emerging Role of Data Scientists on Software Development Teams,,"Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.",2016,,10.1145/2884781.2884783
2055,"Wang, Lei and Wang, Yang",Application of Machine Learning for Process Control in Semiconductor Manufacturing,"Virtual metrology, Data analytics, Machine learning, Advanced process control, Semiconductor manufacturing","In this article, the authors attempt to describe the core quality inspection during semiconductor manufacturing in terms of production efficiency and yield. Special focus is therefore given to photolithography, which is the most critical step for the fabrication of wafer patterns in front-end processes. Further, machine learning approaches are demonstrated and their applicability in semiconductor manufacturing industry is discussed. Also, a technical concept regarding virtual metrology for advanced process control in semiconductor production is introduced as a potential utilization case. Finally, current status and future trends in technology as well as application are summarized based on authors' perspective in the concluding section.",2020,,10.1145/3424311.3424326
2056,"Lin, Yu-Ru and Castillo, Carlos and Yin, Jie","The 5th International Workshop on Social Web for Disaster Management(SWDM'18): Collective Sensing, Trust, and Resilience in Global Crises","emergency management, disaster response, social media","During large-scale emergencies such as natural and man-made disasters, a massive amount of information is posted by the public in social media. Collecting, aggregating, and presenting this information to stakeholders can be extremely challenging, particularly if an understanding of the ""big picture»» is sought. This international workshop, the fifth in the series, is a key venue for researchers and practitioners to discuss research challenges and technical issues around the usage of social media in disaster management. Workshop»s website: https://sites.google.com/site/swdm2018/",2018,,10.1145/3159652.3160594
2057,"Suaprae, Phanintorn and Nilsook, Prachyanun and Wannapiroon, Panita",System Framework of Intelligent Consulting Systems with Intellectual Technology,,"The purposes of this research were: 1) Analyze factors affecting the student retention of higher education students, 2) Develop intelligent consulting system models with intellectual technology for the student retention of higher education students, 3) Design intelligent consulting system architecture with intellectual technology for the student retention of higher education students, 4) Develop intelligent consulting systems with intellectual technology for the student retention of higher education students, and 5) Study the results of intelligent consultation systems with intellectual technology for the student retention of higher education students. An intelligent counseling system with intellectual technology for the student retention of higher education students is a system that can reduce students' mid-exit rates and increase student retention rates. The research has synthesized analysis of factors that affect Student retention applied to Cognitive technology, machine learning can provide accurate student retention forecasts. Counselors can know before students drop out.",2021,,
2058,"Xie, Xiaoyuan and Poon, Pak-Lok and Pullum, Laura L.",Workshop Summary: 2019 IEEE / ACM Fourth International Workshop on Metamorphic Testing (MET 2019),"software verification and validation, metamorphic testing, software engineering, software testing","MET is a relatively new workshop on metamorphic testing for academic researchers and industry practitioners. The first international workshop on MET (MET 2016) was co-located with the 38th International Conference on Software Engineering (ICSE 2016) in Austin TX, USA on May 16, 2016. Since then the workshop has become an annual event. This paper reports on the fourth International Workshop on Metamorphic Testing (MET 2019) held in Montr\'{e}al, Canada on May 26, 2019, as part of the 41st International Conference on Software Engineering (ICSE 2019). We first outline the aims of the workshop, followed by a discussion of its keynote speech and technical program.",2019,,10.1145/3356773.3356810
2059,"Pei, Jian",Data Pricing -- From Economics to Data Science,"revenue maximization, trustfulness, subscription, fairness, arbitrage, information goods, auctions, privacy, bundling, data pricing, data products, digital products","Data are invaluable. How can we assess the value of data objectively and quantitatively? Pricing data, or information goods in general, has been studied and practiced in dispersed areas and principles, such as economics, data management, data mining, electronic commerce, and marketing. In this tutorial, we present a unified and comprehensive overview of this important direction. We examine various motivations behind data pricing, understand the economics of data pricing, review the development and evolution of pricing models, and compare the proposals of marketplaces of data. We cover both digital products, such as ebooks and MP3 music, and data products, such as data sets, data queries and machine learning models. We also connect data pricing with the highly related areas, such as cloud service pricing, privacy pricing, and decentralized privacy preserving infrastructure like blockchains.",2020,,10.1145/3394486.3406473
2060,"Cao, Tien-Dung and Pham, Tran-Vu and Vu, Quang-Hieu and Truong, Hong-Linh and Le, Duc-Hung and Dustdar, Schahram",MARSA: A Marketplace for Realtime Human Sensing Data,"Internet of Things, platform, data contract, cost model","This article introduces a dynamic cloud-based marketplace of near-realtime human sensing data (MARSA) for different stakeholders to sell and buy near-realtime data. MARSA is designed for environments where information technology (IT) infrastructures are not well developed but the need to gather and sell near-realtime data is great. To this end, we present techniques for selecting data types and managing data contracts based on different cost models, quality of data, and data rights. We design our MARSA platform by leveraging different data transferring solutions to enable an open and scalable communication mechanism between sellers (data providers) and buyers (data consumers). To evaluate MARSA, we carry out several experiments with the near-realtime transportation data provided by people in Ho Chi Minh City, Vietnam, and simulated scenarios in multicloud environments.",2016,,10.1145/2883611
2061,"Lehmann, Jens and Auer, S\""{o}ren and Capadisli, Sarven and Janowicz, Krzysztof and Bizer, Christian and Heath, Tom and Hogan, Aidan and Berners-Lee, Tim",LDOW2017: 10th Workshop on Linked Data on the Web,"semantic web, linked data","The 10th Linked Data on the Web workshop (LDOW2017) was held in Perth, Western Australia on April 3, 2017, co-located with the 26th International World Wide Web Conference (WWW2017). In its 10th anniversary edition, the LDOW workshop aims to stimulate discussion and further research into the challenges of publishing, consuming, and integrating structured data on the Web as well as mining knowledge from said data.",2017,,10.1145/3041021.3055510
2062,"Bednarz, Tomasz and Hughes, Rowan T. and Mathews, Alex and Chen, Dawei and Zhu, Liming and Filonik, Daniel","Visual Analytics for Large Networks: Theory, Art and Practice",,,2021,,10.1145/3450508.3464558
2063,"Maz\'{o}n, Jose-Norberto and Garrig\'{o}s, Irene and Daniel, Florian and Castellanos, Malu",Report of the International Workshop on Business Intelligence and the Web: BEWEB 2011,,"The 2nd International Workshop on Business intelligencE and the WEB (BEWEB) was co-located with the EDBT/ICDT 2011 Joint Conference in Uppsala (Sweden) on March 25, 2011. BEWEB intends to be an international forum for researchers and practitioners to exchange ideas on how to leverage the huge amount of data that is available on the Web in BI applications and on how to apply Web engineering methods and techniques to the design of BI applications. This report summarizes the 2011 edition of BEWEB.",2012,,10.1145/2380776.2380789
2064,"Netten, Niels and Bargh, Mortaza S. and van den Braak, Susan and Choenni, Sunil and Leeuw, Frans",On Enabling Smart Government: A Legal Logistics Framework for Future Criminal Justice Systems,"legal design, Law enforcement, efficiency, open justice, smart governance, and penal law, effectivity","While in business and private settings the disruptive impact of advanced information communication technology (ICT) have already been felt, the legal sector is now starting to face great disruptions due to such ICTs. Bits and pieces of innovations in the legal sector have been emerging for some time, affecting the performance of core functions and the legitimacy of public institutions.In this paper, we present our framework for enabling the smart government vision, particularly for the case of criminal justice systems, by unifying different isolated ICT-based solutions. Our framework, coined as Legal Logistics, supports the well-functioning of a legal system in order to streamline the innovations in these legal systems. The framework targets the exploitation of all relevant data generated by the ICT-based solutions. As will be illustrated for the Dutch criminal justice system, the framework may be used to integrate different ICT-based innovations and to gain insights about the well-functioning of the system. Furthermore, Legal Logistics can be regarded as a roadmap towards a smart and open justice.",2016,,10.1145/2912160.2912180
2065,"Hoffmann, Leah",Seeing Light at the End of the Cybersecurity Tunnel,,"After decades of cybersecurity research, Elisa Bertino remains optimistic.",2020,,10.1145/3403976
2066,"Zalmout, Nasser and Zhang, Chenwei and Li, Xian and Liang, Yan and Dong, Xin Luna",All You Need to Know to Build a Product Knowledge Graph,"knowledge graphs, taxonomy, data cleaning, information extraction","Knowledge graphs have been pivotal in supporting downstream applications like search, recommendation, and question answering, among others. Therefore, knowledge graphs have naturally become key enabling technologies in e-Commerce platforms. Developing a high coverage product knowledge graph is more challenging than generic knowledge graphs. The highly specific and complex domain, the sparsity of training data, along with the dynamic taxonomies and product types, can constrain the resulting knowledge graphs. In this tutorial we present best practices and ML innovations in industry towards building a scalable product knowledge graph. Contributions in this domain benefit from the general literature in areas including information extraction and data mining, tailored to address the specific characteristics of e-Commerce platforms.",2021,,10.1145/3447548.3470825
2067,"Stakoulas, Konstantinos and Georgiou, Konstantinos and Mittas, Nikolaos and Angelis, Lefteris",An Analysis of User Profiles from Covid-19 Questions in Stack Overflow,,"The COVID-19 pandemic brought many changes in society, with one of the most important being an explosion of software development concerning technological solutions for combatting its crippling effects. In this global crisis, many software enthusiasts, combined with seasoned developers and specialists turned their attention to Questions and Answers platforms such as Stack Overflow to expand their knowledge and ask questions regarding their COVID-19 related solutions. This paper examines the different characteristics of these users, dividing them into Newcomers and Oldcomers and pinpoints popularity differences, scientific and technological backgrounds by analyzing key technologies, as well as the role of gender in their participation.",2021,,10.1145/3503823.3503900
2068,"Parycek, P. and Pereira, G. Viale",Drivers of Smart Governance: Towards to Evidence-Based Policy-Making,"Decision-making, Collaborative Governance, Evaluation, Data Governance","This paper presents the preliminary framework proposed by the authors for drivers of Smart Governance. The research question of this study is: What are the drivers for Smart Governance to achieve evidence-based policy-making? The framework suggests that in order to create a smart governance model, data governance and collaborative governance are the main drivers. These pillars are supported by legal framework, normative factors, principles and values, methods, data assets or human resources, and IT infrastructure. These aspects will guide a real time evaluation process in all levels of the policy cycle, towards to the implementation of evidence-based policies.",2017,,10.1145/3085228.3085255
2069,"Barhak, Jacob",Modeling Clinical Data from Publications,"clinical trial, publications, high performance computing, reference modeling, Monte-Carlo, disease modeling","Medical data is becoming increasingly available. Access to such data is generally restricted and researchers cannot access it easily. On the other hand, clinical trial data is freely available and published without restriction for access to the public at the summary level. With proper analysis, it is possible to extract valuable conclusions from such data. This paper will review new methods to look at such public data and will discuss possible future trends.",2015,,
2070,"Gupta, Suyash and Hellings, Jelle and Rahnama, Sajjad and Sadoghi, Mohammad",Blockchain Consensus Unraveled: Virtues and Limitations,"permissioned blockchains, resilient transaction processing, cluster-sending, byzantine learning, geo-scale, sharding, consensus","Since the introduction of Bitcoin---the first wide-spread application driven by blockchains---the interest of the public and private sector in blockchains has skyrocketed. At the core of this interest are the ways in which blockchains can be used to improve data management, e.g., by enabling federated data management via decentralization, resilience against failure and malicious actors via replication and consensus, and strong data provenance via a secured immutable ledger.In practice, high-performance blockchains for data management are usually built in permissioned environments in which the participants are vetted and can be identified. In this setting, blockchains are typically powered by Byzantine fault-tolerant consensus protocols. These consensus protocols are used to provide full replication among all honest blockchain participants by enforcing an unique order of processing incoming requests among the participants.In this tutorial, we take an in-depth look at Byzantine fault-tolerant consensus. First, we take a look at the theory behind replicated computing and consensus. Then, we delve into how common consensus protocols operate. Finally, we take a look at current developments and briefly look at our vision moving forward.",2020,,10.1145/3401025.3404099
2071,"Han, Yang and Li, Victor O.K. and Lam, Jacqueline C.K. and Lu, Zhiyi",UMeAir: Predicting Momentary Happiness Towards Air Quality via Machine Learning,"Subjective well-being prediction, Machine learning, Air quality, Short-term happiness, Data interpretability","Subjective well-being (SWB) refers to people's subjective evaluation of their own quality of life. Previous studies show that environmental pollution, such as air pollution, has generated significant negative impacts on one's SWB. However, such works are often constrained by the lack of appropriate representation of SWB specifically related to air quality. In this study, we develop UMeAir, which collects one's real-time SWB, specifically, one's momentary happiness at a given air quality, pre-processes input data and detects outliers via Isolation Forests, trains and selects the best model via Support Vector Machine and Random Forests, and predicts the momentary happiness towards any air quality one experienced. Unlike traditional representation of air quality by pollution concentration/Air Pollution Index, UMeAir intends to represent air quality in a more user-comprehensible way, by connecting the air quality experienced at a particular time and location with the corresponding momentary happiness perceived towards the air. The higher the momentary happiness, the better the air quality one experienced. Our work is the first attempt to predict momentary happiness towards air quality in real-time, with the development of the-first-of-its-kind UMeAir Happiness Index (HAPI) towards air quality via machine learning.",2018,,10.1145/3267305.3267694
2072,"Khadivizand, Sam and Beheshti, Amin and Sobhanmanesh, Fariborz and Sheng, Quan Z. and Istanbouli, Elias and Wood, Steven and Pezaro, Damon",Towards Intelligent Feature Engineering for Risk-Based Customer Segmentation in Banking,"banking processes, risk-based customer segmentation, business process, feature engineering","Business Processes, i.e., a set of coordinated tasks and activities to achieve a business goal, and their continuous improvements are key to the operation of any organization. In banking, business processes are increasingly dynamic as various technologies have made dynamic processes more prevalent. For example, customer segmentation, i.e., the process of grouping related customers based on common activities and behaviors, could be a data-driven and knowledge-intensive process. In this paper, we present an intelligent data-driven pipeline composed of a set of processing elements to move customers' data from one system to another, transforming the data into the contextualized data and knowledge along the way. The goal is to present a novel intelligent customer segmentation process which automates the feature engineering, i.e., the process of using (banking) domain knowledge to extract features from raw data via data mining techniques, in the banking domain. We adopt a typical scenario for analyzing customer transaction records, to highlight how the presented approach can significantly improve the quality of risk-based customer segmentation in the absence of feature engineering.",2020,,10.1145/3428690.3429172
2073,"Gupta, Suyash and Hellings, Jelle and Rahnama, Sajjad and Sadoghi, Mohammad",An In-Depth Look of BFT Consensus in Blockchain: Challenges and Opportunities,,"Since the introduction of Bitcoin---the first wide-spread application driven by blockchains---the interest of the public and private sector in blockchains has skyrocketed. At the core of this interest are the ways in which blockchains can be used to improve data management, e.g., by enabling federated data management via decentralization, resilience against failure and malicious actors via replication and consensus, and strong data provenance via a secured immutable ledger.In practice, high-performance blockchains for data management are usually built in permissioned environments in which the participants are vetted and can be identified. In this setting, blockchains are typically powered by Byzantine fault-tolerant consensus protocols. These consensus protocols are used to provide full replication among all honest blockchain participants by enforcing an unique order of processing incoming requests among the participants.In this tutorial, we take an in-depth look at Byzantine fault-tolerant consensus. First, we take a look at the theory behind replicated computing and consensus. Then, we delve into how common consensus protocols operate. Finally, we take a look at current developments and briefly look at our vision moving forward.",2019,,10.1145/3366625.3369437
2074,"Anjum, Shahid W.",Risk Magnification Framework for Clouds Computing Architects in Business Intelligence,"Business Intelligence Architecture, Cloud Computing, IT Risk Management, Multi Criteria Decision Making, A'WOT Analysis","IT infrastructure and applications in enterprise systems started with traditional client-server architecture and have gone through key paradigm shifts in infrastructure, software, enterprise, and service architectures to current age of cloud computing and internet of everything. Using strengths-weaknesses-opportunities-threats and analytical hierarchy process of multi-criteria decision making frameworks together, various aspects of cloud computing characteristics related to opportunities, benefits, costs, value and risks can be understood in a more detailed way and can be ranked. This article has combined these two frameworks for the ranking of various business intelligence architects for cloud computing by using 'business automation with sustainable hedging for information risks' framework for cloud computing from conservative perspective where risk relevancy attracts the prime focus. The results have shown that moving operational business intelligence is the best business intelligence architecture for cloud computing as its strengths are more than inherent risks as has become evident by using this approach.",2017,,10.1145/3029387.3029421
2075,"Li, Xinming and Talburt, John R. and Li, Ting and Liu, Xiangwen",Scoring Matrix Combined with Machine Learning for Heterogeneously Structured Entity Resolution,,"This paper describes how machine learning works with ""coring matrix"", which is designed for measuring the similarity between heterogeneously structured references, to get a better performance in Entity Resolution (ER). In the scoring matrix, each entity reference is tokenized and all pairs of tokens between the references are scored by a similarity scoring function such as the Levenshtein edit distance. In so doing, a similarity score vector can measure the similarity between references. With the similarity score vector, machine learning is used to make the linking decision. Our experiments show that machine learning based on score vector outperforms TF-IDF and FuzzyWuzzy benchmarks. One possible explanation is that a similarity score vector conveys much more information than a single similarity score. Random forest and neural network even get better performance with raw score vector input than with the statistic characteristic input.",2019,,
2076,"Zhou, Xiaofang and Sadiq, Shazia",Data Centric Research at the University of Queensland,,,2013,,10.1145/2536669.2536682
2077,"Wu, Ji and Zhou, Ming and Xu, Min and Zhang, Jin and Wu, Yue and Zha, Weiwei and Zhang, Chengping",Design and Research of IoT Management Architecture for Power Grid Enterprises Based on Digital Transformation: Application of IoT in Power Grid Enterprises According to Enterprise Architecture Method,"Intelligent IoT management system, IoT, Digital transformation","Internet of things technology, as the core technology in digital transformation, helps enterprises in digital transformation to carry out comprehensive perception, intelligent management and secure transmission. Following the information architecture of State Grid Corporation of China and combined with the business objectives and Strategies of electric power company, carry out the differentiated design of power Internet of things architecture, put forward the improvement direction of power Internet of things architecture, clarify the application scenario and future evolution route of power Internet of things business, and ensure the realization of technology.",2021,,10.1145/3503928.3503944
2078,"Yan, Feng",A Building Integrated Control Platform Oriented Towards Intelligent Building,,,2021,,10.1145/3469213.3470424
2079,"Schuch de Azambuja, Luiza",Drivers and Barriers for the Development of Smart Sustainable Cities: A Systematic Literature Review,"challenges, enablers, sustainable city, smart city","The term Smart Sustainable City (SSC) has been gaining popularity due to the growth of initiatives to address urban problems towards sustainable development. SSC can be considered as a combination of sustainable city and smart city, and some variance between the concepts may be expected. As this is a modern term, the literature falls short of studies presenting factors that hinder and/or facilitate the complex phenomenon of SSC development. Therefore, this paper aims to analyse scientific studies to identify aspects that influence the progress of smart sustainable cities. The methodological approach undertaken was a systematic literature review that included 169 papers. The results offer a comprehensive list of 57 drivers and 63 barriers, classified according to five main dimensions of a smart sustainable city, which are the three sustainability pillars (society, environment, and economy), combined to governance, and urban infrastructure. The findings revealed ‘governance’ as the most significant domain for SSC development, and multistakeholder engagement as one of the main challenges. This study shows that SSC is not a research field itself, but an interdisciplinary concept, contributing to academics, government, and policymakers for eradicating potential interferences in the development of smart and sustainable cities.",2021,,10.1145/3494193.3494250
2080,,Introduction,,"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.",2019,,
2081,"Douglas, David M.",Should Researchers Use Data from Security Breaches?,,Evaluating the arguments for and against using digital data derived from security breaches.,2019,,10.1145/3368091
2082,"Dang, Yingnong and Lin, Qingwei and Huang, Peng",AIOps: Real-World Challenges and Research Innovations,"AIOps, software analytics, DevOps","AIOps is about empowering software and service engineers (e.g., developers, program managers, support engineers, site reliability engineers) to efficiently and effectively build and operate online services and applications at scale with artificial intelligence (AI) and machine learning (ML) techniques. AIOps can help improve service quality and customer satisfaction, boost engineering productivity, and reduce operational cost. In this technical briefing, we first summarize the real-world challenges in building AIOps solutions based on our practice and experience in Microsoft. We then propose a roadmap of AIOps related research directions, and share a few successful AIOps solutions we have built for Microsoft service products.",2019,,10.1109/ICSE-Companion.2019.00023
2083,"Yatim, Ir. Fazilah Mat and Majid, Zulkepli and Amerudin, Shahabuddin",Locating Success Within A Geographic Information System,"KSF-Key Success Factors, TNB-Tenaga Nasional Berhad, GIS-Geographic Information System","Tenaga Nasional Berhad (TNB) being one of the largest utilities in the Southeast Asia has embarked on enriching their GIS solutions suite for its business operations. A distribution station was chosen as a pilot project to run the business processes using GIS. The successful implementation is to be measured through its impact on the station day-today operations. Key success factors (KSF) were defined and will be measured with reference to component of GIS. The outcome of the measurement will guide the implementation of GIS nation-wide in TNB Distribution, Malaysia. This paper is aimed at providing insights for utilities who are keen in identifying those success factors and methodology of measuring the success of the GIS implementation.",2017,,10.1145/3132300.3132305
2084,"Hastings, Justine S. and Howison, Mark and Lawless, Ted and Ucles, John and White, Preston",Unlocking Data to Improve Public Policy,,"When properly secured, anonymized, and optimized for research, administrative data can be put to work to help government programs better serve those in need.",2019,,10.1145/3335150
2085,"Talburt, John R.",SPECIAL ISSUE ON ENTITY RESOLUTION Overview: The Criticality of Entity Resolution in Data and Information Quality,,,2013,,10.1145/2435221.2435222
2086,"Calyam, Prasad and Swany, Martin",Research Challenges in Future Multi-Domain Network Performance Measurement and Monitoring,"next-generation measurement infrastructures, future multi-domain network monitoring, research challenges","The perfSONAR-based Multi-domain Network Performance Measurement and Monitoring Workshop was held on February 20-21, 2014 in Arlington, VA. The goal of the workshop was to review the state of the perfSONAR effort and catalyze future directions by cross-fertilizing ideas, and distilling common themes among the diverse perfSONAR stakeholders that include: network operators and managers, end-users and network researchers. The timing and organization for the second workshop is significant because there are an increasing number of groups within NSF supported data-intensive computing and networking programs that are dealing with measurement, monitoring and troubleshooting of multi-domain issues. These groups are forming explicit measurement federations using perfSONAR to address a wide range of issues. In addition, the emergence and wide-adoption of new paradigms such as software-defined networking are taking shape to aid in traffic management needs of scientific communities and network operators. Consequently, there are new challenges that need to be addressed for extensible and programmable instrumentation, measurement data analysis, visualization and middleware security features in perfSONAR. This report summarizes the workshop efforts to bring together diverse groups for delivering targeted short/long talks, sharing latest advances, and identifying gaps that exist in the community for solving end-to-end performance problems in an effective, scalable fashion.",2015,,10.1145/2805789.2805795
2087,"Frey, Remo Manuel and Hardjono, Thomas and Smith, Christian and Erhardt, Keeley and Pentland, Alex 'Sandy'",Secure Sharing of Geospatial Wildlife Data,"crime, animal, GPS, hunting, geospatial, privacy, data sharing, species protection, security, wildlife, cyber-poaching, blockchain","Modern tracking technologies enables new ways for data mining in the wild. It allows wildlife monitoring centers to permanently collect geospatial data in a non-intrusive manner in real-time and at low cost. Unfortunately, wildlife data is exposed to crime and there is already a first reported case of 'cyber-poaching'. Based on stolen geospatial data, poachers can easily track and kill animals. As a result, cautious monitoring centers limited data access for research and public use. This means that the data cannot fully exploit its potential. We propose a novel solution to overcome the security problem. It allows monitoring centers to securely answer questions from the research community and to provide aggregated data to the public while the raw data is protected against unauthorized third parties. This data service can also be monetized. Several new applications are conceivable, such as a mobile app for preventing conflicts between human and wildlife or for engaging people in wildlife donation. Besides presenting the solution and potential use cases, the intention of present article is to start a discussion about the need for data protection and privacy in the animal world.",2017,,10.1145/3080546.3080550
2088,"Hoel, Tore and Griffiths, Dai and Chen, Weiqin",The Influence of Data Protection and Privacy Frameworks on the Design of Learning Analytics Systems,"learning analytics, data protection by design, privacy frameworks, data protection by default, privacy by design, data protection, personal information, learning analytics process requirements, learning analytics systems design","Learning analytics open up a complex landscape of privacy and policy issues, which, in turn, influence how learning analytics systems and practices are designed. Research and development is governed by regulations for data storage and management, and by research ethics. Consequently, when moving solutions out the research labs implementers meet constraints defined in national laws and justified in privacy frameworks. This paper explores how the OECD, APEC and EU privacy frameworks seek to regulate data privacy, with significant implications for the discourse of learning, and ultimately, an impact on the design of tools, architectures and practices that now are on the drawing board. A detailed list of requirements for learning analytics systems is developed, based on the new legal requirements defined in the European General Data Protection Regulation, which from 2018 will be enforced as European law. The paper also gives an initial account of how the privacy discourse in Europe, Japan, South-Korea and China is developing and reflects upon the possible impact of the different privacy frameworks on the design of LA privacy solutions in these countries. This research contributes to knowledge of how concerns about privacy and data protection related to educational data can drive a discourse on new approaches to privacy engineering based on the principles of Privacy by Design. For the LAK community, this study represents the first attempt to conceptualise the issues of privacy and learning analytics in a cross-cultural context. The paper concludes with a plan to follow up this research on privacy policies and learning analytics systems development with a new international study.",2017,,10.1145/3027385.3027414
2089,"Winslett, Marianne and Braganholo, Vanessa",Michael Franklin Speaks Out on Data Science,,"Welcome to ACM SIGMOD Record series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we're at the 2017 SIGMOD and PODS conference in Chicago. I have here with me Mike Franklin, who is the chair of the Computer Science department at the University of Chicago. Before that, for many years, Mike was a professor at Berkeley where he also served as a chair of the Computer Science division. Mike was a co-founder and director of the Algorithms, Machines, and People Lab, better known as the AMPLab. He is an ACM fellow, a two-time winner of the SIGMOD Ten Year Test of Time Award, and a founder of the successful startup, Truviso. Mike's Ph.D. is from the University of Wisconsin Madison. So, Mike, welcome!",2019,,10.1145/3377391.3377398
2090,"Gupta, Suyash and Hellings, Jelle and Rahnama, Sajjad and Sadoghi, Mohammad",Building High Throughput Permissioned Blockchain Fabrics: Challenges and Opportunities,,"Since the introduction of Bitcoin---the first widespread application driven by blockchains---the interest in the design of blockchain-based applications has increased tremendously. At the core of these applications are consensus protocols that securely replicate client requests among all replicas, even if some replicas are Byzantine faulty. Unfortunately, these consensus protocols typically have low throughput, and this lack of performance is often cited as the reason for the slow wider adoption of blockchain technology. Consequently, many works focus on designing more efficient consensus protocols to increase throughput of consensus.We believe that this focus on consensus protocols only explains part of the story. To investigate this belief, we raise a simple question: Can a well-crafted system using a classical consensus protocol outperform systems using modern protocols? In this tutorial, we answer this question by diving deep into the design of blockchain systems. Further, we take an in-depth look at the theory behind consensus, which can help users select the protocol that best-fits their requirements. Finally, we share our vision of high-throughput blockchain systems that operate at large scales.",2020,,10.14778/3415478.3415565
2091,"Pan, Zhiwen and Zhao, Shuangye and Pacheco, Jesus and Zhang, Yuxin and Song, Xiaofan and Chen, Yiqiang and Dai, Lianjun and Zhang, Jun",Comprehensive Data Management and Analytics for General Society Survey Dataset,"Data management, Decision support systems, Society survey, Data mining, Knowledge discovery","The General Society Survey(GSS) is a kind of government-funded survey which aims at examining the Socio-economic status, quality of life, and structure of contemporary society. GSS dataset is regarded as one of the authoritative source for the government and organization practitioners to make data-driven policies. The previous analytic approaches for GSS dataset are designed by combining expert knowledges and simple statistics. In this paper, we proposed a comprehensive data management and data mining approach for GSS datasets. The approach is designed to be operated in a two-phase manner: a data management phase which can improve the quality of GSS data by performing attribute preprocessing and filter-based attribute selection; a data mining phase which can extract hidden knowledges from the dataset by performing data mining analysis including prediction analysis, classification analysis, association analysis and clustering analysis. By leveraging the power of data mining techniques, our proposed approach can explore knowledges in a fine-grained manner with minimum human interference. Experiments on Chinese General Social Survey dataset are conducted at the end to evaluate the performance of our approach.",2019,,10.1145/3371238.3371269
2092,"van Donge, W. and Bharosa, N. and Janssen, M. F. W. H. A.",Future Government Data Strategies: Data-Driven Enterprise or Data Steward? Exploring Definitions and Challenges for the Government as Data Enterprise,"data enterprise, data stewardship, Data-driven government, data governance, e-government","Comparable to the concept of a data(-driven) enterprise, the concept of a ‘government as data (-driven) enterprise’ is gaining popularity as a data strategy. However, what it implies is unclear. The objective of this paper is to clarify the concept of the government as data (-driven) enterprise, and identify the challenges and drivers that shape future data strategies. Drawing on literature review and expert interviews, this paper provides a rich understanding of the challenges for developing sound future government data strategies. Our analysis shows that two contrary data strategies dominate the debate. On the one hand is the data-driven enterprise strategy that focusses on collecting and using data to improve or enrich government processes and services (internal orientation). On the other hand, respondents point to the urgent need for governments to take on data stewardship, so other parties can use data to develop value for society (external orientation). Since these data strategies are not mutually exclusive, some government agencies will attempt to combine them, which is very difficult to pull off. Nonetheless, both strategies demand a more data minded culture. Moreover, the successful implementation of either strategy requires mature data governance – something most organisations still need to master. This research contributes by providing more depth to these strategies. The main challenge for policy makers is to decide on which strategy best fits their agency's roles and responsibilities and develop a shared roadmap with the external actors while at the same time mature on data governance.",2020,,10.1145/3396956.3396975
2093,"Wang, Chen and Huang, Xiangdong and Qiao, Jialin and Jiang, Tian and Rui, Lei and Zhang, Jinrui and Kang, Rong and Feinauer, Julian and McGrail, Kevin A. and Wang, Peng and Luo, Diaohan and Yuan, Jun and Wang, Jianmin and Sun, Jiaguang",Apache IoTDB: Time-Series Database for Internet of Things,,"The amount of time-series data that is generated has exploded due to the growing popularity of Internet of Things (IoT) devices and applications. These applications require efficient management of the time-series data on both the edge and cloud side that support high throughput ingestion, low latency query and advanced time series analysis. In this demonstration, we present Apache IoTDB managing time-series data to enable new classes of IoT applications. IoTDB has both edge and cloud versions, provides an optimized columnar file format for efficient time-series data storage, and time-series database with high ingestion rate, low latency queries and data analysis support. It is specially optimized for time-series oriented operations like aggregations query, down-sampling and sub-sequence similarity search. An edge-to-cloud time-series data management application is chosen to demonstrate how IoTDB handles time-series data in real-time and supports advanced analytics by integrating with Hadoop and Spark. An end-to-end IoT data management solution is shown by integrating IoTDB with PLC4x, Calcite, and Grafana.",2020,,10.14778/3415478.3415504
2094,"Dong, Xin Luna and Rekatsinas, Theodoros",Data Integration and Machine Learning: A Natural Synergy,"data integration, schema mapping, data fusion, entity linkage, data cleaning","As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.",2019,,10.1145/3292500.3332296
2095,"Empl, Philip and Pernul, G\""{u}nther",A Flexible Security Analytics Service for the Industrial IoT,"security as a service, industrial IoT, security analytics","In Cloud Computing, the cloud serves as a central data hub for the Industrial Internet of Things' (IIoT) data and is deployed in diverse application fields, e.g., Smart Grid or Smart Manufacturing. Therefore, the aggregated and contextualized data is bundled in a central data hub, bringing tremendous cybersecurity advantages. Given the threat landscape in IIoT systems, especially SMEs (small and medium-sized enterprises) need to be prepared regarding their cybersecurity, react quickly, and strengthen their overall cybersecurity. For instance, with the application of machine learning algorithms, security-related data can be analyzed predictively in order to be able to ward off a potential attack at an early stage. Since modern reference architectures for IIoT systems, such as RAMI 4.0 or IIRA, consider cybersecurity approaches on a high level and SMEs lack financial funds and knowledge, this paper conceptualizes a security analytics service used as a security add-on to these reference architectures. Thus, this paper conceptualizes a flexible security analytics service that implements security capabilities with flexible analytical techniques that fit specific SMEs' needs. The security analytics service is also evaluated with a real-world use case.",2021,,10.1145/3445969.3450427
2096,"Drachsler, Hendrik and Greller, Wolfgang",Privacy and Analytics: It's a DELICATE Issue a Checklist for Trusted Learning Analytics,"educational data mining, trust, data management, privacy, learning analytics, legal aspects, implementation, ethics","The widespread adoption of Learning Analytics (LA) and Educational Data Mining (EDM) has somewhat stagnated recently, and in some prominent cases even been reversed following concerns by governments, stakeholders and civil rights groups about privacy and ethics applied to the handling of personal data. In this ongoing discussion, fears and realities are often indistinguishably mixed up, leading to an atmosphere of uncertainty among potential beneficiaries of Learning Analytics, as well as hesitations among institutional managers who aim to innovate their institution's learning support by implementing data and analytics with a view on improving student success. In this paper, we try to get to the heart of the matter, by analysing the most common views and the propositions made by the LA community to solve them. We conclude the paper with an eight-point checklist named DELICATE that can be applied by researchers, policy makers and institutional managers to facilitate a trusted implementation of Learning Analytics.",2016,,10.1145/2883851.2883893
2097,"Wong, Ka-Chun and Zhang, Jiao and Yan, Shankai and Li, Xiangtao and Lin, Qiuzhen and Kwong, Sam and Liang, Cheng",DNA Sequencing Technologies: Sequencing Data Protocols and Bioinformatics Tools,"bioinformatics, DNA sequencing, tools, third-generation sequencing (TGS), technology, software, data protocols, history, computational biology","The recent advances in DNA sequencing technology, from first-generation sequencing (FGS) to third-generation sequencing (TGS), have constantly transformed the genome research landscape. Its data throughput is unprecedented and severalfold as compared with past technologies. DNA sequencing technologies generate sequencing data that are big, sparse, and heterogeneous. This results in the rapid development of various data protocols and bioinformatics tools for handling sequencing data.In this review, a historical snapshot of DNA sequencing is taken with an emphasis on data manipulation and tools. The technological history of DNA sequencing is described and reviewed in thorough detail. To manipulate the sequencing data generated, different data protocols are introduced and reviewed. In particular, data compression methods are highlighted and discussed to provide readers a practical perspective in the real-world setting. A large variety of bioinformatics tools are also reviewed to help readers extract the most from their sequencing data in different aspects, such as sequencing quality control, genomic visualization, single-nucleotide variant calling, INDEL calling, structural variation calling, and integrative analysis. Toward the end of the article, we critically discuss the existing DNA sequencing technologies for their pitfalls and potential solutions.",2019,,10.1145/3340286
2098,"Kritzinger, A.K. and Calitz, A.P. and Westraadt, L.",Data Wrangling for South African Smart City Crime Data,"Data Cleaning, Open Data, Smart City Data, Data Wrangling","South Africa (S.A.) is currently facing economic and social challenges that could benefit from the implementation of international smart city guidelines. Crucial to transforming a city into a smart city is the collection and access to reliable data. One of the main problems experienced by S.A. cities is the limited access to data, resulting from a traditionally fragmented approach to data collection, sharing and use. Crime-related data is one of the most commonly collected datasets in smart cities. In S.A., crime data is predominantly collected by the S.A. Police Services (SAPS) and security companies. While the latter are not readily available for public use, SAPS crime data is consolidated and disseminated at the national level. Initial data exploration, however, shows that temporal, spatial and structural inconsistencies in the data limits the usefulness of available crime data. In this study, the inconsistencies in SAPS crime data are summarised, and standard data wrangling techniques are implemented and evaluated to clean the data. The study proposes a data wrangling model for S.A. crime data. Furthermore, this study will further developments that could benefit S.A. cities in general as they transform into smart cities.",2020,,10.1145/3410886.3410913
2099,"Hu, Bo and Rodrigues, Eduarda Mendes and Viel, Emeric",Capri: Programmable Analytics for Linked Data,"RDF, R, Linked Data","Link Data (LD) initiative has fundamentally changed the way how data are published, distributed, and consumed. It advocates data transparency and accessibility to fulfill the Web of Data vision. Thus far, tens of billions of data items have been made publicly available in machine-understandable forms (e.g. RDF). The sheer size of LD data, however, has not resulted in a significant increase of data consumption and thus a self-sustainable consumption-driven publication. We contend that this is primarily due to the lack of tooling for exploiting LD. A new programming paradigm is necessary to simplify and encourage value-add LD data utilisation.This paper reports an on-going project towards programmable Linked Open Data. We propose to tap into a distributed computing environment underpinning the popular statistical toolkit R. Where possible, native R operators and functions are used in our approach so as to lower the learning curve for experienced data scientists.We believe a report to the relevant community at this stage can help us to collect critical requirements before moving into the next stage of development. The crux of our future work lies in comprehensive and extensive evaluations, in terms of, but not limited to, system performance, system stability, system scalability, programming productivity and user experience.",2014,,10.1145/2684200.2684336
2100,"Verma, Nitya and Dombrowski, Lynn",Confronting Social Criticisms: Challenges When Adopting Data-Driven Policing Strategies,"challenges, data-driven organizations, policing, metis, law enforcement, data practices","Proponents of data-driven policing strategies claim that it makes policing organizations more effective, efficient, and accountable and has the potential to address some policing social criticisms (e.g. racial bias, lack of accountability and training). What remains less understood are the challenges when adopting data-driven policing as a response to these criticisms. We present results from a qualitative field study about the adoption of data-driven policing strategies in a Midwestern police department in the United States. We identify three key challenges police face with data-driven adoption efforts: data-driven frictions, precarious and inactionable insights, and police metis concerns. We demonstrate the issues that data-driven initiatives create for policing and the open questions police agents face. These findings contribute an empirical account of how policing agents attend to the strengths and limits of big data's knowledge claims. Lastly, we present data and design implications for policing.",2018,,10.1145/3173574.3174043
2101,"Huang, Qibao and Huang, Yiqi",The Significance of Urban Cockpit for Urban Brain Construction,"Urban cockpit, Urban brain, Data","The urban cockpit will comprehensively perceive and process all kinds of data in the city operation, establish the data chassis of the smart city, objectively, comprehensively and multi dimensionally display the operation situation of the city, and carry out early warning, prediction and scientific disposal of outstanding problems and emergencies in the city operation. Moreover, in the near future, with the introduction and use of 5G, artificial intelligence, big data, data Luan Sheng and edge computing in the city brain project, the city cockpit will also give the city managers and visitors a better and more beautiful feeling in the display effect (such as immersion and three-dimensional), thus accelerating the promotion and landing of the city brain project and promoting social governance Intelligent and professional, improve the level of comprehensive city governance, and change the transformation and upgrading of the city from extensive to precise and refined.",2020,,10.1145/3414752.3414800
2102,"Siemens, George",Learning Analytics: Envisioning a Research Discipline and a Domain of Practice,"collaboration, practice, theory, learning analytics, research, data integration, ethics","Learning analytics are rapidly being implemented in different educational settings, often without the guidance of a research base. Vendors incorporate analytics practices, models, and algorithms from datamining, business intelligence, and the emerging ""big data"" fields. Researchers, in contrast, have built up a substantial base of techniques for analyzing discourse, social networks, sentiments, predictive models, and in semantic content (i.e., ""intelligent"" curriculum). In spite of the currently limited knowledge exchange and dialogue between researchers, vendors, and practitioners, existing learning analytics implementations indicate significant potential for generating novel insight into learning and vital educational practices. This paper presents an integrated and holistic vision for advancing learning analytics as a research discipline and a domain of practices. Potential areas of collaboration and overlap are presented with the intent of increasing the impact of analytics on teaching, learning, and the education system.",2012,,10.1145/2330601.2330605
2103,"Martins, Denis Mayr Lima and Vossen, Gottfried and de Lima Neto, Fernando Buarque",Intelligent Decision Support for Data Purchase,"computational intelligence, personalization, decision support, data purchase","The Big Data era is affording a paradigm change on decision-making approaches. More and more, companies as well as individuals are relying on data rather than on the so called ""gut feeling"" to make decisions. However, searching the Web for carrying out purchases is not completely satisfactory yet, given the arduousness of finding suitable quality data. This has contributed to the emergence of data marketplaces as an alternative to traditional data commerce, as they provide appropriate online environments for data offering and purchasing. Nevertheless, as the number of available datasets to purchase increases, the task of buying appropriate offers is, very often, challenging. In this sense, we propose an intelligent decision support system to help buyers in purchasing data offers based on a multiple-criteria decision analysis. Experimental results show that our approach provides an interactive way that addresses buyers' needs, allowing them to state and easily refine their preferences, without any specific order, via a series of dataset recommendations.",2017,,10.1145/3106426.3106434
2104,"Santana, Eduardo Felipe Zambom and Chaves, Ana Paula and Gerosa, Marco Aurelio and Kon, Fabio and Milojicic, Dejan S.","Software Platforms for Smart Cities: Concepts, Requirements, Challenges, and a Unified Reference Architecture","Wireless sensor networks, software platforms","Information and communication technologies (ICT) can be instrumental in progressing towards smarter city environments, which improve city services, sustainability, and citizens’ quality of life. Smart City software platforms can support the development and integration of Smart City applications. However, the ICT community must overcome current technological and scientific challenges before these platforms can be widely adopted. This article surveys the state of the art in software platforms for Smart Cities. We analyzed 23 projects concerning the most used enabling technologies, as well as functional and non-functional requirements, classifying them into four categories: Cyber-Physical Systems, Internet of Things, Big Data, and Cloud Computing. Based on these results, we derived a reference architecture to guide the development of next-generation software platforms for Smart Cities. Finally, we enumerated the most frequently cited open research challenges and discussed future opportunities. This survey provides important references to help application developers, city managers, system operators, end-users, and Smart City researchers make project, investment, and research decisions.",2017,,10.1145/3124391
2105,"Zhu, Qiang and Guo, Songtao and Ogilvie, Paul and Liu, Yan",Business Applications of Predictive Modeling at Scale,"business analytics, machine learning, machine learning platforms, predictive modeling","Predictive modeling is the art of building statistical models that forecast probabilities and trends of future events. It has broad applications in industry across different domains. Some popular examples include user intention predictions, lead scoring, churn analysis, etc. In this tutorial, we will focus on the best practice of predictive modeling in the big data era and its applications in industry, with motivating examples across a range of business tasks and relevance products. We will start with an overview of how predictive modeling helps power and drive various key business use cases. We will introduce the essential concepts and state of the art in building end-to-end predictive modeling solutions, and discuss the challenges, key technologies, and lessons learned from our practice, including case studies of LinkedIn feed relevance and a platform for email response prediction. Moreover, we will discuss some practical solutions of building predictive modeling platform to scale the modeling efforts for data scientists and analysts, along with an overview of popular tools and platforms used across the industry.",2016,,10.1145/2939672.2945388
2106,"Yuwei, Sun and Jianbao, Zhu and Qingshan, Ma and Xinchun, Yu and Ye, Shi and Yu, Chen",Picture Management of Power Supply Safety Management System Based on Deep Learning Technology,"deep learning, convolutional neural network, Picture management","With the advent of the era of big data, power supply security management systems will get a lot of picture data. In the face of massive image data, this paper studies the image management technology based on convolutional neural network. Aiming at the high repetition rate of self built image database samples and the problem that many sample classes contain uncorrelated images, two algorithms are proposed to improve the quality of the database: de duplication and de uncorrelation. By using the depth convolution neural network, the Embedding represented by the corresponding image is taken, and the distance between Embedding is calculated in the Euclidean space to achieve the purpose of de duplication and de uncorrelation. In this paper, ""time"" and ""accuracy"" are used to evaluate the performance of de duplication and de uncorrelation algorithms. The comparison examples of some sample classes before and after removing repetition and before and after removing uncorrelation are shown. The Recall-value of the database after removing duplicate and uncorrelated is tested based on the GoogLe Netplus-model respectively, which proves the effectiveness of the two filtering algorithms and overcomes the complexity of the traditional filtering process.",2019,,10.1145/3377458.3377464
2107,"Fekete, Alan and Kay, Judy and R\""{o}hm, Uwe",A Data-Centric Computing Curriculum for a Data Science Major,"data science, curriculum","Many universities are introducing a new major in Data Science into their offering, to reflect the explosive growth in this field and the career opportunities it provides. As a field Data Science has elements from Computer Science and from Statistics, and curricula plans differ widely, both in the balance between the CS and Stats aspects, and also in the emphasis within the computing topics. This paper reports on the curriculum that has been taught for three years now at the University of Sydney. In particular, we describe the approach of a sequence of computing subjects which were developed specifically for the major, in order to bring students over several years to a sophisticated understanding of the data-handling aspects of Data Science. Students also take traditional subjects from both CS (such as Data Structures or AI) and from Statistics (such as Learning from Data and Statistical Inference). The data-centric specially-designed subjects we discuss in this paper are (i) Informatics: Data and Computation (in the first year), (ii) Big Data and Data Diversity (in the second year), and then upper-division subjects on (iii) Data Science Platforms, and (iv) Human-in-the-Loop Data Analytics.",2021,,10.1145/3408877.3432457
2108,"Neumann, Alexander and Schnier, Christian and Hermann, Thomas and Pitsch, Karola",Interaction Analysis and Joint Attention Tracking in Augmented Reality,"conversation analysis, data mining, interaction studies, multimodality","Multimodal research in human interaction has to consider a variety of factors, ranging from local short-time phenomena to complex interaction patterns. As of today, no single discipline engaged in communication research offers the methods and tools to investigate the full complexity continuum in a time-efficient way. A synthesis of qualitative and quantitative analysis is required to merge insights about micro-sequential structures with big data patterns. Using the example of a co-present dyadic negotiation analysis to combine methods offered by Conversation Analysis and Data Mining, we show how such a partnership can benefit each discipline and lead to insights as well as new hypotheses evaluation opportunities.",2013,,10.1145/2522848.2522892
2109,"Omitola, Tope and Davies, John and Duke, Alistair and Glaser, Hugh and Shadbolt, Nigel","Linking Social, Open, and Enterprise Data","Navigation, Architectures, Semantic networks, Hypertext/Hypermedia, User issues","The new world of big data, of the LOD cloud, of the app economy, and of social media means that organisations no longer own, much less control, all the data they need to make the best informed business decisions. In this paper, we describe how we built a system using Linked Data principles to bring in data from Web 2.0 sites (LinkedIn, Salesforce), and other external business sites such as OpenCorporates, linking these together with pertinent internal British Telecommunications enterprise data into that enterprise data space. We describe the challenges faced during the implementation, which include sourcing the datasets, finding the appropriate ""join points"" from the individual datasets, as well as developing the client application used for data publication. We describe our solutions to these challenges and discuss the design decisions made. We conclude by drawing some general principles from this work.",2014,,10.1145/2611040.2611086
2110,"Alsubaiee, Sattam and Behm, Alexander and Grover, Raman and Vernica, Rares and Borkar, Vinayak and Carey, Michael J. and Li, Chen",ASTERIX: Scalable Warehouse-Style Web Data Integration,"ASTERIX, hyracks, semistructured data, data-intensive computing, cloud computing","A growing wealth of digital information is being generated on a daily basis in social networks, blogs, online communities, etc. Organizations and researchers in a wide variety of domains recognize that there is tremendous value and insight to be gained by warehousing this emerging data and making it available for querying, analysis, and other purposes. This new breed of ""Big Data"" applications poses challenging requirements against data management platforms in terms of scalability, flexibility, manageability, and analysis capabilities. At UC Irvine, we are building a next-generation database system, called ASTERIX, in response to these trends. We present ongoing work that approaches the following questions: How does data get into the system? What primitives should we provide to better cope with dirty/noisy data? How can we support efficient data analysis on spatial data? Using real examples, we show the capabilities of ASTERIX for ingesting data via feeds, supporting set-similarity predicates for fuzzy matching, and answering spatial aggregation queries.",2012,,10.1145/2331801.2331803
2111,"Deeb-Swihart, Julia and Endert, Alex and Bruckman, Amy",Understanding Law Enforcement Strategies and Needs for Combating Human Trafficking,"law enforcement, human trafficking, qualitative, needs analysis","In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.",2019,,10.1145/3290605.3300561
2112,"Caruccio, Loredana and Cirillo, Stefano and Deufemia, Vincenzo and Polese, Giuseppe",Efficient Discovery of Functional Dependencies from Incremental Databases,," With the advent of Big Data there is an increasing necessity to incrementally mine information from data originating from sensors and other dynamic sources. Thus, it is necessary to devise algorithms capable of mining useful information upon possible evolutions of databases. Among these, there are certainly data profiling info, such as functional dependencies (fd for short), which are particularly useful for data integration and for assessing the quality of data. The incremental scenario requires the definition of search strategies and validation methods able to analyze only the portion of the dataset affected by the last changes. In this paper, we propose a new validation method, which exploits regular expressions and compressed data structures to efficiently verify whether a candidate fd holds on an updated version of the dataset. Experimental results demonstrate the effectiveness of the proposed method on real-world datasets adapted for incremental scenarios, also compared with a baseline incremental fd discovery algorithm.",2021,,
2113,"Castro Fernandez, Raul and Deng, Dong and Mansour, Essam and Qahtan, Abdulhakim A. and Tao, Wenbo and Abedjan, Ziawasch and Elmagarmid, Ahmed and Ilyas, Ihab F. and Madden, Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan",A Demo of the Data Civilizer System,"data cleaning, data integration, polystore queries, join path discovery, data discovery, data stitching","Finding relevant data for a specific task from the numerous data sources available in any organization is a daunting task. This is not only because of the number of possible data sources where the data of interest resides, but also due to the data being scattered all over the enterprise and being typically dirty and inconsistent. In practice, data scientists are routinely reporting that the majority (more than 80%) of their effort is spent finding, cleaning, integrating, and accessing data of interest to a task at hand. We propose to demonstrate DATA CIVILIZER to ease the pain faced in analyzing data ""in the wild"". DATA CIVILIZER is an end-to-end big data management system with components for data discovery, data integration and stitching, data cleaning, and querying data from a large variety of storage engines, running in large enterprises.",2017,,10.1145/3035918.3058740
2114,"Zhao, Kai and Feng, Jie and Xu, Zhao and Xia, Tong and Chen, Lin and Sun, Funing and Guo, Diansheng and Jin, Depeng and Li, Yong",DeepMM: Deep Learning Based Map Matching with Data Augmentation,"map matching, deep learning, data driven system","Map matching is important in many trajectory based applications like route optimization and traffic schedule, etc. As the widely used methods, Hidden Markov Model and its variants are well studied to provide accurate and efficient map matching service. However, HMM based methods fail to utilize the value of enormous trajectory big data, which are useful for the map matching task. Furthermore, with many following-up works, they are still easily influenced by the noisy records, which are very common in the real system. To solve these problems, we revisit the map matching task from the data perspective, and propose to utilize the great power of data to help solve these problems. We build a deep learning based model to utilize all the trajectory data for joint training and knowledge sharing. With the help of embedding techniques and sequence learning model with attention enhancement, our system does the map matching in the latent space, which is tolerant to the noise in the physical space. Extensive experiments demonstrate that our model outperforms the widely used HMM based methods more than 10% (absolute accuracy) and works robustly in the noisy settings in the meantime.",2019,,10.1145/3347146.3359090
2115,"Chen, Juan and Lu, Yan and Zhang, Ting and Ouyang, Zhaolian","Artificial Intelligence in Medicine in the United States, China and India",,"Objective: To compare the development status of artificial intelligence (AI) in medicine among the United States (US), China and India with bibliometric analysis. Methods: Articles involving AI in medicine published from 2015 to 2019 were retrieved on March 30, 2020 from Web of Science Core Collection. The country-level and the institution-level performance of the US, China and India in the field of AI in medicine were compared with indicators including the amount of papers, 5-year Compound Annual Growth Rate (CAGR) of the amount of papers, the amount of highly-cited papers, the proportion of highly-cited papers and the average citations per paper. In addition, the research hotspots and international cooperation of the three countries in recent 5 years were compared by conducting keywords co-occurrence analysis and co-authorship analysis in VOSviewer. Results: From 2015 to 2019, The US has published 7838 papers and 154 highly-cited papers in the field of AI in medicine, with an average citations per paper to be 9.3, and the proportion of highly-cited papers to be 2.0 %. China has output 6635 papers and 73 highly-cited papers in this field, with an average citations per paper to be 5.3, and the proportion of highly-cited papers to be 1.1%. India has output 3895 papers and 22 highly-cited papers in this field, with an average citations per paper to be 3.6, and the proportion of highly-cited papers to be 0.6%. The 5-year CAGR of the US, China and India in the period of 2015~2019 were 16.0%, 25.4% and 2.4%, respectively. At the institutional level, most of these indicators were significantly better for the US institutions than for Chinese and Indian ones. There were four research hotspots in this field, namely medical imaging technology, health big data mining, disease prediction with biomarkers and genetic information, and early diagnosis of neurological disease. The three countries focused on different hotspots, with China focusing relatively less on health big data mining, while the US and India being complementary to each other. As to international cooperation, the average links per paper to other countries were 0.60, 0.40 and 0.20, respectively, for the US, China and India. Conclusions: In the field of AI in medicine, the US, with a number of competitive institutions in AI and medical researches, is taking a definitely leading role, having conducted many innovative researches and cooperated extensively with other countries. China is taking the second leading role at the country level, with top institutions somewhat less productive than those in the US. India is the third productive country, with top institutions obvious less productive than those in the US, and with research hotspots exactly complementary to the US.",2020,,
2116,"H G, Monika Rani and R, Sapna and Mishra, Shakti",An Investigative Study on the Quality Aspects of Linked Open Data,"Linked open data, Semantic Web, Quality of linked open data","Linked Open Data refers to a set of best practices that empowers enterprises to publish and interlink their data using existing ontologies on the Semantic Web. The focus of linked open data is to move from document-based Web to a Web of interlinked data, created by typed links between data from different data sources. Linked open data expert group has taken cognizance of data quality importance, as the amount of linked data publications grown on the Web substantially. Measures have been taken to check the linked data quality. But, these measures are diverse in nature with respect to quality terms. This makes the comparison and evaluation difficult, leading to an incorrect selection of accurate data sources based on quality requirements. In this paper, we carried out an analysis on linked data, the quality of linked data, the frameworks to assess the quality of linked data and the challenges to achieve the quality of linked open data.",2018,,10.1145/3291064.3291074
2117,"Chowdhury, Tahiya and Ding, Qizhen and Mandel, Ilan and Ju, Wendy and Ortiz, Jorge",Tracking Urban Heartbeat and Policy Compliance through Vision and Language-Based Sensing,"computer vision and language, COVID-19, urban sensing","Sensing activities at the city scale using big data can enable applications to improve the quality of citizen life. While there are approaches to sense the urban heartbeat using sound, vision, radio frequency (RF), and other sensors, capturing changes at urban scale using such sensing modalities is challenging. Due to the enormous amount of data they produce and the associated annotation and processing requirement, such data can be of limited use. In this paper, we present a vision-to-language modeling approach to capture patterns and transitions that occur in New York City from March 2020 to August 2020. We use the model on ~1 million street images captured by dashcams over 6 months. We then use the captions to train a language model based on Latent Dirichlet Allocation [4] and compare models from different periods using probabilistic distance measures. We observe distribution shifts in the model that correlate well with social distancing policies and are corroborated by different data sources, such as mobility traces. This language-based sensing introduces a new sensing modality to capture dynamics in the city with lower storage requirements and privacy concerns.",2021,,10.1145/3486611.3491133
2118,"Strey, Mateus Rambo and Pereira, Roberto and de Castro Salgado, Luciana C.",Human Data-Interaction: A Systematic Mapping,"Human-Computer Interaction, Systematic Mapping Review, Human-Data Interaction","Big Data, e-Science and Internet of Things have contributed to increase the production, processing and storage of data, changing the way people deal and live with data. Although the problem is not new, the ""human aspect"" of data and the possible impact of Human-Data Interaction (HDI) in human life have been explored and discussed as an emerging research area. On the one hand, HDI offers plenty of opportunities for research and development, and on the other hand it demands characterization, grounding, critical discussions, empirical results and thinking tools to support research and practice. This paper presents a Systematic Mapping of Literature on HDI in Computer Science, identifying the different definitions for the area, elements or objects of investigation, contexts of application, stakeholders, etc. Based on 28 selected papers, results point out to a lack of definition or agreement on what HDI is, but suggest that there are different aspects that can characterize it, and allow identifying concerns and objects of study, such as privacy, ownership and transparency. Results suggest a demand for theoretical and methodological frameworks to support the understanding, design and evaluation of HDI via computing systems.",2018,,10.1145/3274192.3274219
2119,"Efros, Pavel and Buchmann, Erik and Englhardt, Adrian and B\""{o}hm, Klemens",How to Quantify the Impact of Lossy Transformations on Change Detection,,"To ease the proliferation of big data, it frequently is transformed, be it by compression, be it by anonymization. Such transformations however modify characteristics of the data, such as changes in the case of time series. Changes however are important for subsequent analyses. The impact of those modifications depends on the application scenario, and quantifying it is far from trivial. This is because a transformation can shift or modify existing changes or introduce new ones. In this paper, we propose MILTON, a flexible and robust Measure for quantifying the Impact of Lossy Transformations on subsequent change detectiON. MILTON is applicable to any lossy transformation technique on time-series data and to any general-purpose change-detection approach. We have evaluated it with three real-world use cases. Our evaluation shows that MILTON allows to quantify the impact of lossy transformations and to choose the best one from a class of transformation techniques for a given application scenario.",2015,,10.1145/2791347.2791371
2120,"Horne, Benjamin D. and Nevo, Dorit and Adal\i{}, Sibel",Recognizing Experts on Social Media: A Heuristics-Based Approach,"social media, expertise location, data analytics.","Knowing who is an expert on social media is a challenging yet important task, especially in a world where misleading information is commonplace and where social media is an important information source for knowledge seekers. In this paper we investigate expertise heuristics by comparing features of experts versus non-experts in big data settings. We employ a large set of features to classify experts and non-experts using data collected on two social media platform (Twitter and reddit). Our results show a good ability to predict who is an expert, especially using language-based features, validating that heuristics can be developed to differentiate experts from novices organically, based on social media use. Our results contribute to the development of expertise location and identification systems as well as our understanding on how experts present themselves on social media.",2019,,10.1145/3353401.3353406
2121,"Dasu, Tamraparni and Shkapenyuk, Vladislav and Srivastava, Divesh and Swayne, Deborah F.",FIT to Monitor Feed Quality,,"While there has been significant focus on collecting and managing data feeds, it is only now that attention is turning to their quality. In this paper, we propose a principled approach to online data quality monitoring in a dynamic feed environment. Our goal is to alert quickly when feed behavior deviates from expectations.We make contributions in two distinct directions. First, we propose novel enhancements to permit a publish-subscribe approach to incorporate data quality modules into the DFMS architecture. Second, we propose novel temporal extensions to standard statistical techniques to adapt them to online feed monitoring for outlier detection and alert generation at multiple scales along three dimensions: aggregation at multiple time intervals to detect at varying levels of sensitivity; multiple lengths of data history for varying the speed at which models adapt to change; and multiple levels of monitoring delay to address lagged data arrival.FIT, or Feed Inspection Tool, is the result of a successful implementation of our approach. We present several case studies outlining the effective deployment of FIT in real applications along with user testimonials.",2015,,10.14778/2824032.2824070
2122,"Mehta, Paras and Voisard, Agn\`{e}s",Analysis of User Mobility Data Sources for Multi-User Context Modeling,"multi-user, context, model, dataset, situation, mobility","Finding the right data source for research is a challenge that many of us face. Although we live in times where 'Open Data' and 'Big Data' have become buzzwords, getting hold of a reasonable size and quality dataset is often hard. When it comes to user data such as mobility data, this becomes even tougher due to privacy-related concerns. This paper briefly explains our research in the area of multi-user context modeling and presents some criteria that we believe are important while selecting a dataset for testing different approaches in this domain. To find the right dataset, some relevant publicly available human mobility datasets are examined using these criteria. The following are the datasets that have been analyzed: Microsoft Research GeoLife Trajectory Dataset, Tracking Delft I Pedestrian Trajectory Dataset, MIT Media Lab Reality Mining Dataset and LifeMap Dataset. Besides these, some other useful data sources for researchers have been cited.",2012,,10.1145/2442952.2442955
2123,"McKenzie, Grant and Janowicz, Krzysztof and Adams, Benjamin",Weighted Multi-Attribute Matching of User-Generated Points of Interest,"point of interest, volunteered geographic information, similarity, location-based services, conflation, POI","To a large degree, the attraction of Big Data lies in the variety of its heterogeneous multi-thematic and multi-dimensional data sources and not merely its volume. To fully exploit this variety, however, requires conflation. This is a two step process. First, one has to establish identity relations between information entities across the different data sources; and second, attribute values have to be merged according to certain procedures which avoid logical contradictions. The first step, also called matching, can be thought of as a weighted combination of common attributes according to some similarity measures. In this work, we propose such a matching based on multiple attributes of Points of Interests (POI) from the Location-based Social Network Foursquare and the Yelp local directory service. While both contain overlapping attributes that can be use for matching, they have specific strengths and weaknesses which makes their conflation desirable. We present a weighted multi-attribute matching strategy and evaluate its performance. Our strategy can automatically match 97% of randomly selected Yelp POI to their corresponding Foursquare entities.",2013,,10.1145/2525314.2525455
2124,"Yang, Longqi and Zhang, Liangliang and Tang, Yuhua",Scalable Auto-Weighted Discrete Multi-View Clustering,"parameter selection, binary coding, graph regularization, multi-view clustering"," Multi-view clustering has been widely studied in machine learning, which uses complementary information to improve clustering performance. However, challenges remain when handling large-scale multi-view data due to the traditional approaches’ high time complexity. Besides, the existing approaches suffer from parameter selection. Due to the lack of labeled data, parameter selection in practical clustering applications is difficult, especially in big data. In this paper, we propose a novel approach for large-scale multi-view clustering to overcome the above challenges. Our approach focuses on learning the low-dimensional binary embedding of multi-view data, preserving the samples’ local structure during binary embedding, and optimizing the embedding and clustering in a unified framework. Furthermore, we proposed to learn the parameters using a combination of data-driven and heuristic approaches. Experiments on five large-scale multi-view datasets show that the proposed method is superior to the state-of-the-art in terms of clustering quality and running time.",2021,,10.1145/3442381.3449956
2125,"Aalst, Wil Van Der and Zhao, J. Leon and Wang, Harry Jiannan",Editorial: “Business Process Intelligence: Connecting Data and Processes”,"business process intelligence, performance analysis, Process mining, compliance checking, process modeling","This introduction to the special issue on Business Process Intelligence (BPI) discusses the relation between data and processes. The recent attention for Big Data illustrates that organizations are aware of the potential of the torrents of data generated by today's information systems. However, at the same time, organizations are struggling to extract value from this overload of data. Clearly, there is a need for data scientists able to transform event data into actionable information. To do this, it is crucial to take a process perspective. The ultimate goal of BPI is not to improve information systems or the recording of data; instead the focus should be in improving the process. For example, we may want to aim at reducing costs, minimizing response times, and ensuring compliance. This requires a “confrontation” between process models and event data. Recent advances in process mining allow us to automatically learn process models showing the bottlenecks from “raw” event data. Moreover, given a normative model, we can use conformance checking to quantify and understand deviations. Automatically learned models may also be used for prediction and recommendation. BPI is rapidly developing as a field linking data science to business process management. This article aims to provide an overview thereby paving the way for the other contributions in this special issue.",2015,,10.1145/2685352
2126,"Wang, Chang and Zhu, Yongxin and Shi, Weiwei and Chang, Victor and Vijayakumar, P. and Liu, Bin and Mao, Yishu and Wang, Jiabao and Fan, Yiping",A Dependable Time Series Analytic Framework for Cyber-Physical Systems of IoT-Based Smart Grid,"dependable time series analytics, IoT-based smart grid, sensor-network-regularization-based matrix factorization, cyber-physical-systems","With the emergence of cyber-physical systems (CPS), we are now at the brink of next computing revolution. The Smart Grid (SG) built on top of IoT (Internet of Things) is one of the foundations of this CPS revolution, which involves a large number of smart objects connected by networks. The volume of time series of SG equipment is tremendous and the raw time series are very likely to contain missing values because of undependable network transferring. The problem of storing a tremendous volume of raw time series thereby providing a solid support for precise time series analytics now becomes tricky. In this article, we propose a dependable time series analytics (DTSA) framework for IoT-based SG. Our proposed DTSA framework is capable of providing a dependable data transforming from CPS to the target database with an extraction engine to preliminary refining raw data and further cleansing the data with a correction engine built on top of a sensor-network-regularization-based matrix factorization method. The experimental results reveal that our proposed DTSA framework is capable of effectively increasing the dependability of raw time series transforming between CPS and the target database system through the online lightweight extraction engine and the offline correction engine. Our proposed DTSA framework would be useful for other industrial big data practices.",2018,,10.1145/3145623
2127,"Sang, Go Muan and Xu, Lai and de Vrieze, Paul and Bai, Yuewei and Pan, Fangyu",Predictive Maintenance in Industry 4.0,"Blockchain, Collaborative business process, Industry 4.0, Industrial data space, Predictive maintenance, FIWARE","In the context of Industry 4.0, the manufacturing related processes have shifted from conventional processes within one organization to collaborative processes cross different organizations, for example, product design processes, manufacturing processes, and maintenance processes across different factories and enterprises. The application of Internet of things, i.e. smart devices and sensors increases collection and availability of diverse data. Advanced technologies such as big data analytics and cloud computing offer new opportunities for effective optimization of manufacturing related processes, e.g. predictive maintenance. Predictive maintenance provides a detailed examination of the detection, location and diagnosis of faults in related machineries using various analyses. RAMI4.0 is a framework for thinking about the various efforts that constitute Industry 4.0. It spans the entire product life cycle &amp; value stream axis, hierarchical structure axis and functional classification axis. The Industrial Data Space (now International Data Space) is a virtual data space using standards and common governance models to facilitate the secure exchange and easy linkage of data in business ecosystems. It thereby provides a basis for creating and using smart services and innovative business processes, while at the same time ensuring digital sovereignty of data owners. This paper looks at how to support predictive maintenance in the context of Industry 4.0? Especially, applying RAMI 4.0 architecture supports the predictive maintenance using FIWARE framework, which leads to deal with data exchanging among different organizations with different security requirements as well as modularizing of related functions.",2020,,10.1145/3447568.3448537
2128,"Kumar, Santosh","Sensitivity, Specificity, Generalizability, and Reusability Aspirations for Machine Learning (ML) Models in MHealth","Mobile Health (mHealth), Machine Learning Models","Mobile sensor big data collected from smartphones, smartwatches, fitness trackers, and other wearables can be mined for signatures (called mHealth biomarkers) of subtle changes in daily behaviors (e.g., mobility, gait, sleep, etc.) and/or physiology (e.g., heart function, breathing, sweating, etc.). Clinical adoption of these mHealth biomarkers can lead to potent temporally-precise interventions, enabling patients to initiate and sustain the healthy lifestyle choices and treatment regimes that are necessary to prevent and/or successfully manage the growing burden of multiple chronic conditions.However, for any new biomarker to be successfully used for clinical diagnosis or treatment, its clinical utility must be established. mHealth biomarkers are usually derived by training a machine learning (ML) algorithm on mobile sensor data. The published models differ in feature construction (domain-derived features fed to a supervised ML model vs. data-driven features discovered by a deep learning (DL) model), data collection setting (lab vs. field), data selection and preparation (covering all twenty-four hours of the day vs. awake hours, vs. only when performing certain tasks), data labeling (retrospective self-reported aggregate labels vs. time-synchronized labels from first-person video), data size and diversity (e.g., number of participants, gender, ethnicity, age group, number of days, hours per day, etc.), experiment design (cross-validation vs. cross-subject validation), and performance (e.g., accuracy, F1 score, confusion matrix, AUC, etc.). As a result, there is wide diversity in published models on their potential for reusability, generalizability, and eventual clinical utility.This talk will describe an aspirational framework for specificity, sensitivity, generalizability, and reusability of mHealth biomarkers with some concrete performance targets so that they have a higher chance of widespread clinical utility. It will draw upon the presenter's decade-long transdisciplinary research experience in developing machine learning models to detect a wide variety of daily behaviors such as stress, speaking, smoking, and brushing from wearable physiological and inertial sensors.The talk will use the analogy of five nines (i.e., 99.999%) paradigm in the area of service-level agreements to quantify high-availability. Similar to how these managed services are expected to be available 24-7-365, with the downtime limited to 5.26 minutes per year, we can express the performance requirements of mHealth biomarkers that are expected to detect subtle signs of health and behaviour deterioration anytime and anywhere. Five nines guarantee for the detection of a health event (e.g., fall, stress) translates to one false positive every 100 person-days, if the model runs on 1,000 minutes of sensor data collected each day. Achieving five nines to claim the detection of non-event (e.g., smoking abstinence) is even more challenging, as there are several other failure scenarios for missing an event, in addition to model failure, such as the non-wearing of sensors when performing the event of interest, poor data quality, mismatch of model to where (on the body) and how the sensor is worn (e.g., smartwatch on non-dominant hand), battery failure, and data loss. To achieve generalizability, the model performance must be achieved on independent test data that covers all aspects of daily life, without any data selection. Finally, for the model to be used by others for real-life, the model should not only be accessible to the community, it should also be possible to train and test the model on different datasets by independent non-ML-expert researchers.",2020,,10.1145/3396868.3402495
2129,"Xiao, Houping and Wang, Shiyu",Toward Quality of Information Aware Distributed Machine Learning,"quality of information, distributed machine learning","In the era of big data, data are usually distributed across numerous connected computing and storage units (i.e., nodes or workers). Under such an environment, many machine learning problems can be reformulated as a consensus optimization problem, which consists of one objective and constraint terms splitting into N parts (each corresponds to a node). Such a problem can be solved efficiently in a distributed manner via Alternating Direction Method of Multipliers (ADMM). However, existing consensus optimization frameworks assume that every has the same quality of information (QoI), i.e., the data from all the nodes are equally informative for the estimation of global model parameters. As a consequence, they may lead to inaccurate estimates in the presence of nodes with low QoI. To overcome this challenge, in this paper, we propose a novel consensus optimization framework for distributed machine learning that incorporates the crucial metric, quality of information. Theoretically, we prove that the convergence rate of the proposed framework is linear to the number of iterations but has a tighter upper bound compared with ADMM. Experimentally, we show that the proposed framework is more efficient and effective than existing ADMM based solutions on both synthetic and real-world datasets due to its faster convergence rate and higher accuracy.",2022,,10.1145/3522591
2130,"Duan, Xuliang and Guo, Bing and Shen, Yan and Shen, Yuncheng and Dong, Xiangqian and Zhang, Hong",Research on Parallel Data Currency Rule Algorithms,"data currency rule, Data currency, parallel algorithm, dynamic data","Data currency is a temporal reference of data, which is related to the value of data and affects the results of data analysis and mining. The currency rules that reflect the time series features of data can be used not only for data repairing, but also for data quality evaluation. However, with the rapid growth and dynamic update of data volume, both the forms and algorithms of basic currency rule are facing severe challenges in application. Therefore, based on the research on data currency repairing, we extended the basic currency rule form, and proposed rule extraction and incremental updating algorithms that can run in parallel on dynamic data set. The experimental results show that, compared with non-parallel methods, the efficiency of parallel algorithms is significantly improved.",2020,,10.1145/3388176.3388210
2131,"Zhang, Ce and Shin, Jaeho and R\'{e}, Christopher and Cafarella, Michael and Niu, Feng",Extracting Databases from Dark Data with DeepDive,"dark data, information extraction, data integration, knowledge base construction","DeepDive is a system for extracting relational databases from dark data: the mass of text, tables, and images that are widely collected and stored but which cannot be exploited by standard relational tools. If the information in dark data --- scientific papers, Web classified ads, customer service notes, and so on --- were instead in a relational database, it would give analysts access to a massive and highly-valuable new set of ""big data"" to exploit.DeepDive is distinctive when compared to previous information extraction systems in its ability to obtain very high precision and recall at reasonable engineering cost; in a number of applications, we have used DeepDive to create databases with accuracy that meets that of human annotators. To date we have successfully deployed DeepDive to create data-centric applications for insurance, materials science, genomics, paleontologists, law enforcement, and others. The data unlocked by DeepDive represents a massive opportunity for industry, government, and scientific researchers.DeepDive is enabled by an unusual design that combines large-scale probabilistic inference with a novel developer interaction cycle. This design is enabled by several core innovations around probabilistic training and inference.",2016,,10.1145/2882903.2904442
2132,"Skorin-Kapov, Lea and Varela, Mart\'{\i}n and Ho\ss{}feld, Tobias and Chen, Kuan-Ta",A Survey of Emerging Concepts and Challenges for QoE Management of Multimedia Services,"NFV, monitoring probes, encrypted traffic, crowdsourcing, data analytics, SDN, QoE monitoring, QoE modeling, QoE management","Quality of Experience (QoE) has received much attention over the past years and has become a prominent issue for delivering services and applications. A significant amount of research has been devoted to understanding, measuring, and modelling QoE for a variety of media services. The next logical step is to actively exploit that accumulated knowledge to improve and manage the quality of multimedia services, while at the same time ensuring efficient and cost-effective network operations. Moreover, with many different players involved in the end-to-end service delivery chain, identifying the root causes of QoE impairments and finding effective solutions for meeting the end users’ requirements and expectations in terms of service quality is a challenging and complex problem. In this article, we survey state-of-the-art findings and present emerging concepts and challenges related to managing QoE for networked multimedia services. Going beyond a number of previously published survey articles addressing the topic of QoE management, we address QoE management in the context of ongoing developments, such as the move to softwarized networks, the exploitation of big data analytics and machine learning, and the steady rise of new and immersive services (e.g., augmented and virtual reality). We address the implications of such paradigm shifts in terms of new approaches in QoE modeling and the need for novel QoE monitoring and management infrastructures.",2018,,10.1145/3176648
2133,"Mendes, Yan and Braga, Regina and Str\""{o}ele, Victor and de Oliveira, Daniel",Polyflow: A SOA for Analyzing Workflow Heterogeneous Provenance Data in Distributed Environments,"polystore, heterogeneous provenance data integration, Workflows interoperability","In the last decade the (big) data-driven science paradigm became a wide-spread reality. However, this approach has some limitations such as a performance dependency on the quality of the data and the lack of reproducibility of the results. In order to enable this reproducibility, many tools such as Workflow Management Systems were developed to formalize process pipelines and capture execution traces. However, interoperating data generated by these solutions became a problem, since most systems adopted proprietary data models. To support interoperability across heterogeneous provenance data, we propose a Service Oriented Architecture with a polystore storage design in which provenance is conceptually represented utilizing the ProvONE model. A wrapper layer is responsible for transforming data described by heterogeneous formats into ProvONE-compliant. Moreover, we propose a query layer that provides location and access transparency to users. Furthermore, we conduct two feasibility studies, showcasing real usecase scenarios. Firstly, we illustrate how two research groups can compare their processes and results. Secondly, we show how our architecture can be used as a queriable provenance repository. We show Polyflow's viability for both scenarios using the Goal-Question-Metric methodology. Finally, we show our solution usability and extensibility appeal by comparing it to similar approaches.",2019,,10.1145/3330204.3330259
2134,"Lv, Zhining and Hu, Ziheng and Ning, Baifeng and Li, Wei and Yan, Gangfeng and Ding, Lifu and Shi, Xiasheng and Guo, Ningxuan",Safety Monitoring of Power Industrial Control Terminals Based on Data Cleaning,"Chebyshev theory, Power monitoring, Data cleaning, Proximity data averaging","Stable and high-quality electric energy is the main driving force for the development of social science, technology, and the national economic leap. The assessment and monitoring of electrical safety rely on the generation, collection and statistics of large amounts of data by the power system. For the possible problems and impurities in these data, this paper uses the 'local Chebyshev theorem' and the 'near data averaging method' for the attribute values. The error is cleaned, and the 'sorting neighbor algorithm' is used to clean the duplicate data, thereby improving the data quality and realizing the accuracy of the safety monitoring of the power grid of the smart grid.",2019,,10.1145/3357777.3357781
2135,"Aiken, Peter",EXPERIENCE: Succeeding at Data Management—BigCo Attempts to Leverage Data,"data architecture, data stewardship, data integration, strategy, policy, CIO, Data management, data warehousing, business intelligence, chief data officer, IT management, BigCo, enterprise architecture, analytics, CDO, information systems, chief information officer, organizational design, conceptual modeling, data governance, enterprise data executive, data","In a manner similar to most organizations, BigCompany (BigCo) was determined to benefit strategically from its widely recognized and vast quantities of data. (U.S. government agencies make regular visits to BigCo to learn from its experiences in this area.) When faced with an explosion in data volume, increases in complexity, and a need to respond to changing conditions, BigCo struggled to respond using a traditional, information technology (IT) project-based approach to address these challenges. As BigCo was not data knowledgeable, it did not realize that traditional approaches could not work. Two full years into the initiative, BigCo was far from achieving its initial goals. How much more time, money, and effort would be required before results were achieved? Moreover, could the results be achieved in time to support a larger, critical, technology-driven challenge that also depended on solving the data challenges? While these questions remain unaddressed, these considerations increase our collective understanding of data assets as separate from IT projects. Only by reconceiving data as a strategic asset can organizations begin to address these new challenges. Transformation to a data-driven culture requires far more than technology, which remains just one of three required “stool legs” (people and process being the other two). Seven prerequisites to effectively leveraging data are necessary, but insufficient awareness exists in most organizations—hence, the widespread misfires in these areas, especially when attempting to implement the so-called big data initiatives. Refocusing on foundational data management practices is required for all organizations, regardless of their organizational or data strategies.",2016,,10.1145/2893482
2136,"Fan, Ju and Chen, Junyou and Liu, Tongyu and Shen, Yuwei and Li, Guoliang and Du, Xiaoyong",Relational Data Synthesis Using Generative Adversarial Networks: A Design Space Exploration,,"The proliferation of big data has brought an urgent demand for privacy-preserving data publishing. Traditional solutions to this demand have limitations on effectively balancing the tradeoff between privacy and utility of the released data. Thus, the database community and machine learning community have recently studied a new problem of relational data synthesis using generative adversarial networks (GAN) and proposed various algorithms. However, these algorithms are not compared under the same framework and thus it is hard for practitioners to understand GAN's benefits and limitations. To bridge the gaps, we conduct so far the most comprehensive experimental study that investigates applying GAN to relational data synthesis. We introduce a unified GAN-based framework and define a space of design solutions for each component in the framework, including neural network architectures and training strategies. We conduct extensive experiments to explore the design space and compare with traditional data synthesis approaches. Through extensive experiments, we find that GAN is very promising for relational data synthesis, and provide guidance for selecting appropriate design solutions. We also point out limitations of GAN and identify future research directions.",2020,,10.14778/3407790.3407802
2137,"Wang, Jingjing and Jiang, Wenjun and Li, Kenli and Wang, Guojun and Li, Keqin",Incremental Group-Level Popularity Prediction in Online Social Networks,"popularity prediction, information diffusion, incremental approach, tensor analysis, Group level, online social networks","Predicting the popularity of web contents in online social networks is essential for many applications. However, existing works are usually under non-incremental settings. In other words, they have to rebuild models from scratch when new data occurs, which are inefficient in big data environments. It leads to an urgent need for incremental prediction, which can update previous results with new data and conduct prediction incrementally. Moreover, the promising direction of group-level popularity prediction has not been well treated, which explores fine-grained information while keeping a low cost. To this end, we identify the problem of incremental group-level popularity prediction, and propose a novel model IGPP to address it. We first predict the group-level popularity incrementally by exploiting the incremental CANDECOMP/PARAFCAC (CP) tensor decomposition algorithm. Then, to reduce the cumulative error by incremental prediction, we propose three strategies to restart the CP decomposition. To the best of our knowledge, this is the first work that identifies and solves the problem of incremental group-level popularity prediction. Extensive experimental results show significant improvements of the IGPP method over other works both in the prediction accuracy and the efficiency.",2021,,10.1145/3461839
2138,"Bleifu\ss{}, Tobias and Bornemann, Leon and Johnson, Theodore and Kalashnikov, Dmitri V. and Naumann, Felix and Srivastava, Divesh",Exploring Change: A New Dimension of Data Analytics,,"Data and metadata in datasets experience many different kinds of change. Values are inserted, deleted or updated; rows appear and disappear; columns are added or repurposed, etc. In such a dynamic situation, users might have many questions related to changes in the dataset, for instance which parts of the data are trustworthy and which are not? Users will wonder: How many changes have there been in the recent minutes, days or years? What kind of changes were made at which points of time? How dirty is the data? Is data cleansing required? The fact that data changed can hint at different hidden processes or agendas: a frequently crowd-updated city name may be controversial; a person whose name has been recently changed may be the target of vandalism; and so on. We show various use cases that benefit from recognizing and exploring such change.We envision a system and methods to interactively explore such change, addressing the variability dimension of big data challenges. To this end, we propose a model to capture change and the process of exploring dynamic data to identify salient changes. We provide exploration primitives along with motivational examples and measures for the volatility of data. We identify technical challenges that need to be addressed to make our vision a reality, and propose directions of future work for the data management community.",2018,,10.14778/3282495.3282496
2139,"Meyer, Jochen and Simske, Steven and Siek, Katie A. and Gurrin, Cathal G. and Hermens, Hermie",Beyond Quantified Self: Data for Wellbeing,"wellbeing, data analysis, user oriented design","Sustaining our health and wellbeing requires lifelong efforts for prevention and healthy living. Continuously observing ourselves is one of the fundamental measures to be taken. While many devices support monitoring and quantifying our health behavior and health state, they all are facing the same trade-off: the higher the data quality is the higher are the efforts of data acquisition. However, for lifelong use, minimizing efforts for the user is crucial. Nowadays, few devices find a good balance between cost and value. In this interdisciplinary workshop we discuss how this trade-off can be approached by addressing three topics: understanding the user's information needs, exploring options for data acquisition, and discussing potential designs for life-long use.",2014,,10.1145/2559206.2560469
2140,"Sangogboye, Fisayo Caleb and Jia, Ruoxi and Hong, Tianzhen and Spanos, Costas and Kj\ae{}rgaard, Mikkel Baun",A Framework for Privacy-Preserving Data Publishing with Enhanced Utility for Cyber-Physical Systems,"cyber-physical systems, smart buildings, Privacy preservation, k-anonymity, deep learning","Cyber-physical systems have enabled the collection of massive amounts of data in an unprecedented level of spatial and temporal granularity. Publishing these data can prosper big data research, which, in turn, helps improve overall system efficiency and resiliency. The main challenge in data publishing is to ensure the usefulness of published data while providing necessary privacy protection. In our previous work&nbsp;(Jia et al. 2017a), we presented a privacy-preserving data publishing framework (referred to as PAD hereinafter), which can guarantee k-anonymity while achieving better data utility than traditional anonymization techniques. PAD learns the information of interest to data users or features from their interactions with the data publishing system and then customizes data publishing processes to the intended use of data. However, our previous work is only applicable to the case where the desired features are linear in the original data record. In this article, we extend PAD to nonlinear features. Our experiments demonstrate that for various data-driven applications, PAD can achieve enhanced utility while remaining highly resilient to privacy threats.",2018,,10.1145/3275520
2141,"Nie, Yu and Talburt, John and Li, Xinming and Xiao, Zhongdong",Chief Data Officer (CDO) Role and Responsibility Analysis,"business management, role and responsibility analysis, data analytics, chief data officer (CDO)","While the number of organizations creating the role of Chief Data Officer (CDO) is increasing each year, the nature of the role is still emerging. CDO management responsibilities can vary widely from company to company. The study focuses on the various management responsibilities of the CDO role and their commonalities across organizations. After collecting and analyzing CDO job description from 411 organizations, we came to the following conclusions. Data analytics and business management are the most often cited and thus the most important management responsibilities for the CDO. Second is the management of data quality and data governance programs. Third, the CDO should keep abreast of new information technologies that could help firms design and execute an enterprise data strategy that coordinates the firm's business intelligence processes, leads to the development of new products, and acquires new customers through new data media.",2018,,
2142,"Tang, Mengfan and Pongpaichet, Siripen and Jain, Ramesh",Research Challenges in Developing Multimedia Systems for Managing Emergency Situations,"disaster, situation prediction, situation recognition, eventshop, micro-reports","With an increasing amount of diverse heterogeneous data and information, the methodology of multimedia analysis has become increasingly relevant in solving challenging societal problems such as managing emergency situations during disasters. Using cybernetic principles combined with multimedia technology, researchers can develop effective frameworks for using diverse multimedia (including traditional multimedia as well as diverse multimodal) data for situation recognition, and determining and communicating appropriate actions to people stranded during disasters. We present known issues in disaster management and then focus on emergency situations. We show that an emergency management problem is fundamentally a multimedia information assimilation problem for situation recognition and for connecting people's needs to available resources effectively, efficiently, and promptly. Major research challenges for managing emergency situations are identified and discussed. We also present a intelligently detecting evolving environmental situations, and discuss the role of multimedia micro-reports as spontaneous participatory sensing data streams in emergency responses. Given enormous progress in concept recognition using machine learning in the last few years, situation recognition may be the next major challenge for learning approaches in multimedia contextual big data. The data needed for developing such approaches is now easily available on the Web and many challenging research problems in this area are ripe for exploration in order to positively impact our society during its most difficult times.",2016,,10.1145/2964284.2976761
2143,"Karmaker (“Santu”), Shubhra Kanti and Hassan, Md. Mahadi and Smith, Micah J. and Xu, Lei and Zhai, Chengxiang and Veeramachaneni, Kalyan",AutoML to Date and Beyond: Challenges and Opportunities,"Automated machine learning, interactive data science, predictive analytics, democratization of artificial intelligence","As big data becomes ubiquitous across domains, and more and more stakeholders aspire to make the most of their data, demand for machine learning tools has spurred researchers to explore the possibilities of automated machine learning (AutoML). AutoML tools aim to make machine learning accessible for non-machine learning experts (domain experts), to improve the efficiency of machine learning, and to accelerate machine learning research. But although automation and efficiency are among AutoML’s main selling points, the process still requires human involvement at a number of vital steps, including understanding the attributes of domain-specific data, defining prediction problems, creating a suitable training dataset, and selecting a promising machine learning technique. These steps often require a prolonged back-and-forth that makes this process inefficient for domain experts and data scientists alike and keeps so-called AutoML systems from being truly automatic. In this review article, we introduce a new classification system for AutoML systems, using a seven-tiered schematic to distinguish these systems based on their level of autonomy. We begin by describing what an end-to-end machine learning pipeline actually looks like, and which subtasks of the machine learning pipeline have been automated so far. We highlight those subtasks that are still done manually—generally by a data scientist—and explain how this limits domain experts’ access to machine learning. Next, we introduce our novel level-based taxonomy for AutoML systems and define each level according to the scope of automation support provided. Finally, we lay out a roadmap for the future, pinpointing the research required to further automate the end-to-end machine learning pipeline and discussing important challenges that stand in the way of this ambitious goal.",2021,,10.1145/3470918
2144,"Mierswa, Ingo",The Wisdom of Crowds: Best Practices for Data Prep &amp; Machine Learning Derived from Millions of Data Science Workflows,"machine learning tools, analytics, visual workflow, wisdom of the crowds, data visualization","With hundreds of thousands of users, RapidMiner is the most frequently used visual workflow platform for machine learning. It covers the full spectrum of analytics from data preparation to machine learning and model validation. In this presentation, I will take you on a tour of machine learning which spans the last 15 years of research and industry applications and share key insights with you about how data scientists perform their daily analysis tasks. These patterns are extracted from mining millions of analytical workflows that have been created with RapidMiner over the past years. This talk will address important questions around the data mining process such as: What are the most frequently used solutions for typical data quality problems? How often are analysts using decision trees or neural networks? And does this behavior change over time or depend on the users experience level?",2016,,10.1145/2939672.2945365
2145,"Lopez, M. Graham and Young, Jeffrey and Meredith, Jeremy S. and Roth, Philip C. and Horton, Mitchel and Vetter, Jeffrey S.",Examining Recent Many-Core Architectures and Programming Models Using SHOC,"performance, accelerators, benchmarking","The Scalable HeterOgeneous Computing (SHOC) benchmark suite was released in 2010 as a tool to evaluate the stability and performance of emerging heterogeneous architectures and to compare different programming models for compute devices used in those architectures. Since then, high-performance computing (HPC) system architectures have increasingly incorporated both discrete and fused multi-core and many-core processors. The TOP500 list illustrates this trend: heterogeneous systems grew from a 3.4% to 18.0% share of the list between June 2010 and June 2015. Not only are there more heterogeneous systems on the TOP500 list today, those machines are responsible for a disproportionately large percentage of list's aggregate performance: as of June 2015, the performance share for heterogeneous systems has grown to 33.7%.Part of this shift toward heterogeneous architectures has stemmed from new products in the hardware accelerator market, such as Intel's Xeon Phi coprocessor, and improvements in the approaches for programming such accelerators. Existing approaches such as CUDA and OpenCL have become more powerful and easy to use, and directive-based programming models such as OpenACC, OpenMP 4.0, and Intel's Language Extensions for Offload (LEO) are rapidly gaining user acceptance. The benefits of these hardware and software advances are not limited to HPC; other problem domains such as ""big data"" are reaping the rewards also.The original SHOC benchmarks had adequate support for CUDA and OpenCL for graphics processing units, but did not support more recent programming models and devices. We extended SHOC to support evaluation of recent heterogeneous architectures and programming models such as OpenACC and LEO, and we added new benchmarks to increase SHOC's application domain coverage. In this paper, we describe our modifications to the stock SHOC distribution and present several examples of using our augmented version of SHOC for evaluation of recent heterogeneous architectures and programming models.",2015,,10.1145/2832087.2832090
2146,"Rheinl\""{a}nder, Astrid and Lehmann, Mario and Kunkel, Anja and Meier, J\""{o}rg and Leser, Ulf",Potential and Pitfalls of Domain-Specific Information Extraction at Web Scale,"focused crawling, massively parallel data analysis, information extraction","In many domains, a plethora of textual information is available on the web as news reports, blog posts, community portals, etc. Information extraction (IE) is the default technique to turn unstructured text into structured fact databases, but systematically applying IE techniques to web input requires highly complex systems, starting from focused crawlers over quality assurance methods to cope with the HTML input to long pipelines of natural language processing and IE algorithms. Although a number of tools for each of these steps exists, their seamless, flexible, and scalable combination into a web scale end-to-end text analytics system still is a true challenge. In this paper, we report our experiences from building such a system for comparing the ""web view"" on health related topics with that derived from a controlled scientific corpus, i.e., Medline. The system combines a focused crawler, applying shallow text analysis and classification to maintain focus, with a sophisticated text analytic engine inside the Big Data processing system Stratosphere. We describe a practical approach to seed generation which led us crawl a corpus of ~1 TB web pages highly enriched for the biomedical domain. Pages were run through a complex pipeline of best-of-breed tools for a multitude of necessary tasks, such as HTML repair, boilerplate detection, sentence detection, linguistic annotation, parsing, and eventually named entity recognition for several types of entities. Results are compared with those from running the same pipeline (without the web-related tasks) on a corpus of 24 million scientific abstracts and a third corpus made of ~250K scientific full texts. We evaluate scalability, quality, and robustness of the employed methods and tools. The focus of this paper is to provide a large, real-life use case to inspire future research into robust, easy-to-use, and scalable methods for domain-specific IE at web scale.",2016,,10.1145/2882903.2903736
2147,"Zhou, Han and Zou, Wentao and Jiang, Yan and Shao, Qizhuan and Wu, Yang and Liu, Shuangquan",Rule-Based Data Verification Method in Electricity Spot Market,,"In the electric power spot market, input data quality is critical to an accurate and reliable clearing result. Missing and abnormal data will lead to the result that the clearing algorithm diverges or the results mismatch reality. This would seriously affect power system security and reduce the efficiency of the market. Therefore, to ensure a smooth convergence of the algorithm and reasonable results, this paper proposes a rule-based verification method for the input data in the power spot market. Data integrity and logical verification rules are established. During the verification process, information will be divided into different warning levels and displayed so that users can modify relevant data according to the actual situation.",2021,,10.1145/3469213.3470246
2148,"Sharbatdar, Nasim and Lamine, Yassine and Milord, Brigitte and Morency, Catherine and Cheng, Jinghui","Capturing the Practices, Challenges, and Needs of Transportation Decision-Makers","transportation management and planning, decision-making, persona, user study, decision-maker","Transportation decision-makers from government agencies play an important role in addressing the traffic network conditions, which in turn, have a major impact on the well-being of citizens. The practices, challenges, and needs of this group of practitioners are less represented in the HCI literature. We address this gap through an interview study with 19 practitioners from Transports Qu\'{e}bec, a government agency responsible for transportation infrastructures in Qu\'{e}bec, Canada. We found that this group of decision-makers can most benefit from research about data analysis tools and platforms that (1) provide information to support data quality awareness, (2) are interoperable with other tools in the complex workflow of the practitioners, and (3) support intuitive and customizable visual analytics. These implications can also be informative to the design of tools supporting other decision-making tasks and domains.",2020,,10.1145/3334480.3382864
2149,"Neto, Nelson Novaes and Madnick, Stuart and Paula, Anchises Moraes G. De and Borges, Natasha Malara",Developing a Global Data Breach Database and the Challenges Encountered,"data breach, data aggregation, Cyber security, semantics of data, privacy","If the mantra “data is the new oil” of our digital economy is correct, then data leak incidents are the critical disasters in the online society. The initial goal of our research was to present a comprehensive database of data breaches of personal information that took place in 2018 and 2019. This information was to be drawn from press reports, industry studies, and reports from regulatory agencies across the world. This article identified the top 430 largest data breach incidents among more than 10,000 data breach incidents.In the process, we encountered many complications, especially regarding the lack of standardization of reporting. This article should be especially interesting to the readers of JDIQ because it describes both the range of data quality and consistency issues found as well as what was learned from the database created.The database that was created, available at https://www.databreachdb.com, shows that the number of data records breached in those top 430 incidents increased from around 4B in 2018 to more than 22B in 2019. This increase occurred despite the strong efforts from regulatory agencies across the world to enforce strict rules on data protection and privacy, such as the General Data Protection Regulation (GDPR) that went into effect in Europe in May 2018. Such regulatory effort could explain the reason why there is such a large number of data breach cases reported in the European Union when compared to the U.S. (more than 10,000 data breaches publicly reported in the U.S. since 2018, while the EU reported more than 160,0001 data breaches since May 2018). However, we still face the problem of an excessive number of breach incidents around the world.This research helps to understand the challenges of proper visibility of such incidents on a global scale. The results of this research can help government entities, regulatory bodies, security and data quality researchers, companies, and managers to improve the data quality of data breach reporting and increase the visibility of the data breach landscape around the world in the future.",2021,,10.1145/3439873
2150,"Yang, Jie and Cao, Yong",The Classification of Gene Sequencer Based on Machine Learning,"Machine learning, Classification of gene Sequencer, Quality of sequencing","Abstract: Biological sequencing plays a very important role in life science, especially with the improvement of sequencing technology and the development of sequencing instruments, and a large number of biological sequencing quality data are produced every day. Because of different sequencers, the quality of sequencing is different. In the process of sequencing quality control, the model of sequencer can be deduced according to the quality of gene sequence. Therefore, in this paper, five sequencers of Illumina HiSeq series, Illumina HiSeq 2000, Illumina HiSeq 2500, Illumina HiSeq 3000, Illumina HiSeq 4000 and Illumina HiSeq XTen, are selected as the classification objects. Firstly, the sequencing quality data of the five sequencers are preprocessed. Then, the classification model is trained by three machine learning algorithms: decision tree, logistic regression and support vector machine. The experimental results show that the accuracy rates of the three machine learning algorithms are 96.67%, 97.50% and 97.50% respectively. These algorithms are very good to solve the problem of using biological sequencing data quality to classify sequencer.",2022,,10.1145/3511716.3511730
2151,"Ahmadov, Ahmad and Thiele, Maik and Lehner, Wolfgang and Wrembel, Robert",Context Similarity for Retrieval-Based Imputation,,"Completeness as one of the four major dimensions of data quality is a pervasive issue in modern databases. Although data imputation has been studied extensively in the literature, most of the research is focused on inference-based approach. We propose to harness Web tables as an external data source to effectively and efficiently retrieve missing data while taking into account the inherent uncertainty and lack of veracity that they contain.Existing approaches mostly rely on standard retrieval techniques and out-of-the-box matching methods which result in a very low precision, especially when dealing with numerical data. We, therefore, propose a novel data imputation approach by applying numerical context similarity measures which results in a significant increase in the precision of the imputation procedure, by ensuring that the imputed values are of the same domain and magnitude as the local values, thus resulting in an accurate imputation.We use Dresden Web Table Corpus which is comprised of more than 125 million web tables extracted from the Common Crawl as our knowledge source. The comprehensive experimental results demonstrate that the proposed method well outperforms the default out-of-the-box retrieval approach.",2017,,10.1145/3110025.3110161
2152,"Li, Pei and Dai, Chaofan and Wang, Wenqian",Application of Attribute Correlation in Unsupervised Data Cleaning,"Unsupervised data cleaning, minimum repair cost, weak logic errors, attribute correlation, machine learning","Referring to the supervised learning and unsupervised learning in machine learning, we divide the data cleaning processes into supervised and unsupervised two forms too, and then, we reclassify the data quality problems into canonicalization error, redundancy error, strong logic error and weak logic error according to the characteristics of unsupervised cleaning. For the weak logic errors, we propose a repair framework AC-Framework and an algorithm AC-Repair based on the attribute correlation. When repairing, we first establish a priority queue(PQ) for elements to be repaired according to the minimum cost idea and take the corresponding conflict-free data set(Icf) as a training set to learn the correlation among attributes. Then, we select the first element in PQ list as the candidate element to repair, and recompute the PQ list after one repair round to improve the efficiency. Finally, in order to prevent the algorithm from endless loops, we set a label flag to mark the repaired elements, in this way, every error element will be repaired at most once. In the experimental part, we compare the AC-Repair algorithm with the interpolation-based repair algorithm to verify its validity.",2019,,10.1145/3312714.3312717
2153,"Gupta, Somit and Shi, Xiaolin and Dmitriev, Pavel and Fu, Xin and Mukherjee, Avijit","Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments",,"A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the ""why"" behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program [11, 14]. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions. A version of this tutorial was also present at KDD 2019 [23]. It was attended by around 150 participants.",2020,,
2154,"Killeen, Patrick and Kiringa, Iluju and Yeap, Tet",Unsupervised Dynamic Sensor Selection for IoT-Based Predictive Maintenance of a Fleet of Public Transport Buses,"internet of things, sensor selection, predictive analytics, Performance, fleet management, Algorithms, J1939, machine learning, controller area network, predictive maintenance, Design","In recent years, big data produced by the Internet of Things (IoT) has enabled new kinds of useful applications. One such application is monitoring a fleet of vehicles in real-time to predict their remaining useful life. Consensus self-organized models (COSMO) approach is an example of a predictive maintenance system. The present work proposes a novel IoT-based architecture for predictive maintenance that consists of three primary nodes: namely, the vehicle node (VN), the server leader node (SLN), and the root node (RN), which enable on-board vehicle data processing, heavy-duty data processing, and fleet administration, respectively. A minimally viable prototype (MVP) of the proposed architecture was implemented and deployed to a local bus garage in Gatineau, Canada. The present work proposes an improved COSMO (ICOSMO), a fleet-wide unsupervised dynamic sensor selection algorithm. To analyze the performance of ICOSMO, a fleet simulation was implemented. The J1939 data gathered from a hybrid bus was used to generate synthetic data in the simulations. Simulation results that compared the performance of the COSMO and ICOSMO approaches revealed that in general ICOSMO improves the average area under the curve of COSMO by approximately 1.5% when using the Cosine distance and 0.6% when using the Hellinger distance.",2022,,10.1145/3530991
2155,"Fan, Wenfei and Lu, Ping and Tian, Chao and Zhou, Jingren",Deducing Certain Fixes to Graphs,,"This paper proposes to deduce certain fixes to graphs G based on data quality rules Σ and ground truth Γ (i.e., validated attribute values and entity matches). We fix errors detected by Σ in G such that the fixes are assured correct as long as Σand Γ are correct. We deduce certain fixes in two paradigms. (a) We interact with users and ""incrementally"" fix errors online. Whenever users pick a small set V0 of nodes in G, we fix all errors pertaining to V0 and accumulate ground truth in the process. (b) Based on accumulated Γ, we repair the entire graph G offline; while this may not correct all errors in G, all fixes are guaranteed certain.We develop techniques for deducing certain fixes. (1) We define data quality rules to support conditional functional dependencies, recursively defined keys and negative rules on graphs, such that we can deduce fixes by combining data repairing and object identification. (2) We show that deducing certain fixes is Church-Rosser, i.e., the deduction converges at the same fixes regardless of the order of rules applied. (3) We establish the complexity of three fundamental problems associated with certain fixes. (4) We provide (parallel) algorithms for deducing certain fixes online and offline, and guarantee to reduce running time when given more processors. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of our methods.",2019,,10.14778/3317315.3317318
2156,"Civili, Cristina and Console, Marco and De Giacomo, Giuseppe and Lembo, Domenico and Lenzerini, Maurizio and Lepore, Lorenzo and Mancini, Riccardo and Poggi, Antonella and Rosati, Riccardo and Ruzzi, Marco and Santarelli, Valerio and Savo, Domenico Fabio",Mastro Studio: Managing Ontology-Based Data Access Applications,,"Ontology-based data access (OBDA) is a novel paradigm for accessing large data repositories through an ontology, that is a formal description of a domain of interest. Supporting the management of OBDA applications poses new challenges, as it requires to provide effective tools for (i) allowing both expert and non-expert users to analyze the OBDA specification, (ii) collaboratively documenting the ontology, (iii) exploiting OBDA services, such as query answering and automated reasoning over ontologies, e.g., to support data quality check, and (iv) tuning the OBDA application towards optimized performances. To fulfill these challenges, we have built a novel system, called MASTRO STUDIO, based on a tool for automated reasoning over ontologies, enhanced with a suite of tools and optimization facilities for managing OBDA applications. To show the effectiveness of MASTRO STUDIO, we demonstrate its usage in one OBDA application developed in collaboration with the Italian Ministry of Economy and Finance.",2013,,10.14778/2536274.2536304
2157,"Lin, Xueling and Chen, Lei",Domain-Aware Multi-Truth Discovery from Conflicting Sources,,"In the Big Data era, truth discovery has served as a promising technique to solve conflicts in the facts provided by numerous data sources. The most significant challenge for this task is to estimate source reliability and select the answers supported by high quality sources. However, existing works assume that one data source has the same reliability on any kinds of entity, ignoring the possibility that a source may vary in reliability on different domains. To capture the influence of various levels of expertise in different domains, we integrate domain expertise knowledge to achieve a more precise estimation of source reliability. We propose to infer the domain expertise of a data source based on its data richness in different domains. We also study the mutual influence between domains, which will affect the inference of domain expertise. Through leveraging the unique features of the multi-truth problem that sources may provide partially correct values of a data item, we assign more reasonable confidence scores to value sets. We propose an integrated Bayesian approach to incorporate the domain expertise of data sources and confidence scores of value sets, aiming to find multiple possible truths without any supervision. Experimental results on two real-world datasets demonstrate the feasibility, efficiency and effectiveness of our approach.",2018,,10.1145/3177732.3177739
2158,"Gupta, Somit and Shi, Xiaolin and Dmitriev, Pavel and Fu, Xin","Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments",,"A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the “why” behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program 11, 14. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions. A version of this tutorial was also present at KDD 2019 23. It was attended by around 150 participants. This tutorial has also been accepted for the WSDM 2020 conference.",2020,,
2159,"Cohen, L.",Impacts of Business Intelligence on Population Health: A Systematic Literature Review,"population health, business intelligence, systematic literature review","""Business Intelligence"" is an area of Information Technology (IT) that involves the collection, analysis and presentation of large amounts of data. BI has been successfully applied to promote good decision making in a variety of environments, and has high potential to make a significant impact in the domain of population health. The promotion of population health is a key concern of government authorities and various health institutions and officials making decisions about interventions that may impact on population health would benefit from the use of information on population health. BI could clearly be a facilitator in this regard, but evidence of its current application and impact in this field is not easily accessible to policy makers. This systematic literature review explored the literature and provided a synthesis of information available on the current use of BI in this area, and evidence of the impact of its use on population health. An array of applications of BI for population health were found, including data warehouses, analytics, reports, data warehouse browsers, OLAP, GIS, Dashboards and Alerts. Evidence of the impact of these applications on population health was mainly anecdotal, with only one empirical study found. Issues and challenges encountered in the development and use of BI are Privacy and Security, Data Quality and Development and Maintenance of BI infrastructure",2017,,10.1145/3129416.3129441
2160,"Lin, Xueling and Chen, Lei",Domain-Aware Multi-Truth Discovery from Conflicting Sources,,"In the Big Data era, truth discovery has served as a promising technique to solve conflicts in the facts provided by numerous data sources. The most significant challenge for this task is to estimate source reliability and select the answers supported by high quality sources. However, existing works assume that one data source has the same reliability on any kinds of entity, ignoring the possibility that a source may vary in reliability on different domains. To capture the influence of various levels of expertise in different domains, we integrate domain expertise knowledge to achieve a more precise estimation of source reliability. We propose to infer the domain expertise of a data source based on its data richness in different domains. We also study the mutual influence between domains, which will affect the inference of domain expertise. Through leveraging the unique features of the multi-truth problem that sources may provide partially correct values of a data item, we assign more reasonable confidence scores to value sets. We propose an integrated Bayesian approach to incorporate the domain expertise of data sources and confidence scores of value sets, aiming to find multiple possible truths without any supervision. Experimental results on two real-world datasets demonstrate the feasibility, efficiency and effectiveness of our approach.",2018,,10.1145/3187009.3177739
2161,"Dorn, Amelie and Wandl-Vogt, Eveline and Palfinger, Thomas and D\'{\i}az, Jos\'{e} Luis Preza and Piringer, Barbara and Schatek, Alexander and Zoubek, Rainer",Applying Commercial Computer Vision Tools to Cope with Uncertainties in a Citizen-Driven Archive: The Case Study Topothek@exploreAT!,"citizen-driven archive, AI, computer vision, uncertainty, Digital Humanities","Uncertainties in data, e.g., incomplete data sets, data quality issues or inconsistencies in annotations, are a common phenomenon across disciplines. How to address these issues is context dependent. In this paper, we address uncertainties in the citizen-driven archive Topotheque as a concrete use-case in the Digital Humanities project exploreAT!, and demonstrate, how to deal with uncertainties by benchmarking a set of selected commercial computer vision (CV) tools. The approach aims to enrich Topotheque's data to enable better access, connectivity and analysis for both researchers and citizens. Results show that by applying CV, existing uncertainties are noticeably reduced, but new ones also introduced. Better grounds for semantic structuring are provided, enabling higher connectivity and linking within Topotheque, but also across other data sets. Ultimately, the enrichment of the archive is for the benefit of both researchers and citizens enabled by addressing and tackling apparent uncertainties.",2018,,10.1145/3284179.3284322
2162,"Ye, Yumeng and Talburt, John R.",Generating Synthetic Data to Support Entity Resolution Education and Research,,"Almost all organizations use some type of Entity Resolution (ER) methods to uniquely identify their customers and vendors across different channels of contact. In the case of persons, this requires the use of personally identifying information (PII) such as name, address, phone number, and email address. Because of the growing concerns over data privacy and identity theft, organizations are reluctant to release personally-identifiable customer information even for education and research purposes. An alternative is to generate synthetic data to use in student exercises and for research related to entity resolution methods and techniques. One advantage of synthetically generated data for ER is it can be fully annotated with the correct linking making it very easy to calculate the precision and recall of linking operations. This paper discusses a simple method to generate synthetic data as input for ER processes. The method allows the user to randomly assign certain types and levels of data quality errors along with other types of non-error variations to the data, such as nicknames, different date formats, and changes in address. For ER research in particular, the method can create introduce data redundancy by copying records referencing the same person into the same file or into different files with different record layouts.",2019,,
2163,"Li, Xiang and Zhang, Zhaoqian and Zhao, Zhigang and Wu, Lu and Huo, Jidong and Zhang, Jian and Wang, Yinglong",ECNN: One Online Deep Learning Model for Streaming Ocean Data Prediction,"Attention Network, Ocean Data, Online Learning, Time Series Prediction, CNN","Despite been extensively explored, current techniques in sequential data modeling and prediction are generally designed for solving regression tasks in a batch learning setting, making them not only computationally inefficient but also poorly scalable in real-world applications, especially for real-time intelligent ocean data quality control (QC), where the data arrives sequentially and the QC should be conducted in real time. This paper investigates the online learning for ocean data streams by resolving two main challenges: (i) how to develop a deep learning model to capture the complex ocean data distribution that could evolve dynamically, namely tackling the 'concept drift' problem for non-stationary time series; (ii) how to develop a deep learning model that can dynamically adapt its structure from shallow to deep with the inflow of the data to overcome under-fitting problem, namely tackling the 'model selection' problem. To tackle these challenges, we propose one Evolutive Convolutional Neural Network (ECNN) that dynamically re-weighting the sub-structure of the model from data streams in a sequential or online learning fashion, by which the capacity scalability and sustainability are introduced into the model. The experiments on real ocean observation data verify the effectiveness of our model. As far as we know, it is the first work that introduce online deep learning techniques into ocean data prediction research.",2022,,10.1145/3491396.3506519
2164,"Wu, Yi and Song, Yan and Yang, Hongshan",Intelligent Distributed Web Crawler Based on Attention Mechanism,"Distributed Framework, Artificial Intelligence, Deep Learning, Intelligent Web Crawler","With the rapid development of the Internet, webpages' content has become the central platform for people to publish and retrieve information. Recently, web crawlers could quickly and accurately find the information users need from the massive network information resources. There have been many different types of web crawlers in the literature, developed for data retrieval. However, most of the existing web crawlers have significant limitations. For example, they focus on the effective overall architecture instead of paying attention to the actual data's complexity. Moreover, the advertising links in the news and the public platform's promotional content have become ubiquitous noise. The existing web crawler collection strategy lacks sufficient identification of advertising information. The degree of automation to detect advertisements is low, so it isn't easy to form a complete and deployable large-scale distributed data crawling system. Therefore, the research and improvement of distributed web crawlers that intelligently distinguish advertisements is a work of practical significance. The distributed intelligent web crawler system designed and implemented in this paper solves low manual crawler efficiency and poor data quality. The crawler system can effectively identify and eliminate advertising information and significantly improve the automatically extracted data in the distributed crawler system from the experimental results.",2020,,10.1145/3438872.3439085
2165,"Shi, Xiaolin and Dmitriev, Pavel and Gupta, Somit and Fu, Xin","Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled Experiments","controlled experiments, user experience evaluation, a/b testing, online metrics","A/B Testing is the gold standard to estimate the causal relationship between a change in a product and its impact on key outcome measures. It is widely used in the industry to test changes ranging from simple copy change or UI change to more complex changes like using machine learning models to personalize user experience. The key aspect of A/B testing is evaluation of experiment results. Designing the right set of metrics - correct outcome measures, data quality indicators, guardrails that prevent harm to business, and a comprehensive set of supporting metrics to understand the ""why"" behind the key movements is the #1 challenge practitioners face when trying to scale their experimentation program [18, 22]. On the technical side, improving sensitivity of experiment metrics is a hard problem and an active research area, with large practical implications as more and more small and medium size businesses are trying to adopt A/B testing and suffer from insufficient power. In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating experiment results, focusing on both lessons learned and practical guidelines as well as open research questions.",2019,,10.1145/3292500.3332297
2166,"Ilyas, Ihab F. and Chu, Xu",Data Cleaning,,"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.",2019,,
2167,"Cong, Phan Thanh and Tam, Nguyen Thanh and Yin, Hongzhi and Zheng, Bolong and Stantic, Bela and Hung, Nguyen Quoc Viet",Efficient User Guidance for Validating Participatory Sensing Data,"trust management, probabilistic database, Participatory sensing","Participatory sensing has become a new data collection paradigm that leverages the wisdom of the crowd for big data applications without spending cost to buy dedicated sensors. It collects data from human sensors by using their own devices such as cell phone accelerometers, cameras, and GPS devices. This benefit comes with a drawback: human sensors are arbitrary and inherently uncertain due to the lack of quality guarantee. Moreover, participatory sensing data are time series that exhibit not only highly irregular dependencies on time but also high variance between sensors. To overcome these limitations, we formulate the problem of validating uncertain time series collected by participatory sensors. In this article, we approach the problem by an iterative validation process on top of a probabilistic time series model. First, we generate a series of probability distributions from raw data by tailoring a state-of-the-art dynamical model, namely &lt;u&gt;G&lt;/u&gt;eneralised &lt;u&gt;A&lt;/u&gt;uto &lt;u&gt;R&lt;/u&gt;egressive &lt;u&gt;C&lt;/u&gt;onditional &lt;u&gt;H&lt;/u&gt;eteroskedasticity (GARCH), for our joint time series setting. Second, we design a feedback process that consists of an adaptive aggregation model to unify the joint probabilistic time series and an efficient user guidance model to validate aggregated data with minimal effort. Through extensive experimentation, we demonstrate the efficiency and effectiveness of our approach on both real data and synthetic data. Highlights from our experiences include the fast running time of a probabilistic model, the robustness of an aggregation model to outliers, and the significant effort saving of a guidance model.",2019,,10.1145/3326164
2168,"Zhu, Feida and Pei, Jian",The Third International Workshop on Smart Data for Blockchain and Distributed Ledger (SDBD2021): Joint Workshop with SIGKDD 2021 Trust Day,"data auditing, data pricing, privacy, data asset, distributed ledger technology, data governance","Today's computing is characterized by an increasing degree of complexity, comprehensiveness and collaboration. The complexity can be observed by the wide application of gigantic models with a huge number of parameters and structures of an unprecedented level of sophistication. The comprehensiveness is best illustrated by the high heterogeneity of data both in terms of format and source. The collaboration, finally, becomes an obvious trend when computing systems grow more open and decentralized in which various entities interact to achieve collective intelligence with the presence of potentially malicious behavior. Trust, therefore, has become critical at multiple levels: At model level to assure its integrity, fairness and interpretability; At data level to safeguard data quality, compliance and privacy; At system level to govern resilience, performance and incentive. Moreover, the notion of trust has long been discussed in different domains in both academia and industry with different definition and understanding. The Third International Workshop on Smart Data for Blockchain and Distributed Ledger (SDBD'21) will be held as a joint workshop with the special-themed ""Trust Day"" of KDD 2021, which has therefore aimed to bring together researchers, practitioners and experts from various communities to exchange and explore ideas, frontiers, opportunities and challenges under the broad theme of ""trust"" in a highly interdisciplinary manner.",2021,,10.1145/3447548.3469441
2169,"Perkins, Patrick and Heber, Steffen",Using a Novel Negative Selection Inspired Anomaly Detection Algorithm to Identify Corrupted Ribo-Seq and RNA-Seq Samples,"negative selection algorithm, rna-seq, ribosome profiling, sample quality, machine learning, anomaly detection","RNA-seq and Ribo-seq are popular techniques for quantifying cellular transcription and translation. These experiments use next-generation sequencing to produce genome-wide high-resolution snapshots of the total populations of mRNAs and translating ribosomes within the investigated samples. When performed in concert, these experiments yield valuable information about protein synthesis rates and translational efficiency. Due to their intricate experimental protocols and demanding data processing requirements, quality control and analysis of such experiments are often challenging. Therefore, methods for accurately assessing data quality, and for identifying contaminated samples, are greatly needed. In the following we use a novel negative selection inspired algorithm called Boundary Detection Using Nearest Neighbors (BDUNN), for the identification of corrupted samples. Our algorithm constructs a detector set and reduced training set that defines the boundaries between normal data points and potential anomalies. Subsequently, a nearest neighbor algorithm is used to classify unseen observations. We compare the performance of BDUNN with other popular negative selection and one-class classification algorithms, and show that BDUNN is capable of accurately and efficiently detecting anomalies in standard anomaly detection datasets and simulated RNA-seq and Ribo-seq data sets. Furthermore, we have implemented our method within an existing R Shiny platform for analyzing RNA-seq an Ribo-seq datasets, which permits downstream analysis of anomalous samples.",2019,,10.1145/3307339.3342169
2170,"Lipuntsov, Yuri P.",On the Relationship Between the Information and Analytical Components in the Shared E-Government,"Economic and mathematical modeling, Data exchange, Information modeling, Shared environment, Simulation","Economic and mathematical models and information models are the two main components of the information environment. These two components perform different functions - the information models is responsible for the data quality, the data delivery, and the economic and mathematical models defines data mining and intelligence. This category of models is constantly being developed often independently of each other. The information models as methods of data presentation and data integration are considered as separate from economic and mathematical modeling area. This paper discuss the relationship between the two types of models as sequence of steps for models development with the horizontal and vertical traceability. The connection between two types of models presented as the reflection of the real word logic to the data layer and after that to the software layer, and the feedback from the application to the information and to the operation logic.",2015,,10.1145/2846012.2846026
2171,"Pine, Kathleen and Bossen, Claus and Holten M\o{}ller, Naja and Miceli, Milagros and Lu, Alex Jiahong and Chen, Yunan and Horgan, Leah and Su, Zhaoyuan and Neff, Gina and Mazmanian, Melissa",Investigating Data Work Across Domains: New Perspectives on the Work of Creating Data,"Data-Driven, Data Work, Labor, Occupations, Datafication","In the wake of the hype around big data, artificial intelligence, and “data-drivenness,” much attention has been paid to developing novel tools to capitalize upon the deluge of data being recorded and gathered automatically through IT systems. While much of this literature tends to overlook the data itself—sometimes even characterizing it as “data exhaust” that is readily available to be fed into algorithms, which will unlock the insights held within it—a growing body of literature has recently been directed at the (often intensive and skillful) work that goes into creating, collecting, managing, curating, analyzing, interpreting, and communicating data. These investigations detail the practices and processes involved in making data useful and meaningful so that aims of becoming ‘data-driven’ or ‘data-informed’ can become real. Further, In some cases, increased demands for data work have led to the formation of new occupations, whereas at other times data work has been added to the task portfolios of existing occupations and professions, occasionally affecting their core identity. Thus, the evolving forms of data work are requiring individual and organizational resources, new and re-tooled practices and tools, development of new competences and skills, and creation of new functions and roles. While differences exist across the global North and the global South experience of data work, such factors of data production remain paramount even as they exist largely for the benefit of the data-driven system [21, 32]. This one-day workshop will investigate existing and emerging tasks of data work. Further, participants will seek to understand data work as it impacts: individual data workers; occupations tasked with data work (existing and emerging); organizations (e.g. changing their skill-mix and infrastructuring to support data work); and teaching institutions (grappling with incorporation of data work into educational programs). Participants are required to submit a position paper or a case study drawn from their research to be reviewed and accepted by the organizing committee (submissions should be up to four pages in length). Upon acceptance, participants will read each other's paper, prepare to shortly present and respond to comments by two discussants and other participants. Subsequently, the workshop will focus on developing a set of core processes and tasks as well as an outline of a research agenda for a CHI-perspective on data work in the coming years.",2022,,10.1145/3491101.3503724
2172,"El-Atawy, Sameh S. and Khalefa, Mohamed E.",Building an Ontology-Based Electronic Health Record System,"Query Language, Electronic Health Record Management, Ontology","Electronic health record (EHR) solutions are complex, spanning multiple specialties and domains of expertise. These systems need to handle clinical concepts, temporal data, documents, and financial transactions, which leads to a large code base that is tightly coupled with data models and inherently hard to maintain. These difficulties can greatly increase the cost of developing EHR systems, result in a high failure rate of implementation, and threaten investments in this sector. Moreover, due to the wide variance in the level of detail across different settings, data exchange is becoming a serious problem, further increasing the cost of development and maintenance.To overcome these issues, we adopt ontologies to model our proposed EHR solution, not only allowing code reuse; but also enabling later extension and customization. Adopting software factory techniques, we build tools to transform ontological models into deployment-ready code. This automatically provides handling of data persistence, access, and exchange. Business logic is expressed as ontology-based process flows and rules, ensuring data quality and supporting special needs. This logic is enforced transparently and can be modified on the fly. We optimized the user experience by facilitating fast data entry and retrieval.In this paper, we present the requirements of an effective EHR solution, explain the techniques we employed, describe the main modules of our proposed system, and discuss the technical decisions we made.",2016,,10.1145/2944165.2944172
2173,"Samel, Karan and Miao, Xu",Active Deep Learning to Tune Down the Noise in Labels,"active learning, denoising, deep neural networks, classification","The great success of supervised learning has initiated a paradigm shift from building a deterministic software system to a probabilistic artificial intelligent system throughout the industry. The historical records in enterprise domains can potentially bootstrap the traditional business into the modern data-driven approach almost everywhere. The introduction of the Deep Neural Networks (DNNs) significantly reduces the efforts of feature engineering so that supervised learning becomes even more automated. The last bottleneck is to ensure the data quality, particularly the label quality, because the performance of supervised learning is bounded by the errors present in labels. In this paper, we present a new Active Deep Denoising (ADD) approach that first builds a DNN noise model, and then adopts an active learning algorithm to identify the optimal denoising function. We prove that under the low noise condition, we only need to query the oracle with log n examples where n is the total number in the data. We apply ADD on one enterprise application and show that it can effectively reduce 1/3 of the prediction error with only 0.1% of examples verified by the oracle.",2018,,10.1145/3219819.3219914
2174,"Thirumuruganathan, Saravanan and Kunjir, Mayuresh and Ouzzani, Mourad and Chawla, Sanjay",Automated Annotations for AI Data and Model Transparency,"data cleaning, machine learning, Data transparency","The data and Artificial Intelligence revolution has had a massive impact on enterprises, governments, and society alike. It is fueled by two key factors. First, data have become increasingly abundant and are often available openly. Enterprises have more data than they can process. Governments are spearheading open data initiatives by setting up data portals such as data.gov and releasing large amounts of data to the public. Second, AI engineering development is becoming increasingly democratized. Open source frameworks have enabled even an individual developer to engineer sophisticated AI systems. But with such ease of use comes the potential for irresponsible use of data.Ensuring that AI systems adhere to a set of ethical principles is one of the major problems of our age. We believe that data and model transparency has a key role to play in mitigating the deleterious effects of AI systems. In this article, we describe a framework to synthesize ideas from various domains such as data transparency, data quality, data governance among others to tackle this problem. Specifically, we advocate an approach based on automated annotations (of both data and the AI model), which has a number of appealing properties. The annotations could be used by enterprises to get visibility of potential issues, prepare data transparency reports, create and ensure policy compliance, and evaluate the readiness of data for diverse downstream AI applications. We propose a model architecture and enumerate its key components that could achieve these requirements. Finally, we describe a number of interesting challenges and opportunities.",2021,,10.1145/3460000
2175,"K\""{o}hler, Henning and Link, Sebastian and Zhou, Xiaofang",Possible and Certain SQL Keys,,"Driven by the dominance of the relational model, the requirements of modern applications, and the veracity of data, we revisit the fundamental notion of a key in relational databases with NULLs. In SQL database systems primary key columns are NOT NULL by default. NULL columns may occur in unique constraints which only guarantee uniqueness for tuples which do not feature null markers in any of the columns involved, and therefore serve a different function than primary keys. We investigate the notions of possible and certain keys, which are keys that hold in some or all possible worlds that can originate from an SQL table, respectively. Possible keys coincide with the unique constraint of SQL, and thus provide a semantics for their syntactic definition in the SQL standard. Certain keys extend primary keys to include NULL columns, and thus form a sufficient and necessary condition to identify tuples uniquely, while primary keys are only sufficient for that purpose. In addition to basic characterization, axiomatization, and simple discovery approaches for possible and certain keys, we investigate the existence and construction of Armstrong tables, and describe an indexing scheme for enforcing certain keys. Our experiments show that certain keys with NULLs do occur in real-world databases, and that related computational problems can be solved efficiently. Certain keys are therefore semantically well-founded and able to maintain data quality in the form of Codd's entity integrity rule while handling the requirements of modern applications, that is, higher volumes of incomplete data from different formats.",2015,,
2176,"K\""{o}hler, Henning and Link, Sebastian and Zhou, Xiaofang",Possible and Certain SQL Keys,,"Driven by the dominance of the relational model, the requirements of modern applications, and the veracity of data, we revisit the fundamental notion of a key in relational databases with NULLs. In SQL database systems primary key columns are NOT NULL by default. NULL columns may occur in unique constraints which only guarantee uniqueness for tuples which do not feature null markers in any of the columns involved, and therefore serve a different function than primary keys. We investigate the notions of possible and certain keys, which are keys that hold in some or all possible worlds that can originate from an SQL table, respectively. Possible keys coincide with the unique constraint of SQL, and thus provide a semantics for their syntactic definition in the SQL standard. Certain keys extend primary keys to include NULL columns, and thus form a sufficient and necessary condition to identify tuples uniquely, while primary keys are only sufficient for that purpose. In addition to basic characterization, axiomatization, and simple discovery approaches for possible and certain keys, we investigate the existence and construction of Armstrong tables, and describe an indexing scheme for enforcing certain keys. Our experiments show that certain keys with NULLs do occur in real-world databases, and that related computational problems can be solved efficiently. Certain keys are therefore semantically well-founded and able to maintain data quality in the form of Codd's entity integrity rule while handling the requirements of modern applications, that is, higher volumes of incomplete data from different formats.",2015,,10.14778/2809974.2809975
2177,"Li, Yanying and Sun, Haipei and Dong, Boxiang and Wang, Hui (Wendy)",Cost-Efficient Data Acquisition on Online Data Marketplaces for Correlation Analysis,,"Incentivized by the enormous economic profits, the data marketplace platform has been proliferated recently. In this paper, we consider the data marketplace setting where a data shopper would like to buy data instances from the data marketplace for correlation analysis of certain attributes. We assume that the data in the marketplace is dirty and not free. The goal is to find the data instances from a large number of datasets in the marketplace whose join result not only is of high-quality and rich join informativeness, but also delivers the best correlation between the requested attributes. To achieve this goal, we design DANCE, a middleware that provides the desired data acquisition service. DANCE consists of two phases: (1) In the off-line phase, it constructs a two-layer join graph from samples. The join graph includes the information of the datasets in the marketplace at both schema and instance levels; (2) In the online phase, it searches for the data instances that satisfy the constraints of data quality, budget, and join informativeness, while maximizing the correlation of source and target attribute sets. We prove that the complexity of the search problem is NP-hard, and design a heuristic algorithm based on Markov chain Monte Carlo (MCMC). Experiment results on two benchmark and one real datasets demonstrate the efficiency and effectiveness of our heuristic data acquisition algorithm.",2018,,10.14778/3297753.3297757
2178,"Fu, Qingwen and Zhu, Jiahui and Chen, Yuepeng and Wan, Jintao and He, Bin",An Automatic Learning Model for Trajectory Outlier Detection,"Bi-LSTM, outlier detection, spatial-temporal data, attention mechanism","The rapid development of global positioning system has given birth to a large number of spatial-temporal data, and there are many outliers of points obviously in these trajectory data. It is very important to detect the outliers in the trajectory to improve the data quality and accuracy of trajectory mining. In this paper, we propose a trajectory outlier detection algorithm based on bi-directional long short-term memory model and attention mechanism. Firstly, an eight-dim eigenvector is extracted from each point of trajectory, and then a two-layer bi-directional long short-term memory model is constructed. Finally, representing the trajectory points in an interactive way which is called attention mechanism. The input of the model is the trajectory point with a certain length, and the output is the type of the trajectory point. The model can automatically learn the difference between the normal point and the adjacent abnormal point with motion features. Experimental dataset based on real trajectory data of taxi from Beijing, and results showed that the performance of this algorithm is significantly better than constant speed threshold method or classical machine learning classification. Especially the precision and recall reaches 0.93 and 0.90 separately, which proves the effectiveness of this algorithm.",2020,,10.1145/3414274.3414505
2179,"Spaniol, Marc and Baeza-Yates, Ricardo and Masan\`{e}s, Julien",TempWeb 2018 Chairs' Welcome and Organization,"distributed data analytics, data quality metrics, web science, large scale data storage, content evolution on the web, community detection and evolution, web scale data analytics, large scale data processing, web spam evolution, data aggregation, temporal web analytics, web trends, time aware web archiving, systematic exploitation of web archives, topic mining, web dynamics, terminology evolution","Time is a key dimension to understand the Web. It is fair to say that it has not received yet all the attention it deserves and TempWeb is an attempt to help remedy this situation by putting time as the center of its reflection. Studying time in this context actually covers a large spectrum, from the extraction of temporal information and knowledge, to diachronic studies for the design of infrastructural and experimental settings enabling a proper observation of this dimension.",2018,,10.1145/3184558.3192324
2180,"Shi, Zhiguo and Yang, Guang and Gong, Xiaowen and He, Shibo and Chen, Jiming",Quality-Aware Incentive Mechanisms Under Social Influences in Data Crowdsourcing,,"Incentive mechanism design and quality control are two key challenges in data crowdsourcing, because of the need for recruitment of crowd users and their limited capabilities. Without considering users’ social influences, existing mechanisms often result in low efficiency in terms of the platform’s cost. In this paper, we exploit social influences among users as incentives to motivate users’ participation, in order to reduce the cost of recruiting users. Based on social influences, we design incentive mechanisms with the goal of achieving high quality of crowdsourced data and low cost of incentivizing users’ participation. Specifically, we consider three scenarios. In the full information scenario, we design task assignment and user recruitment mechanisms to optimize the data quality while reducing the incentive cost. In the partial information scenario, users’ qualities and costs are unknown. We exploit the correlation between tasks to overcome the information asymmetry, for both cases of opportunistic crowdsourcing and participatory crowdsourcing. Further, in the dynamic social influence scenario, we investigate the dynamics of users’ social influences and design extra rewards for users to make full use of the social influence and achieve maximum cost saving. We evaluate the incentive mechanisms using numerical results, which demonstrate their effectiveness.",2022,,10.1109/TNET.2021.3105427
2181,"Dvo\v{r}\'{a}kov\'{a}, Eli\v{s}ka and Kumar, Sajal and Kl\'{e}ma, Ji\v{r}\'{\i} and \v{Z}elezn\'{y}, Filip and Drbal, Karel and Song, Mingzhou",Evaluating Model-Free Directional Dependency Methods on Single-Cell RNA Sequencing Data with Severe Dropout,"model-free, directional dependency, single-cell sequencing","As severe dropout in single-cell RNA sequencing (scRNA-seq) degrades data quality, current methods for network inference face increased uncertainty from such data. To examine how dropout influences directional dependency inference from scRNA-seq data, we thus studied four methods based on discrete data that are model-free without parametric model assumptions. They include two established methods: conditional entropy and Kruskal-Wallis test, and two recent methods: causal inference by stochastic complexity and function index. We also included three non-directional methods for a contrast. On simulated data, function index performed most favorably at varying dropout rates, sample sizes, and discrete levels. On an scRNA-seq dataset from developing mouse cerebella, function index and Kruskal-Wallis test performed favorably over other methods in detecting expression of developmental genes as a function of time. Overall among the four methods, function index is most resistant to dropout for both directional and dependency inference. The next best choice, Kruskal-Wallis test, carries a directional bias towards a uniformly distributed variable. We conclude that a method robust to marginal distributions with a sufficiently large sample size can reap benefits of single-cell over bulk RNA sequencing in understanding molecular mechanisms at the cellular resolution.",2019,,10.1145/3383783.3383793
2182,"Jia, Ruoxi and Sangogboye, Fisayo Caleb and Hong, Tianzhen and Spanos, Costas and Kj\ae{}rgaard, Mikkel Baun",PAD: Protecting Anonymity in Publishing Building Related Datasets,"occupancy privacy, k-anonymity, convex optimization, clustering","The diffusion of low-cost sensor network technologies in smart buildings has enabled the collection of massive amounts of data regarding indoor environments, energy use and occupants, which, in turn, creates opportunities for knowledge- and information-based building management. Driven by benefits mutual to occupants, building managers, and research communities, there is a demand for data publication to foster more sophisticated and robust models and algorithms. Data in the original form, however, contains sensitive information about occupants' behavioral patterns, and publishing such data will violate individuals' privacy. The current practice on publishing building-related datasets relies primarily on policies for dictating which types of data can be published and agreements on the use of published data. This approach alone provides insufficient protection as it does not prevent privacy breaches from occurring in the first place.In this paper, we present PAD, which to our knowledge is the first system that provides a technological solution for publishing building related datasets in a privacy-preserving manner while maintaining high data quality. PAD is able to offer a strong anonymity guarantee by perturbing data records. The unique feature of PAD is that it offers an interface to incorporate dataset users into the loop of data publication and customizes the perturbation such that useful information in the dataset can be better retained. We study the efficacy of PAD using occupancy and plug load data collected in real buildings. The experiments demonstrate that PAD can achieve high resilience to privacy threats without introducing any significant data fidelity penalties.",2017,,10.1145/3137133.3137140
2183,"Li, Furong and Lee, Mong Li and Hsu, Wynne",Entity Profiling with Varying Source Reliabilities,"source reliability, entity profiling, record linkage, truth discovery","The rapid growth of information sources on the Web has intensified the problem of data quality. In particular, the same real world entity may be described by different sources in various ways with overlapping information, and possibly conflicting or even erroneous values. In order to obtain a more complete and accurate picture for a real world entity, we need to collate the data records that refer to the entity, as well as correct any erroneous values. We observe that these two tasks are often tightly coupled: rectifying erroneous values will facilitate data collation, while linking similar records provides us with a clearer view of the data and additional evidence for error correction. In this paper, we present a framework called Comet that interleaves record linkage with error correction, taking into consideration the source reliabilities on various attributes. The proposed framework first utilizes confidence based matching to discriminate records in terms of ambiguity and source reliability. Then it performs adaptive matching to reduce the impact of erroneous values. Experiment results demonstrate that Comet outperforms the state-of-the-art techniques and is able to build complete and accurate profiles for real world entities.",2014,,10.1145/2623330.2623685
2184,"Hao, Fei and Jiao, Mingjie and Min, Geyong and Yang, Laurence T.",Launching an Efficient Participatory Sensing Campaign: A Smart Mobile Device-Based Approach,"tensor, DTA, deployment, recruitment, trajectory, Participatory sensing","Participatory sensing is a promising sensing paradigm that enables collection, processing, dissemination and analysis of the phenomena of interest by ordinary citizens through their handheld sensing devices. Participatory sensing has huge potential in many applications, such as smart transportation and air quality monitoring. However, participants may submit low-quality, misleading, inaccurate, or even malicious data if a participatory sensing campaign is not launched effectively. Therefore, it has become a significant issue to establish an efficient participatory sensing campaign for improving the data quality. This article proposes a novel five-tier framework of participatory sensing and addresses several technical challenges in this proposed framework including: (1) optimized deployment of data collection points (DC-points); and (2) efficient recruitment strategy of participants. Toward this end, the deployment of DC-points is formulated as an optimization problem with maximum utilization of sensor and then a Wise-Dynamic DC-points Deployment (WD3) algorithm is designed for high-quality sensing. Furthermore, to guarantee the reliable sensing data collection and communication, a trajectory-based strategy for participant recruitment is proposed to enable campaign organizers to identify well-suited participants for data sensing based on a joint consideration of temporal availability, trust, and energy. Extensive experiments and performance analysis of the proposed framework and associated algorithms are conducted. The results demonstrate that the proposed algorithm can achieve a good sensing coverage with a smaller number of DC-points, and the participants that are termed as social sensors are easily selected, to evaluate the feasibility and extensibility of the proposed recruitment strategies.",2015,,10.1145/2808198
2185,"Madera, Cedrine and Laurent, Anne",The next Information Architecture Evolution: The Data Lake Wave,"data lakes, data lab, data warehouses, data reservoirs, data governance, data laboratory, internet of things, digital transformation","Data warehouses and data marts have long been considered as the unique solution for providing end-users with decisional information. More recently, data lakes have been proposed in order to govern data swamps. However, no formal definition has been proposed in the literature. Existing works are not complete and miss important parts of the topic. In particular, they do not focus on the influence of the data gravity, the infrastructure role of those solutions and of course are proposing divergent definitions and positioning regarding the usage and the interaction with existing decision support system.In this paper, we propose a novel definition of data lakes, together with a comparison with other over several criteria as the way to populate them, how to use, what is the Data Lake end user profile. We claim that data lakes are complementary components in decisional information systems and we discuss their position and interactions regarding the other components by proposing an interaction model.",2016,,10.1145/3012071.3012077
2186,"Golab, Lukasz and Johnson, Theodore",Data Stream Warehousing,"real-time analytics, data streams, data warehousing",,2013,,10.1145/2463676.2465337
2187,"Dong, Xin Luna and Dragut, Eduard Constantin",10th International Workshop on Quality in Databases: QDB 2012,,,2013,,10.1145/2430456.2430472
2188,"Elmekki, Hanae and Chiadmi, Dalila and Lamharhar, Hind",Open Government Data: Towards a Comparison of Data Lifecycle Models,"Open Government Data, data lifecycle, data value creation","Government, through Open Government Data ""OGD, becomes one of the important producers of open data. OGD is an opportunity to create valuable services and innovative products useful for citizens as a primarily targeted consumer. However, the expected benefits of OGD are not yet met. That is to say, several research communities' studies insist on the necessity of creating valuable data in order to generate valuable services. These studies are still insufficient for a shared understanding of how OGD contribute to the creation of value. For this purpose, this paper presents a review of a set of data lifecycle models compared against their contribution to the creation of value in the context of OGD.",2019,,10.1145/3333165.3333180
2189,"Tao, Shibo and Wang, Xiaorong and Huang, Weijing and Chen, Wei and Wang, Tengjiao and Lei, Kai",From Citation Network to Study Map: A Novel Model to Reorganize Academic Literatures,"study map, academic papers, topic analysis, double-damping pagerank, reference injection","As the number of academic papers and new technologies soars, it has been increasingly difficult for researchers, especially beginners, to enter a new research field. Researchers often need to study a promising paper in depth to keep up with the forefront of technology. Traditional Query-Oriented study method is time-consuming and even tedious. For a given paper, existent academic search engines like Google Scholar tend to recommend relevant papers, failing to reveal the knowledge structure. The state-of-the-art Map-Oriented study methods such as AMiner and AceMap can structure scholar information, but they're too coarse-grained to dig into the underlying principles of a specific paper. To address this problem, we propose a Study-Map Oriented method and a novel model called RIDP (Reference Injection based Double-Damping PageRank) to help researchers study a given paper more efficiently and thoroughly. RIDP integrates newly designed Reference Injection based Topic Analysis method and Double-Damping PageRank algorithm to mine a Study Map out of massive academic papers in order to guide researchers to dig into the underlying principles of a specific paper. Experiment results on real datasets and pilot user studies indicate that our method can help researchers acquire knowledge more efficiently, and grasp knowledge structure systematically.",2017,,10.1145/3041021.3053059
2190,"Zhang, Bo and Kong, Dehua",Dynamic Estimation Model of Insurance Product Recommendation Based on Naive Bayesian Model,"insurance products, dynamic estimation, Naive Bayes, recommendation","Aiming at the dynamic estimation of insurance product recommendation, considering the particularity and complexity of purchasing insurance product and the uncertainty of influencing factors, a dynamic estimation model of insurance product recommendation based on Naive Bayes is proposed. The model combines customer insurance information with machine learning. The results show that the naive Bayesian classification algorithm can be compared with the decision tree and neural network classification algorithm, showing high accuracy and high speed.",2020,,10.1145/3444370.3444575
2191,"Wang, Chunxia and Xie, Jian",Constructing a Computer Model for Discipline Data Governance Using the Contingency Theory and Data Mining,,"Data governance is an important part of modernizing the governance capacity of universities. Discipline data governance plays an important role in promoting the development of university disciplines, and is a key factor in improving the governance of university disciplines, the science of educational decision-making and the effectiveness of management. It is an important way to promote the ""precision"" and ""science"" of discipline governance. In this paper, we construct a model of discipline data governance based on the Contingency Theory with a view to shedding light on discipline governance.",2021,,10.1145/3482632.3484077
2192,"Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix",Data Profiling: A Tutorial,"data profiling, data exploration, dependency discovery","is to understand the dataset at hand and its metadata. The process of metadata discovery is known as data profiling. Profiling activities range from ad-hoc approaches, such as eye-balling random subsets of the data or formulating aggregation queries, to systematic inference of structural information and statistics of a dataset using dedicated profiling tools. In this tutorial, we highlight the importance of data profiling as part of any data-related use-case, and we discuss the area of data profiling by classifying data profiling tasks and reviewing the state-of-the-art data profiling systems and techniques. In particular, we discuss hard problems in data profiling, such as algorithms for dependency discovery and profiling algorithms for dynamic data and streams. We also pay special attention to visualizing and interpreting the results of data profiling. We conclude with directions for future research in the area of data profiling. This tutorial is based on our survey on profiling relational data [2].",2017,,10.1145/3035918.3054772
2193,"Nugroho, Heru and Gumilang, Soni Fajar Surya",Recommendations for Improving Data Management Process in Government of Bandung Regency Using COBIT 4.1 Framework,"COBIT 4.1, Data, Recommendations, DS-11, Maturity","Data is an valuable asset that potentially provides substantial benefits for the government and society. To make the performance of local government apparatus runs optimally and the public gets the best service, the government of Bandung Regency strives to improve data management. The initial stage of optimizing data management is the assessment of the maturity level in managing data (DS-11) using COBIT 4.1. Base on the assessment maturity level for DS-11, the government of Bandung Regency needs to raise the level from 2.46 (Repeatable but Intuitive) to 3.0 (Defined). Recommendations given to improve data management in Government with focuses on maintaining the completeness, accuracy, availability, and protection of data.",2020,,10.1145/3384544.3384588
2194,"Bertino, Elisa and Jahanshahi, Mohammad R.",Adaptive and Cost-Effective Collection of High-Quality Data for Critical Infrastructure and Emergency Management in Smart Cities—Framework and Challenges,"Civil engineering, device swarms, edge computing",,2018,,10.1145/3190579
2195,"Pedersen, Torben Bach and Castellanos, Malu and Dayal, Umesh",Report on the Seventh International Workshop on Business Intelligence for the Real Time Enterprise (BIRTE 2013),,,2015,,10.1145/2737817.2737831
2196,"Rhazal, Oumaima El and Tomader, Mazri",Study of Smart City Data: Categories and Quality Challenges,"internet of things (IoT), quality of information (QoI), smart city","Lately, the world attention is directed to transforming daily life to a smarter one, we cannot deny the smart city concept that became pervading. This concept will give every device the chance to communicate with other devices, it will simply create the smarter version of everything. However, data heterogeneity and quality changes are one of the best priorities and challenges that should be handled in this promising concept.In this paper we present a review about data categories circulating in a smart city depending on its required services. We also study the quality of information as one of both, major challenges and treasures in a smart city.",2019,,10.1145/3368756.3368965
2197,ACM Data Science Task Force,Computing Competencies for Undergraduate Data Science Curricula,,,2021,,
2198,"Cao, Longbing",Data Science: Challenges and Directions,,"While it may not be possible to build a data brain identical to a human, data science can still aspire to imaginative machine thinking.",2017,,10.1145/3015456
2199,"Peek, Geerten and Taspinar, Ahmet",One Thousand Interviews,,"How customer insights keep one company agile, and challenge these data scientist to stay ahead in an ever-changing world.",2016,,10.1145/2983463
2200,"Rahman, Md Mahbubur and Nathan, Viswam and Nemati, Ebrahim and Vatanparvar, Korosh and Ahmed, Mohsin and Kuang, Jilong",Towards Reliable Data Collection and Annotation to Extract Pulmonary Digital Biomarkers Using Mobile Sensors,"Digital Biomarkers, Breathlessness, Breathing, Data Quality, Cough, mHealth, Crowdsourced Annotation","Proliferation of sensors embedded in smartphones and smartwatches helps capture rich dataset for machine learning algorithms to extract meaningful digital bio-markers on consumer devices for monitoring disease progression and treatment response. However, development and validation of machine learning algorithms depend on gathering high fidelity sensor data and reliable ground-truth. We conduct a study, called mLungStudy, with 131 subjects with varying pulmonary conditions to collect mobile sensor data including audio, accelerometer, gyroscope using a smartphone and a smartwatch, in order to extract pulmonary biomarkers such as breathing, coughs, spirometry, and breathlessness. Our study shows that commonly used breathing ground-truth data from chestband may not always be reliable as a gold-standard. Our analysis shows that breathlessness biomarkers such as pause time and pause frequency from 2.15 minutes of audio can be as reliable as those extracted from 5 minutes' worth of speech data. This finding can be useful for future studies to trade-off between the reliability of breathlessness data and patient comfort in generating continuous speech data. Furthermore, we use crowdsourcing techniques to annotate pulmonary sound events for developing signal processing and machine learning algorithms. In this paper, we highlight several practical challenges to collect and annotate physiological data and acoustic symptoms from chronic pulmonary patients and ways to improve data quality. We show that the waveform visualization of the audio signal improves annotation quality which leads to a 6.59% increase in cough classification accuracy and a 6% increase in spirometry event classification accuracy. Findings from this study inform future studies focusing on developing explainable machine learning models to extract pulmonary digital bio-markers using mobile sensors.",2019,,10.1145/3329189.3329204
2201,"Romualdo-Suzuki, Larissa and Finkelstein, Anthony and Gann, David",A Middleware Framework for Urban Data Management,"value chain., smart cities, software architecture, big data","The domain of inquiry of this research is the collection, organization, integration, distribution and consumption of knowledge derived from urban open data, and how it can be best offered to application cities' stakeholders through a software middleware. We argue that the extensive investigation proposed in this research will contribute to a growing body of knowledge about data integration and application in smart cities, and offer opportunities to re-think an integrated urban infrastructure.",2013,,10.1145/2494091.2499223
2202,"Kir\'{a}ly, P\'{e}ter",Towards an Extensible Measurement of Metadata Quality,"big data, REST API, metadata quality, design patterns","This paper describes the structure of an extensible metadata quality assessment framework, which supports multiple metadata schemas, and is flexible enough to work with new schemas. The software has to be scalable to be able to process huge amount of metadata records within a reasonable time. Fundamental requirements that need to be considered during the design of such a software are i) the abstraction of the metadata schema (in the context of the measurement process), ii) how to address distinct parts within metadata records, iii) the workflow of the measurement, iv) a common and powerful interface for the individual metrics, and v) interoperability with Java and REST APIs.",2017,,10.1145/3078081.3078109
2203,"Hamdi, Sana and Bouazizi, Emna and Faiz, Sami",A New QoS Management Approach in Real-Time GIS with Heterogeneous Real-Time Geospatial Data Using a Feedback Control Scheduling,"Quality of Service, Transaction, Heterogeneous Real-Time Geospatial Data, Geographic Information System, Real-Time Spatial Big Data, Feedback Control Scheduling","Geographic Information System (GIS) is a computer system designed to capture, store, manipulate, analyze, manage, and present all types of spatial data. Spatial data, whether captured through remote sensors or large scale simulations becomes more and big and heterogenous. As a result, structured data and unstructured content are simultaneously accessed via an integrated user interface. The issue of real-time and heterogeneity is extremely important for taking effective decision. Thus, heterogeneous real-time spatial data management is a very active research domain nowadays. Existing research are interested in querying of real-time spatial data and their updates without taking into account the heterogeneity of real-time geospatial data. In this paper, we propose the use of the real-time Spatial Big Data and we define a new architecture called FCSA-RTSBD (Feedback Control Scheduling Architecture for Real-Time Spatial Big Data). The main objectives of this architecture are the following: take in account the heterogeneity of data, guarantee the data freshness, enhance the deadline miss ratio even in the presence of conflicts and finally satisfy the requirements of users by the improving of the quality of service (QoS).",2015,,10.1145/2790755.2790774
2204,"Noussair, Lazrak and Jihad, Zahir and Hajar, Mousannif",Responsive Cities and Data Gathering: Challenges and Opportunities,"Responsive cities, Big Data, Quality of data, Data gathering","For the last two decades, data driven cities have emerged as an efficient way of improving the city performance, enhancing life quality, and providing more choices to city planners and decision makers. A significant change in data driven cities in recent years is that much more data are collected from a variety of sources and can be processed into various forms for different stakeholders. The availability of a large amount of data can potentially lead to a revolution in city development, changing the city operation system from a conventional technology-driven system into a more powerful multifunctional data-driven intelligent system. But with more data collected the more questions raised about the optimization and space saving methods, then the quality of data collected and the efficiency of the its treatment. In this paper, we provide a survey on the data driven cities requirements, and the tools made available for the responsive cities to maintain its data.",2018,,10.1145/3286606.3286794
2205,"Wang, Guang and Fang, Zhihan and Xie, Xiaoyang and Wang, Shuai and Sun, Huijun and Zhang, Fan and Liu, Yunhuai and Zhang, Desheng",Pricing-Aware Real-Time Charging Scheduling and Charging Station Expansion for Large-Scale Electric Buses,"charging pattern, charging scheduling, data driven, MDP, Electric bus","We are witnessing a rapid growth of electrified vehicles due to the ever-increasing concerns on urban air quality and energy security. Compared to other types of electric vehicles, electric buses have not yet been prevailingly adopted worldwide due to their high owning and operating costs, long charging time, and the uneven spatial distribution of charging facilities. Moreover, the highly dynamic environment factors such as unpredictable traffic congestion, different passenger demands, and even the changing weather can significantly affect electric bus charging efficiency and potentially hinder the further promotion of large-scale electric bus fleets. To address these issues, in this article, we first analyze a real-world dataset including massive data from 16,359 electric buses, 1,400 bus lines, and 5,562 bus stops. Then, we investigate the electric bus network to understand its operating and charging patterns, and further verify the necessity and feasibility of a real-time charging scheduling. With such understanding, we design busCharging, a pricing-aware real-time charging scheduling system based on Markov Decision Process to reduce the overall charging and operating costs for city-scale electric bus fleets, taking the time-variant electricity pricing into account. To show the effectiveness of busCharging, we implement it with the real-world data from Shenzhen, which includes GPS data of electric buses, the metadata of all bus lines and bus stops, combined with data of 376 charging stations for electric buses. The evaluation results show that busCharging dramatically reduces the charging cost by 23.7% and 12.8% of electricity usage simultaneously. Finally, we design a scheduling-based charging station expansion strategy to verify our busCharging is also effective during the charging station expansion process.",2020,,10.1145/3428080
2206,"Palacio, Ana Le\'{o}n and L\'{o}pez, \'{O}scar Pastor",Towards an Effective Medicine of Precision by Using Conceptual Modelling of the Genome: Short Paper,"conceptual modelling, precision medicine, data quality","The continuous improvement in our understanding of the human genome is leading to an increasing viable and effective Precision Medicine. Its intention is to provide a personalized solution to any individual health problem. Nevertheless, three main issues must be considered to make Precision Medicine a reality: i) the understanding of the huge amount of genomic data, spread out in hundreds of genome data sources, with different formats and contents, whose semantic interoperability is a must; ii) the development of information systems intended to guide the search of relevant genomic repositories related with a disease, the identification of significant information for its prevention, diagnosis and/or treatment and its management in an efficient software platform; iii) the high variability in the quality of the publicly available information. This paper presents a conceptual framework for solving these problems by i) using a precise conceptual schema of the human genome, and ii) introducing a method to search, identify, load and adequately interpret the required data, assuring its quality during the entire process.",2018,,10.1145/3194696.3194700
2207,"Lin, Zihan and Tian, Changxin and Hou, Yupeng and Zhao, Wayne Xin",Improving Graph Collaborative Filtering with Neighborhood-Enriched Contrastive Learning,"Collaborative Filtering, Graph Neural Network, Contrastive Learning, Recommender System"," Recently, graph collaborative filtering methods have been proposed as an effective recommendation approach, which can capture users’ preference over items by modeling the user-item interaction graphs. Despite the effectiveness, these methods suffer from data sparsity in real scenarios. In order to reduce the influence of data sparsity, contrastive learning is adopted in graph collaborative filtering for enhancing the performance. However, these methods typically construct the contrastive pairs by random sampling, which neglect the neighboring relations among users&nbsp;(or items) and fail to fully exploit the potential of contrastive learning for recommendation. To tackle the above issue, we propose a novel contrastive learning approach, named Neighborhood-enriched Contrastive Learning, named NCL, which explicitly incorporates the potential neighbors into contrastive pairs. Specifically, we introduce the neighbors of a user&nbsp;(or an item) from graph structure and semantic space respectively. For the structural neighbors on the interaction graph, we develop a novel structure-contrastive objective that regards users&nbsp;(or items) and their structural neighbors as positive contrastive pairs. In implementation, the representations of users&nbsp;(or items) and neighbors correspond to the outputs of different GNN layers. Furthermore, to excavate the potential neighbor relation in semantic space, we assume that users with similar representations are within the semantic neighborhood, and incorporate these semantic neighbors into the prototype-contrastive objective. The proposed NCL can be optimized with EM algorithm and generalized to apply to graph collaborative filtering methods. Extensive experiments on five public datasets demonstrate the effectiveness of the proposed NCL, notably with 26% and 17% performance gain over a competitive graph collaborative filtering base model on the Yelp and Amazon-book datasets, respectively. Our implementation code is available at: https://github.com/RUCAIBox/NCL.",2022,,10.1145/3485447.3512104
2208,"Chen, Qingyu and Wan, Yu and Zhang, Xiuzhen and Lei, Yang and Zobel, Justin and Verspoor, Karin",Comparative Analysis of Sequence Clustering Methods for Deduplication of Biological Databases,"clustering, databases, Deduplication, validation","The massive volumes of data in biological sequence databases provide a remarkable resource for large-scale biological studies. However, the underlying data quality of these resources is a critical concern. A particular challenge is duplication, in which multiple records have similar sequences, creating a high level of redundancy that impacts database storage, curation, and search. Biological database deduplication has two direct applications: for database curation, where detected duplicates are removed to improve curation efficiency, and for database search, where detected duplicate sequences may be flagged but remain available to support analysis.Clustering methods have been widely applied to biological sequences for database deduplication. Since an exhaustive all-by-all pairwise comparison of sequences cannot scale for a high volume of data, heuristic approaches have been recruited, such as the use of simple similarity thresholds. In this article, we present a comparison between CD-HIT and UCLUST, the two best-known clustering tools for sequence database deduplication. Our contributions include a detailed assessment of the redundancy remaining after deduplication, application of standard clustering evaluation metrics to quantify the cohesion and separation of the clusters generated by each method, and a biological case study that assesses intracluster function annotation consistency to demonstrate the impact of these factors on a practical application of the sequence clustering methods. Our results show that the trade-off between efficiency and accuracy becomes acute when low threshold values are used and when cluster sizes are large. This evaluation leads to practical recommendations for users for more effective uses of the sequence clustering tools for deduplication.",2018,,10.1145/3131611
2209,"Wu, Jinze and Huang, Zhenya and Liu, Qi and Lian, Defu and Wang, Hao and Chen, Enhong and Ma, Haiping and Wang, Shijin",Federated Deep Knowledge Tracing,"knowledge tracing, intelligent education, data quality evaluation, data isolation, federated learning","Knowledge tracing is a fundamental task in intelligent education for tracking the knowledge states of students on necessary concepts. In recent years, Deep Knowledge Tracing (DKT) utilizes recurrent neural networks to model student learning sequences. This approach has achieved significant success and has been widely used in many educational applications. However, in practical scenarios, it tends to suffer from the following critical problems due to data isolation: 1) Data scarcity. Educational data, which is usually distributed across different silos (e.g., schools), is difficult to gather. 2) Different data quality. Students in different silos have different learning schedules, which results in unbalanced learning records, meaning that it is necessary to evaluate the learning data quality independently for different silos. 3) Data incomparability. It is difficult to compare the knowledge states of students with different learning processes from different silos. Inspired by federated learning, in this paper, we propose a novel Federated Deep Knowledge Tracing (FDKT) framework to collectively train high-quality DKT models for multiple silos. In this framework, each client takes charge of training a distributed DKT model and evaluating data quality by leveraging its own local data, while a center server is responsible for aggregating models and updating the parameters for all the clients. In particular, in the client part, we evaluate data quality incorporating different education measurement theories, and we construct two quality-oriented implementations based on FDKT, i.e., FDKTCTT and FDKTIRT-where the means of data quality evaluation follow Classical Test Theory and Item Response Theory, respectively. Moreover, in the server part, we adopt hierarchical model interpolation to uptake local effects for model personalization. Extensive experiments on real-world datasets demonstrate the effectiveness and superiority of the FDKT framework.",2021,,10.1145/3437963.3441747
2210,"Jaakkola, Hannu and M\""{a}kinen, Timo and Etel\""{a}aho, Anna",Open Data: Opportunities and Challenges,"open data, big data, public data, networking, data analysis","Open data is seen as a promising source of new business, especially in the SME sector, in the form of new products, services and innovative solutions. High importance is seen also in fostering citizens' participation in political and social life and increasing the transparency of public authorities. The forerunners of the open data movement in the public sector are the USA and the UK, which started to open their public data resources in 2009. The first European Union open data related directive was drawn up as early as 2003; however progress in putting the idea into practice has been slow and adoptions by the wider member states are placed in the early 2010s. The beneficial use of open data in real applications has progressed hand in hand with the improvement of other ICT-related technologies. The (raw) data itself has no high value. The economic value comes from a balanced combination of high quality open (data) resources combined with the related value chain. This paper builds up a ""big picture"" of the role of open data in current society. The approach is analytical and it clarifies the topic from the viewpoints of both opportunities and challenges. The paper covers both general aspects related to open data and results of the research and regional development project conducted by the authors.",2014,,10.1145/2659532.2659594
2211,"Choudhury, Pranab Ranjan and Behera, Manoj Kumar",Using Administrative Data for Monitoring and Improving Land Policy and Governance in India,"Women Land Rights, India, Big Data, SDGs, Forest Rights","Demands for production and dissemination of reliable data is growing with increasing demand from public policies to monitor, compare and improve global and national developmental status and targets. Implementation of intentionally agreed commitments like Millennium Development Goals (MDGs), Sustainable development Goals (SDGs) are influencing data production and availability, and the development of national statistical capacities. They also trigger challenges and opportunities in production of internationally comparable data to induce fair comparability among nations. Being a signatory to major international treaties, India has considerably improved data production, accessibility and availability over the years to ensure proper alignment of national level statistics and induce international comparison. However, very little efforts have been made to assess India's progress around data production and dissemination around growingly important land governance. This assessment attempts to identify key opportunities and challenges at the country level to improve data availability, access, timeliness and quality.India has made many progressive reforms around land laws and institutions to make land governance more inclusive and equitable; however its assessment with respect to global best practices through World Bank's Land Governance Assessment Framework (LGAF) indicate the need of improvements around different land dimensions. Movement towards good land governance outcomes is incumbent upon robust and regular monitoring mechanism of land indicators across spatial (viz. administrative boundaries, land being a state subject in India) and temporal scales.India has traditions of collecting, maintaining and reporting land information through nation-wide surveys, census, administrative and judicial reports/ databases. Its flagship program Digital India Land Record Modernization Program (DILRMP), has been supporting universal digitization of spatial and textual land records by the states. Together, these administrative and survey-derived datasets provide seamless opportunity for routine generation of data on key land indicators at low cost on a regularbasis. Land is a state subject in India. Monitoring and reporting land-indicators at state levels would help in systematically discovering and identifying good practice that can then be documented and disseminated across states, manage change, and gradually move towards a more performance-based approach to improving land governance in India. However, there have been lack of institutionalized attempts, so far, to report land-indicators at national scale.We have tried to assess the state of data in India, particularly to track and report two critical land governance indicators viz. women land rights and forest rights, critical to ensure equity and sustainability in terms of public policy. With UN's SDG, defining similar indicators, we also attempt aligning them around SDG indicators. Status of these two parameters were analyzed using nation-wide datasets collecting whole population data, through legitimate institutions following robust processes and reporting them open access.Census (human population) data and Forest Survey of India (FSI) data were used to assess village-wise forest areas eligible for recognition of rights under India's historic Forest Rights Act, 2005. Using the FSI data and meta-analysis of census data, we calculated the estimated population (150 million including 90 million tribal) living in villages that have forest land within administrative revenue boundaries, potential area (40 million ha) that can be recognized under FRA and number of villages (0.17 million) that are eligible to initiate the claim. These data were made available across administrative boundaries of state, district and village, providing opportunities for relevant Government Ministries at Central and State level and civil society to expedite the forest rights recognition under India's largest land reform process.In order to assess women's land rights (WLR) in India in the context of the SDGs, after examining the existing data sets, we used Agricultural Census data, conducted by Government of India every fifth year following the guidelines of World Census on Agriculture (WCA). Using Agricultural census data, we have developed atlas of women land rights (based on operational holdings) in India with state and district wise granularity with further disaggregation across ethnicity (caste) and other socio-economic parameters. The study also attempted to analyze the link between the inter-regional and temporal variability of WLR and relevant policies and legal-institutional frameworks among the states to see if the correlations can better inform public policy and also induce healthy competition among states to appreciate and follow best practices. This paper presents the process, methodology and results of the data-analysis for these two land indicators while delving into the scope and challenges of dealing with existing and upcoming big datasets in India to report the land governance indicators and the potential policy spinoffs.",2017,,10.1145/3047273.3047296
2212,"Zeng, Xian and Han, Minglei and Li, Ning and Liu, Peng",Research on Real-Time Data Warehouse Technology for Sea Battlefield,"Big Data, Data Warehouse, Real-time, Sea Battlefield","Aiming at the data governance problems in the sea battlefield, this paper proposes a real-time data warehouse construction method for naval battlefields, which realizes the functions of storage, analysis and mining of battle data. This paper completes the construction of the data warehouse from the aspects of real-time data life cycle, real-time data application scenarios, data warehouse real-time safeguard measures, and data warehouse theme design. It can effectively provide data support for naval combat forces and provide auxiliary decision-making for commanders.",2021,,10.1145/3503928.3503930
2213,"Blake, Catherine and Souden, Maria and Anderson, Caryn L. and Twidale, Michael and Stelmack, Jenifer E.",Online Question Answering Practices to Support Healthcare Data Re-Use,"health, communities of practice, social question answering, forums, big data","Institutional data collection practices inevitably evolve over time, especially in a distributed clinical setting. Clinical and administrative data can improve health and healthcare, but only if researchers ensure that the data is well-aligned to their reuse goals and that they have adequately accounted for changes in data collection practices over time. Our goal is to understand information behaviors of health services data users as they bridge the gap between the historical data and their intended data reuse goals. This project leverages more than a decade of listserv posts related to the use of clinical and administrative data by US Department of Veterans Affairs (VA) employees, providing longitudinal insight into data reuse practices in both research and operational settings. In this paper we report the results of a pilot study that highlighted questions raised in the use of data and the knowledge engaged to answer them.",2015,,
2214,"Guo, Xusheng and Liang, Likeng and Liu, Yuanxia and Weng, Heng and Hao, Tianyong",The Construction of a Diabetes-Oriented Frequently Asked Question Corpus for Automated Question-Answering Services,"Diabetes, corpus construction, frequently-asked questions, visualization","In recent years, the prevalence of diabetes has been increasing rapidly worldwide. With the advancement of information technology, automated question-answering services for healthcare, which are commonly based on annotated corpus in health domain, have positive effects on health knowledge spread and daily health management for high-risk populations. This paper proposes to construct a large scale diabetes corpus of frequently-asked questions for automated question-answering services and evaluations. Concentrating on the characteristics of diabetes-related factors that reflect conditions of diabetes, this work establishes an annotated dataset containing professional question &amp; answer pairs about diabetes and their annotated question target categories. The corpus is applicable for various question-answering applications, supporting users to retrieve needed information, arrange diets, adhere to scientific medication as well as prevent and control disease complications.",2020,,10.1145/3433996.3434008
2215,"Deng, Song and Chen, Fulin and Dong, Xia and Gao, Guangwei and Wu, Xindong",Short-Term Load Forecasting by Using Improved GEP and Abnormal Load Recognition,"adaptive evolution, probability distribution, power load forecasting, abnormal load recognition, Gene expression programming","Load forecasting in short term is very important to economic dispatch and safety assessment of power system. Although existing load forecasting in short-term algorithms have reached required forecast accuracy, most of the forecasting models are black boxes and cannot be constructed to display mathematical models. At the same time, because of the abnormal load caused by the failure of the load data collection device, time synchronization, and malicious tampering, the accuracy of the existing load forecasting models is greatly reduced. To address these problems, this article proposes a Short-Term Load Forecasting algorithm by using Improved Gene Expression Programming and Abnormal Load Recognition (STLF-IGEP_ALR). First, the Recognition algorithm of Abnormal Load based on Probability Distribution and Cross Validation is proposed. By analyzing the probability distribution of rows and columns in load data, and using the probability distribution of rows and columns for cross-validation, misjudgment of normal load in abnormal load data can be better solved. Second, by designing strategies for adaptive generation of population parameters, individual evolution of populations and dynamic adjustment of genetic operation probability, an Improved Gene Expression Programming based on Evolutionary Parameter Optimization is proposed. Finally, the experimental results on two real load datasets and one open load dataset show that compared with the existing abnormal data detection algorithms, the algorithm proposed in this article have higher advantages in missing detection rate, false detection rate and precision rate, and STLF-IGEP_ALR is superior to other short-term load forecasting algorithms in terms of the convergence speed, MAE, MAPE, RSME, and R2.",2021,,10.1145/3447513
2216,"Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M","“Everyone Wants to Do the Model Work, Not the Data Work”: Data Cascades in High-Stakes AI","Nigeria, India, data politics, ML, raters, developers, USA, AI, application-domain experts, Ghana, data collectors, Kenya, Data, high-stakes AI, data quality, Uganda, data cascades"," AI models are increasingly applied in high-stakes domains like health and conservation. Data quality carries an elevated significance in high-stakes AI due to its heightened downstream impact, impacting predictions like cancer detection, wildlife poaching, and loan allocations. Paradoxically, data is the most under-valued and de-glamorised aspect of AI. In this paper, we report on data practices in high-stakes AI, from interviews with 53 AI practitioners in India, East and West African countries, and USA. We define, identify, and present empirical evidence on Data Cascades—compounding events causing negative, downstream effects from data issues—triggered by conventional AI/ML practices that undervalue data quality. Data cascades are pervasive (92% prevalence), invisible, delayed, but often avoidable. We discuss HCI opportunities in designing and incentivizing data excellence as a first-class citizen of AI, resulting in safer and more robust systems for all.",2021,,10.1145/3411764.3445518
2217,"Palumbo, Rachel and Thompson, Laura and Thakur, Gautam",SONET: A Semantic Ontological Network Graph for Managing Points of Interest Data Heterogeneity,"points of interest, openstreetmap, ontology, graph database, big data","Scalability, standardization, and management are important issues when working with very large Volunteered Geographic Information (VGI). VGI is a rich and valuable source of Points of Interest (POI) information, but its inherent heterogeneity in content, structure, and scale across sources present major challenges for interlinking data sources for analysis. To be useful at scale, the raw information needs to be transformed into a standardized schema that can be easily and reliably used by data analysts. In this work, we tackle the problem of unifying POI categories (e.g. restaurants, temple, and hotel) across multiple data sources to aid in improving land use maps and population distribution estimation as well as support data analysts wishing to fuse multiple data sources with the OpenStreetMap (OSM) mapping platform or working with projects that are already configured in the OSM schema and wish to add additional sources of information. Graph theory and its implementation through the SONET graph database, provides a programmatic way to organize, store, and retrieve standardized POI categories at multiple levels of abstraction. Additionally, it addresses category heterogeneity across data sources by standardizing and managing categories in a way that makes cross-domain analysis possible.",2019,,10.1145/3356991.3365474
2218,"Birkel, Hendrik and Kopyto, Matthias and Lutz, Corinna",Challenges of Applying Predictive Analytics in Transport Logistics,"big data, challenges, predictive analytics, supply chain management, transport logistics","The field of Predictive Analytics (PA) provides the possibility to utilize large amounts of data to improve forecasting, data-driven decision-making, and competitive advantage. Especially the transport logistics sector, which is characterized by high business-related uncertainties, time-sensitivity, and volatility, highly benefits from accurate resource and production planning. While success factors and framework conditions of applying PA are well-investigated on a theoretical SCM level, findings on internal and external challenges of transport logistics organizations remain scarce. Therefore, based on a multiple case approach, this study offers in-depth insights into six real-world cases of freight forwarders, ocean carriers, and air carriers. The results uncover both internal and external challenges. From the internal perspective, the biggest challenges are related to the technical implementation including the acquisition of globally generated, internal and external data and its harmonization. In addition, stakeholder management and target setting impede the development of PA. Regarding external challenges, relational and external conditions hamper the application. Therefore, especially actions of third-party institutions in terms of standardization and security enhancements are required. This study contributes to the existing literature in various ways as the systematic identification addresses real-world issues of PA in the neglected but crucial area of transport logistics, discussing urgent research needs and highlighting potential solutions. Additionally, the results offer valuable guidance for managers when implementing PA in transport logistics.",2020,,10.1145/3378539.3393864
2219,"Huang, Qunying and Cao, Guofeng and Wang, Caixia",From Where Do Tweets Originate? A GIS Approach for User Location Inference,"big data, geography, spatial clustering, spatiotemporal clustering, human mobility","A number of natural language processing and text-mining algorithms have been developed to extract the geospatial cues (e.g., place names) to infer locations of content creators from publicly available information, such as text content, online social profiles, and the behaviors or interactions of users from social networks. These studies, however, can only successfully infer user locations at city levels with relatively decent accuracy, while much higher resolution is required for meaningful spatiotemporal analysis in geospatial fields. Additionally, geographical cues exploited by current text-based approaches are hidden in the unreliable, unstructured, informal, ungrammatical, and multilingual data, and therefore are hard to extract and make meaningful correctly. Instead of using such hidden geographic cues, this paper develops a GIS approach that can infer the true origin of tweets down to the zip code level by using and mining spatial (geo-tags) and temporal (timestamps when a message was posted) information recorded on user digital footprints. Further, individual major daily activity zones and mobility can be successfully inferred and predicted. By integrating GIS data and spatiotemporal clustering methods, this proposed approach can infer individual daily physical activity zones with spatial resolution as high as 20 m by 20 m or even higher depending on the number of digit footprints collected for social media users. The research results with detailed spatial resolution are necessary and useful for various applications such as human mobility pattern analysis, business site selection, disease control, or transportation systems improvement.",2014,,10.1145/2755492.2755494
2220,"Chu, Xu and Ilyas, Ihab F. and Krishnan, Sanjay and Wang, Jiannan",Data Cleaning: Overview and Emerging Challenges,"integrity constraints, data cleaning, sampling, data quality, statistical cleaning","Detecting and repairing dirty data is one of the perennial challenges in data analytics, and failure to do so can result in inaccurate analytics and unreliable decisions. Over the past few years, there has been a surge of interest from both industry and academia on data cleaning problems including new abstractions, interfaces, approaches for scalability, and statistical techniques. To better understand the new advances in the field, we will first present a taxonomy of the data cleaning literature in which we highlight the recent interest in techniques that use constraints, rules, or patterns to detect errors, which we call qualitative data cleaning. We will describe the state-of-the-art techniques and also highlight their limitations with a series of illustrative examples. While traditionally such approaches are distinct from quantitative approaches such as outlier detection, we also discuss recent work that casts such approaches into a statistical estimation framework including: using Machine Learning to improve the efficiency and accuracy of data cleaning and considering the effects of data cleaning on statistical analysis.",2016,,10.1145/2882903.2912574
2221,"Mountantonakis, Michalis and Tzitzikas, Yannis",Large-Scale Semantic Integration of Linked Data: A Survey,"Data integration, RDF, big data, data discovery, semantic web","A large number of published datasets (or sources) that follow Linked Data principles is currently available and this number grows rapidly. However, the major target of Linked Data, i.e., linking and integration, is not easy to achieve. In general, information integration is difficult, because (a) datasets are produced, kept, or managed by different organizations using different models, schemas, or formats, (b) the same real-world entities or relationships are referred with different URIs or names and in different natural languages,&lt;?brk?&gt;(c) datasets usually contain complementary information, (d) datasets can contain data that are erroneous, out-of-date, or conflicting, (e) datasets even about the same domain may follow different conceptualizations of the domain, (f) everything can change (e.g., schemas, data) as time passes. This article surveys the work that has been done in the area of Linked Data integration, it identifies the main actors and use cases, it analyzes and factorizes the integration process according to various dimensions, and it discusses the methods that are used in each step. Emphasis is given on methods that can be used for integrating several datasets. Based on this analysis, the article concludes with directions that are worth further research.",2019,,10.1145/3345551
2222,"Ferguson, Holly T. and Vardeman, Charles F. and Buccellato, Aimee P. C.",Capturing an Architectural Knowledge Base Utilizing Rules Engine Integration for Energy and Environmental Simulations,"semantic web, machine learning, experimentation, design, RIF, OWL, HCI, linked data, SWIRL, standardization, ontological knowledge engine, SPARAQL, performance, expert systems, green scale tool, verification, SPIN, big data, algorithms, sustainable data, REST, PyKE, reliability, knowledge based rules","The era of ""Big Data"" presents new challenges and opportunities to impact how the built environment is designed and constructed. Modern design tools and material databases should be more scalable, reliable, and accessible to take full advantage of the quantity of available building data. New approaches providing well-structured information can lead to robust decision support for architectural simulations earlier in the design process; rule-based decision engines and knowledge bases are the link between current data and useful decision frameworks. Integrating distributed API-based systems means that material data silos existing in modern tools can become enriched and extensible for future use with additional data from building documents, other databases, and the minds of design professionals. The PyKE rules engine extension to the Green Scale (GS) Tool improves material searches, creates the opportunity for incorporating additional rules via a REST interface, and enables integration with the Semantic Web via Linked Data principles.",2015,,
2223,"Agrawal, Divy and Ba, Lamine and Berti-Equille, Laure and Chawla, Sanjay and Elmagarmid, Ahmed and Hammady, Hossam and Idris, Yasser and Kaoudi, Zoi and Khayyat, Zuhair and Kruse, Sebastian and Ouzzani, Mourad and Papotti, Paolo and Quiane-Ruiz, Jorge-Arnulfo and Tang, Nan and Zaki, Mohammed J.",Rheem: Enabling Multi-Platform Task Execution,"big data, cross-platform execution, data analytics","Many emerging applications, from domains such as healthcare and oil &amp; gas, require several data processing systems for complex analytics. This demo paper showcases system, a framework that provides multi-platform task execution for such applications. It features a three-layer data processing abstraction and a new query optimization approach for multi-platform settings. We will demonstrate the strengths of system by using real-world scenarios from three different applications, namely, machine learning, data cleaning, and data fusion.",2016,,10.1145/2882903.2899414
2224,"Harley, Kelsey and Cooper, Rodney",Information Integrity: Are We There Yet?,"Clark-Wilson model, Integrity, information flow, noninterference, data quality, quality assessment, information security, quality dimension, information integrity, security requirements, information trustworthiness, information quality, Biba’s model","The understanding and promotion of integrity in information security has traditionally been underemphasized or even ignored. From implantable medical devices and electronic voting to vehicle control, the critical importance of information integrity to our well-being has compelled review of its treatment in the literature. Through formal information flow models, the data modification view, and the relationship to data quality, information integrity will be surveyed. Illustrations are given for databases and information trustworthiness. Integrity protection is advancing but lacks standardization in terminology and application. Integrity must be better understood, and pursued, to achieve devices and systems that are beneficial and safe for the future.",2021,,10.1145/3436817
2225,"Alaa, Mostafa and Bolock, Alia El and Abas, Mostafa and Abdennadher, Slim and Herbert, Cornelia",AppGen: A Framework for Automatic Generation of Data Collection Apps,,"Data, and its collection, is one core aspect of technology and research, nowadays. Various scientific disciplines are interested in collecting human data in practically any context (at home, at work, during leisure time). For example, experts from the field of Psychology design studies for reliable and valid data collection in the laboratory and in the wild. We propose a generic platform for data-collection software development to be used by scientists without a programming background. This is done by adapting a basic Unity project through a configuration file provided by the platform users through an easy to use user interface. The scientific user can adapt and rearrange pre-defined data collection modules targeting a desired research question, implement it as application within the data collection platform and use and manage the application for data collection and later data analysis. As a proof of concept, the platform was embedded with build-in application modules for wide-spread Psychology data collection experiments. The versatility of the platform was tested by creating three diverse prototypical applications. Finally, the usability of the proposed platform evaluated using the System Usability Scale obtained high usability results. The robust module-based nature of the platform architecture makes is possible to create a various range of of psychologically-proven applications with different features to be decided by the researcher. This holds true for both the development phase of the applications, as well as, for after deployment.",2020,,
2226,"Xin, SHEN and Yang, Hongxia and Xian, Weizhao and Ester, Martin and Bu, Jiajun and Wang, Zhongyao and Wang, Can",Mobile Access Record Resolution on Large-Scale Identifier-Linkage Graphs,"scalable algorithms, big data, graph algorithms, mobile access record resolution","The e-commerce era is witnessing a rapid increase of mobile Internet users. Major e-commerce companies nowadays see billions of mobile accesses every day. Hidden in these records are valuable user behavioral characteristics such as their shopping preferences and browsing patterns. And, to extract these knowledge from the huge dataset, we need to first link records to the corresponding mobile devices. This Mobile Access Records Resolution (MARR) problem is confronted with two major challenges: (1) device identifiers and other attributes in access records might be missing or unreliable; (2) the dataset contains billions of access records from millions of devices. To the best of our knowledge, as a novel challenge industrial problem of mobile Internet, no existing method has been developed to resolve entities using mobile device identifiers in such a massive scale. To address these issues, we propose a SParse Identifier-linkage Graph (SPI-Graph) accompanied with the abundant mobile device profiling data to accurately match mobile access records to devices. Furthermore, two versions (unsupervised and semi-supervised) of Parallel Graph-based Record Resolution (PGRR) algorithm are developed to effectively exploit the advantages of the large-scale server clusters comprising of more than 1,000 computing nodes. We empirically show superior performances of PGRR algorithms in a very challenging and sparse real data set containing 5.28 million nodes and 31.06 million edges from 2.15 billion access records compared to other state-of-the-arts methodologies.",2018,,10.1145/3219819.3219916
2227,"Bowyer, Alex and Montague, Kyle and Wheater, Stuart and McGovern, Ruth and Lingam, Raghu and Balaam, Madeline","Understanding the Family Perspective on the Storage, Sharing and Handling of Family Civic Data","social care, data security, personal data, ubicomp, data privacy, ethnographic interviews, design games, dynamic consent, data sharing, family design games, big data, family, user-centered design, civic data, healthcare, boundary objects, family research","Across social care, healthcare and public policy, enabled by the ""big data"" revolution (which has normalized large-scale data-based decision-making), there are moves to ""join up"" citizen databases to provide care workers with holistic views of families they support. In this context, questions of personal data privacy, security, access, control and (dis-)empowerment are critical considerations for system designers and policy makers alike. To explore the family perspective on this landscape of what we call Family Civic Data, we carried out ethnographic interviews with four North-East families. Our design-game-based interviews were effective for engaging both adults and children to talk about the impact of this dry, technical topic on their lives. Our findings, delivered in the form of design guidelines, show support for dynamic consent: families would feel most empowered if involved in an ongoing co-operative relationship with state welfare and civic authorities through shared interaction with their data.",2018,,10.1145/3173574.3173710
2228,"Bj\""{o}rk, Kaj-Mikael and Eirola, Emil and Miche, Yoan and Lendasse, Amaury",A New Application of Machine Learning in Health Care,"Big Data, Health care, Missing values, Huntington's disease, Machine Learning","In our ever more complex world, the field of analytics has dramatically increased its importance. Gut feeling is no longer sufficient in decision making, but intuition has to be combined with support from the huge amount of data available today. Even if the amount of data is enormous, the quality of the data is not always good. Problems arise in at least two situations: i) the data is imprecise by nature and ii) the data is incomplete (or there are missing parts in the data set). Both situations are problematic and need to be addressed appropriately. If these problems are solved, applications are to be found in various interesting fields. We aim at achieving significant methodology development as well as creative solutions in the domain of medicine, information systems and risk management. This paper sets focus especially on missing data problems in the field of medicine when presenting a new project in its very first phase.",2016,,10.1145/2910674.2935861
2229,"Fei, Yiming and Yuan, Xiaoyue and Ren, Mengmeng and Fan, Shuhai",Research on Horizontal Integration Scheme for Mass Customization Data Quantity and Quality Problem: Horizontal Integration Scheme for MC,"Data Quality, LiDAR Camera Technology, Horizontal Integration, Mass Customization","To solve the problem of Mass Customization Data Quantity and Quality Problem, a Horizontal Integration Scheme of MC is proposed. By using LiDAR technology to scan and identify parts information, the machining route and required parts of the workpiece are automatically planned by comparing and matching with the documents using STEP-NC standard, to realize the efficient acquisition and utilization of MC data and ensure the automation of enterprise production.",2021,,10.1145/3478905.3478920
2230,"Coletti, Paolo and Murgia, Maurizio",Design and Construction of a Historical Financial Database of the Italian Stock Market 1973--2011,"stock market, data quality, Financial database, data integration","This article presents the technical aspects of designing and building a historical database of the Italian Stock Market. The database contains daily market data from 1973 to 2011 and is constructed by merging two main digital sources and several other hand-collected data sources. We analyzed and developed semiautomatic tools to deal with problems related to time-series matchings, quality of data, and numerical errors. We also developed a concatenation structure to allow the handling of company name changes, mergers, and spin-offs without artificially altering numerical series. At the same time, we maintained the transparency of the historical information on each individual company listed. Thanks to the overlapping of digital and hand-collected data, the completed database has a very high level of detail and accuracy. The dataset is particularly suited for any empirical research in financial economics and for more practically oriented numerical applications and forecasting simulations.",2015,,10.1145/2822898
2231,"Ukil, Arijit and Marin, Leandro and Jara, Antonio and Farserotu, John",On the Knowledge-Driven Analytics and Systems Impacting Human Quality of Life,"sensors, artificial intelligence, knowledge, security, human life, big data, deep learning, privacy","The present scenario of Covid-19 pandemic has disrupted the human life to a larger extent. In such context, human-centric applications and systems that endeavor to positively impact the human quality of life is of utmost importance. Knowledge-driven analytics that help to build such intelligent systems play important role to construct the required eco-system on the macro-scale. It is worth mentioning that Knowledge-Driven Analytics and Systems Impacting Human Quality of Life (KDAH) workshop in ACM International Conference on Information and Knowledge Management (CIKM), attempts to bring out the intricate research direction for enabling a sustainable human society through the positive co-existence of human beings and intelligent systems.",2020,,10.1145/3340531.3414077
2232,"Chabin, Jacques and Gomes-Jr., Luiz and Halfeld-Ferrari, Mirian",A Context-Driven Querying System for Urban Graph Analysis,"data graph, data quality, smart city, Query language, constraints","This paper presents a context-driven query system for urban computing where users are responsible for defining their own restrictions over which datalog-like queries are built. Instead of imposing constraints on databases, our goal is to filter consistent data during the query process. Our query language is able to express aggregates in recursive rules, allowing it to capture network properties typical of graph analysis. This paper presents our query system and analyzes its capabilities using use cases in Urban Computing.",2018,,10.1145/3216122.3216148
2233,"Martins, Pedro and Cec\'{\i}lio, Jos\'{e} and Abbasi, Maryam and Furtado, Pedro",GPII: A Benchmark for Generic Purpose Image Information,"Benchmark, performance, pattern-detection, experimentation, algorithms, GIS, Big-data, spatio-temporal databases","The growing number of different models and approaches for Geographic Information Systems (GIS) brings high complexity when we want to develop new approaches and compare a new GIS algorithm. In order to test and compare different processing models and approaches, in a simple way, we identified the need of defining uniform testing methods, able to compare processing algorithms in terms of performance and accuracy regarding large image processing, algorithms for GIS pattern-detection.Taking into account, for instance, images collected during a done flight or a satellite, it is important to know the processing cost to extract data when applying different processing models and approaches, as well as their accuracy (compare execution time vs. extracted data quality). In this work, we propose a GIS Benchmark (GPII), a benchmark that allows evaluating different approaches to detect/extract selected features from a GIS dataset. Considering a given dataset (or two data-sets, from different years, of the same region), it provides linear methods to compare different performance parameters regarding GIS information, making possible to access the most relevant information in terms of features and processing efficiency. Moreover, our approach to test algorithms makes possible to change the data-set in order to support different purpose algorithms.",2016,,10.1145/2948992.2949009
2234,"Xu, Tong and Zhu, Hengshu and Zhao, Xiangyu and Liu, Qi and Zhong, Hao and Chen, Enhong and Xiong, Hui",Taxi Driving Behavior Analysis in Latent Vehicle-to-Vehicle Networks: A Social Influence Perspective,"social influence, taxi trajectories, mobile data mining","With recent advances in mobile and sensor technologies, a large amount of efforts have been made on developing intelligent applications for taxi drivers, which provide beneficial guide and opportunity to improve the profit and work efficiency. However, limited scopes focus on the latent social interaction within cab drivers, and corresponding social propagation scheme to share driving behaviors has been largely ignored. To that end, in this paper, we propose a comprehensive study to reveal how the social propagation affects for better prediction of cab drivers' future behaviors. To be specific, we first investigate the correlation between drivers' skills and their mutual interactions in the latent vehicle-to-vehicle network, which intuitively indicates the effects of social influences. Along this line, by leveraging the classic social influence theory, we develop a two-stage framework for quantitatively revealing the latent driving pattern propagation within taxi drivers. Comprehensive experiments on a real-word data set collected from the New York City clearly validate the effectiveness of our proposed framework on predicting future taxi driving behaviors, which also support the hypothesis that social factors indeed improve the predictability of driving behaviors.",2016,,10.1145/2939672.2939799
2235,"Hohmann, Matthias R. and Hackl, Michelle and Wirth, Brian and Zaman, Talha and Enficiaud, Raffi and Grosse-Wentrup, Moritz and Sch\""{o}lkopf, Bernhard",MYND: A Platform for Large-Scale Neuroscientific Studies,"user-centered design, neuroscience, electrophysiology, smartphone application, wearable sensors, medical studies, big data","We present a smartphone application for at-home participation in large-scale neuroscientific studies. Our goal is to establish user-experience design as a paradigm in basic neuroscientific research to overcome the limits of current studies, especially in rare neurological disorders.The presented application guides users through the fitting procedure of the EEG headset and automatically encrypts and uploads recorded data to a remote server. User-feedback and neurophysiological data from a pilot study with eighteen subjects indicate that the application can be used outside of a laboratory, without the need for external guidance. We hope to inspire future work on the intersection between basic neuroscience and human-computer interaction as a promising paradigm to accelerate research on rare neurological diseases and assistive neurotechnology.",2019,,10.1145/3290607.3313002
2236,"Shemshadi, Ali and Sheng, Quan Z. and Qin, Yongrui and Sun, Aixin and Zhang, Wei Emma and Yao, Lina",Searching for the Internet of Things: Where It is and What It Looks Like,"Information retrieval, Web mapping, Internet of things, Big data, Web of things","The Internet of Things (IoT), in general, is a compelling paradigm that aims to connect everyday objects to the Internet. Nowadays, IoT is considered as one of the main technologies which contribute towards reshaping our daily lives in the next decade. IoT unlocks many exciting new opportunities in a variety of applications in research and industry domains. However, many have complained about the absence of the real-world IoT data. Unsurprisingly, a common question that arises regularly nowadays is ""Does the IoT already exist?"". So far, little has been known about the real-world situation on IoT, its attributes, the presentation of data, and user interests. To answer this question, in this work, we conduct an in-depth analytical investigation on real IoT data. More specifically, we identify IoT data sources over the Web and develop a crawler engine to collect large-scale real-world IoT data for the first time. We make the results of our work available to the public in order to assist the community in the future research. In particular, we collect the data of nearly two million Internet connected objects and study trends in IoT using a real-world query set from an IoT search engine. Based on the collected data and our analysis, we identify the typical characteristics of IoT data. The most intriguing finding of our study is that IoT data is mainly disseminated using Web Mapping while the emerging IoT solutions such as the Web of Things are currently not well adopted. On top of our findings, we further discuss future challenges and open research problems in the IoT area.",2017,,10.1007/s00779-017-1034-0
2237,"Garriga, Martin and Aarns, Koen and Tsigkanos, Christos and Tamburri, Damian A. and Heuvel, Wjan Van Den",DataOps for Cyber-Physical Systems Governance: The Airport Passenger Flow Case,"big data, DataOps, airport management, Data-intensive systems, systems governance, cyber-physical systems","Recent advancements in information technology have ushered a new wave of systems integrating Internet technology with sensing, wireless communication, and computational resources over existing infrastructures. As a result, myriad complex, non-traditional Cyber-Physical Systems (CPS) have emerged, characterized by interaction among people, physical facilities, and embedded sensors and computers, all generating vast amounts of complex data. Such a case is encountered within a contemporary airport hall setting: passengers roaming, information systems governing various functions, and data being generated and processed by cameras, phones, sensors, and other Internet of Things technology. This setting has considerable potential of contributing to goals entertained by the CPS operators, such as airlines, airport operators/owners, technicians, users, and more. We model the airport setting as an instance of such a complex, data-intensive CPS where multiple actors and data sources interact, and generalize a methodology to support it and other similar systems. Furthermore, this article instantiates the methodology and pipeline for predictive analytics for passenger flow, as a characteristic manifestation of such systems requiring a tailored approach. Our methodology also draws from DataOps principles, using multi-modal and real-life data to predict the underlying distribution of the passenger flow on a flight-level basis (improving existing day-level predictions), anticipating when and how the passengers enter the airport and move through the check-in and baggage drop-off process. This allows to plan airport resources more efficiently while improving customer experience by avoiding passenger clumping at check-in and security. We demonstrate results obtained over a case from a major international airport in the Netherlands, improving up to 60% upon predictions of daily passenger flow currently in place.",2021,,10.1145/3432247
2238,"Maqboul, Jaouad and Jaouad, Bouchaib Bounabat",Contribution of Artificial Neural Network in Predicting Completeness Through the Impact and Complexity of Its Improvement,"cost/benefit analysis, Data quality improvement project, artificial neural network, cost of data quality, data quality assessment and improvement","The technological evolution and the immensity of the data produced, circulated into company makes these data, the real capital of the companies to the detriment of the customers. The erroneous data put the knockout to relationships with customers, the company must address this problem and identify the quality projects on which it must make an effort. In this article, we will present an approach based on qualitative and quantitative analysis to help the decision-makers to target data by its impacts and complexities of process improvement. The Qualitative study will be a survey and a quantitative to learn from survey data to decide the prediction and the completeness of data.",2020,,10.1145/3386723.3387850
2239,"Zhang, Xu and Xu, Yong and Lin, Qingwei and Qiao, Bo and Zhang, Hongyu and Dang, Yingnong and Xie, Chunyu and Yang, Xinsheng and Cheng, Qian and Li, Ze and Chen, Junjie and He, Xiaoting and Yao, Randolph and Lou, Jian-Guang and Chintalapati, Murali and Shen, Furao and Zhang, Dongmei",Robust Log-Based Anomaly Detection on Unstable Log Data,"Log Analysis, Anomaly Detection, Deep Learning, Data Quality, Log Instability","Logs are widely used by large and complex software-intensive systems for troubleshooting. There have been a lot of studies on log-based anomaly detection. To detect the anomalies, the existing methods mainly construct a detection model using log event data extracted from historical logs. However, we find that the existing methods do not work well in practice. These methods have the close-world assumption, which assumes that the log data is stable over time and the set of distinct log events is known. However, our empirical study shows that in practice, log data often contains previously unseen log events or log sequences. The instability of log data comes from two sources: 1) the evolution of logging statements, and 2) the processing noise in log data. In this paper, we propose a new log-based anomaly detection approach, called LogRobust. LogRobust extracts semantic information of log events and represents them as semantic vectors. It then detects anomalies by utilizing an attention-based Bi-LSTM model, which has the ability to capture the contextual information in the log sequences and automatically learn the importance of different log events. In this way, LogRobust is able to identify and handle unstable log events and sequences. We have evaluated LogRobust using logs collected from the Hadoop system and an actual online service system of Microsoft. The experimental results show that the proposed approach can well address the problem of log instability and achieve accurate and robust results on real-world, ever-changing log data.",2019,,10.1145/3338906.3338931
2240,"Zhu, Ruyi",Traffic Condition Prediction of Urban Roads Based on Neural Network,"Urban road system, real-time traffic condition, big data, video surveillance system, forecasting","Real-time and reliable traffic flow estimation is the basis of urban traffic management and control. However, the existing research focuses on how to use the historical data of surveillance intersection to predict future traffic conditions. As we know, there are few effective algorithms to infer the real-time traffic state of non-surveillance intersections from limited road surveillance by using traffic information in the urban road system. In this paper, we introduce a new solution to solve the prediction task of traffic flow analysis by using traffic data, especially taxi historical data, traffic network data and intersection historical data. The proposed solution takes advantage of GCN and CGAN, and we improved the Unet to realize an important part of the generator. Then, we capture the relationship between the intersections with surveillance and the intersections without surveillance by floating taxi-cabs covered in the whole city. The framework of CGAN can adjust the weights and enhance the inference ability to generate complete traffic status under current conditions. The experimental results show that our method is superior to other methods on the accuracy of traffic volume inference.",2020,,10.1145/3404555.3404621
2241,"Chen, Zhangbin and Liu, Yang",Research and Construction of University Data Governance Platform Based on Smart Campus Environment,,"Campus data is a subset of education big data. It is a variety of data generated by teachers and students in the life, teaching, scientific research, management and service process, as well as various school affairs management status data. It has the characteristics of a wide variety of data. Contains great information value, and giving full play to its role is an indispensable part of achieving the school's strategic goals. Taking the opportunity of building a smart campus, using advanced technologies such as cloud computing, big data, Internet of Things, and artificial intelligence, through a big data management platform, the full collection of existing business data inside and outside the school is provided, and a normal data governance model is provided to eliminate data islands. Realize normal data sharing services. The article conducts research on the data governance platform, and builds a data governance platform system with functions such as a normalized data quality monitoring system, information resource catalog system and full-link data monitoring to realize university data integration, business collaboration, service upgrades and assistance Decision-making provides a solid foundation for the application and expansion of smart campuses in universities and provides a reference for smart campus builders in universities.",2021,,10.1145/3495018.3495097
2242,"Frischbier, Sebastian and Paic, Mario and Echler, Alexander and Roth, Christian",A Real-World Distributed Infrastructure for Processing Financial Data at Scale,"publish/subscribe, stream-processing, infrastructure, quality of information, financial data, broker network, big data, Event-processing","Financial markets are event- and data-driven to an extremely high degree. For making decisions and triggering actions stakeholders require notifications about significant events and reliable background information that meet their individual requirements in terms of timeliness, accuracy, and completeness. As one of Europe's leading providers of financial data and regulatory solutions vwd: processes an average of 18 billion event notifications from 500+ data sources for 30 million symbols per day. Our large-scale distributed event-based systems handle daily peak rates of 1+ million event notifications per second and additional load generated by singular pivotal events with global impact.In this poster we give practical insights into our IT systems. We outline the infrastructure we operate and the event-driven architecture we apply at vwd. In particular we showcase the (geo)distributed publish/subscribe broker network we operate across locations and countries to provide market data to our customers with varying quality of information (QoI) properties.",2019,,10.1145/3328905.3332513
2243,"Cao, Paul Y. and Li, Gang and Chen, Guoxing and Chen, Biao",Mobile Data Collection Frameworks: A Survey,"data collection framework, mobile data","Mobile phones equipped with powerful sensors have become ubiquitous in recent years. Mobile sensing applications present an unprecedented opportunity to collect and analyze information from mobile devices. Much of the work in mobile sensing has been done on designing monolithic applications but inadequate attention has been paid to general mobile data collection frameworks. In this paper, we provide a survey on how to build a general purpose mobile data collection framework. We identify the basic requirements and present an architecture for such a framework. We survey existing works to summarize existing approaches to address the basic requirements. Eight major mobile data collection frameworks are compared with respect to the requirements as well as additional issues on privacy, energy and incentives.",2015,,10.1145/2757384.2757396
2244,"Khan, Suleman and Gani, Abdullah and Wahab, Ainuddin Wahid Abdul and Bagiwa, Mustapha Aminu and Shiraz, Muhammad and Khan, Samee U. and Buyya, Rajkumar and Zomaya, Albert Y.","Cloud Log Forensics: Foundations, State of the Art, and Future Directions","confidentiality, authenticity, cloud log forensics, integrity, Cloud computing, correlation of cloud logs, big data","Cloud log forensics (CLF) mitigates the investigation process by identifying the malicious behavior of attackers through profound cloud log analysis. However, the accessibility attributes of cloud logs obstruct accomplishment of the goal to investigate cloud logs for various susceptibilities. Accessibility involves the issues of cloud log access, selection of proper cloud log file, cloud log data integrity, and trustworthiness of cloud logs. Therefore, forensic investigators of cloud log files are dependent on cloud service providers (CSPs) to get access of different cloud logs. Accessing cloud logs from outside the cloud without depending on the CSP is a challenging research area, whereas the increase in cloud attacks has increased the need for CLF to investigate the malicious activities of attackers. This paper reviews the state of the art of CLF and highlights different challenges and issues involved in investigating cloud log data. The logging mode, the importance of CLF, and cloud log-as-a-service are introduced. Moreover, case studies related to CLF are explained to highlight the practical implementation of cloud log investigation for analyzing malicious behaviors. The CLF security requirements, vulnerability points, and challenges are identified to tolerate different cloud log susceptibilities. We identify and introduce challenges and future directions to highlight open research areas of CLF for motivating investigators, academicians, and researchers to investigate them.",2016,,10.1145/2906149
2245,"Bonifati, Angela and Holubov\'{a}, Irena and Prat-P\'{e}rez, Arnau and Sakr, Sherif",Graph Generators: State of the Art and Open Challenges,"benchmarks, Big data management, generators, graph data, synthetic data","The abundance of interconnected data has fueled the design and implementation of graph generators reproducing real-world linking properties or gauging the effectiveness of graph algorithms, techniques, and applications manipulating these data. We consider graph generation across multiple subfields, such as Semantic Web, graph databases, social networks, and community detection, along with general graphs. Despite the disparate requirements of modern graph generators throughout these communities, we analyze them under a common umbrella, reaching out the functionalities, the practical usage, and their supported operations. We argue that this classification is serving the need of providing scientists, researchers, and practitioners with the right data generator at hand for their work. This survey provides a comprehensive overview of the state-of-the-art graph generators by focusing on those that are pertinent and suitable for several data-intensive tasks. Finally, we discuss open challenges and missing requirements of current graph generators along with their future extensions to new emerging fields.",2020,,10.1145/3379445
2246,"Esteva, Maria and Xu, Weijia and Simone, Nevan and Gupta, Amit and Jah, Moriba",Modeling Data Curation to Scientific Inquiry: A Case Study for Multimodal Data Integration,,"Scientific data publications may include interactive data applications designed by scientists to explore a scientific problem. Defined as knowledge systems, their development is complex when data are aggregated from multiple sources over time. Multimodal data are created, encoded, and maintained differently, and even when reporting about identical phenomena, fields and their values may be inconsistent across datasets. To assure the validity and accuracy of the application, the data has to abide by curation requirements similar to those ruling digital libraries. We present a novel, inquiry-driven curation approach aimed to optimize multimodal datasets curation and maximize data reuse by domain researchers. We demonstrate the method through the ASTRIAGraph project, in which multiple data sources about near earth space objects are aggregated into a central knowledge system. The process involves multidisciplinary collaboration, resulting in the design of a data model as the backbone for both data curation and scientific inquiry. We demonstrate a) how data provenance information is needed to assess the uncertainty of the results of scientific inquiries involving multiple data sources, and b) that continuous curation of integrated datasets is facilitated when undertaken as integral to the research project. The approach provides flexibility to support expansion of scientific inquiries and data in the knowledge system, and allows for transparent and explainable results.",2020,,
2247,"Wieczorkowski, Jundefineddrzej",Barriers to Using Open Government Data,"OGD, LOD, Linked Data, Central Repository for Public Information, Open Data, Big Data, Linked Open Data, E-government, Open Government Data, CRPI","The article describes the issues of Open Government Data (OGD) and problems with the use of such data. Good quality and proper publishing of OGD enable (apart from the control function) their business use. This affects the economic benefits. The author has identified the main problems of data publication based on Central Repositories for Public Information (CRPI) in Poland, the USA, the UK and Germany. The article focuses on the maturity of data formats, automated processing with Application Programming Interface (API), using the concept of Linked Open Data (LOD). The aim of the article is to identify barriers to the implementation of OGD-based solutions and to indicate recommendations to overcome these barriers. The research shows that the methods of sharing OGD differ significantly between countries despite common guidelines. The main problem is the use of unstructured data, unsuitable for the use of LOD.",2019,,10.1145/3340017.3340022
2248,"Borgman, Christine L. and Darch, Peter T. and Sands, Ashley E. and Wallis, Jillian C. and Traweek, Sharon",The Ups and Downs of Knowledge Infrastructures in Science: Implications for Data Management,"big data, astronomy, little science, knowledge infrastructures, big science, biology, digital libraries, small science, sensor networks, data management","The promise of technology-enabled, data-intensive scholarship is predicated upon access to knowledge infrastructures that are not yet in place. Scientific data management requires expertise in the scientific domain and in organizing and retrieving complex research objects. The Knowledge Infrastructures project compares data management activities of four large, distributed, multidisciplinary scientific endeavors as they ramp their activities up or down; two are big science and two are small science. Research questions address digital library solutions, knowledge infrastructure concerns, issues specific to individual domains, and common problems across domains. Findings are based on interviews (n=113 to date), ethnography, and other analyses of these four cases, studied since 2002. Based on initial comparisons, we conclude that the roles of digital libraries in scientific data management often depend upon the scale of data, the scientific goals, and the temporal scale of the research projects being supported. Digital libraries serve immediate data management purposes in some projects and long-term stewardship in others. In small science projects, data management tools are selected, designed, and used by the same individuals. In the multi-decade time scale of some big science research, data management technologies, policies, and practices are designed for anticipated future uses and users. The need for library, archival, and digital library expertise is apparent throughout all four of these cases. Managing research data is a knowledge infrastructure problem beyond the scope of individual researchers or projects. The real challenges lie in designing digital libraries to assist in the capture, management, interpretation, use, reuse, and stewardship of research data.",2014,,
2249,"Chu, Xu and Ilyas, Ihab F.",Qualitative Data Cleaning,,"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and wrong business decisions.Data cleaning exercise often consist of two phases: error detection and error repairing. Error detection techniques can either be quantitative or qualitative; and error repairing is performed by applying data transformation scripts or by involving human experts, and sometimes both.In this tutorial, we discuss the main facets and directions in designing qualitative data cleaning techniques. We present a taxonomy of current qualitative error detection techniques, as well as a taxonomy of current data repairing techniques. We will also discuss proposals for tackling the challenges for cleaning ""big data"" in terms of scale and distribution.",2016,,10.14778/3007263.3007320
2250,"Hongmeng, Zhang and Zhiqiang, Zhu and Lei, Sun and Xiuqing, Mao and Yuehan, Wang",A Detection Method for DeepFake Hard Compressed Videos Based on Super-Resolution Reconstruction Using CNN,"Deep Learning, DeepFake detection, Super-resolution reconstruction, Hard compressed video","The DeepFake video detection method based on convolutional neural networks has a poor performance in the dataset of hard compressed DeepFake video. And a large number of false tests will occur to the real data. To solve this problem, a networks model detection method for super-resolution reconstruction of DeepFake video is proposed. First of all, the face area of real data is processed by Gaussian blur, which is converted into negative data, and the real data and processing data are input into neural network for training. Then the residual network is used for super-resolution reconstruction of test data. Finally, the trained model is used to test the video after super-resolution reconstruction. Experiments show that the proposed method can reduce the false detection rate and improve the accuracy in detection of single frames.",2020,,10.1145/3409501.3409542
2251,"Silva, Thiago H. and Viana, Aline Carneiro and Benevenuto, Fabr\'{\i}cio and Villas, Leandro and Salles, Juliana and Loureiro, Antonio and Quercia, Daniele",Urban Computing Leveraging Location-Based Social Network Data: A Survey,"urban informatics, city dynamics, urban sensing, location-based social networks, Urban computing, urban societies, big data",Urban computing is an emerging area of investigation in which researchers study cities using digital data. Location-Based Social Networks (LBSNs) generate one specific type of digital data that offers unprecedented geographic and temporal resolutions. We discuss fundamental concepts of urban computing leveraging LBSN data and present a survey of recent urban computing studies that make use of LBSN data. We also point out the opportunities and challenges that those studies open.,2019,,10.1145/3301284
2252,"Deng, Alex and Li, Yicheng and Lu, Jiannan and Ramamurthy, Vivek",On Post-Selection Inference in A/B Testing,"A/B testing, bias correction, post-selection inference, winner's curse, machine learning, online metrics, randomization, big data, regression, empirical Bayes","When interpreting A/B tests, we typically focus only on the statistically significant results and take them by face value. This practice, termed post-selection inference in the statistical literature, may negatively affect both point estimation and uncertainty quantification, and therefore hinder trustworthy decision making in A/B testing. To address this issue, in this paper we explore two seemingly unrelated paths, one based on supervised machine learning and the other on empirical Bayes, and propose post-selection inferential approaches that combine the strengths of both. Through large-scale simulated and empirical examples, we demonstrate that our proposed methodologies stand out among other existing ones in both reducing post-selection biases and improving confidence interval coverage rates, and discuss how they can be conveniently adjusted to real-life scenarios.",2021,,10.1145/3447548.3467129
2253,"Kamel, Mohammed B. M. and Wallis, Kevin and Ligeti, Peter and Reich, Christoph",Distributed Data Validation Network in IoT: A Decentralized Validator Selection Model,"cluster-based data \^{A}\u{a}Validation, data validation, data validation network, distributed hash table, industrial internet of things, internet of things, big data","The generated real-time data on the Internet of Things (IoT) and the ability to gather and manipulate them are positively affecting various fields. One of the main concerns in IoT is how to provide trustworthy data. The data validation network ensures that the generated data by data sources in the IoT are trustworthy. However, the existing data validation network depends on a centralized entity for the selection of data validators. In this paper, a decentralized validator selection model is proposed. The proposed model creates multiple clusters using the distributed hash table (DHT) technique. The selection process of data validators from different clusters in the model is done randomly in a decentralized scheme. It provides a global method of assignment, selection, and verification of the selected validators in the network.",2020,,10.1145/3410992.3411027
2254,"Maziku, Hellen",Improved Data Accuracy Assessment Tool for Information Management Systems,"Information Management Systems, Human Centered Design, Data Quality Assessment, Accuracy","Developing countries are increasingly taking advantage of the rapid advancement in ICT to replace paper-based operations with Information Management Systems (IMS) such as District Health Information Software (DHIS). While the adoption of IMS presents significant benefits, challenges exist in the quality of IMS data. Inaccurate, incomplete or redundant data in IMS has misled organisations into making incorrect decisions leading to customer dissatisfaction and high cost implications. There is an urgent need for IMS stakeholders to have mechanisms of assessing the quality of data prior to data analysis, data sharing or decision making. Data Accuracy Assessment Tool (DAAT) assesses and identifies errors in a pair of context related datasets. DAAT provides ability for Data Managers to easily compare datasets by choosing attributes of their interest from a pool of diverse attributes that define the data. Through reports and visualization, the tool reveals the accuracy of data in real time using metrics such as validity, completeness and duplication of data. DAAT is scalable since it can be integrated with any IMS such as DHIS. The tool has been tested using four years Voluntary Medical Male Circumcision (VMMC) program data from JHPIEGO's AIDSFree project in Tanzania.",2020,,10.1145/3442555.3442579
2255,"Jiang, Yu and Deng, Dong and Wang, Jiannan and Li, Guoliang and Feng, Jianhua",Efficient Parallel Partition-Based Algorithms for Similarity Search and Join with Edit Distance Constraints,"similarity search, content filter, similarity join, parallel algorithms","The quantity of data in real-world applications is growing significantly while the data quality is still a big problem. Similarity search and similarity join are two important operations to address the poor data quality problem. Although many similarity search and join algorithms have been proposed, they did not utilize the abilities of modern hardware with multi-core processors. It calls for new parallel algorithms to enable multi-core processors to meet the high performance requirement of similarity search and join on big data. To this end, in this paper we propose parallel algorithms to support efficient similarity search and join with edit-distance constraints. We adopt the partition-based framework and extend it to support parallel similarity search and join on multi-core processors. We also develop two novel pruning techniques. We have implemented our algorithms and the experimental results on two real datasets show that our parallel algorithms achieve high performance and obtain good speedup.",2013,,10.1145/2457317.2457382
2256,"Yan, Cairong and Song, Yalong and Wang, Jian and Guo, Wenjing",Eliminating the Redundancy in MapReduce-Based Entity Resolution,"blocking, MapReduce, redundancy elimination, entity resolution","Entity resolution is the basic operation of data quality management, and the key step to find the value of data. The parallel data processing framework based on MapReduce can deal with the challenge brought by big data. However, there exist two important issues, avoiding redundant pairs led by the multi-pass blocking method and optimizing candidate pairs based on the transitive relations of similarity. In this paper, we propose a multi-signature based parallel entity resolution method, called multi-sig-er, which supports unstructured data and structured data. Two redundancy elimination strategies are adopted to prune the candidate pairs and reduce the number of similarity computation without affecting the resolution accuracy. Experimental results on real-world datasets show that our method tends to handle large datasets and it is more suitable for complex similarity computation than simple object matching.",2015,,10.1109/CCGrid.2015.24
2257,"Bowles, Juliana and Broccia, Giovanna and Nanni, Mirco","DataMod2020: 9th International Symposium ""From Data to Models and Back""","deep learning, text mining, process calculi, formal methods, big data analytics, processing mining, machine learning","DataMod 2020 aims to bring together practitioners and researchers from academia, industry and research institutions interested in the combined application of computational modelling methods with data-driven techniques from the areas of knowledge management, data mining and machine learning. Modelling methodologies of interest include automata, agents, Petri nets, process algebras and rewriting systems. Application domains include social systems, ecology, biology, medicine, smart cities, governance, security, education, software engineering, and any other field that deals with complex systems and large amounts of data. Papers can present research results in any of the themes of interest for the symposium as well as application experiences, tools and promising preliminary ideas. Papers dealing with synergistic approaches that integrate modelling and knowledge management/discovery or that exploit knowledge management/discovery to develop/syntesise system models are especially welcome.",2020,,10.1145/3340531.3414073
2258,"Soliman, Aiman and Terstriep, Jeffrey",Leveraging Geospatial Data Gateways to Support the Operational Application of Deep Learning Models: Vision Paper,"Scientific Reproducibility, Geospatial Data Gateway, Remote Sensing, Geospatial Big Data, Deep Learning, Image Preprocessing","Geospatial data providers have adopted a variety of science gateways as the primary method for accessing remote geospatial data. Early systems provided little more than a simple file transfer mechanism but over the past decade, advanced features were incorporated to allow users to retrieve data seamlessly without concern for native file formats, data resolution, or even spatial projections. However, the recent growth in Deep Learning models in the geospatial domains has exposed additional requirements for accessing geospatial repositories. In this paper we discussed the major data accessibility challenges faced by the Deep Learning community namely: (1) reproducibility of data preprocessing workflows, (2) optimizing data transfer between gateways and computational environments, and (3) minimizing local storage requirements using on-the-fly augmentation. In this paper, we present our vision of spatial data generators to act as middleware between geospatial data gateways and Deep Learning models. We propose advanced features for spatial data generators and describe how they could satisfy the data accessibility requirements of the geospatial Deep Learning community. Lastly, we argue that satisfying these data accessibility requirements will not only enhance the reproducibility of Deep Learning workflows and speed their development but will also improve the quality of training and prediction of operational Deep Learning models.",2020,,10.1145/3397536.3422232
2259,"Zhou, Xiaofang and Zheng, Kai and Jueng, Hoyoung and Xu, Jiajie and Sadiq, Shazia",Making Sense of Spatial Trajectories,"trajectory mining, trajectory data management, spatiotemporal database","Spatial trajectory data is widely available today. Over a sustained period of time, trajectory data has been collected from numerous GPS devices, smartphones, sensors and social media applications. Daily increases of real-time trajectory data have also been phenomenal in recent years. More and more new applications have emerged to derive business values from both trajectory data warehouses and real-time trajectory data. Due to their very large volumes, their nature of streaming, their highly variable levels of data quality, as well as many possible links with other types of data, making sense of spatial trajectory data becomes one of the crucial areas for big data analytics. In this paper we will present a review of the extensive work in spatiotemporal data management and trajectory mining, and discuss new challenges and new opportunities in the context of new applications, focusing on recent advances in trajectory data management and trajectory mining from their foundations to high performance processing with modern computing infrastructure.",2015,,10.1145/2806416.2806418
2260,"Colborne, Adrienne and Smit, Michael",Characterizing Disinformation Risk to Open Data in the Post-Truth Era,"risk mitigation, data quality assurance, fake news, Open data, risk identification, post-truth","Curated, labeled, high-quality data is a valuable commodity for tasks such as business analytics and machine learning. Open data is a common source of such data—for example, retail analytics draws on open demographic data, and weather forecast systems draw on open atmospheric and ocean data. Open data is released openly by governments to achieve various objectives, such as transparency, informing citizen engagement, or supporting private enterprise. Critical examination of ongoing social changes, including the post-truth phenomenon, suggests the quality, integrity, and authenticity of open data may be at risk. We introduce this risk through various lenses, describe some of the types of risk we expect using a threat model approach, identify approaches to mitigate each risk, and present real-world examples of cases where the risk has already caused harm. As an initial assessment of awareness of this disinformation risk, we compare our analysis to perspectives captured during open data stakeholder consultations in Canada.",2020,,10.1145/3328747
2261,"Singh, Lisa and Deshpande, Amol and Zhou, Wenchao and Banerjee, Arindam and Bowers, Alex and Friedler, Sorelle and Jagadish, H.V. and Karypis, George and Obradovic, Zoran and Vullikanti, Anil and Zuo, Wangda",NSF BIGDATA PI Meeting - Domain-Specific Research Directions and Data Sets,,"In March 2017, PIs and co-PIs funded through the NSF BIGDATA program were brought together along with selected industry and government invitees to discuss current research, identify current challenges, discuss promising future directions, foster new collaborations, and share accomplishments, at BDPI-2017. Given that two recent NITRD [2] and NSF [1] meeting reports contained a set of recommendations, grand challenges, and high impact priorities for Big Data, the organizers of this meeting shifted the focus of the breakout sessions to discuss problems and available data sets that exist in five application domains - policy, health, education, economy &amp; finance, and environment &amp; energy. These domains were selected based on a survey of the PIs/co-PIs and should not be interpreted as being more important than others. Slides that were presented by the different breakout group leaders are available at https://www.bi.vt.edu/ nsf-big-data/. We hope this report will serve as a blueprint for promising big data research in five application domains.",2019,,10.1145/3316416.3316425
2262,"Piety, Philip J. and Hickey, Daniel T. and Bishop, M. J.","Educational Data Sciences: Framing Emergent Practices for Analytics of Learning, Organizations, and Systems","theories and theoretical concepts for understanding learning, learner analytics, educational data mining, methods, tools for sense-making in learning analytics, educational data science, big data, analytic approaches, data-driven decisions, learning analytics","In this paper, we develop a conceptual framework for organizing emerging analytic activities involving educational data that can fall under broad and often loosely defined categories, including Academic/Institutional Analytics, Learning Analytics/Educational Data Mining, Learner Analytics/Personalization, and Systemic Instructional Improvement. While our approach is substantially informed by both higher education and K-12 settings, this framework is developed to apply across all educational contexts where digital data are used to inform learners and the management of learning. Although we can identify movements that are relatively independent of each other today, we believe they will in all cases expand from their current margins to encompass larger domains and increasingly overlap. The growth in these analytic activities leads to the need to find ways to synthesize understandings, find common language, and develop frames of reference to help these movements develop into a field.",2014,,10.1145/2567574.2567582
2263,"Whang, Steven Euijong and Lee, Jae-Gil",Data Collection and Quality Challenges for Deep Learning,,"Software 2.0 refers to the fundamental shift in software engineering where using machine learning becomes the new norm in software with the availability of big data and computing infrastructure. As a result, many software engineering practices need to be rethought from scratch where data becomes a first-class citizen, on par with code. It is well known that 80--90% of the time for machine learning development is spent on data preparation. Also, even the best machine learning algorithms cannot perform well without good data or at least handling biased and dirty data during model training. In this tutorial, we focus on data collection and quality challenges that frequently occur in deep learning applications. Compared to traditional machine learning, there is less need for feature engineering, but more need for significant amounts of data. We thus go through state-of-the-art data collection techniques for machine learning. Then, we cover data validation and cleaning techniques for improving data quality. Even if the data is still problematic, hope is not lost, and we cover fair and robust training techniques for handling data bias and errors. We believe that the data management community is well poised to lead the research in these directions. The presenters have extensive experience in developing machine learning platforms and publishing papers in top-tier database, data mining, and machine learning venues.",2020,,10.14778/3415478.3415562
2264,"Ba, Huafeng and Gao, Xiaoming and Zhang, Xiaofeng and He, Zhenyu",Protecting Data Privacy from Being Inferred from High Dimensional Correlated Data,"algorithm, privacy preserving data mining, data publishing","In the era of big data, privacy becomes a challenging issue which already attracts a good number of research efforts. In the literature, most of existing privacy preserving algorithms focus on protecting users' privacy from being disclosed by making the set of designated semi-id features indiscriminate. However, how to automatically determine the appropriate semi-id features from high-dimensional correlated data is seldom studied. Therefore, in this paper we first theoretically study the problem and propose the IPFS algorithm to find all possible features forming the candidate semi-id feature set which can infer users' privacy. Then, the KIPFS algorithm is proposed to find the key features from the candidate semi-id feature set. By anonymizing the key feature set, called as key inferring privacy features (KIPFS), users' privacy is protected. To evaluate the effectiveness and the efficacy of the proposed approach, two state-of-the-art algorithms, i.e., K-anonymity and t-closeness, applied on the designated semi-id feature set are chose as the baseline algorithms and their revised versions are applied on the KIPFS for the performance comparison. The promising results showed that by anonymizing the identified KIPFS, both aforementioned algorithms can achieve better performance than the original ones in terms of efficiency and data quality.",2014,,10.1109/WI-IAT.2014.139
2265,"Artikis, Alexander and Etzion, Opher and Feldman, Zohar and Fournier, Fabiana",Event Processing under Uncertainty,"event processing, event recognition, artificial intelligence, pattern matching, uncertainty","Big data is recognized as one of the three technology trends at the leading edge a CEO cannot afford to overlook in 2012. Big data is characterized by volume, velocity, variety and veracity (""data in doubt""). As big data applications, many of the emerging event processing applications must process events that arrive from sources such as sensors and social media, which have inherent uncertainties associated with them. Consider, for example, the possibility of incomplete data streams and streams including inaccurate data. In this tutorial we classify the different types of uncertainty found in event processing applications and discuss the implications on event representation and reasoning. An area of research in which uncertainty has been studied is Artificial Intelligence. We discuss, therefore, the main Artificial Intelligence-based event processing systems that support probabilistic reasoning. The presented approaches are illustrated using an example concerning crime detection.",2012,,10.1145/2335484.2335488
2266,"Fang, Minghong and Sun, Minghao and Li, Qi and Gong, Neil Zhenqiang and Tian, Jin and Liu, Jia",Data Poisoning Attacks and Defenses to Crowdsourcing Systems,"truth discovery, crowdsourcing, Data poisoning attacks"," A key challenge of big data analytics is how to collect a large volume of (labeled) data. Crowdsourcing aims to address this challenge via aggregating and estimating high-quality data (e.g., sentiment label for text) from pervasive clients/users. Existing studies on crowdsourcing focus on designing new methods to improve the aggregated data quality from unreliable/noisy clients. However, the security aspects of such crowdsourcing systems remain under-explored to date. We aim to bridge this gap in this work. Specifically, we show that crowdsourcing is vulnerable to data poisoning attacks, in which malicious clients provide carefully crafted data to corrupt the aggregated data. We formulate our proposed data poisoning attacks as an optimization problem that maximizes the error of the aggregated data. Our evaluation results on one synthetic and two real-world benchmark datasets demonstrate that the proposed attacks can substantially increase the estimation errors of the aggregated data. We also propose two defenses to reduce the impact of malicious clients. Our empirical results show that the proposed defenses can substantially reduce the estimation errors of the data poisoning attacks.",2021,,10.1145/3442381.3450066
2267,"Li, Yunze and Wu, Yuxuan and Tang, Ruisen",Data Aggregation and Anomaly Detection System for Isomerism and Heterogeneous Data,"serialization and deserialization, isomerism and heterogeneous data, anomaly detection, Kafka","With the development of big data technology, data accessed by big data platforms maintain the features of mass, isomerism, heterogeneous, and streaming. Therefore, how to access the varied data sources of isomerism and heterogeneous data and how to process and analyze the data become the current challenges. In this paper, we design and implement a data aggregation and anomaly detection system for isomerism and heterogeneous data. The system proposes a novel isomerism and heterogeneous data access sub-system. The sub-system applies improved Avro as the unified data description format and presents different storage algorithms for data serialization to raise the data adaption efficiency. The system adopts Kafka as the message middleware for data aggregation and distribution. Also, we design the anomaly detection and alarming sub-system for detecting the anomalies of streaming data on time and notifying the users. The data aggregation and anomaly detection system has passed all the tests and applied in small and medium-sized enterprises.",2022,,10.1145/3520084.3520099
2268,"Zhang, Mingjie and Sheng, Yan and Tian, Nuo and Liu, Wei and Wang, Hui and Zhu, Longzhu and Xu, Qing",Exploring and Analyzing Data Mining Algorithm Technology in Internet Customer Ranking,,"Based on the characteristics of online customers, such as user characteristics, interaction behavior, frequency of visits and business queries, this paper uses big data analysis mining algorithm to conduct exploratory analysis on each business data, and builds a weight division model (Entropy Value Method) to achieve online customer ranking.",2021,,10.1145/3495018.3495398
2269,"Fox, Edward A. and Xie, Zhiwu and Klein, Martin",WADL 2016: Third International Workshop on Web Archiving and Digital Libraries,"internet archive, web archiving","This workshop will explore integration of Web archiving and digital libraries, so the complete life cycle involved is covered: creation/authoring, uploading/publishing in the Web (2.0), (focused) crawling, indexing, exploration (searching, browsing), archiving (of events), etc. It will include particular coverage of current topics of interest, like: big data, mobile web archiving, and systems (e.g., Memento, SiteStory, Hadoop processing).",2016,,10.1145/2910896.2926735
2270,"Sun, Wen",Cloud Service Context and Feedback Fusion of Product Design Creative Demand Perception Technology,,"After experiencing the stages of document delivery service, information service and knowledge service, the traditional product design creative demand perception technology has gradually transformed to the cloud service stage driven by new technologies such as big data and cloud computing. With the advent of the era of big data and artificial intelligence, multi-source heterogeneous and massive data resources have specific fusion characteristics and application trends. The generation of new artificial intelligence technologies and methods is to respond to the above characteristics and trends. Multi-source heterogeneous resources and vast amounts of data driven product design requirements perception from the user requirements perception, perception technology content and creative product design requirements capturing perception technology scenario-based push these three core function implementation requirements perception technology cloud service mode, implement the product design requirements perception technology innovation process. Of this study was to study the cloud service situation and feedback the fusion of product design requirements perception technology, cloud service components analysis demand perception technology and its features, with case studies to explore data driven era product design demand perception technology to the cloud service model transformation of ideas, requirements for perception technology transformation provides scientific theory and practice.",2021,,
2271,"Fox, Edward A. and Xie, Zhiwu and Klein, Martin",Web Archiving and Digital Libraries (WADL),"internet archive, web archiving","This workshop will explore integration of Web archiving and digital libraries, so the complete life cycle involved is covered: creation/authoring, uploading/publishing in the Web (2.0), (focused) crawling, indexing, exploration (searching, browsing), archiving (of events), etc. It will include particular coverage of current topics of interest, like: big data, mobile web archiving, and systems (e.g., Memento, SiteStory, Hadoop processing).",2017,,
2272,"Darmont, J\'{e}r\^{o}me and Favre, C\'{e}cile and Loudcher, Sabine and No\^{u}s, Camille",Data Lakes for Digital Humanities,"digital humanities, data lakes, metadata","Traditional data in Digital Humanities projects bear various formats (structured, semi-structured, textual) and need substantial transformations (encoding and tagging, stemming, lemmatization, etc.) to be managed and analyzed. To fully master this process, we propose the use of data lakes as a solution to data siloing and big data variety problems. We describe data lake projects we currently run in close collaboration with researchers in humanities and social sciences and discuss the lessons learned running these projects.",2020,,10.1145/3423603.3424004
2273,"Fan, Wenfei and Liu, Xueli and Lu, Ping and Tian, Chao",Catching Numeric Inconsistencies in Graphs,"graph dependencies, incremental validation, numeric errors","Numeric inconsistencies are common in real-life knowledge bases and social networks. To catch such errors, we propose to extend graph functional dependencies with linear arithmetic expressions and comparison predicates, referred to as NGDs. We study fundamental problems for NGDs. We show that their satisfiability, implication and validation problems are Σ 2 p-complete, ¶II2 p-complete and coNP-complete, respectively. However, if we allow non-linear arithmetic expressions, even of degree at most 2, the satisfiability and implication problems become undecidable. In other words, NGDs strike a balance between expressivity and complexity.To make practical use of NGDs, we develop an incremental algorithm IncDect to detect errors in a graph G using NGDs, in response to updates Δ G to G. We show that the incremental validation problem is coNP-complete. Nonetheless, algorithm IncDect is localizable, i.e., its cost is determined by small neighbors of nodes in Δ G instead of the entire G. Moreover, we parallelize IncDect such that it guarantees to reduce running time with the increase of processors. Using real-life and synthetic graphs, we experimentally verify the scalability and efficiency of the algorithms.",2018,,10.1145/3183713.3183753
2274,"Moreira, Fernando and Gon\c{c}alves, Ramiro and Martins, Jos\'{e} and Branco, Frederico and Au-Yong-Oliveira, Manuel",Learning Analytics as a Core Component for Higher Education Disruption: Governance Stakeholder,"Governance, Learning Analytics, Disruption, Higher Education Institutions","Higher education institutions are at this stage, on the one hand, faced with challenges never seen before and, on the other hand, their action is moving very rapidly into digital learning spaces. These challenges are increasingly complex because of the global competition for resources, students and teachers. In addition, the amount of data produced inside and outside higher education institutions has grown exponentially, so more and more institutions are exploring the potential of Big Data to meet these challenges. In this context, higher education institutions and key stakeholders (students, teachers, and governance) can derive multiple benefits from learning analytics using different data analysis strategies to produce summative, real-time and predictive information and recommendations. However, it may be questioned whether institutions, academic administrative staff as well as including those with responsibility for governance, are prepared for learning analytics? As a response to the question raised in this paper is presented an extension of a disruptive conceptual approach to higher education, using information gathered by IoT and based on Big Data &amp; Cloud Computing and Learning Analytics analysis tools, with the main focus on the stakeholder governance.",2017,,10.1145/3144826.3145387
2275,"Smith, Jeffrey and Rege, Manjeet",The Data Warehousing (R) Evolution: Where's It Headed Next?,"ETL, intelligence, business, warehouse, Data","This paper provides an overview of the history and current state of data warehousing and corporate analytics. It begins with a quick review of the history of the data warehouse and then does a deeper dive into subsets of this space including data integration, the DBMS, business intelligence and analytics, advanced analytics, and information stewardship. It finishes with a quick review of some of the leading trends in data warehousing including Big Data and the Logical Data Warehouse, Hybrid Transaction Analytical Processing and In-Memory Computing.",2017,,10.1145/3093241.3093268
2276,"Saberi, Morteza and Hussain, Omar K. and Chang, Elizabeth",An Online Statistical Quality Control Framework for Performance Management in Crowdsourcing,"statistical quality control, multiple choice HIT, crowd workers, crowdsourcing management","The big data research topic has grown rapidly for the past decade due to the advent of the ""data deluge"". Recent advancements in the literature leverage human computing power known as crowdsourcing to manage and harness big data for various applications. However, human involvement in the completion of crowdsourcing tasks is an error-prone process that affects the overall performance of the crowd. Thus, controlling the quality of workers is an essential step for crowdsourcing systems, which due to unavailability of ground-truth data for any task at hand becomes increasingly challenging. To propose a solution to this problem, in this study, we propose OSQC (Online Statistical Quality Control Framework) for managing the performance of workers in crowdsourcing. OSQC ascertains the worker's performance by using a statistical model and then leverages the traditional statistical control techniques to decide whether to retain a worker for crowdsourcing or to evict him. We evaluate our proposed framework on a real dataset and demonstrate how OSQC assists crowdsourcing to maintain its accuracy.",2017,,10.1145/3106426.3106436
2277,"Petrou, Lambros and Larkou, George and Laoudias, Christos and Zeinalipour-Yazti, Demetrios and Panayiotou, Christos G.",Demonstration Abstract: Crowdsourced Indoor Localization and Navigation with Anyplace,"indoor localization, crowdsourcing, navigation","In this demonstration paper, we present the Anyplace system that relies on the abundance of sensory data on smartphones (e.g., WiFi signal strength and inertial measurements) to deliver reliable indoor geolocation information. Our system features two highly desirable properties, namely crowdsourcing and scalability. Anyplace implements a set of crowdsourcing-supportive mechanisms to handle the enormous amount of crowdsensed data, filter incorrect user contributions and exploit WiFi data from heterogeneous mobile devices. Moreover, Anyplace follows a big-data architecture for efficient and scalable storage and retrieval of localization and mapping data.",2014,,
2278,"Gao, Mengke and Zhang, Yan and Gao, Yue",Research Progress of User Portrait Technology in Medical Field,,"In recent years, due to the rise of big data mining and intelligent recommendation, user portrait technology has gradually become a hot topic. As a new data analysis method, user portrait technology aims to mine user characteristics from a large number of user behavior data, and complete the user information panorama of the monomer or group through information mining, so as to prepare for the realization of precision service and personalized recommendation in various industries. It has been widely used in personalized recommendation and precision recommendation, personalized service and intelligent service, group characteristics analysis, prediction analysis and auxiliary decision-making. User portrait technology in the domestic research started late, currently in e-commerce, commercial precision marketing, book recommendation management, network personalized search, video entertainment and other fields of application is relatively mature. However, the application of user portrait technology in the field of medical and health is still in the preliminary exploration stage, and the combination of medical big data and user portrait technology can vividly depict the portraits of patients, doctors, residents and other different groups in promoting health and disease prevention. According to the different characteristics extracted, it can provide reference for meeting the needs of patients and achieving precision medicine. Therefore, this study reviews the concept, elements, implementation process and application of user portrait technology in the medical field, and provides reference for the subsequent application research of user portrait technology in the medical field",2021,,
2279,"Jirovsk\'{y}, V\'{a}clav and Pastorek, Andrej and M\""{u}hlh\""{a}user, Max and Tundis, Andrea",Cybercrime and Organized Crime,"Cyber-crime, Data Security, Cyber-security, Privacy","The way of live in the modern society has changed radically over the past few decades. In particular, thanks to the strong use of information technology, many activities have moved from the real world to the digital world. This has obviously introduced advantages in terms of data management and communication efficiency. Nevertheless, it has given also to the criminals the possibility to move into cybernetic space and, as a consequence, to exploit all the technological advantages available for carrying out their activities. In this context the paper provide an overview on the cybercrime and organized crime by focusing on the concept of crime as a service as well as the main issues related to big data by highlighting the social aspects.",2018,,10.1145/3230833.3233288
2280,"Boyapati, Rahul and Huang, Jiayi and Majumder, Pritam and Yum, Ki Hwan and Kim, Eun Jung",APPROX-NoC: A Data Approximation Framework for Network-On-Chip Architectures,"Data Compression, Networks-On-Chip, Approximate Computing","The trend of unsustainable power consumption and large memory bandwidth demands in massively parallel multicore systems, with the advent of the big data era, has brought upon the onset of alternate computation paradigms utilizing heterogeneity, specialization, processor-in-memory and approximation. Approximate Computing is being touted as a viable solution for high performance computation by relaxing the accuracy constraints of applications. This trend has been accentuated by emerging data intensive applications in domains like image/video processing, machine learning and big data analytics that allow inaccurate outputs within an acceptable variance. Leveraging relaxed accuracy for high throughput in Networks-on-Chip (NoCs), which have rapidly become the accepted method for connecting a large number of on-chip components, has not yet been explored. We propose APPROX-NoC, a hardware data approximation framework with an online data error control mechanism for high performance NoCs. APPROX-NoC facilitates approximate matching of data patterns, within a controllable value range, to compress them thereby reducing the volume of data movement across the chip.Our evaluation shows that APPROX-NoC achieves on average up to 9% latency reduction and 60% throughput improvement compared with state-of-the-art NoC data compression mechanisms, while maintaining low application error. Additionally, with a data intensive graph processing application we achieve a 36.7% latency reduction compared to state-of-the-art compression mechanisms.",2017,,10.1145/3079856.3080241
2281,"Boyapati, Rahul and Huang, Jiayi and Majumder, Pritam and Yum, Ki Hwan and Kim, Eun Jung",APPROX-NoC: A Data Approximation Framework for Network-On-Chip Architectures,"Approximate Computing, Networks-On-Chip, Data Compression","The trend of unsustainable power consumption and large memory bandwidth demands in massively parallel multicore systems, with the advent of the big data era, has brought upon the onset of alternate computation paradigms utilizing heterogeneity, specialization, processor-in-memory and approximation. Approximate Computing is being touted as a viable solution for high performance computation by relaxing the accuracy constraints of applications. This trend has been accentuated by emerging data intensive applications in domains like image/video processing, machine learning and big data analytics that allow inaccurate outputs within an acceptable variance. Leveraging relaxed accuracy for high throughput in Networks-on-Chip (NoCs), which have rapidly become the accepted method for connecting a large number of on-chip components, has not yet been explored. We propose APPROX-NoC, a hardware data approximation framework with an online data error control mechanism for high performance NoCs. APPROX-NoC facilitates approximate matching of data patterns, within a controllable value range, to compress them thereby reducing the volume of data movement across the chip.Our evaluation shows that APPROX-NoC achieves on average up to 9% latency reduction and 60% throughput improvement compared with state-of-the-art NoC data compression mechanisms, while maintaining low application error. Additionally, with a data intensive graph processing application we achieve a 36.7% latency reduction compared to state-of-the-art compression mechanisms.",2017,,10.1145/3140659.3080241
2282,"Huang, Ruihong",Approximate Event Pattern Matching over Heterogeneous and Dirty Sources,"heterogeneous source, cep, dirty source, approximate match, low-quality data, complex event processing","Pattern matching is an important task in the field of Complex Event Processing (CEP). However, exact event pattern matching methods could suffer from low hit rate and loss for meaningful events identification due to the heterogeneous and dirty sources in the big data era. Since both events and patterns could be imprecise, the actual event trace may have different event names as well as structures from the pre-defined pattern. The low-quality data even intensifies the difficulty of matching. In this work, we propose to learn embedding representations for patterns and event traces separately and calculate their similarity as the scores for approximate matching.",2020,,10.1145/3340531.3418506
2283,"Verma, Nitya and Voida, Amy",Mythologies of Business Intelligence,"values, business intelligence, mythology, analytics","We present results from a case study of the use of business intelligence (BI) systems in a human services organization. In their organizational trajectory towards a ""culture of data,"" our informants perceived four values associated with BI: data-driven, predictive and proactive, shared accountability, and inquisitive. Each value corresponds to a mythology of big data and BI. For each, we highlight the ways in which the enactment of the mythology is problematized by disconnects between aggregate and drill-down views of data that often impede the desired actionability. Our findings contribute initial empirical evidence of the ways in which the epistemological biases of BI systems influence organizations. We suggest design implications for better enabling data-driven decision making.",2016,,10.1145/2851581.2892379
2284,"Li, Ruixue and Peng, Can and Sun, Huiliang",Product Selection Strategy Analysis of Crowdsourcing Platform from the Full Cost Perspective,"Crowdsourcing platform, Selection Strategy Analysis, Appropriate products, Full cost","From the perspective of full cost, this paper uses Coase's transaction cost theory to analyze the causes of crowdsourcing, and on this basis to analyze the applicability of crowdsourcing platform products. At the same time, based on the crowdsourcing platform--zbj.com, we use the big data technology to grasp and analyze the related data of the crowdsourcing platform's successful cases in the past five months, and use the relevant statistical analysis method to categorize and analyze the industry attributes of the top five orders of the success cases of the zbj.com, in order to verify the theory mentioned in the article.",2019,,10.1145/3335550.3335577
2285,"Tang, Haijing and Zhou, Yangdong and Yang, Xu and Gao, Keyan and Zheng, Wenhao and Zhao, Jinfeng",Adopting Data Analysis and Visualization Technology to Construct Clinical Research Data Management and Analysis System,"Clinical data, Data analysis, Visualization","With the development of information technology, information systems have been widely used in medical institutions, and more and more clinical research data has been digitized, which provides the possibility to carry out clinical research with data as the source. However, the complexity and multi-dimensionality of clinical data make medical scientists' progress slow, and comprehensive use of various big data technologies is needed to help improve the efficiency of clinical research. Visualization technology can display data in an intuitive and easy-to-read way, helping medical researchers understand data, while parallel computing can greatly improve computing efficiency. Therefore, this paper explores the application strategies of data analysis technology and visualization technology in the management and analysis of clinical research data, and builds a set of clinical research data management analysis system, which combines various technologies to help effectively promote medical clinical.",2018,,10.1145/3301761.3301767
2286,"Abadi, Daniel and Agrawal, Rakesh and Ailamaki, Anastasia and Balazinska, Magdalena and Bernstein, Philip A. and Carey, Michael J. and Chaudhuri, Surajit and Dean, Jeffrey and Doan, AnHai and Franklin, Michael J. and Gehrke, Johannes and Haas, Laura M. and Halevy, Alon Y. and Hellerstein, Joseph M. and Ioannidis, Yannis E. and Jagadish, H. V. and Kossmann, Donald and Madden, Samuel and Mehrotra, Sharad and Milo, Tova and Naughton, Jeffrey F. and Ramakrishnan, Raghu and Markl, Volker and Olston, Christopher and Ooi, Beng Chin and R\'{e}, Christopher and Suciu, Dan and Stonebraker, Michael and Walter, Todd and Widom, Jennifer",The Beckman Report on Database Research,,"Every few years a group of database researchers meets to discuss the state of database research, its impact on practice, and important new directions. This report summarizes the discussion and conclusions of the eighth such meeting, held October 14- 15, 2013 in Irvine, California. It observes that Big Data has now become a defining challenge of our time, and that the database research community is uniquely positioned to address it, with enormous opportunities to make transformative impact. To do so, the report recommends significantly more attention to five research areas: scalable big/fast data infrastructures; coping with diversity in the data management landscape; end-to-end processing and understanding of data; cloud services; and managing the diverse roles of people in the data life cycle.",2014,,10.1145/2694428.2694441
2287,"Yan, Boyang and Yecies, Brian and Zhou, Zhi Quan",Metamorphic Relations for Data Validation: A Case Study of Translated Text Messages,"sentiment analysis, metamorphic relation, Oracle problem, social media, metamorphic testing, data validation, natural language processing, Douban, data quality assessment, machine translation","In conventional metamorphic testing, metamorphic relations (MRs) are identified as necessary properties of a computer program's intended functionality, whereby violations of MRs reveal faults in the program---under the assumption that the source and follow-up inputs (test cases used in metamorphic testing) are valid. In the present study, the authors argue that MRs can also be used to validate and assess the quality of the program's input data---under the assumption that the source or follow-up inputs can be inappropriately generated. Using this new perspective, a case study in the natural language processing domain is used to explore the different types of text messages that are difficult to interpret by (Chinese-English) machine translation. A total of 46,180 short user comments on Personal Tailor (a 2013 Chinese film), collected from Douban (a popular Chinese social media platform), has been used as the primary dataset of this study, and the analysis of results demonstrates that the proposed MR-based data validation method is useful for the automatic identification of poorly translated text messages.",2019,,10.1109/MET.2019.00018
2288,"Ensan, Faezeh and Noorian, Zeinab and Bagheri, Ebrahim",Mining Actionable Insights from Social Networksat WSDM 2017,"web mining, social network analysis, predictive modeling","The first international workshop on Mining Actionable Insights from Social Networks (MAISoN'17) is to be held on February 10, 2017; co-located with the Tenth ACM International Web Search and Data Mining (WSDM) Conference in Cambridge, UK. MAISoN'17 aims at bringing together researchers and participants from different disciplines such as computer science, big data mining, machine learning, social network analysis and other related areas in order to identify challenging problems and share ideas, algorithms, and technologies for mining actionable insight from social network data. We organized a workshop program that includes the presentation of eight peer-reviewed papers and keynote talks, which foster discussions around state-of-the-art in social network mining and will hopefully lead to future collaborations and exchanges.",2017,,10.1145/3018661.3022759
2289,"Heintz, Benjamin and Chandra, Abhishek and Sitaraman, Ramesh K.",Wide-Area Streaming Analytics: Distributing the Data Cube,,"To date, much research in data-intensive computing has focused on batch computation. Increasingly, however, it is necessary to derive knowledge from big data streams. As a motivating example, consider a content delivery network (CDN) such as Akamai [4], comprising thousands of servers in hundreds of globally distributed locations. Each of these servers produces a stream of log data, recording for example every user it serves, along with each video stream they access, when they play and pause streams, and more. Each server also records network- and system-level data such as TCP connection statistics. In aggregate, the servers produce billions of lines of log data from over a thousand locations daily.",2013,,10.1145/2523616.2525963
2290,"De Masi, Alexandre and Ciman, Matteo and Gustarini, Mattia and Wac, Katarzyna",MQoL Smart Lab: Quality of Life Living Lab for Interdisciplinary Experiments,"people centric sensing, data analysis, data science, data collection, smartphones, platforms","As a base for hypothesis formulation and testing, accurate, timely and reproducible data collection is a challenge for all researchers. Data collection is especially challenging in uncontrolled environments, outside of the lab and when it involves many collaborating disciplines, where the data must serve quality research in all of them. In this paper, we present own ""mQoL Smart Lab"" for interdisciplinary research efforts on individuals' ""Quality of Life"" improvement. We present an evolution of our current in-house living lab platform enabling continuous, pervasive data collection from individuals' smartphones. We discuss opportunities for mQoL stemming from developments in machine learning and big data for advanced data analytics in different disciplines, better meeting the requirements put on the platform.",2016,,10.1145/2968219.2971593
2291,"B\""{u}chler, Marco and Riegert, Sarah and Alpi, Federico and Cadeddu, Francesca",Towards Big Religious Data: RESILIENCE Research Infrastructure for Data on Religion in the Digital Age,"religious studies, digital transformation, big religious data, research infrastructures","Data in and for religion is arguably as old as humanity. Religious significance has been attached to an immense variety of artifacts and documents, often in written form, in nearly all spoken and written languages over the past millennia. The rise of the digital age gives to the scholar in religious studies the opportunity to build research over a much wider array of data than ever before; institutions which have data repositories (such as libraries, museums, universities, etc.) similarly have the chance to make their collections available to a larger community. On the other hand, however, there is a serious risk that a considerable amount of data gets lost during the ""Digital transition"". This paper presents the approach of the RESILIENCE Research Infrastructure in dealing with the issue of big data and data loss within the field of religious studies.",2020,,10.1145/3423603.3424007
2292,"Zhong, Junmei and Gao, Chuangui and Yi, Xiu",Categorization of Patient Disease into ICD-10 with NLP and SVM for Chinese Electronic Health Record Analysis,"SVM, ICD-10, machine learning, NLP, Electronic health record","The electronic health record (EHR) analysis has become an increasingly important application for artificial intelligence (AI) algorithms to leverage the insight from the big data for improving the quality of human healthcare. In a lot of Chinese EHR analysis applications, it is very important to categorize the patients' diseases according to the medical coding standard. In this paper, we develop NLP and machine learning algorithms to automatically categorize each patient's individual diseases into the ICD-10 coding standard. Experimental results show that the support vector machine algorithm (SVM) accomplishes very promising classification results.",2018,,10.1145/3268866.3268877
2293,"Zhan, Lin and Junhua, Zhao and Fan, Li and Zhifei, Wang",Research on Intelligent Management Platform of Highspeed Railway Traffic Safety Equipment Based on CPS,"CPS, traffic safety, equipment fault diagnosis, High-speed railway","From the view of high-speed railway traffic safety, this paper establishes an intelligent management platform for operation safety equipment based on CPS for ""person-equipment-environment"", and designs a framework of traffic safety system composed of perception control hardware, Internet of Things, cognitive decision-making and information services. The deep fusion of information system and traffic safety equipment is discussed, and the fault diagnosis method of driving equipment based on complex sensing technology is given, such as intelligent identification, online monitoring and ubiquitous sensing of the characteristics of safety protection equipment. Through the application of equipment fault diagnosis, it realizes the rapid retrieval and active collection of safety information, provides early warning and auxiliary decision-making, big data analysis and prediction, and improves the traffic safety.",2020,,10.1145/3390557.3394127
2294,"Wang, Maximilian J. and Mao, Guifen and Chen, Haiquan",Mining Multivariate Outliers: A Mixture Model-Based Framework,"mahalanobis distance, EM algorithm, normal mixture models, data mining, k-means clustering algorithm, outlier detection","Mining outliers has become more and more important in recent years. It has wide applications in military surveillance for enemy activities, detection of potential terrorist attacks, credit card fraud detection, network intrusion, computer virus attack, clinical trials, severe weather prediction, athlete performance analysis, and many other data mining tasks. In today's big data age, multivariate data sets are very complex. Variables among different dimensions are usually correlated with different variations. Classical data mining methods with Euclidean distance measure are not working well for mining multivariate outliers. In this study, we propose a normal mixture model-based framework of multivariate outlier detection. We fit the model parameters through the robust EM algorithm. The K-means clustering algorithm is used to provide the initial inputs for the EM algorithm. The well-know Mahalanobis distance is used to determine the cutoff points for outlier detection via the chi-square distribution critical values. Implementation details of this framework are also discussed.",2014,,10.1145/2638404.2638526
2295,"Gao, Song and Rao, Jinmeng and Kang, Yuhao and Liang, Yunlei and Kruse, Jake",Mapping County-Level Mobility Pattern Changes in the United States in Response to COVID-19,,"To contain the COVID-19 epidemic, one of the non-pharmacological epidemic control measures is reducing the transmission rate of SARS-COV-2 in the population through social distancing. An interactive web-based mapping platform that provides timely quantitative information on how people in different counties and states reacted to the social distancing guidelines was developed by the GeoDS Lab @UW-Madison with the support of the National Science Foundation RAPID program. The web portal integrates geographic information systems (GIS) and daily updated human mobility statistical patterns (median travel distance and stay-at-home dwell time) derived from large-scale anonymized and aggregated smartphone location big data at the county-level in the United States, and aims to increase risk awareness of the public, support data-driven public health and governmental decision-making, and help enhance community responses to the COVID-19 pandemic.",2020,,10.1145/3404820.3404824
2296,"Edge, Darren and Larson, Jonathan and White, Christopher",Bringing AI to BI: Enabling Visual Analytics of Unstructured Data in a Modern Business Intelligence Platform,"data, business intelligence, visual analytics, hci, ai","The Business Intelligence (BI) paradigm is challenged by emerging use cases such as news and social media analytics in which the source data are unstructured, the analysis metrics are unspecified, and the appropriate visual representations are unsupported by mainstream tools. This case study documents the work undertaken in Microsoft Research to enable these use cases in the Microsoft Power BI product. Our approach comprises: (a) back-end pipelines that use AI to infer navigable data structures from streams of unstructured text, media and metadata; and (b) front-end representations of these structures grounded in the Visual Analytics literature. Through our creation of multiple end-to-end data applications, we learned that representing the varying quality of inferred data structures was crucial for making the use and limitations of AI transparent to users. We conclude with reflections on BI in the age of AI, big data, and democratized access to data analytics.",2018,,10.1145/3170427.3174367
2297,"Harrison, Teresa and F. Luna-Reyes, Luis and Pardo, Theresa and De Paula, Nic and Najafabadi, Mahdi and Palmer, Jillian",The Data Firehose and AI in Government: Why Data Management is a Key to Value and Ethics,"Artificial Intelligence, DMBOK, Policy Analysis, Data Analytics, Data Management","Technical and organizational innovations such as Open Data, Internet of Things and Big Data have fueled renewed interest in policy analytics in the public sector. This revamped version of policy analysis continues the long-standing tradition of applying statistical modeling to better understand policy effects and decision making, but also incorporates other computational approaches such as artificial intelligence (AI) and computer simulation. Although much attention has been given to the development of capabilities for data analysis, there is much less attention to understanding the role of data management in a context of AI in government. In this paper, we argue that data management capabilities are foundational to data analysis of any kind, but even more important in the present AI context. This is so because without proper data management, simply acquiring data or systems will not produce desired outcomes. We also argue that realizing the potential of AI for social good relies on investments specifically focused on this social outcome, investments in the processes of building trust in government data, and ensuring the data are ready and suitable for use, for both immediate and future uses.",2019,,10.1145/3325112.3325245
2298,"Almeida, Ricardo and Maio, Paulo and Oliveira, Paulo and Barroso, Jo\~{a}o",Ontology Based Rewriting Data Cleaning Operations,"Vocabulary, Schema, Rewriting Process, Ontology, Data Cleaning","Dealing with increasing amounts of data creates the need to deal with redundant, inconsistent and/or complementary repositories which may be different in their data models and/or in their schema. Current data cleaning techniques developed to tackle data quality problems are just suitable for scenarios were all repositories share the same model and schema. Recently, an ontology-based methodology was proposed to overcome this limitation. In this paper, this methodology is briefly described and applied to a real scenario in the health domain with data quality problems.",2016,,10.1145/2948992.2949007
2299,"Mack, Vincent Z. W. and Kam, Tin Seong",Is There Space for Violence? A Data-Driven Approach to the Exploration of Spatial-Temporal Dimensions of Conflict,"hotspot detection, knowledge discovery, geospatial autocorrelation, political violence, Africa","With recent increases in incidences of political violence globally, the world has now become more uncertain and less predictable. Of particular concern is the case of violence against civilians, who are often caught in the crossfire between armed state or non-state actors. Classical methods of studying political violence and international relations need to be updated. Adopting the use of data analytic tools and techniques of studying big data would enable academics and policy makers to make sense of a rapidly changing world.",2018,,10.1145/3282933.3282935
2300,"Rabe, Markus and Scheidler, Anne Antonia",An Approach for Increasing the Level of Accuracy in Supply Chain Simulation by Using Patterns on Input Data,,"Setting up simulation scenarios in the field of Supply Chains (SCs) is a big challenge because complex input data must be specified and careful input data management as well as precise model design are necessary. SC simulation needs a large amount of input data -- especially in times of big data, in which the data is often approximated by statistical distributions from real world observations. This paper deals with the question how the model itself and its input can be effectively complemented. This takes into account the commonly known fact, that the accuracy of a model output depends on the model input. Therefore an approach for using techniques of Knowledge Discovery in Databases is introduced to derive logical relations from the data. We discuss how Knowledge Discovery would be applied, as a preprocessing step for simulation scenario setups, in order to provide benefits for the level of accuracy in simulation models.",2014,,
2301,"Xie, Dingding and Li, Junyi and Yuan, Lening and Peng, Peng",Multi: Source Data Inconsistency Detection and Repair Based on CRC Algorithm,"Data quality, Distributed database system, CRC, Data inconsistency","Most existing systems suffer from data quality problems. Data quality has been affected by many factors such as manual operation, software problems and hardware problems, especially data inconsistencies. As an important carrier of data, database system plays an important role in distributed systems. In order to reduce the impact of data inconsistency on data quality in distributed database systems, we design and implement a multi-source data inconsistency detection and repair method based on CRC algorithm.The idea of the proposed techniques is to use the rolling checksum in the rsync algorithm. In the process of data inconsistency detection, the method divides the table into chunks and calculates the checksums of data chunks in parallel for multiple data tables to detect and repair the inconsistent data. The experimental results show that the detection effect of this method is consistent with that of the traditional method which comparing source data with target data. The detection rate is as high as 99%, but it performs better than the traditional method, and the running time is reduced by about 20%.",2018,,10.1145/3220199.3220206
2302,"Cohen, Jeffrey and Dolan, Brian and Dunlap, Mark and Hellerstein, Joseph M. and Welton, Caleb",MAD Skills: New Analysis Practices for Big Data,,"As massive data acquisition and storage becomes increasingly affordable, a wide variety of enterprises are employing statisticians to engage in sophisticated data analysis. In this paper we highlight the emerging practice of Magnetic, Agile, Deep (MAD) data analysis as a radical departure from traditional Enterprise Data Warehouses and Business Intelligence. We present our design philosophy, techniques and experience providing MAD analytics for one of the world's largest advertising networks at Fox Audience Network, using the Greenplum parallel database system. We describe database design methodologies that support the agile working style of analysts in these settings. We present dataparallel algorithms for sophisticated statistical techniques, with a focus on density methods. Finally, we reflect on database system features that enable agile design and flexible algorithm development using both SQL and MapReduce interfaces over a variety of storage mechanisms.",2009,,10.14778/1687553.1687576
2303,"Abiteboul, Serge and Dong, Luna and Etzioni, Oren and Srivastava, Divesh and Weikum, Gerhard and Stoyanovich, Julia and Suchanek, Fabian M.",The Elephant in the Room: Getting Value from Big Data,,,2015,,10.1145/2767109.2770014
2304,"Marchionini, Gary and Lee, Christopher A. and Bowden, Heather and Lesk, Michael",Curating for Quality: Ensuring Data Quality to Enable New Science,,"Science is built on observations. If our observational data is bad, we are building a house on sand. Some of our data banks have quality measurements and maintenance, such as the National Climate Data Center and the National Center for Biotechnology Information; but others do not, and we do not even know which scientific data services have quality metrics or what they are.Data quality is an assertion about data properties, typically assumed within a context defined by a collection that holds the data. The assertion is made by the creator of the data. The collection context includes both metadata that describe provenance and representation information, and procedures that are able to parse and manipulate the data. However data quality from the perspective of users is defined based on the data properties that are required for use within their scientific research. The user believes data is of high quality when assertions about compliance can be shown to their research requirements.Digital data can accumulate rich contextual and derivative data as it is collected, analyzed, used, and reused, and planning for the management of this history requires new kinds of tools, techniques, standards, workflows, and attitudes. As science and industry recognize the need for digital curation, scientists and information professionals recognize that access and use of data depend on trust in the accuracy and veracity of data. In all data sets trust and reuse depend on accessible context and metadata that make explicit provenance, precision, and other traces of the datum and data life cycle. Poor data quality can be worse than missing data because it can waste resources and lead to faulty ideas and solutions, or at minimum challenges trust in the results and implications drawn from the data. Improvement in data quality can thus have significant benefits.",2012,,
2305,"Rocuts, Schweitzer and Alier, Marc",A Methodology Exploration to Motivate Teachers to Place Mathematics at the Center of a Transdisciplinary Experience Using Videogames and Big Data Combined with the 21st Century Skills,"Connected mathematics, creativity and collaborative work in education, transdisciplinary, 21st Century Skills","Human knowledge is highly connected and under continuous and collaborative evolution, where all of us can co-create and contribute. But the execution of our standardized educational systems mostly offers a standardized experience and teaching practices that do not reflect this point. This is particularly acute in teachers and students with mathematics. In this research, I will review some strategies to help teachers to effectively use connected mathematics with the real and daily world, promoting transdisciplinary work with an emphasis on creativity and collaboration, and using the Game-Based Assessment methodology combined with the unique environment surrounding each teacher.",2021,,10.1145/3486011.3486555
2306,"Rahm, Erhard",Discovering Product Counterfeits in Online Shops: A Big Data Integration Challenge,,,2014,,10.1145/2629605
2307,"Al-janabi, Samir and Hamid, Abubaker and Janicki, Ryszard",DatumPIPE: Data Generator and Corrupter for Multiple Data Quality Aspects,,"Organizations use data to support different business processes. Data may become unclean because of corruptions in the central quality aspects due to factors such as duplicate records, outdated data, inconsistent values, incomplete information, or inaccurate values. Real datasets are usually not available for reasons such as privacy constraints. In the existing systems that generate or corrupt synthetic data, the intrinsic characteristics of data may not satisfy the quality aspects, and the injected types of errors do not corrupt multiple data quality aspects. Also, a lack of common datasets is a primary reason that representative comparisons between algorithms of different data quality management approaches are not possible. To address these issues, we present datumPIPE, a system that allows for the generation of data that satisfies a set of integrity constraints, including functional dependencies (FDs), conditional functional dependencies (CFDs), and inclusion dependencies (INDs). Also, datumPIPE provides the functionality to generate other types of attribute values such as sensors and personal data. It also allows for the corruption of the generated data through the introduction of quality issues in the central data quality aspects.",2017,,10.1145/3110025.3120958
2308,"Wang, Chun-Yu and Fuh, Shih-Hao and Lo, Ta-Chun and Cheng, Qi-Jun and Chen, Yu-Cheng and Cho, Feng-Min and Chang, Jyh-Biau and Shieh, Ce-Kuen",A Data Compacting Technique to Reduce the NetFlow Size in Botnet Detection with BotCluster,"data compression, botnet, netflow, data compacting, p2p botnet, data reduction, big data, mapreduce framework","Big data analytics helps us to find potentially valuable knowledge, but as the size of the dataset increases, the computing cost also grows exponentially. In our previous work, BotCluster, we had designed a pre-processing filtering pipeline, including whitelist filter and flow loss-response rate (FLR) filter, for data reduction, which intended to wipe out irrelative noises and reduce the computing overhead. However, we still face a data redundancy phenomenon in which some of the same feature vectors repeatedly emerged. In this paper, we propose a data compacting approach aimed to reduce the input volume and keep enough representative feature vectors to fit DBSCAN's (Density-based spatial clustering of applications with noise) criteria. It purges the redundant vectors according to a purging threshold and keeps the primary representatives. Experimental results have shown that the average data reduction ratio is about 81.34%, while the precision has only slightly decreased by 1.6% on average, and the results still have 99.88% of IPs overlapped with the previous system.",2019,,10.1145/3365109.3368778
2309,"Nguyen, Minh Chau and Won, Hee Sun",Advanced Multitenant Hadoop in Smart Open Data Platform,"Advanced multitenant Hadoop, Metadata, Data authorization, Big data, Open data","Nowadays, there has been an immense amount of data coming from various devices sensors, social networks and IoT services. Among these data, open data is playing more and more important role in practice. Many individuals and organizations collect a broad range of different types of data in order to perform their analytic tasks. However, the current open data platforms still have many limitations. Among the drawbacks, data management, an important process of analytic service development, needs to be improved significantly. The main reason is that the emergence of massive data explosion coming from various sources has been making the process become more and more complicated and costly. Therefore, we propose here a system related to the field of data management to allow multitenant users to find and access easily their desired data as well as metadata. It also helps improve the performance of platform.",2017,,10.1145/3175684.3175719
2310,"Zhang, Jie and Qu, Zhihao and Chen, Chenxi and Wang, Haozhao and Zhan, Yufeng and Ye, Baoliu and Guo, Song",Edge Learning: The Enabling Technology for Distributed Big Data Analytics in the Edge,"edge computing, machine learning, security and privacy, federated learning, Edge learning","Machine Learning (ML) has demonstrated great promise in various fields, e.g., self-driving, smart city, which are fundamentally altering the way individuals and organizations live, work, and interact. Traditional centralized learning frameworks require uploading all training data from different sources to a remote data server, which incurs significant communication overhead, service latency, and privacy issues.To further extend the frontiers of the learning paradigm, a new learning concept, namely, Edge Learning (EL) is emerging. It is complementary to the cloud-based methods for big data analytics by enabling distributed edge nodes to cooperatively training models and conduct inferences with their locally cached data. To explore the new characteristics and potential prospects of EL, we conduct a comprehensive survey of the recent research efforts on EL. Specifically, we first introduce the background and motivation. We then discuss the challenging issues in EL from the aspects of data, computation, and communication. Furthermore, we provide an overview of the enabling technologies for EL, including model training, inference, security guarantee, privacy protection, and incentive mechanism. Finally, we discuss future research opportunities on EL. We believe that this survey will provide a comprehensive overview of EL and stimulate fruitful future research in this field.",2021,,10.1145/3464419
2311,"Wu, Jian and Liang, Chen and Yang, Huaiyu and Giles, C. Lee",CiteSeerX Data: Semanticizing Scholarly Papers,"semantic entity extraction, scholarly big data, digital library search engine, CiteSeerX, citation graph","Scholarly big data is, for many, an important instance of Big Data. Digital library search engines have been built to acquire, extract, and ingest large volumes of scholarly papers. This paper provides an overview of the scholarly big data released by CiteSeerX, as of the end of 2015, and discusses various aspects such as how the data is acquired, its size, general quality, data management, and accessibility. Preliminary results on extracting semantic entities from body text of scholarly papers with Wikifier show biases towards general terms appearing in Wikipedia and against domain specific terms. We argue that the latter will play a more important role in extracting important facts from scholarly papers.",2016,,10.1145/2928294.2928306
2312,"Polese, Giuseppe and Deufemia, Vincenzo and Song, Shaoxu",Editorial: Special Issue on Metadata Discovery for Assessing Data Quality,,,2020,,10.1145/3423321
2313,"Filonik, Daniel and Bednarz, Tomasz",Visual Analytics of Big Networks: Novel Approaches for Exploring Complex Networks in Big Data,,"Four ""Paradigms"" of Science• Empirical Science• Theoretical Science• Computational Science• Data Science",2018,,10.1145/3277644.3277803
2314,"Mountantonakis, Michalis and Tzitzikas, Yannis",Scalable Methods for Measuring the Connectivity and Quality of Large Numbers of Linked Datasets,"Data quality, big data, spark, linked data, mapreduce, dataset selection, lattice of measurements, connectivity, dataset discovery","Although the ultimate objective of Linked Data is linking and integration, it is not currently evident how connected the current Linked Open Data (LOD) cloud is. In this article, we focus on methods, supported by special indexes and algorithms, for performing measurements related to the connectivity of more than two datasets that are useful in various tasks including (a) Dataset Discovery and Selection; (b) Object Coreference, i.e., for obtaining complete information about a set of entities, including provenance information; (c) Data Quality Assessment and Improvement, i.e., for assessing the connectivity between any set of datasets and monitoring their evolution over time, as well as for estimating data veracity; (d) Dataset Visualizations; and various other tasks. Since it would be prohibitively expensive to perform all these measurements in a na\""{\i}ve way, in this article, we introduce indexes (and their construction algorithms) that can speed up such tasks. In brief, we introduce (i) a namespace-based prefix index, (ii) a sameAs catalog for computing the symmetric and transitive closure of the owl:sameAs relationships encountered in the datasets, (iii) a semantics-aware element index (that exploits the aforementioned indexes), and, finally, (iv) two lattice-based incremental algorithms for speeding up the computation of the intersection of URIs of any set of datasets. For enhancing scalability, we propose parallel index construction algorithms and parallel lattice-based incremental algorithms, we evaluate the achieved speedup using either a single machine or a cluster of machines, and we provide insights regarding the factors that affect efficiency. Finally, we report measurements about the connectivity of the (billion triples-sized) LOD cloud that have never been carried out so far.",2018,,10.1145/3165713
2315,"Yu, Jie and Li, Danning and Chen, Kai and Huang, Wei and Qin, Meiyuan and Qin, Xianjin",Research on Food Safety Data Sharing and Exchange Mechanism,,"With the development of the economy and the improvement of people's quality of life, the public demand for food taste has gradually changed to the demand for food safety. In order to better facilitate the government to strengthen food safety supervision and protect people's food safety, it is necessary for the government to realize information interaction with enterprises related to the food supply chain and ensure the traceability of food flowing into the market by exchanging and sharing the data of food safety information. With the rapid promotion and popularization of various mobile terminals in the Internet era, the data analysis technology based on artificial intelligence technology is more accurate, which makes the value contained in the data more and more important to people. At present, many fields need the opening and sharing of big data, but there is no reliable data sharing environment in the field of food supervision, and it is still difficult to ensure the traceability of data related to food safety. Blockchain has unique advantages of decentralization and distribution, which can help break the current obstacles of big data sharing and exchange and achieve a high degree of data sharing, interconnection and exchange. Based on the blockchain technology, this paper studies the food safety data sharing and exchange mechanism, combines the blockchain with the distributed file system, constructs the data connection model, stores the shared information on the blockchain, then introduces IPFs and zigzag coding, designs the corresponding control method, and establishes a reliable data sharing and exchange mechanism. The analysis shows that the data sharing and exchange mechanism proposed in this paper can meet the needs of food safety data sharing and exchange.",2022,,10.1145/3523181.3523193
2316,"Baijens, Jeroen and Helms, Remko and Kusters, Rob",Data Analytics Project Methodologies: Which One to Choose?,"Project Methodologies, Project characteristics, Data Analytics","Developments in big data have led to an increase in data analytics projects conducted by organizations. Such projects aim to create value by improving decision making or enhancing business processes. However, many data analytics projects still fail to deliver the expected value. The use of process models or methodologies is recommended to increase the success rate of these projects. Nevertheless, organizations are hardly using them because they are considered too rigid and hard to implement. The existing methodologies often do not fit the specific project characteristics. Therefore, this research suggests grouping different project characteristics to identify the most appropriate project methodology for a specific type of project. More specifically, this research provides a structured description that helps to determine what type of project methodology works for different types of data analytics projects. The results of six different case studies show that continuous projects would benefit from an iterative methodology.",2020,,10.1145/3437075.3437087
2317,"Chirkova, Rada and Doyle, Jon and Reutter, Juan",Ensuring Data Readiness for Quality Requirements with Help from Procedure Reuse,"Big Data quality management processes, frameworks, Big Data quality and analytics, Data and information quality, Big Data quality in business process, data integration in Big Data, data cleaning in Big Data, and models","Assessing and improving the quality of data are fundamental challenges in Big-Data applications. These challenges have given rise to numerous solutions targeting transformation, integration, and cleaning of data. However, while schema design, data cleaning, and data migration are nowadays reasonably well understood in isolation, not much attention has been given to the interplay between standalone tools in these areas. In this article, we focus on the problem of determining whether the available data-transforming procedures can be used together to bring about the desired quality characteristics of the data in business or analytics processes. For example, to help an organization avoid building a data-quality solution from scratch when facing a new analytics task, we ask whether the data quality can be improved by reusing the tools that are already available, and if so, which tools to apply, and in which order, all without presuming knowledge of the internals of the tools, which may be external or proprietary.Toward addressing this problem, we conduct a formal study in which individual data cleaning, data migration, or other data-transforming tools are abstracted as black-box procedures with only some of the properties exposed, such as their applicability requirements, the parts of the data that the procedure modifies, and the conditions that the data satisfy once the procedure has been applied. As a proof of concept, we provide foundational results on sequential applications of procedures abstracted in this way, to achieve prespecified data-quality objectives, for the use case of relational data and for procedures described by standard relational constraints. We show that, while reasoning in this framework may be computationally infeasible in general, there exist well-behaved cases in which these foundational results can be applied in practice for achieving desired data-quality results on Big Data.",2021,,10.1145/3428154
2318,"Aljohani, Asmaa and Jones, James",Conducting Malicious Cybersecurity Experiments on Crowdsourcing Platforms,"Online experiments, Participants, Recruitment, Cybersecurity, Crowdsourcing","Evaluating the effectiveness of defense technologies mandates the inclusion of a human element, specifically if these technologies target human cognition and emotions. One of the biggest challenges that face researchers in the realm of behavioral cybersecurity is participant recruitment. Researchers often rely on college students, the general public, real-world hackers, or a hard-to-reach population (e.g., professional red teamers) to test the effectiveness of cybersecurity defense techniques. However, recruiting participants from these populations has drawbacks, including but not limited to: high cost, time constraints, and manageability and accessibility issues. This research explored the applicability of using two popular crowdsourcing platforms, Amazon Mechanical Turk and Prolific, to conduct web hacking experiments. Our study is the first to use crowdsourcing platforms to run hacking experiments for scientific purposes. While the recruitment is challenging, the paradigm of existing crowdsourcing platforms can be useful for understanding adversarial behavior, as it facilitates access to a diverse set of participants and allows researchers to conduct longitudinal and cross-cultural assessments. In particular, crowdsourcing platforms offer a great opportunity for cybersecurity researchers to investigate the Oppositional Human Factors (OHFs) in a manageable and flexible way.",2021,,10.1145/3468920.3468942
2319,"Liang, Yu and Duan, Xuliang and Ding, Yuanjun and Kou, Xifeng and Huang, Jingcheng",Data Mining of Students' Course Selection Based on Currency Rules and Decision Tree,"decision tree, course selection information, currency rules, data mining","The currency of data can ensure that data is not obsolete and outdated. As one of the important bases for evaluating data quality, it plays an important role in the availability of data. Data currency rules can effectively discriminate the temporal relationship between data sets. The decision tree can availably classify and predict the data, and can test the attribute values very well. In this paper, the currency rules are combined with the C4.5 algorithm in the decision tree, and the improved algorithm is applied to the college elective data in recent years. Through experiments, the algorithm used in this paper can extract the statute rules from the student elective database. According to the currency rules, the college teaching plan can be planned in advance and the curriculum resources can be allocated reasonably.",2019,,10.1145/3335484.3335541
2320,"Pan, Zhengjun and Zhao, Lianfen and Zhong, Xingyu and Xia, Zitong",Application of Collaborative Filtering Recommendation Algorithm in Internet Online Courses,"recommendation system, Collaborative filtering algorithm, online course, education platform","Aiming at the problem that the overload of online education platform course resources leads to the difficulty of user selection, this paper mainly studies the improvement and application of collaborative filtering algorithm based on online course recommendation system, which organically combines personalized recommendation technology and online course system to meet the needs of users and online education platform. In the process of recommendation, firstly, user preferences are collected to establish a data model, and user login information and learning behavior information are used as implicit characteristics of user preferences. The loss rate of users in the computing platform is defined, the popularity of each course is calculated, and the relationship between users and courses is constructed, and the correlation and comparative analysis are carried out, Then, the traditional collaborative filtering algorithm is improved by introducing the implicit features after analysis, and the cosine similarity method is used to calculate the course similarity. Finally, the topN recommendation list is generated to get the recommendation results. Based on the desensitization data of an education platform, the experimental results show that the improved recommendation model can improve the precision of recommendation by introducing implicit features.",2021,,10.1145/3469968.3469992
2321,"Wu, Mingming",Multi-Task Representation Learning Network for Trajectory Recovery,"Trajectory recovery, Multi-task learning, Representation learning","Trajectory recovery can benefit many applications such as migration pattern studies of animal and finding hot routes in the urban city. It is necessary to recover trajectory with limited trajectory points to utilize collected trajectory data in a reasonable and efficient way and to provide the better location based service for users. However, the trajectory data involves complex and nonlinear spatial-temporal impacts which cannot be captured by traditional trajectory recovery methods. Moreover, the existing methods consider little about the correlations between trajectory and traffic pattern in the urban city. The superiority of deep neural network makes it possible to recover trajectory with low data quality. We propose a Multi-Task Representation Learning Network (MRL-Net) framework which models the complex nonlinear spatial-temporal correlations in trajectory data with representation learning technique and capture the dependencies of trajectory points with recurrent neural networks. To the best of our knowledge, it is the first paper to address the trajectory recovery problem with representation learning and multi-task learning. Experiments on real-world trajectory data show that our model is superior to state-of-the-art methods.",2020,,10.1145/3404687.3404703
2322,"Esteves, Diego and Rula, Anisa and Reddy, Aniketh Janardhan and Lehmann, Jens",Toward Veracity Assessment in RDF Knowledge Bases: An Exploratory Analysis,"DeFacto, exploratory data analysis, fact checking, benchmark, data quality, linked data, trustworthiness","Among different characteristics of knowledge bases, data quality is one of the most relevant to maximize the benefits of the provided information. Knowledge base quality assessment poses a number of big data challenges such as high volume, variety, velocity, and veracity. In this article, we focus on answering questions related to the assessment of the veracity of facts through Deep Fact Validation (DeFacto), a triple validation framework designed to assess facts in RDF knowledge bases. Despite current developments in the research area, the underlying framework faces many challenges. This article pinpoints and discusses these issues and conducts a thorough analysis of its pipeline, aiming at reducing the error propagation through its components. Furthermore, we discuss recent developments related to this fact validation as well as describing advantages and drawbacks of state-of-the-art models. As a result of this exploratory analysis, we give insights and directions toward a better architecture to tackle the complex task of fact-checking in knowledge bases.",2018,,10.1145/3177873
2323,"El Bacha, Oussama and Jmad, Othmane and El Bouzekri El Idrissi, Younes and Hmina, Nabil",Exploiting Open Data to Improve the Business Intelligence &amp; Business Discovery Experience,"business discovery, business intelligence, knowledge discovery in databases, linked, middleware, data mining, open data, smart city","The extent to which data mining tools are able to make efficient use of an open data oriented strategy in a smart city is limited. In a sense that it is not fully automated, incompatible or has to be supervised. These sets of tools may offer the possibility to import a dataset in a certain predefined standardized format, still, they do not make it a part of their workflow and algorithms in a fully unsupervised manner (i.e without ongoing human guidance). In a departure from previous research works, in this paper, we present a middleware architecture that exploits open data as background knowledge by acting as a bridge between data mining tools and open data resources.",2017,,10.1145/3090354.3090382
2324,"Zhang, Guilan and Wang, Jian and Zhou, Guomin and Liu, Jianping and Wei, Caoyuan",Scientific Data Relevance Criteria Classification and Usage,"Relevance, scientific data, relevance criteria, information carrier","In1 the big data era, scientific data plays a crucial role in scientific research. Data sharing, retrieval and usage has become an inevitable trend. We study how the users of scientific data select relevant data from the data sharing platform. The study was conducted in two stages. In the first stage, a total of 14 subjects were selected to obtain their relevance criteria and usage of scientific data through semi-structured interviews. In the second stage, 671 questionnaires were collected in order to classify criteria. Finally, we determined 9 relevance criteria for scientific data: topicality, availability, comprehensiveness, currency, authority, quality, convenience, standardization, and usability, and divided them to 5 groups. In order to truly make a better data search engine and improve its search efficiency, moving beyond the criteria often used by users, we need to determine those criteria that are not often used, but still very important. What's more, a more convenient data search platform needs to be considered.",2018,,10.1145/3207677.3278010
2325,"Sun, Donglei and Zeng, Jun and Zhu, Yi and Cao, Xiangyang and Wang, Yiqun and Yang, Bo and Yang, Bin and Wang, Nan and Bo, Qibin and Fu, Yimu and Wei, Jia and Liu, Dong",Mechanism Design for Unified Management of Power Grid Planning Data,"mechanism design, power grid planning, data management, data fusion","In order to deal with diversity of massive data structures and the variety of information formats, a novel mechanism is designed for unified management of power grid planning data. By integrating many business systems including production management system (PMS), geographic information system (GIS), energy management system (EMS), distribution network information acquisition system in the power supply company's system and adopting various technical means such as data warehouse technology (e.g. ETL, Extract-Transform-Load) and incremental capture, data structure that supports the whole process management of power grid business is designed, data correlation analysis and integration &amp; migration are carried out, and efficient access and deep fusion of massive relational data, file-type data, distributed data and spatial data are realized. Besides, through computing modes such as diagnostic analysis, load analysis and spatial analysis, the integrated database for power grid planning that integrates data fusion, storage, mining, modeling, computing, analysis and intelligent perception is finally constructed based on the designed data management mechanism, which could provide the comprehensive model and data support for power grid development. Field application shows the engineering benefit of the designed data management mechanism.",2018,,10.1145/3291801.3291826
2326,"Al Fanah, Muna and Ansari, Muhammad Ayub",Understanding E-Learners' Behaviour Using Data Mining Techniques,"Association Rules, Accuracy, Radom Forests, Precision, Bayesian Networks","The information from Higher Education Institutions (HEIs) is primarily relevant for decision maker and educators. This study tackles e-learners behaviour using machine learning, particularly association rules and classifiers. Learners are characterized by a set of behaviours and attitudes that determine their learning abilities and skills. Learning from data generated by online learners may have significant impacts, however, few studies cover this resource from machine learning perspectives. We examine different data mining techniques including Random Forests, Logistic Regressions and Bayesian Networks as classifiers used for predicting e-learners' classes (High, Medium and Low). The novelty of this study is that it explores and compares classifiers performance on the behaviour of online learners on four variables: raise hands, visiting IT resources, view announcement and discussion impact on e-learners. The results of this study indicate an 80% accuracy level obtained by Bayesian Networks; in contrast, the Random Forests have only 63% accuracy level and Logistic Regressions for 58%.",2019,,10.1145/3322134.3322145
2327,"Ennajjar, Ibtissam and Tabii, Youness and Benkaddour, Abdelhamid",Securing Data in Cloud Computing by Classification,"Cloud Computing, Classification, Data Security, Cloud Storage","Cloud computing is a wide architecture based on diverse models for providing different services of software and hardware. Cloud computing paradigm attracts different users because of its several benefits such as high resource elasticity, expense reduction, scalability and simplicity which provide significant preserving in terms of investment and work force. However, the new approaches introduced by the cloud, related to computation outsourcing, distributed resources, multi-tenancy concept, high dynamism of the model, data warehousing and the nontransparent style of cloud increase the security and privacy concerns and makes building and handling trust among cloud service providers and consumers a critical security challenge. This paper proposes a new approach to improve security of data in cloud computing. It suggests a classification model to categorize data before being introduced into a suitable encryption system according to the category. Since data in cloud has not the same sensitivity level, encrypting it with the same algorithms can lead to a lack of security or of resources. By this method we try to optimize the resources consumption and the computation cost while ensuring data confidentiality.",2017,,10.1145/3090354.3090404
2328,"Chahidi, Hamza and Omara, Hicham and Lazaar, Mohamed and Al Achhab, Mohammed",Impact of Neural Network Architectures on Arabic Sentiment Analysis,"Arabic, Multi-layer Perceptron, Machine Learning, Deep Learning, Sentiment Analysis","Sentiment Analysis (SA), commonly known as opinion mining, during last couple of years, it becomes the fastest growing research areas in computer science. Conventionally, it helps to automatically detect if a text express is a positive, negative or neutral opinion. It enables us to identify and extract subjective information in a piece of writing, and this leads to gain an overview of wider public opinions or attitudes toward topics, products or services. Many researches have been done in this area, but most of them have focused on English and other Indo-European languages. Insufficient studies have actually accosted Sentiment Analysis in morphologically rich language such as Arabic. Regardless, given the increasing number of Arabic users and the exponential growth of online content, SA in this language has gained the attention of many researches last years, since Arabic raises many challenges because of its derivational, inflectional and agglutinative morphology. The objective of this paper is to promote the performance of Arabic Sentiment Analysis (ASA) by using Deep learning techniques. For that we implement Multi-Layer perceptron model in order to process and classify a dataset (Tweets). In fact, the experimental results prove that MLP as a deep learning model has a better performance for ASA than classical approaches.",2019,,10.1145/3372938.3372950
2329,"El Kafhali, Said and Chahir, Chorouk and Hanini, Mohamed and Salah, Khaled",Architecture to Manage Internet of Things Data Using Blockchain and Fog Computing,"Internet of Things, Fog computing, NFV, SDN, Smart Contracts, Blockchain, Edge Computing","In this paper, we propose a novel architecture that utilizes features of Blockchain, fog computing, and cloud computing to manage IoT data. Blockchain allows to have a distributed peer-to-peer network in which non-trusting participants can interact with each other without a trusted intermediary or third party. We evaluate how this mechanism works to face the challenges of IoT with respect to multiple accessibility to IoT devises. We consider a Blockchain architecture in presence of edge computing layer. With fog or fog computing, the sensitive data can be analyzed locally instead of sending it to the cloud for analysis. Edge nodes can also keep track and control of the IoT devices that collect, analyze and store data. We show that this control can be better executed when Software Defined Network (SDN) and Network Functions Virtualization (NFV) are integrated into our process for optimal resource management. In this paper, we present our system architecture with a detailed description of the different interactions. We remark that the integration of Blockchain, IoT, and edge computing when coupled with SDN and NFV-enabled cloud infrastructure can bring to more superior and efficient platform for accessing, managing, and processing the huge influx of IoT data.",2019,,10.1145/3372938.3372970
2330,"Srivastava, Divesh and Scannapieco, Monica and Redman, Thomas C.",Ensuring High-Quality Private Data for Responsible Data Science: Vision and Challenges,"quality of private data, Responsible data science, data trust, private data","High-quality data is critical for effective data science. As the use of data science has grown, so too have concerns that individuals’ rights to privacy will be violated. This has led to the development of data protection regulations around the globe and the use of sophisticated anonymization techniques to protect privacy. Such measures make it more challenging for the data scientist to understand the data, exacerbating issues of data quality. Responsible data science aims to develop useful insights from the data while fully embracing these considerations.We pose the high-level problem in this article, “How can a data scientist develop the needed trust that private data has high quality?” We then identify a series of challenges for various data-centric communities and outline research questions for data quality and privacy researchers, which would need to be addressed to effectively answer the problem posed in this article.",2019,,10.1145/3287168
2331,"Chaofan, Dai and Ran, Zhang and Pei, Li and Wenqian, Wang",Design of ETL Provenance Tool Based on Minimal Attribute Set,"minimal attribute set, Metadata, data provenance, ETL, PROV","For the ETL process, this paper designs a provenance tool based on inversible transformation, and describes the meta-information of ETL and data provenance process in two ways: one is to take the database two-dimensional table to describe the relevant information in logical level, easy to record; the other is the use of PROV model information on the xml description, and shows the ETL and the provenance process in the directed acyclic graph, easy to understand.",2017,,10.1145/3152723.3152730
2332,"Bian, Shuqing and Zhao, Wayne Xin and Zhou, Kun and Cai, Jing and He, Yancheng and Yin, Cunxiang and Wen, Ji-Rong",Contrastive Curriculum Learning for Sequential User Behavior Modeling via Data Augmentation,,"Within online platforms, it is critical to capture the semantics of sequential user behaviors for accurately modeling user interests. However, dynamic characteristics and sparse behaviors make it difficult to train effective user representations for sequential user behavior modeling.Inspired by the recent progress in contrastive learning, we propose a novel Contrastive Curriculum Learning framework for producing effective representations for modeling sequential user behaviors. We make important technical contributions in two aspects, namely data quality and sample ordering. Firstly, we design a model-based data generator by generating high-quality samples confirming to users' attribute information. Given a target user, it can leverage the fused attribute semantics for generating more close-to-real sequences. Secondly, we propose a curriculum learning strategy to conduct contrastive learning via an easy-to-difficult learning process. The core component is a learnable difficulty evaluator, which can score augmented sequences, and schedule them in curriculums. Extensive results on both public and industry datasets demonstrate the effectiveness of our approach on downstream tasks.",2021,,
2333,"Huang, Huijun and Zheng, Jiguang",Quality Earned Value Analysis Based on IFPUG Method in Software Project,"earned value analysis, function point method, software project, software quality","Earned value method is an important tool to evaluate and control the schedule and cost of the project. It is widely used in engineering construction projects, but rarely used in software projects. Due to the characteristics of the software project, the accuracy of the calculation of the basic parameters of the earned value analysis is low, which leads to the fact that the credibility of the result of the earned value analysis becomes very low or even meaningless. In order to make Earned value method applied to software projects better , IFPUG function points method is used to measure the actual completion of software projects, then used earned value method on the basis of IFPUG function point method. This can improved the accuracy of earned value analysis better. In order to analyze the cost and schedule of software projects better, the quality factors of software are also taken into account in the analysis of earned value, this can monitor the cost and schedule of software projects accurately.",2018,,10.1145/3226116.3226135
2334,"Majthoub, Manar and Odeh, Yousra and Hijjawi, Mohammed",Non-Functional Requirements Classification for Aligning Business with Information Systems,"Business Models, Business/IT Alignment, Non-Functional Requirements, Quality Requirements, Business Process Model, Use Case, System Model","Non-Functional Requirements (NFR) are defined as the desired quality requirements, such as availability, that restrict software product being developed where some external restrictions may apply. Since information systems have been introduced, organizations in the business world align their functional activities with systems without paying attention to quality-based alignment. Few research works have been conducted in order to classify and integrate the NFR with business or system models. But these classifications and integrations are only confined to either the business side or the system side, which in turn have caused in having a gap in mapping the classifications between the two sides. Because business models and system models mutually affect each other in many ways, their NFR integration and classification should be aligned with each other. Having a NFR alignment-based classification between business and information systems contributes to assist the stakeholders in reflecting the quality requirements at the business side for a particular task on the related tasks integrated with NFRs at the systems side. Also having an alignment-oriented classification contributes to trace quality/NFR-based changes from the business organization to its systems and vice versa.In this research, we propose a NFR classification for aligning quality requirements in business with their NFRs in information systems. The work in business side is represented through business process models designed using Business Process Model and Notation (BPMN) where the use case models represents the system side in this research. The proposed classification is demonstrated in both business and systems using the academic advising and registration case study at Applied Science University in Jordan.",2020,,10.1145/3437075.3437091
2335,"Zhang, Le and Ren, Junda and Yang, Zhi and Yin, Zenan and Chen, Yiting and Gu, Yiming",Analysis of The Advancement of Rpa Technology and Its Application in the Financial Field of Electric Power Enterprises,"intelligence, electric power enterprise, financial robot, digitization, RPA","Under the background of the new technology era of cloud, big things, mobile intelligence, RPA (RoboticsProcessAutomation) technology, as an important and mature application in the field of artificial intelligence, can help financial personnel to free themselves from a large number of simple and complex transactional work and invest in Financial analysis, scientific decision-making and other high value-added work. At present, financial robot products based on RPA technology can be extended to be compatible with OCR, voice, intelligent customer service, deep learning and other functions, supporting the establishment of risk management and control systems and intelligent application scenarios, and ultimately improve the cross-business collaboration capabilities and operation automation efficiency of financial management. Effectively control financial risks, improve the efficiency of data asset use and financial analysis and decision-making capabilities, and provide power companies with good management and economic benefits. This article first analyzes the advantages and technical characteristics of RPA technology, then summarizes the practical application of financial robotics technology in power companies, explores the role of RPA technology in financial digital transformation, and studies its risk management and control models, which are of great significance to improving the comprehensive management level of power grid companies.",2021,,10.1145/3513142.3513235
2336,"Polpinij, Jantima and Namee, Khanista",Internet Usage Patterns Mining from Firewall Event Logs,"Event logs, Generalized Sequential Pattern, Data mining, Sequential pattern mining, Inappropriate user pattern, Internet usage","Understanding users' behavior of internet usage is essential for the quality of service (QoS) analysis on the internet. If the internet providers can better understand their users, they may be able to provide better service, and also enhance the quality of the service. In general, the information about users' behavior is stored as the internet access log files, called event logs, on the server. To have the patterns of users' behavior from the event logs, this work aims to extract an interesting pattern of inappropriate user behaviors through the method of internet usage patterns mining. The primary mechanism of the proposed method is the Generalized Sequential Pattern (GSP) algorithm, which is an algorithm of sequential pattern mining. This study uses real event logs from an organization in Thailand. The results have identified exciting findings that have made possible to propose some improvements and increasing the QoS of the internet service.",2019,,10.1145/3322134.3322155
2337,"Gao, Jing and Li, Qi and Zhao, Bo and Fan, Wei and Han, Jiawei",Truth Discovery and Crowdsourcing Aggregation: A Unified Perspective,,"In the era of Big Data, data entries, even describing the same objects or events, can come from a variety of sources, where a data source can be a web page, a database or a person. Consequently, conflicts among sources become inevitable. To resolve the conflicts and achieve high quality data, truth discovery and crowdsourcing aggregation have been studied intensively. However, although these two topics have a lot in common, they are studied separately and are applied to different domains. To answer the need of a systematic introduction and comparison of the two topics, we present an organized picture on truth discovery and crowdsourcing aggregation in this tutorial. They are compared on both theory and application levels, and their related areas as well as open questions are discussed.",2015,,10.14778/2824032.2824136
2338,"Li, Huan and Lu, Hua and Shi, Feichao and Chen, Gang and Chen, Ke and Shou, Lidan",TRIPS: A System for Translating Raw Indoor Positioning Data into Visual Mobility Semantics,,"The rapid accumulation of indoor positioning data is increasingly booming the interest in indoor mobility analyses. As a fundamental analysis, it is highly relevant to translate raw indoor positioning data into mobility semantics that describe what, where and when in a more concise and semantics-oriented way. Such a translation is challenging as multiple data sources are involved, raw indoor positioning data is of low quality, and translation results are hard to assess. We demonstrate a system TRIPS that streamlines the entire translation process by three functional components. The Configurator provides a standard but concise means to configure multiple input sources, including the indoor positioning data, indoor space information, and relevant contexts. The Translator cleans the indoor positioning data and exports reliable mobility semantics without manual interventions. The Viewer offers a suite of flexible operations to trace the input, output and intermediate data involved in the translation. Data analysts can interact with TRIPS to obtain the desired mobility semantics in a visual and convenient way.",2018,,10.14778/3229863.3236224
2339,"Silva, M\'{a}rio J. and Rijo, Pedro and Francisco, Alexandre",Evaluating the Impact of Anonymization on Large Interaction Network Datasets,"interaction network inference, academic data publishing, privacy-preserving data publishing, privacy of big data","We address the publication of a large academic information dataset addressing privacy issues. We evaluate anonymization techniques achieving the intended protection, while retaining the utility of the anonymized data. The released data could help infer behaviors and subsequently find solutions for daily planning activities, such as cafeteria attendance, cleaning schedules or student performance, or study interaction patterns among an academic population. However, the nature of the academic data is such that many implicit social interaction networks can be derived from the anonymized datasets, raising the need for researching how anonymity can be assessed in this setting.",2014,,10.1145/2663715.2669610
2340,"Liu, Jianping and Wang, Jian and Zhou, Guomin and Zhang, Guilan and Cui, Yunpeng and Liu, Juan",The Cognitive Enhancement Process of Scientific Data Retrieval,"User relevance, Scientific data retrieval, Relevance criteria","Is there a stable cognitive structure of scientific data retrieval process? Based on the theory and method of user relevance research, this study explores the cognitive characteristics of user scientific data query and retrieval. The semi-structured interview method used to collect relevant data, and the content analysis method used to encode and analyze the cognitive process of users' scientific data query and retrieval. The results show that (1) users scientific data relevance judgment not only depend on topicality, but also use accessibility, quality, authority and usefulness. (2) There are 7 combination patterns for the use of user's scientific data relevance criteria, and (3) different patterns correspond to different user relevance types and different user information need states. These 7 criteria usage patterns reveal the cognitive enhancement of user scientific data relevance judgment. The research results have a great inspiration for the development of interactive scientific data retrieval system based on user cognitive enhancement characteristics.",2019,,10.1145/3331453.3360954
2341,"Cao, Yang and Fan, Wenfei and Yu, Wenyuan",Determining the Relative Accuracy of Attributes,"data cleaning, data accuracy","The relative accuracy problem is to determine, given tuples t1 and t2 that refer to the same entity e, whether t1[A] is more accurate than t2A, i.e., t1A is closer to the true value of the A attribute of e than t2A. This has been a longstanding issue for data quality, and is challenging when the true values of e are unknown. This paper proposes a model for determining relative accuracy. (1) We introduce a class of accuracy rules and an inference system with a chase procedure, to deduce relative accuracy. (2) We identify and study several fundamental problems for relative accuracy. Given a set Ie of tuples pertaining to the same entity e and a set of accuracy rules, these problems are to decide whether the chase process terminates, is Church-Rosser, and leads to a unique target tuple te composed of the most accurate values from Ie for all the attributes of e. (3) We propose a framework for inferring accurate values with user interaction. (4) We provide algorithms underlying the framework, to find the unique target tuple te whenever possible; when there is no enough information to decide a complete te, we compute top-k candidate targets based on a preference model. (5) Using real-life and synthetic data, we experimentally verify the effectiveness and efficiency of our method.",2013,,10.1145/2463676.2465309
2342,"Truong, Hong-Linh and Murguzur, Aitor and Yang, Erica",Challenges in Enabling Quality of Analytics in the Cloud,"big data analytics, Cloud computing, service management, data quality",,2018,,10.1145/3138806
2343,"Tekieh, Mohammad Hossein and Raahemi, Bijan",Importance of Data Mining in Healthcare: A Survey,"predictive modelling, health data analysis, health big data, data mining applications, data quality, data mining","In this survey, we collect the related information that demonstrate the importance of data mining in healthcare. As the amount of collected health data is increasing significantly every day, it is believed that a strong analysis tool that is capable of handling and analyzing large health data is essential. Analyzing the health datasets gathered by electronic health record (EHR) systems, insurance claims, health surveys, and other sources, using data mining techniques is very complex and is faced with very specific challenges, including data quality and privacy issues. However, the applications of data mining in healthcare, advantages of data mining techniques over traditional methods, special characteristics of health data, and new health condition mysteries have made data mining very necessary for health data analysis.",2015,,10.1145/2808797.2809367
2344,"Wang, Yiyang",A Comparison of Machine Learning Algorithms in Blood Glucose Prediction for People with Type 1 Diabetes,,"Diabetes is a metabolic disease with the characteristic of hyperglycemia. The pathogenic principle is derived from the defect of insulin secretion or the impairment of biological effects, or both. We use machine learning models and deep learning models for forecasting future blood glucose levels in this paper, and study the efficiency of detecting hypoglycemia and hyperglycemia events. The data set used is in-silico data generated from the UVA/PADOVA type 1 diabetes simulator. We aim to compare support vector machines, random forests, linear regression, K-Nearest Neighbors regression (KNN), XGBoosted trees and other deep learning models in terms of Root Mean Squared Error (RMSE), and other several evaluation metrics to study their effectiveness in predicting future blood sugar, and the accuracy rate of predicting hypoglycemia and hyperglycemia events. In this work, we found a bidirectional long-short term memory (LSTM) model with the best prediction effect, which can predict the blood glucose level of simulated patients with leading accuracy within 30 minutes (RMSE = 7.55±0.19 [mg/dl], R2-SCORE=0.96). The hopeful results show that this method could have practical application value for self-management of blood glucose in patients with type 1 diabetes.",2021,,
2345,"Lu, Shuqi and Dou, Zhicheng and Jun, Xu and Nie, Jian-Yun and Wen, Ji-Rong",PSGAN: A Minimax Game for Personalized Search with Limited and Noisy Click Data,"generative adversarial network, personalized web search","Personalized search aims to adapt document ranking to user's personal interests. Traditionally, this is done by extracting click and topical features from historical data in order to construct a user profile. In recent years, deep learning has been successfully used in personalized search due to its ability of automatic feature learning. However, the small amount of noisy personal data poses challenges to deep learning models to learn the personalized classification boundary between relevant and irrelevant results. In this paper, we propose PSGAN, a Generative Adversarial Network (GAN) framework for personalized search. By means of adversarial training, we enforce the model to pay more attention to training data that are difficult to distinguish. We use the discriminator to evaluate personalized relevance of documents and use the generator to learn the distribution of relevant documents. Two alternative ways to construct the generator in the framework are tested: based on the current query or based on a set of generated queries. Experiments on data from a commercial search engine show that our models can yield significant improvements over state-of-the-art models.",2019,,10.1145/3331184.3331218
2346,"Qin, Haiqing and Liu, Xiaohan and Qin, Haiqi and Zhu, Jiang",How to Be an Enabler of Digital Transformation for Media Organizations?,"Media organization, Big data, Case study, Data empowerment","Big data brings opportunities for the development of the media industry and also poses serious challenges. During the digital transformation period, media organizations have used third-party big data to achieve digital transformation and build a media digital ecology, which has become the focus of strategic development at this stage. This article takes the application of the data assets of China Unicom Big Data Co., Ltd. (UBD) in the media industry as an example, and explores the difficulties and attempts made by UBD. through in-depth interviews. The countermeasures are summarized from different aspects such as technology and data governance, which provide a reference for the digital transformation of media organizations.",2020,,10.1145/3414752.3414772
2347,"Nesen, Alina and Bhargava, Bharat",Towards Situational Awareness with Multimodal Streaming Data Fusion: Serverless Computing Approach,,"The availability of large quantities of data has given an impulse for methods and techniques to extract unseen useful knowledge and process it in a fast and scalable manner. In order to extract the most complete possible knowledge from the continuous data stream, it is necessary to use the heterogeneous data sources and process information from multiple modalities. The systems that utilize multimodal data must take advantage of the up-to-date approaches for data storage, usage, cleaning and storage. Neural networks and machine learning approaches are widely used for data-heavy software where pattern extractions and predictions need to be conducted while serverless computing frameworks are being increasingly used for machine learning solutions to optimize cost and speed of such systems. This work presents a framework for processing data from multimodal sources where the task of feature and pattern extraction is performed on a serverless computing platform. The use cases for public safety solutions to increase situational awareness are described and compared with other implementation approaches.",2021,,
2348,"Farid, Mina and Roatis, Alexandra and Ilyas, Ihab F. and Hoffmann, Hella-Franziska and Chu, Xu",CLAMS: Bringing Quality to Data Lakes,"data quality, data lakes, RDF","With the increasing incentive of enterprises to ingest as much data as they can in what is commonly referred to as ""data lakes"", and with the recent development of multiple technologies to support this ""load-first"" paradigm, the new environment presents serious data management challenges. Among them, the assessment of data quality and cleaning large volumes of heterogeneous data sources become essential tasks in unveiling the value of big data. The coveted use of unstructured and semi-structured data in large volumes makes current data cleaning tools (primarily designed for relational data) not directly adoptable.We present CLAMS, a system to discover and enforce expressive integrity constraints from large amounts of lake data with very limited schema information (e.g., represented as RDF triples). This demonstration shows how CLAMS is able to discover the constraints and the schemas they are defined on simultaneously. CLAMS also introduces a scale-out solution to efficiently detect errors in the raw data. CLAMS interacts with human experts to both validate the discovered constraints and to suggest data repairs.CLAMS has been deployed in a real large-scale enterprise data lake and was experimented with a real data set of 1.2 billion triples. It has been able to spot multiple obscure data inconsistencies and errors early in the data processing stack, providing huge value to the enterprise.",2016,,10.1145/2882903.2899391
2349,"Wang, Mo and Wang, Jing and Song, Yulun",A Map Matching Method for Restoring Movement Routes with Cellular Signaling Data,"map matching, road networks, human mobility, signaling data","Cellular signaling data is a valuable and abundant data source to explore human mobility. Yet challenges remain to restore movement routes from signaling data due to its coarse positioning information. We propose an efficient map matching method based on road network topology. First, a customized spatial-temporal clustering algorithm ST-DBSCAN was employed to find stationary point clusters, which were later used to segment trips into sub-trips. The search space was then clipped with a fixed buffer zone along the line that connects the whole trip. Two optional strategies were provided to find the best matching routes with distance costs. Experiments on real-world data showed that both strategies achieved high map matching accuracies (88.2% and 94.3%). With Deep Mode, the method reached higher accuracy, while with longer computation time. The proposed method has the potential in solving practical problems, in the sense that it could be easily parallelized to deal with mass data.",2020,,10.1145/3446999.3447017
2350,"Lehmberg, Oliver and Bizer, Christian",Profiling the Semantics of N-Ary Web Table Data,"n-ary relations, web tables, data profiling, key detection","The Web contains millions of relational HTML tables, which cover a multitude of different, often very specific topics. This rich pool of data has motivated a growing body of research on methods that use web table data to extend local tables with additional attributes or add missing facts to knowledge bases. Nearly all existing approaches for these tasks build upon the assumption that web table data consists of binary relations, meaning that an attribute value depends on a single key attribute, and that the key attribute value is contained in the HTML table. Inspecting randomly chosen tables on the Web, however, quickly reveals that both assumptions are wrong for a large fraction of the tables. In order to better understand the potential of non-binary web table data for downstream applications, this papers analyses a corpus of 5 million web tables originating from 80 thousand different web sites with respect to how many web table attributes are non-binary, what composite keys are required to correctly interpret the semantics of the non-binary attributes, and whether the values of these keys are found in the table itself or need to be extracted from the page surrounding the table. The profiling of the corpus shows that at least 38% of the relations are non-binary. Recognizing these relations requires information from the title or the URL of the web page in 50% of the cases. We find that different websites use keys of varying length for the same dependent attribute, e.g. one cluster of websites presents employment numbers depending on time, another cluster presents them depending on time and profession. By identifying these clusters, we lay the foundation for selecting Web data sources according to the specificity of the keys that are used to determine specific attributes.",2019,,10.1145/3323878.3325806
2351,"Luckner, Marcin and Grzenda, Maciej and Kunicki, Robert and Legierski, Jaroslaw",IoT Architecture for Urban Data-Centric Services and Applications,"Data stream, public transport, big data, data processing","In this work, we describe an urban Internet of Things (IoT) architecture, grounded in big data patterns and focused on the needs of cities and their key stakeholders. First, the architecture of the dedicated platform USE4IoT (Urban Service Environment for the Internet of Things), which gathers and processes urban big data and extends the Lambda architecture, is proposed. We describe how the platform was used to make IoT an enabling technology for intelligent transport planning. Moreover, key data processing components vital to provide high-quality IoT data streams in a near-real-time manner are defined. Furthermore, tests showing how the IoT platform described in this study provides a low-latency analytical environment for smart cities are included.",2020,,10.1145/3396850
2352,"Dasu, Tamraparni and Loh, Ji Meng and Srivastava, Divesh",Empirical Glitch Explanations,"quantitative data cleaning, glitch explanations, data quality, crossover subsampling","Data glitches are unusual observations that do not conform to data quality expectations, be they logical, semantic or statistical. By applying data integrity constraints, potentially large sections of data could be flagged as being noncompliant. Ignoring or repairing significant sections of the data could fundamentally bias the results and conclusions drawn from analyses. In the context of Big Data where large numbers and volumes of feeds from disparate sources are integrated, it is likely that significant portions of seemingly noncompliant data are actually legitimate usable data.In this paper, we introduce the notion of Empirical Glitch Explanations - concise, multi-dimensional descriptions of subsets of potentially dirty data - and propose a scalable method for empirically generating such explanatory characterizations. The explanations could serve two valuable functions: (1) Provide a way of identifying legitimate data and releasing it back into the pool of clean data. In doing so, we reduce cleaning-related statistical distortion of the data; (2) Used to refine existing data quality constraints and generate and formalize domain knowledge.We conduct experiments using real and simulated data to demonstrate the scalability of our method and the robustness of explanations. In addition, we use two real world examples to demonstrate the utility of the explanations where we reclaim over 99% of the suspicious data, keeping data repair related statistical distortion close to 0.",2014,,10.1145/2623330.2623716
2353,"Cuzzocrea, Alfredo and Gaber, Mohamed Medhat and Lattimer, Staci and Grasso, Giorgio Mario",Clustering-Based Spatio-Temporal Analysis of Big Atmospheric Data,"Big Data Mining, Big Environmental and Atmospheric Data, Clustering-Based Spatio-Temporal Analysis of Big Data","This paper proposes a comprehensive approach for supporting clustering-based spatio-temporal analysis of big atmospheric data via specializing on the interesting applicative setting represented by Greenhouse Gas Emissions (GGEs), a relevant instance of Big Data that empathize the Variety aspect of the well-known 3V Big Data axioms. In particular, in our research we consider GGEs from three EU countries, namely UK, France and Italy. The deriving Big Data Mining model turns to be useful for decision support processes in both the governmental and industrial contexts.",2016,,10.1145/2896387.2900326
2354,"Abdellaoui, Sabrina and Bellatreche, Ladjel and Nader, Fahima",A Quality-Driven Approach for Building Heterogeneous Distributed Databases: The Case of Data Warehouses,"data warehouse, ontologies, ETL design, semantic database sources, data quality","Data Warehouse (DW) is a collection of data, consolidated from several heterogeneous sources, used to perform data analysis and support decision-making in an organization. Extract-Transform-Load (ETL) phase plays a crucial role in designing DW. To overcome the complexity of the ETL phase, different studies have recently proposed the use of ontologies. Ontology-based ETL approaches have been used to reduce heterogeneity between data sources and ensure automation of the ETL process. Existing studies in semantic ETL have largely focused on fulfilling functional requirements. However, the ETL process quality dimension has not been sufficiently considered by these studies. As the amount of data has exploded with the advent of big data era, dealing with quality challenges in the early stages of designing the process become more important than ever. To address this issue, we propose to keep data quality requirements at the center of the ETL phase design. We present in this paper an approach, defining the ETL process at the ontological level. We define a set of quality indicators and quantitative measures that can anticipate data quality problems and identify causes of deficiencies. Our approach checks the quality of data before loading them into the target data warehouse to avoid the propagation of corrupted data. Finally, our proposal is validated through a case study, using Oracle Semantic DataBase sources (SDBs), where each source references the Lehigh University BenchMark ontology (LUBM).",2016,,10.1109/CCGrid.2016.79
2355,"Arlitt, Martin and Marwah, Manish and Bellala, Gowtham and Shah, Amip and Healey, Jeff and Vandiver, Ben",IoTAbench: An Internet of Things Analytics Benchmark,"performance evaluation, benchmarking, big data, internet of things","The commoditization of sensors and communication networks is enabling vast quantities of data to be generated by and collected from cyber-physical systems. This ``Internet-of-Things"" (IoT) makes possible new business opportunities, from usage-based insurance to proactive equipment maintenance. While many technology vendors now offer ``Big Data"" solutions, a challenge for potential customers is understanding quantitatively how these solutions will work for IoT use cases. This paper describes a benchmark toolkit called IoTAbench for IoT Big Data scenarios. This toolset facilitates repeatable testing that can be easily extended to multiple IoT use cases, including a user's specific needs, interests or dataset. We demonstrate the benchmark via a smart metering use case involving an eight-node cluster running the HP Vertica analytics platform. The use case involves generating, loading, repairing and analyzing synthetic meter readings. The intent of IoTAbench is to provide the means to perform ``apples-to-apples"" comparisons between different sensor data and analytics platforms. We illustrate the capabilities of IoTAbench via a large experimental study, where we store 22.8 trillion smart meter readings totaling 727 TB of data in our eight-node cluster.",2015,,10.1145/2668930.2688055
2356,"Ding, Junhua and Li, Xinchuan and Kang, Xiaojun and Gudivada, Venkat N.",A Case Study of the Augmentation and Evaluation of Training Data for Deep Learning,"Data quality, deep learning, machine learning, diffraction image, support vector machine, convolutional neural network","Deep learning has been widely used for extracting values from big data. As many other machine learning algorithms, deep learning requires significant training data. Experiments have shown both the volume and the quality of training data can significantly impact the effectiveness of the value extraction. In some cases, the volume of training data is not sufficiently large for effectively training a deep learning model. In other cases, the quality of training data is not high enough to achieve the optimal performance. Many approaches have been proposed for augmenting training data to mitigate the deficiency. However, whether the augmented data are “fit for purpose” of deep learning is still a question. A framework for comprehensively evaluating the effectiveness of the augmented data for deep learning is still not available. In this article, we first discuss a data augmentation approach for deep learning. The approach includes two components: the first one is to remove noisy data in a dataset using a machine learning based classification to improve its quality, and the second one is to increase the volume of the dataset for effectively training a deep learning model. To evaluate the quality of the augmented data in fidelity, variety, and veracity, a data quality evaluation framework is proposed. We demonstrated the effectiveness of the data augmentation approach and the data quality evaluation framework through studying an automated classification of biology cell images using deep learning. The experimental results clearly demonstrated the impact of the volume and quality of training data to the performance of deep learning and the importance of the data quality evaluation. The data augmentation approach and the data quality evaluation framework can be straightforwardly adapted for deep learning study in other domains.",2019,,10.1145/3317573
2357,"Caruccio, Loredana and Cirillo, Stefano",Incremental Discovery of Imprecise Functional Dependencies,"tuple insertions, discovery algorithm, incremental discovery, Functional dependency, parallelism","Functional dependencies (fds) are one of the metadata used to assess data quality and to perform data cleaning operations. However, to pursue robustness with respect to data errors, it has been necessary to devise imprecise versions of functional dependencies, yielding relaxed functional dependencies (rfds). Among them, there exists the class of rfds relaxing on the extent, i.e., those admitting the possibility that an fd holds on a subset of data. In the literature, several algorithms to automatically discover rfds from big data collections have been defined. They achieve good performances with respect to the inherent problem complexity. However, most of them are capable of discovering rfds only by batch processing the entire dataset. This is not suitable in the era of big data, where the size of a database instance can grow with high-velocity, and the insertion of new data can invalidate previously holding rfds. Thus, it is necessary to devise incremental discovery algorithms capable of updating the set of holding rfds upon data insertions, without processing the entire dataset. To this end, in this article we propose an incremental discovery algorithm for rfds relaxing on the extent. It manages the validation of candidate rfds and the generation of possibly new rfd candidates upon the insertion of the new tuples, while limiting the size of the overall search space. Experimental results show that the proposed algorithm achieves extremely good performances on real-world datasets.",2020,,10.1145/3397462
2358,"Min, Han Xue and Feng, Zheng Gao and Xi, Liu Peng and Dong, Wang Xu and Ping, Xu Zhong",Application Research of Power Grid Full-Business Monitoring and Analysis Based on Multi-Source Business and Data Fusion,"mining analysis, business and data fusion, Big data technology","With the development of power enterprise informationization after more than ten years of development, Achieve full coverage of the company's business, effectively supporting the full-business operation of the power grid, and the accumulated business data has exploded. However, there are still problems such as low data quality, insufficient integration of multi-source business and data fusion, which makes it difficult to monitor and analyze the full-business of the power grid. This paper will combine the big data technology to study how to conduct monitoring and analysis of power grid full-business operation based on multi-source business and data fusion, and realize the three-layer architecture of business and data combination layer, business and data integration layer and business and data aggregation layer. Different levels of analysis and application, such as indicator monitoring analysis, subject monitoring analysis, and special monitoring analysis, effectively support enterprise management analysis and analytical decision.",2019,,10.1145/3357292.3357306
2359,"Lawrenz, Sebastian and Sharma, Priyanka and Rausch, Andreas",Blockchain Technology as an Approach for Data Marketplaces,"Blockchain, Security, Smart Contracts, Data marketplaces, Data quality","In the digital Economy 'Data is the new oil. In the last decade technology has disrupted every filed imaginable. One such booming technology is Blockchain. A blockchain is essentially a distributed database of records or public ledger of all transactions or digital events that have been executed and shared among participating parties. And once entered, the information is immutable. Ongoing projects and prior work in the fields of big data, data mining and data science has revealed how relevant data can be used to enhance products and services. There are uncountable applications and advantages of relevant data. The most valuable companies of today treat data as a commodity, which they trade and earn revenues.But use of relevant data has also drawn attention by the other non-conventional organizations and domains. To facilitate such trading, data marketplaces have emerged. In this paper we present a global data marketplace for users to easily buy and share data. The main focus of this research is to have a central data sharing platform for the recycling industry. This paper is a part of the research project ""Recycling 4.0"" which is focusing on sustainably improving the recycling process through exchange of information. We identify providing secure platform, data integrity and data quality as some major challenges for a data marketplace. In this paper we also explore how global data marketplace could be implemented using blockchain and similar technologies.",2019,,10.1145/3320154.3320165
2360,"Casale, G. and Ardagna, D. and Artac, M. and Barbier, F. and Nitto, E. Di and Henry, A. and Iuhasz, G. and Joubert, C. and Merseguer, J. and Munteanu, V. I. and P\'{e}rez, J. F. and Petcu, D. and Rossi, M. and Sheridan, C. and Spais, I. and Vladu\v{s}i\v{c}, D.",DICE: Quality-Driven Development of Data-Intensive Cloud Applications,"model-driven engineering, big data, quality assurance","Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.",2015,,
2361,"Huang, Yongliang and Yang, Shulin and Li, Xiang and Peng, Jiao and Zhou, Meiqi",Research on Publisher Topic Selection Based on Data Mining,"Decision tree, Big data, Publishing topics, Data mining","As one of the important traditional industries in China, the printing and publishing industry is facing the current situation of the Internet era with the explosion of information and people's demands tend to be personalized and diversified.How to achieve accurate topic selection is the key.In this context, this paper combines the most popular big data technology with the traditional printing industry, improves the quality of the original data of the publishing house through data preprocessing technology, classifies different types of data by decision tree classifier, and finally completes the data mining.It provides a new thought and method for the topic planning of publishing industry.",2021,,10.1145/3478905.3478999
2362,"Deshpande, Mukund and Ray, Dhruva and Dixit, Sameer and Agasti, Avadhoot",ShareInsights: An Unified Approach to Full-Stack Data Processing,"data pipeline, big data, data visualization, data analysis","The field of data analysis seeks to extract value from data for either business or scientific benefit. This field has seen a renewed interest with the advent of big data technologies and a new organizational role called data scientist. Even with the new found focus, the task of analyzing large amounts of data is still challenging and time-consuming.The essence of data analysis involves setting up data pipe-lines which consists of several operations that are chained together - starting from data collection, data quality checks, data integration, data analysis and data visualization (including the setting up of interaction paths in that visualization).In our opinion, the challenges stem from from the technology diversity at each stage of the data pipeline as well as the lack of process around the analysis.In this paper we present a platform that aims to significantly reduce the time it takes to build data pipelines. The platform attempts to achieve this in following ways. Allow the user to describe the entire data pipeline with a single language and idioms - all the way from data ingestion to insight expression (via visualization and end-user interaction).Provide a rich library of parts that allow users to quickly assemble a data analysis pipeline in the language.Allow for a collaboration model that allows multiple users to work together on a data analysis pipeline as well as leverage and extend prior work with minimal effort.We studied the efficacy of the platform for a data hackathon competition conducted in our organization. The hackathon provided us with a way to study the impact of the approach. Rich data pipelines which traditionally took weeks to build were constructed and deployed in hours. Consequently, we believe that the complexity of designing and running the data analysis pipeline can be significantly reduced; leading to a marked improvement in the productivity of data analysts/data scientists.",2015,,10.1145/2723372.2742800
2363,"Howes, Joshua and Solderitsch, James and Chen, Ignatius and Craighead, Jont\'{e}",Enabling Trustworthy Spaces via Orchestrated Analytical Security,"complex event processing, analytics, orchestration, Hadoop, intelligence, trust, big data, security","Cyberspaces require both the implementation of customized functional requirements and the enforcement of policy constraints to be trustworthy. In tailored, distributed and adaptive environments (spaces), monitoring to ensure this enforcement is especially difficult given the wide spectrum of activities performed and the evolving range of threats. Spaces must be monitored from a multitude of perspectives, each of which will generate a vast quantity of disparate information, including structured, semi-structured and unstructured data. However, existing security toolsets and offerings are not yet well equipped to analyze these kinds of data with the necessary speed and agility. Big Data technologies, such as Hadoop, enable the analysis of large and unstructured data sources. We propose security operations teams extend their existing security infrastructure with emerging Big Data analytics and Complex Event Processing platforms. To help them do so, we introduce a conceptual blueprint for the analytics solution. We also present an Orchestrated Analytical Security operational and organizational framework that helps organizations understand how analytical security not only provides monitoring but also creates actionable intelligence from data.",2013,,10.1145/2459976.2459991
2364,"Posadas, Brianna B. and Hanumappa, Mamatha and Gilbert, Juan E.",Opinions Concerning Crowdsourcing Applications in Agriculture in D.C.,"urban agriculture, big data, focus groups, precision agriculture","As big data has become increasingly necessary in modern farming techniques, the dependence on high quality and quantity of ground truthing data has risen. Collecting ground truthing data is one of the most labor-intensive aspects of the research process. A crowdsourcing platform application to aid laypeople in completing ground truthing data can improve the quality and quantity of data for growers and agricultural researchers. Focus groups were conducted to gauge opinions on crowdsourcing initiatives in agriculture to inform the design of the platform. Preliminary results demonstrate that the greatest motivation for the participants was having opportunities to develop their skills and access to educational resources. They also discussed having a finite timeframe for collecting the data, feeling appreciated by the researchers, and being informed on the context and next steps of the research. The results of these focus groups will be used to develop design prototypes for the crowdsourcing platform.",2019,,10.1145/3358961.3358970
2365,"Korachi, Zineb and Bounabat, Bouchaib",Data Driven Maturity Model for Assessing Smart Cities,"maturity model, DQSC-MM, data quality, Smart city, data quality measurement, smart city evaluation, data","Smart cities provide the ability to improve the quality of the citizen's life. Transformation into a smart city consists of defining the way ICT (Information and Communication Technologies) can be used to improve the weaker aspects of the city and improve the quality of services provided by public sectors (education, health, transportation...). The growth of the urban population implies the growing needs of urban services (health, education...) and resources (water, energy...). ICT can be used to meet the growing population needs and solve many of today's problems in the private and public sectors (health, transportation, school...). Using mobile phones all citizens produce data and information every day and everywhere, this data will be used to improve the quality of services provided by the city. The quality of the generated data presents the key element that will impact the success of the transformation into a smart city.This paper describes the proposed data quality driven smart cities model. The proposed model, called DQSC-MM (Data Quality Driven Smart Cities Maturity Model). DQSC-MM is used to evaluate the maturity of a smart city based on the quality of produced and consumed data. It suggests a way to measure the importance of data quality in a city's transformation into a smart city. The paper describes how the model was conceived, designed and developed. It describes also a JEE application conceived to support DQSC-MM. The developed application provides the ability to measure data quality and use these measurements for smart city evaluation.",2018,,10.1145/3289100.3289123
2366,"Sun, Yunchuan and Zeng, Xiaoping and Cui, Xuegang and Zhang, Guangzhi and Bie, Rongfang",An Active and Dynamic Credit Reporting System for SMEs in China,"Information asymmetry, Credit reporting, Blockchain, Big data, SMEs","SMEs in China always face financing constraints and hardly obtain bank loans under unsound financing system due to the information asymmetry, while thousands of SMEs have contributed greatly to Chinese economic development in the last decades. Credit reporting has been verified to be an effective way to lower information asymmetry. However, existed credit reporting systems for SMEs can not meet the development of SMEs and provide enough information to the financial institutions in China. This paper introduces an active and dynamic credit reporting framework based on Big data and Blockchain for SMEs. The framework is composed of five modules, including credit data acquisition, authentication, evaluation, reporting, and interaction. And it features in capturing diversified data online, conducting evaluation and analysis in real time, generating online credit reports for users automatically, and providing an effective way for different entities to interact. A case study from a real credit evaluation company is also proposed finally to show the proposed framework.",2021,,10.1007/s00779-019-01275-4
2367,"Madkour, Amgad and Aref, Walid G. and Prabhakar, Sunil and Ali, Mohamed and Bykau, Siarhei",TrueWeb: A Proposal for Scalable Semantically-Guided Data Management and Truth Finding in Heterogeneous Web Sources,"Linked Data, Truth detection, RDF, Data Management","We envision a responsible web environment, termed TrueWeb, where a user should be able to find out whether any sentence he or she encounters in the web is true or false. The user should be able to track the provenance of any sentence or paragraph in the web. The target of TrueWeb is to compose factual knowledge from Internet resources about any subject of interest and present the collected knowledge in chronological order and distribute facts spatially and temporally as well as assign some belief factor for each fact. Another important target of TrueWeb is to be able to identify whether a statement in the Internet is true or false. The aim is to create an Internet infrastructure that, for each piece of published information, will be able to identify the truthfulness (or the degree of truthfulness) of that piece of information.",2018,,10.1145/3208352.3208357
2368,"Tian, Haiman and Presa-Reyes, Maria and Tao, Yudong and Wang, Tianyi and Pouyanfar, Samira and Miguel, Alonso and Luis, Steven and Shyu, Mei-Ling and Chen, Shu-Ching and Iyengar, Sundaraja Sitharama",Data Analytics for Air Travel Data: A Survey and New Perspectives,"Airline, revenue management, big data","From the start, the airline industry has remarkably connected countries all over the world through rapid long-distance transportation, helping people overcome geographic barriers. Consequently, this has ushered in substantial economic growth, both nationally and internationally. The airline industry produces vast amounts of data, capturing a diverse set of information about their operations, including data related to passengers, freight, flights, and much more. Analyzing air travel data can advance the understanding of airline market dynamics, allowing companies to provide customized, efficient, and safe transportation services. Due to big data challenges in such a complex environment, the benefits of drawing insights from the air travel data in the airline industry have not yet been fully explored. This article aims to survey various components and corresponding proposed data analysis methodologies that have been identified as essential to the inner workings of the airline industry. We introduce existing data sources commonly used in the papers surveyed and summarize their availability. Finally, we discuss several potential research directions to better harness airline data in the future. We anticipate this study to be used as a comprehensive reference for both members of the airline industry and academic scholars with an interest in airline research.",2021,,10.1145/3469028
2369,"Hai, Rihan and Geisler, Sandra and Quix, Christoph",Constance: An Intelligent Data Lake System,"data quality, data lake, data integration","As the challenge of our time, Big Data still has many research hassles, especially the variety of data. The high diversity of data sources often results in information silos, a collection of non-integrated data management systems with heterogeneous schemas, query languages, and APIs. Data Lake systems have been proposed as a solution to this problem, by providing a schema-less repository for raw data with a common access interface. However, just dumping all data into a data lake without any metadata management, would only lead to a 'data swamp'. To avoid this, we propose Constance, a Data Lake system with sophisticated metadata management over raw data extracted from heterogeneous data sources. Constance discovers, extracts, and summarizes the structural metadata from the data sources, and annotates data and metadata with semantic information to avoid ambiguities. With embedded query rewriting engines supporting structured data and semi-structured data, Constance provides users a unified interface for query processing and data exploration. During the demo, we will walk through each functional component of Constance. Constance will be applied to two real-life use cases in order to show attendees the importance and usefulness of our generic and extensible data lake system.",2016,,10.1145/2882903.2899389
2370,"Chen, Lihan and Liang, Jiaqing and Xie, Chenhao and Xiao, Yanghua",Short Text Entity Linking with Fine-Grained Topics,"short text, fine-grained topics, entity linking, concepts","A wide range of web corpora are in the form of short text, such as QA queries, search queries and news titles. Entity linking for these short texts is quite important. Most of supervised approaches are not effective for short text entity linking. The training data for supervised approaches are not suitable for short text and insufficient for low-resourced languages. Previous unsupervised methods are incapable of handling the sparsity and noisy problem of short text. We try to solve the problem by mapping the sparse short text to a topic space. We notice that the concepts of entities have rich topic information and characterize entities in a very fine-grained granularity. Hence, we use the concepts of entities as topics to explicitly represent the context, which helps improve the performance of entity linking for short text. We leverage our linking approach to segment the short text semantically, and build a system for short entity text recognition and linking. Our entity linking approach exhibits the state-of-the-art performance on several datasets for the realistic short text entity linking problem.",2018,,10.1145/3269206.3271809
2371,"Abdellaoui, Sabrina and Nader, Fahima and Chalal, Rachid",QDflows: A System Driven by Knowledge Bases for Designing Quality-Aware Data Flows,"knowledge bases, graph-based repairing, Data flows, data quality","In the big data era, data integration is becoming increasingly important. It is usually handled by data flows processes that extract, transform, and clean data from several sources, and populate the data integration system (DIS). Designing data flows is facing several challenges. In this article, we deal with data quality issues such as (1) specifying a set of quality rules, (2) enforcing them on the data flow pipeline to detect violations, and (3) producing accurate repairs for the detected violations. We propose QDflows, a system for designing quality-aware data flows that considers the following as input: (1) a high-quality knowledge base (KB) as the global schema of integration, (2) a set of data sources and a set of validated users’ requirements, (3) a set of defined mappings between data sources and the KB, and (4) a set of quality rules specified by users. QDflows uses an ontology to design the DIS schema. It offers the ability to define the DIS ontology as a module of the knowledge base, based on validated users’ requirements. The DIS ontology model is then extended with multiple types of quality rules specified by users. QDflows extracts and transforms data from sources to populate the DIS. It detects violations of quality rules enforced on the data flows, constructs repair patterns, searches for horizontal and vertical matches in the knowledge base, and performs an automatic repair when possible or generates possible repairs. It interactively involves users to validate the repair process before loading the clean data into the DIS. Using real-life and synthetic datasets, the DBpedia and Yago knowledge bases, we experimentally evaluate the generality, effectiveness, and efficiency of QDflows. We also showcase an interactive tool implementing our system.",2017,,10.1145/3064173
2372,"Gotterbarn, Don",The Creation of Facts in the Cloud: A Fiction in the Making,"data misuse, professional responsibility, big data, data integrity","Like most significant changes in technology, Cloud Computing and Big Data along with their associated analytic techniques are claimed to provide us with new insights unattainable by any previous knowledge techniques. It is believed that the quantity of virtual data now available requires new knowledge production strategies. Although they have yielded significant results, there are problems with advocated processes and resulting facts. The primary process treats ""pattern recognition"" as a final result rather than using ""pattern recognition"" to lead to yet to be tested testable hypotheses. In data analytics, the discovery of a pattern is treated as knowledge rather than going further to understand the possible causes of those patterns. When this is used as the primary approach to knowledge acquisition unjustified inferences are made - ""fact generation"". These pseudo-facts are used to generate new pseudo-facts as those initial inferences are fed back into analytic engines as established facts. The approach of generating ""facts from data analytics"" is introducing highly risky scenarios where ""fiction becomes fact"" very quickly. These ""facts"" are then given elevated epistemic status and get used in decision making. This, misleading approach is inconsistent with the moral duty of computing professionals embodied in their Codes of Ethics. There are some ways to mitigate the problems generated by this single path approach to knowledge generation.",2016,,10.1145/2874239.2874248
2373,"Alabduljabbar, Reham and Al-Dossari, Hmood",A Task Ontology-Based Model for Quality Control in Crowdsourcing Systems,"HITs, Big Data, Ontology, Crowd Computing, MTurk, Reputation, Human Computation, Quality Control, Crowdsourcing","In the era of big data, a vast amount of data is created every day. Crowdsourcing systems have recently gained significance as an interesting practice in managing and performing big data operations. Crowdsourcing has facilitated the process of performing tasks that cannot be adequately solved by machines including image labeling, transcriptions, data validation and sentiment analysis. However, quality control remains one of the biggest challenges for crowdsourcing. Current crowdsourcing systems use the same quality control mechanism for evaluating different types of tasks. In this paper, we argue that quality mechanisms vary by task type. We propose a task ontology-based model to identify the most appropriate quality mechanism for a given task. The proposed model has been enriched by a reputation system to collect requesters' feedback on quality mechanisms. Accordingly, the reputation of each mechanism can be established and used for mapping between tasks and mechanisms. Description of the model's framework, algorithms, and its components' interaction are presented.",2016,,10.1145/2987386.2987413
2374,"Werner, Martin and Kiermeier, Marie",A Low-Dimensional Feature Vector Representation for Alignment-Free Spatial Trajectory Analysis,"big data, trajectory, moving objects, multi-modal trajectory","Trajectory analysis is a central problem in the era of big data due to numerous interconnected mobile devices generating unprecedented amounts of spatio-temporal trajectories. Unfortunately, datasets of spatial trajectories are quite difficult to analyse because of the computational complexity of the various existing distance measures. A significant amount of work in comparing two trajectories stems from calculating temporal alignments of the involved spatial points. With this paper, we propose an alignment-free method of representing spatial trajectories using low-dimensional feature vectors by summarizing the combinatorics of shape-derived string sequences. Therefore, we propose to translate trajectories into strings describing the evolving shape of each trajectory, and then provide a sparse matrix representation of these strings using frequencies of adjacencies of characters (n-grams). The final feature vectors are constructed by approximating this matrix with low-dimensional column space using singular value decomposition. New trajectories can be projected into this geometry for comparison. We show that this construction leads to low-dimensional feature vectors with surprising expressive power. We illustrate the usefulness of this approach in various datasets.",2016,,10.1145/3004725.3004733
2375,"Grillenberger, Andreas and Romeike, Ralf",Bringing the Innovations in Data Management to CS Education: An Educational Reconstruction Approach,"Secondary Schools, Data Management, CS Education, Educational Reconstruction, Big Data","This paper describes the application of the research framework educational reconstruction for investigating the field data management under a CS education perspective. Like the many other innovations in CS, Big Data and the field data management have strong influences on students' daily lives. In contrast, school does not yet sufficiently prepare students to handle the arising challenges. In this paper we will describe how we apply an educational reconstruction approach to prepare the teaching of essential data management competencies. We will summarize the main goals and principles of educational reconstruction and discuss the application of the framework to the topic data management, as well as first outcomes. Just as educational reconstruction is suitable for finding the essential aspects for teaching data management and for designing classes/courses on this topic, it also seems promising for the curricular development of other CS innovations as well.",2015,,10.1145/2818314.2818330
2376,"Si, Yaqing and Qin, Siyao and Su, Jing and Wang, Mingyue",Research on Factors Influencing the Value of Data Products and Pricing Models,"Pricing strategy, Data product, Pricing model, Value factor","The article focuses on the analysis of data product value influencing factors and establishes a data product pricing model based on value factors. The research reviews the existing research on the value evaluation of data assets, and summarizes the characteristics of data products in the data product trading system based on the alliance chain [1], and then obtains factors for data product value evaluation. Combined with the dynamics and personalized requirements of data products, a value-based three-stage dynamic pricing model for data products is proposed.",2020,,10.1145/3424978.3425146
2377,"Wei, Qin",Information Fusion in Taxonomic Descriptions,"natural language processing, information fusion","Providing a single access point to an information system from multiple documents is helpful for biodiversity researchers as it is true in many fields. It not only saves the time for going back and forth from different sources but also provides the opportunity to generate new information out of the complementary information in different sources and levels of description. This paper investigates the potential of information fusion techniques in biodiversity area since the researchers in this domain desperately need information from different sources to verify their decision. In another sense, there are massive amounts of collections in this area. It is not easy or even possible for the researcher to manually collect information from different places. The proposed system contains 4 steps: Text segmentation and Taxonomic Name Identification, Organ-level and Sub-organ level Information Extraction, Relationship Identification, and Information fusion. Information fusion is based on the seven out of the twenty-four relationships in CST (Cross-document Sentence Theory). We argue that this kind of information fusion system might not only save the researchers the time for going back and forth from different sources but also provides the opportunity to generate new information out of the complementary information in different sources and levels.",2013,,10.1145/2513549.2513552
2378,"Dashdorj, Zolzaya and Sobolevsky, Stanislav and Serafini, Luciano and Ratti, Carlo",Human Activity Recognition from Spatial Data Sources,"big data, human activity recognition, geo-spatial data and knowledge, bank card transactions, statistical matching, spatial data quality and uncertainty, context recognition, urban and environmental planning","Recent availability of big data of digital traces of human activity boosted research on human behavior. However, in most of the datasets such as mobile phone data or GPS traces, an important layer of information is typically missing: providing an extensive information of when and where people go typically does not allow understanding of what they do there. Predicting the context of human behavior in such cases where such information is not directly available from the data is a complex task that addresses context recognition problems. To fill in the contextual information for such data, we developed an ontological and stochastic model (HRBModel) that interprets semantic (high-level) human behaviors from geographical maps like OpenStreetMap, analyzing the distribution of Points of Interest(POIs), in a given region and time period. The semantic human behaviors are human activities that are accompanied by their likelihood, depending on their location and time. In this paper, we perform an extended evaluation of this model based on other qualitative data source, namely a country-wide anonymized bank card transaction data in Spain, which contains contextual information about the locations and the types of business categories where transactions occurred. This allows us to validate the model, by matching our predicted activity patterns with the actually observed ones, so that it can be later applied to the cases where such information is unavailable. This extended evaluation aimed to define the applicability of the predictive model, HRBModel, taking various type of spatial and temporal factors into account.",2014,,10.1145/2675316.2675321
2379,"Mbah, Raymond Blanch K. and Rege, Manjeet and Misra, Bhabani",Using Spark and Scala for Discovering Latent Trends in Job Markets,"Big Data, Scala, Singular Value Decomposition (SVD), Latent Semantic Analysis(LSA), Natural Language Processing(NLP), Spark","Job markets are experiencing an exponential growth in data alongside the recent explosion of big data in various domains including health, security and finance. Staying current with job market trends entails collecting, processing and analyzing huge amounts of data. A typical challenge with analyzing job listings is that they vary drastically with regards to verbiage, for instance a given job title or skill can be referred to using different words or industry jargons. As a result, it becomes incumbent to go beyond words present in job listings and carry out analysis aimed at discovering latent structures and trends in job listings. In this paper, we present a systematic approach of uncovering latent trends in job markets using big data technologies (Apache Spark and Scala) and distributed semantic techniques such as latent semantic analysis (LSA). We show how LSA can uncover patterns/relationships/trends that will otherwise remain hidden if using traditional text mining techniques that rely only on word frequencies in documents.",2019,,10.1145/3314545.3314566
2380,"Cao, Longbing",Data Science: A Comprehensive Overview,"data profession, data DNA, data education, Big data, data engineering, data science, data scientist, informatics, advanced analytics, big data analytics, data innovation, data industry, computing, data analysis, data economy, data analytics, data service, statistics","The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.",2017,,10.1145/3076253
2381,"Fiadino, Pierdomenico and Ponce-Lopez, Victor and Antonio, Juan and Torrent-Moreno, Marc and D'Alconzo, Alessandro","Call Detail Records for Human Mobility Studies: Taking Stock of the Situation in the ""Always Connected Era""","call detail records, mobile networks, human mobility","The exploitation of cellular network data for studying human mobility has been a popular research topic in the last decade. Indeed, mobile terminals could be considered ubiquitous sensors that allow the observation of human movements on large scale without the need of relying on non-scalable techniques, such as surveys, or dedicated and expensive monitoring infrastructures. In particular, Call Detail Records (CDRs), collected by operators for billing purposes, have been extensively employed due to their rather large availability, compared to other types of cellular data (e.g., signaling). Despite the interest aroused around this topic, the research community has generally agreed about the scarcity of information provided by CDRs: the position of mobile terminals is logged when some kind of activity (calls, SMS, data connections) occurs, which translates in a picture of mobility somehow biased by the activity degree of users. By studying two datasets collected by a Nation-wide operator in 2014 and 2016, we show that the situation has drastically changed in terms of data volume and quality. The increase of flat data plans and the higher penetration of ""always connected"" terminals have driven up the number of recorded CDRs, providing higher temporal accuracy for users' locations.",2017,,10.1145/3098593.3098601
2382,"Li, Xiao-Tong and Zhai, Jun and Zheng, Gui-Fu and Yuan, Chang-Feng",Quality Assessment for Open Government Data in China,"quality metric, quality dimension, open government data, quality assessment, Data quality","With the development in research of government open data, the issue of data quality becomes more prominent. It's important to accurately judge the data quality before using it. The microcosmic quality assessment not only provides criteria for users to pick up dataset, but also establishes standards for providers' data quality management. In this paper, it sums up 16 types of quality problems through the investigation of three Chinese local government datasets in Beijing, Guangzhou and Harbin, and then constructs 7 quality dimensions and metrics at different granular level to score three cities. The evaluation results reflect that overall score of completeness, accuracy and consistency is low, which will affect the availability of data and mislead users to make wrong decision. On the basis of this evaluation, government could take measures to overcome the weaknesses observed in the open data quality, addressing specific lower score quality aspects.",2018,,10.1145/3285957.3285962
2383,"Belghait, Fodil and April, Alain and Hamet, Pavel and Tremblay, Johanne and Desrosiers, Christian",A Large-Scale and Extensible Platform for Precision Medicine Research,"genomics, big data, precision medicine, bioinformatics, clinical databases","The massive adoption of high-throughput genomics, deep sequencing technologies and big data technologies have made possible the era of precision medicine. However, the volume of data and its complexity remain important challenges for precision medicine research, hindering development in this field. The literature on precision medicine research describes a few platforms to support specific types of studies, but none of these offer researchers the level of customization required to meet their specific needs [1]. Methods: We propose to design and develop a platform able to import and integrate a very large volume of genetics, clinical, demographical and environmental data in a cloud computing infrastructure. In our previous publication, we presented an approach that can customize existing data models to fit any precision medicine research data requirement [1] and the requirement for future large-scale precision medicine platforms, in terms of data extensibility and the scalability of processing on demand. We also proposed a framework to meet the specific requirement of any precision medicine research [2]. In this paper, we describe how this new framework was implemented and trialed by the precision medicine researchers at the Centre Hospitalier Universitaire de l'Universit\'{e} de Montr\'{e}al (CHUM). Results: The data analysis simulations showed that the random forest algorithm presents better accuracy results. We obtained an F1-Score of 72% for random forest, 69% using linear regression and 62% using the neural network algorithm. Conclusion: The results suggest that the proposed precision medicine data analysis platform allows researchers to configure, prepare the analysis environment and customize the platform data model to their specific research in very optimal delays, at very low cost and with minimal technical skills.",2019,,10.1145/3357729.3357742
2384,"Ara\'{u}jo, Tiago Brasileiro and Cappiello, Cinzia and Kozievitch, Nadia Puchalski and Mestre, Demetrio Gomes and Pires, Carlos Eduardo Santos and Vitali, Monica",Towards Reliable Data Analyses for Smart Cities,"Data Quality, Data Analysis, Smart Cities, Entity Matching","As cities are becoming green and smart, public information systems are being revamped to adopt digital technologies. There are several sources (official or not) that can provide information related to a city. The availability of multiple sources enables the design of advanced analyses for offering valuable services to both citizens and municipalities. However, such analyses would fail if the considered data were affected by errors and uncertainties: Data Quality is one of the main requirements for the successful exploitation of the available information. This paper highlights the importance of the Data Quality evaluation in the context of geographical data sources. Moreover, we describe how the Entity Matching task can provide additional information to refine the quality assessment and, consequently, obtain a better evaluation of the reliability data sources. Data gathered from the public transportation and urban areas of Curitiba, Brazil, are used to show the strengths and effectiveness of the presented approach.",2017,,10.1145/3105831.3105834
2385,"Meurisch, Christian and Planz, Karsten and Sch\""{a}fer, Daniel and Schweizer, Immanuel",Noisemap: Discussing Scalability in Participatory Sensing,"Sensing Campaign, Environmental Pollution Modeling, Participatory Sensing","Environmental pollutants are an ever increasing problem in dense urban environments. To assess the effect of these pollutants, an unprecedented density of data is needed for large areas (cities, states, countries). In the past, participatory sensing has been proposed as a mean to acquire large sets of data. Since the smartphone is ubiquitous, scalability seems to be no problem anymore.In reality this far from the truth. Measuring their environment, people need to invest their time. For Android and iOS the application needs to compete with more than 700,000 other applications. Measuring large amounts of data is only possible, if we can attract large amounts of casual users.Since 2011, we have been working with and on Noisemap. Noisemap is one of many applications that uses the microphone to measure sound pressure. It then uploads the captured data to our backend, where the data is processed and visualized. Noisemap is officially available since February 2012, has been downloaded over 2,500 times, and has more than 1,000 registered users, which have collected over 500,000 unique data points in 39 countries and 58 cities. We want to share the current state of Noisemap as a multi-platform tool on Android and iOS, as well as our experience in scaling the application.",2013,,10.1145/2536714.2536720
2386,"Lin, Jimmy and Milligan, Ian and Wiebe, Jeremy and Zhou, Alice",Warcbase: Scalable Analytics Infrastructure for Exploring Web Archives,"Apache Hadoop, WARC, Big data, Apache HBase, ARC, Apache Spark","Web archiving initiatives around the world capture ephemeral Web content to preserve our collective digital memory. However, unlocking the potential of Web archives for humanities scholars and social scientists requires a scalable analytics infrastructure to support exploration of captured content. We present Warcbase, an open-source Web archiving platform that aims to fill this need. Our platform takes advantage of modern open-source “big data” infrastructure, namely Hadoop, HBase, and Spark, that has been widely deployed in industry. Warcbase provides two main capabilities: support for temporal browsing and a domain-specific language that allows scholars to interrogate Web archives in several different ways. This work represents a collaboration between computer scientists and historians, where we have engaged in iterative codesign to build tools for scholars with no formal computer science training. To provide guidance, we propose a process model for scholarly interactions with Web archives that begins with a question and proceeds iteratively through four main steps: filter, analyze, aggregate, and visualize. We call this the FAAV cycle for short and illustrate with three prototypical case studies. This article presents the current state of the project and discusses future directions.",2017,,10.1145/3097570
2387,"Song, Jie and He, Yeye",Auto-Validate: Unsupervised Data Validation Using Data-Domain Patterns Inferred from Data Lakes,,"Complex data pipelines are increasingly common in diverse applications such as BI reporting and ML modeling. These pipelines often recur regularly (e.g., daily or weekly), as BI reports need to be refreshed, and ML models need to be retrained. However, it is widely reported that in complex production pipelines, upstream data feeds can change in unexpected ways, causing downstream applications to break silently that are expensive to resolve. Data validation has thus become an important topic, as evidenced by notable recent efforts from Google and Amazon, where the objective is to catch data quality issues early as they arise in the pipelines. Our experience on production data suggests, however, that on string-valued data, these existing approaches yield high false-positive rates and frequently require human intervention. In this work, we develop a corpus-driven approach to auto-validate machine-generated data by inferring suitable data-validation ""patterns'' that accurately describe the underlying data-domain, which minimizes false-positives while maximizing data quality issues caught. Evaluations using production data from real data lakes suggest that sj is substantially more effective than existing methods. Part of this technology ships as an Auto-Tag feature in Microsoft Azure Purview.",2021,,
2388,"Maqboul, Jaouad and Bounabat, Bouchaib",An Approach of Data-Driven Framework Alignment to Knowledge Base,"complexity, Business process, impact, completeness, framework, prediction, data quality, Knowledge, java EE, knowledge Base","When we talk about quality, we cannot do without mentioning the cost of quality and non-quality, the cost increases if the quality also increases; to maintain quality in small data is easier than huge data like big data or knowledge base.Companies tend to use the knowledge base to perfect and facilitate their work, thus satisfying the end customer, however the non-quality of these bases will penalize the company, so it is necessary to improve the quality, the question is when and why to improve quality, our proposal is based on the cost and impact of this improvement, if the impact is greater than the cost then it is recommended to improve completeness in our case study.",2018,,10.1145/3230905.3230932
2389,"Fraihat, Salam and Salameh, Walid A. and Elhassan, Ammar and Tahoun, Bushra Abu and Asasfeh, Maisa",Business Intelligence Framework Design and Implementation: A Real-Estate Market Case Study,"predictive analytics, data quality, Business intelligence, real estate","This article builds on previous work in the area of real-world applications of Business Intelligence (BI) technology. It illustrates the analysis, modeling, and framework design of a BI solution with high data quality to provide reliable analytics and decision support in the Jordanian real estate market. The motivation is to provide analytics dashboards to potential investors about specific segments or units in the market. The article ekxplains the design of a BI solution, including background market and technology investigation, problem domain requirements, solution architecture modeling, design and testing, and the usability of descriptive and predictive features. The resulting framework provides an effective BI solution with user-friendly market insights for investors with little or no market knowledge. The solution features predictive analytics based on established Machine Learning modeling techniques, analyzed and contrasted to select the optimum methodology and model combination for predicting market behavior to empower inexperienced users.",2021,,10.1145/3422669
2390,"Lv, Yirong and Sun, Bin and Luo, Qinyi and Wang, Jing and Yu, Zhibin and Qian, Xuehai",CounterMiner: Mining Big Performance Data from Hardware Counters,"performance counters, big data, computer architecture, data mining","Modern processors typically provide a small number of hardware performance counters to capture a large number of microarchitecture events1. These counters can easily generate a huge amount (e.g., GB or TB per day) of data, which we call big performance data in cloud computing platforms with more than thousands of servers and millions of complex workloads running ina""24/7/365"" manner. The big performance data provides a precious foundation for root cause analysis of performance bottlenecks, architecture and compiler optimization, and many more. However, it is challenging to extract value from the big performance data due to: 1) the many unperceivable errors (e.g., outliers and missing values); and 2) the difficulty of obtaining insights, e.g., relating events to performance.In this paper, we propose CounterMiner, a rigorous methodology that enables the measurement and understanding of big performance data by using data mining and machine learning techniques. It includes three novel components: 1) using data cleaning to improve data quality by replacing outliers and filling in missing values; 2) iteratively quantifying, ranking, and pruning events based on their importance with respect to performance; 3) quantifying interaction intensity between two events by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from the Spark 2 version of HiBench) to evaluate CounterMiner. The experimental results show that CounterMiner reduces the average error from 28.3% to 7.7% when multiplexing 10 events on 4 hardware counters. We also conduct a real-world case study, showing that identifying important configuration parameters of Spark programs by event importance is much faster than directly ranking the importance of these parameters.",2018,,10.1109/MICRO.2018.00056
2391,"de Mendon\c{c}a, Rogers Reiche and da Cruz, S\'{e}rgio Manuel Serra and De La Cerda, Jonas F. S. M. and Cavalcanti, Maria Cl\'{a}udia and Cordeiro, Kelli Faria and Campos, Maria Luiza M.",LOP: Capturing and Linking Open Provenance on LOD Cycle,"data quality, linked data, interoperability, ETL, linked open data, provenance","The Web of Data has emerged as a means to expose, share, reuse, and connect information on the Web identified by URIs using RDF as a data model, following Linked Data Principles. However, the reuse of third party data can be compromised without proper data quality assessments. In this context, important questions emerge: how can one trust on published data and links? Which manipulation, modification and integration operations have been applied to the data before its publication? What is the nature of comparisons or transformations applied to data during the interlinking process? In this scenario, provenance becomes a fundamental element. In this paper, we describe an approach for generating and capturing Linked Open Provenance (LOP) to support data quality and trustworthiness assessments, which covers preparation and format transformation of traditional data sources, up to dataset publication and interlinking. The proposed architecture takes advantage of provenance agents, orchestrated by an ETL workflow approach, to collect provenance at any specified level and also link it with its corresponding data. We also describe a real use case scenario where the architecture was implemented to evaluate the proposal.",2013,,10.1145/2484712.2484715
2392,"Tan, Zijing and Ran, Ai and Ma, Shuai and Qin, Sheng",Fast Incremental Discovery of Pointwise Order Dependencies,,"Pointwise order dependencies (PODs) are dependencies that specify ordering semantics on attributes of tuples. POD discovery refers to the process of identifying the set Σ of valid and minimal PODs on a given data set D. In practice D is typically large and keeps changing, and it is prohibitively expensive to compute Σ from scratch every time. In this paper, we make a first effort to study the incremental POD discovery problem, aiming at computing changes ΔΣ to Σ such that Σ ⊕ ΔΣ is the set of valid and minimal PODs on D with a set ΔD of tuple insertion updates. (1) We first propose a novel indexing technique for inputs Σ and D. We give algorithms to build and choose indexes for Σ and D, and to update indexes in response to ΔD. We show that POD violations w.r.t. Σ incurred by ΔD can be efficiently identified by leveraging the proposed indexes, with a cost dependent on log(|D|). (2) We then present an effective algorithm for computing ΔΣ, based on Σ and identified violations caused by ΔD. The PODs in Σ that become invalid on D + ΔD are efficiently detected with the proposed indexes, and further new valid PODs on D + ΔD are identified by refining those invalid PODs in Σ on D + ΔD. (3) Finally, using both real-life and synthetic datasets, we experimentally show that our approach outperforms the batch approach that computes from scratch, up to orders of magnitude.",2020,,10.14778/3401960.3401965
2393,"Teoh, Jason and Gulzar, Muhammad Ali and Xu, Guoqing Harry and Kim, Miryung",PerfDebug: Performance Debugging of Computation Skew in Dataflow Systems,"big data systems, fault localization, data intensive scalable computing, Performance debugging, data provenance","Performance is a key factor for big data applications, and much research has been devoted to optimizing these applications. While prior work can diagnose and correct data skew, the problem of computation skew---abnormally high computation costs for a small subset of input data---has been largely overlooked. Computation skew commonly occurs in real-world applications and yet no tool is available for developers to pinpoint underlying causes.To enable a user to debug applications that exhibit computation skew, we develop a post-mortem performance debugging tool. PerfDebug automatically finds input records responsible for such abnormalities in a big data application by reasoning about deviations in performance metrics such as job execution time, garbage collection time, and serialization time. The key to PerfDebug's success is a data provenance-based technique that computes and propagates record-level computation latency to keep track of abnormally expensive records throughout the pipeline. Finally, the input records that have the largest latency contributions are presented to the user for bug fixing. We evaluate PerfDebug via in-depth case studies and observe that remediation such as removing the single most expensive record or simple code rewrite can achieve up to 16X performance improvement.",2019,,10.1145/3357223.3362727
2394,"Bertino, Elisa and Kundu, Ahish and Sura, Zehra",Data Transparency with Blockchain and AI Ethics,"privacy, accountability, data provenance, Big data","Providing a 360° view of a given data item especially for sensitive data is essential toward not only protecting the data and associated privacy but also assuring trust, compliance, and ethics of the systems that use or manage such data. With the advent of General Data Protection Regulation, California Data Privacy Law, and other such regulatory requirements, it is essential to support data transparency in all such dimensions. Moreover, data transparency should not violate privacy and security requirements. In this article, we put forward a vision for how data transparency would be achieved in a de-centralized fashion using blockchain technology.",2019,,10.1145/3312750
2395,"Cabrera, Anthony M. and Faber, Clayton J. and Cepeda, Kyle and Derber, Robert and Epstein, Cooper and Zheng, Jason and Cytron, Ron K. and Chamberlain, Roger D.",DIBS: A Data Integration Benchmark Suite,"data wrangling, data integration, big data","As the generation of data becomes more prolific, the amount of time and resources necessary to perform analyses on these data increases. What is less understood, however, is the data preprocessing steps that must be applied before any meaningful analysis can begin. This problem of taking data in some initial form and transforming it into a desired one is known as data integration. Here, we introduce the Data Integration Benchmarking Suite (DIBS), a suite of applications that are representative of data integration workloads across many disciplines. We apply a comprehensive characterization to these applications to better understand the general behavior of data integration tasks. As a result of our benchmark suite and characterization methods, we offer insight regarding data integration tasks that will guide other researchers designing solutions in this area.",2018,,10.1145/3185768.3186307
2396,"Saltz, Jeffrey S. and Dewar, Neil I. and Heckman, Robert",Key Concepts for a Data Science Ethics Curriculum,"big data, computing education, data science, ethics","Data science is a new field that integrates aspects of computer science, statistics and information management. As a new field, ethical issues a data scientist may encounter have received little attention to date, and ethics training within a data science curriculum has received even less attention. To address this gap, this article explores the different codes of conduct and ethics frameworks related to data science. We compare this analysis with the results of a systematic literature review focusing on ethics in data science. Our analysis identified twelve key ethics areas that should be included within a data science ethics curriculum. Our research notes that none of the existing codes or frameworks covers all of the identified themes. Data science educators and program coordinators can use our results as a way to identify key ethical concepts that can be introduced within a data science program.",2018,,10.1145/3159450.3159483
2397,"Verma, Nitya and Voida, Amy",On Being Actionable: Mythologies of Business Intelligence and Disconnects in Drill Downs,"big data, business intelligence, data analytics, mythology","We present results from a case study of the use of business intelligence systems in a human services organization. We characterize four mythologies of business intelligence that informants experience as shared organizational values and are core to their trajectory towards a ""culture of data"": data-driven, predictive and proactive, shared accountability, and inquisitive. Yet, for each mythology, we also discuss the ways in which being actionable is impeded by a disconnect between the aggregate views of data that allows them to identify areas of focus for decision making and the desired ""drill down"" views of data that would allow them to understand how to act in a data-driven context. These findings contribute initial empirical evidence for the impact of business intelligence's epistemological biases on organizations and suggest implications for the design of technologies to better support data-driven decision making.",2016,,10.1145/2957276.2957283
2398,"Qi, Yajie and Guo, Chunwei",Deep Learning-Based Hourly Temperature Prediction: A Case Study of Mega-Cites in North China,"Deep learning, North China, Hourly temperature prediction, LSTM","Accurate prediction of temperature is an important part of fine weather forecast services (such as heating energy consumption in winter, Winter Olympic Games, etc.). Therefore, the accurate prediction of hourly temperature is very significant in the management of human health and the decision-making of government. In this study, a long short term (LSTM) memory model was proposed and used to predict the next hour's temperature in mega-cites in North China. It was fully considered for the historic temperature and meteorological condition. As a result, the predictor secured a fast and accurate prediction performance by fully reflecting the long-term historic process of input time series data through LSTM. The meteorological data from Beijing, Tianjin, Shijiazhuang and Taiyuan which represents the mega-cites of North China during October 1 to December 31 in 2016-2018 were used to verify the validity of the proposed method. In conclusion, the proposed method was proved to have a good prediction performance in cooling and turning warming processes, making up for the poor performance of turning weather prediction in the previous research. It confirmed that the forward supplement LSTM model has the best prediction ability for hourly temperature prediction in Beijing among mega-cites in North China. The results also indicate great potential of the machine learning method in improving local weather forecast and the potential to serve the 2022 Winter Olympics.",2020,,10.1145/3422713.3422718
2399,"Vandervort, David",Medical Device Data Goes to Court,"ehealth, crime, big data, law, wearables, forensics","Advances in mobile and computer technology are combining to create massive changes in the way data about human health and well-being are gathered and used. As the trend toward wearable and ubiquitous health tracking devices moves forward, the sheer quantity of new data from a wide variety of devices presents challenges for analysts. In the coming years, this data will inevitably be used in the criminal and civil justice systems. However, the tools to make full use of it are currently lacking. This paper discusses scenarios where data collected from health and fitness related devices may intersect with legal requirements such as investigations into insurance fraud or even murder. The conclusion is that there is much work to be done to enable reliable investigations. This should include at least the establishment of an organization to promote development of the field, development of cross-disciplinary education materials, and the creation of an open data bank for information sharing.",2016,,10.1145/2896338.2896341
2400,"Hossain, Syed Monowar and Hnat, Timothy and Saleheen, Nazir and Nasrin, Nusrat Jahan and Noor, Joseph and Ho, Bo-Jhang and Condie, Tyson and Srivastava, Mani and Kumar, Santosh",MCerebrum: A Mobile Sensing Software Platform for Development and Validation of Digital Biomarkers and Interventions,"software architecture, mHealth, mobile sensor big data","The development and validation studies of new multisensory biomarkers and sensor-triggered interventions requires collecting raw sensor data with associated labels in the natural field environment. Unlike platforms for traditional mHealth apps, a software platform for such studies needs to not only support high-rate data ingestion, but also share raw high-rate sensor data with researchers, while supporting high-rate sense-analyze-act functionality in real-time. We present mCerebrum, a realization of such a platform, which supports high-rate data collections from multiple sensors with realtime assessment of data quality. A scalable storage architecture (with near optimal performance) ensures quick response despite rapidly growing data volume. Micro-batching and efficient sharing of data among multiple source and sink apps allows reuse of computations to enable real-time computation of multiple biomarkers without saturating the CPU or memory. Finally, it has a reconfigurable scheduler which manages all prompts to participants that is burden- and context-aware. With a modular design currently spanning 23+ apps, mCerebrum provides a comprehensive ecosystem of system services and utility apps. The design of mCerebrum has evolved during its concurrent use in scientific field studies at ten sites spanning 106,806 person days. Evaluations show that compared with other platforms, mCerebrum's architecture and design choices support 1.5 times higher data rates and 4.3 times higher storage throughput, while causing 8.4 times lower CPU usage.",2017,,10.1145/3131672.3131694
2401,"Jing, Furong and Cao, Yongsheng and Fang, Wei and Chen, Yanqing",Construction and Implementation of Big Data Framework for Crop Germplasm Resources,"Crop germplasm resources, Data analysis, Big data architecture, Data management","Based on understanding the application of big data and the research status of crop germplasm resources, this paper proposes a system architecture that is suitable for crop germplasm resources big data. Among them, the overall architecture of germplasm resources is elaborated through six functional modules, including data source, data integration, data processing, data application, big data operation and maintenance platform, and data management and security. The logical functional architecture specification was formulated and the technical implementation and selection are defined. The technical implementation framework describes the technical implementation of germplasm resources big data, and jointly supports the construction and operation of germplasm resources big data. Finally, a verification system is established to verify the feasibility of the big data system framework for germplasm resources.",2019,,10.1145/3331453.3361308
2402,"Peng, Michael Yao-Ping and Tuan, Sheng-Hwa and Liu, Feng-Chi",Establishment of Business Intelligence and Big Data Analysis for Higher Education,"Big data, Business Intelligence, Institutional Research, Database","The applications on business intelligence and big data analysis to extract useful information are getting more mature, but the development and operations in higher education institutions are still be lack. This study aims to explore how HEIs employ business intelligence to analysis and mining student learning and HEIs' operation data from database. The outcomes are benefit for universities to support the management of decision-making.",2017,,10.1145/3134271.3134296
2403,"Li, Jiale and Liao, Shunbao",Quality Control Framework of Big Data for Early Warning of Agricultural Meteorological Disasters,"agro-meteorological disasters, big data, framework, early warning, quality control","Agricultural meteorological disasters, including floods, droughts, dry hot winds, low temperature chills, typhoons, hail and continuous rain, can lead to significant reduction in agricultural output. Big data platform for early warning of agricultural meteorological disaster is the basis of business operation system for early warning of agricultural meteorological disasters, and the data quality is an important guarantee for success of the early warning. Quality control of big data for early warning of agricultural meteorological disaster involves names of data sets, metadata, data documents and content of data sets. The quality control for contents of data sets is divided into quality control of attribute data and that of spatial data, and quality control of spatial data is divided into quality control of vector data and that of raster data. Methods for data quality control are divided into fully automatic, semi-automatic and full manual control methods.",2019,,10.1145/3349341.3349371
2404,"Cuzzocrea, Alfredo",Big Data Management and Analytics in Intelligent Smart Environments: State-of-the-Art Analysis and Future Research Directions,"Big data management, Intelligent smart environments, Big data analytics","This paper focuses on big data management and analytics in intelligent smart environments, with particular regards to intelligent transportation and logistics systems, and provides relevant research directions that may represent a milestone for future years.",2019,,10.1145/3366030.3366044
2405,"Liyao, Zhou and Xiaofang, Liu and Chunyu, Hu",Evaluation Method of Equipment Combat Effectiveness Based On Big Data Mining,"combat effectiveness evaluation, Big data mining, combat test","In the evaluation of equipment combat effectiveness, it is necessary to comprehensively analyze the data of outfield test and infield test, including a variety of audio-visual, image and other combat test data. These data can be classified, extracted, stored and managed by building data model through big data mining technology. The evaluation method of equipment combat effectiveness based on big data mining is based on massive data, through machine learning, statistical analysis, neural network, database and other methods to analyze and process the data, mining the correlation between test data, evaluation index and evaluation conclusion, and extracting useful information and finding new knowledge from it to realize the evaluation of the combat effectiveness of the tested system.",2020,,10.1145/3379247.3379282
2406,"Geisler, Sandra and Quix, Christoph and Weber, Sven and Jarke, Matthias",Ontology-Based Data Quality Management for Data Streams,"Data streams, ontologies, data quality assessment, data quality control","Data Stream Management Systems (DSMS) provide real-time data processing in an effective way, but there is always a tradeoff between data quality (DQ) and performance. We propose an ontology-based data quality framework for relational DSMS that includes DQ measurement and monitoring in a transparent, modular, and flexible way. We follow a threefold approach that takes the characteristics of relational data stream management for DQ metrics into account. While (1) Query Metrics respect changes in data quality due to query operations, (2) Content Metrics allow the semantic evaluation of data in the streams. Finally, (3) Application Metrics allow easy user-defined computation of data quality values to account for application specifics. Additionally, a quality monitor allows us to observe data quality values and take counteractions to balance data quality and performance. The framework has been designed along a DQ management methodology suited for data streams. It has been evaluated in the domains of transportation systems and health monitoring.",2016,,10.1145/2968332
2407,"Jia, Fengsheng and Gao, Yang and Wang, Yuming",Study on Standard System of Aerospace Quality Data Resources Integration under the Background of Big Data,"system planning, standard system, quality data resource integration, aerospace products","The integration and application of aerospace product quality data resources is an important way to carry out quality improvement, quality evaluation and precise management. Standardization is the basis for promoting quality data resources integration. The unified and normative standard system is the guarantee for efficient development of integration standards. Firstly, we analyzed the features of quality data resources according to the status quo of integration. Integration structure of quality data resources in terms of vertical and horizontal integration was proposed by adopting the methods of ""decomposition-integration"" and ""classification-association"". Secondly, we constructed a three-dimensions architecture of quality data resource integration using the method of system engineering methodology, from the layer dimension (basis, common, special), technical dimension (description, collection, storage, transmission, processing, comprehensive management) and category dimension (rocket, spacecraft). Thirdly, we worked out 20 lists about basis and common standard by adopting the top-down approach. Some standard development suggestions are proposed based on the characteristics of quality data resources and standard research strategies. Finally, we applied the quality problem data resource standard construction and application to verify the proposed method.",2019,,10.1145/3341620.3341624
2408,"Scavuzzo, Marco and Tamburri, Damian A. and Di Nitto, Elisabetta",Providing Big Data Applications with Fault-Tolerant Data Migration across Heterogeneous NoSQL Databases,,"The recent growing interest on highly-available data-intensive applications sparked the need for flexible and portable storage technologies, e.g., NoSQL databases. Unfortunately, the lack of standard interfaces and architectures for NoSQLs makes it difficult and expensive to create portable applications, which results in vendor lock-in. Building on previous work, we aim at providing guaranteed fault-tolerant techniques and supporting architectures to port or migrate data to and across heterogeneous NoSQL technology. To prove the effectiveness of our approach we evaluate it on an industrial case-study. We conclude that our method and supporting architecture offer an efficient and fault-tolerant mechanism for NoSQL portability and interoperation.",2016,,10.1145/2896825.2896831
2409,"Megler, V. M. and Maier, David",When Big Data Leads to Lost Data,"ranked data search, scientific data","For decades, scientists bemoaned the scarcity of observational data to analyze and against which to test their models. Exponential growth in data volumes from ever-cheaper environmental sensors has provided scientists with the answer to their prayers: ""big data"". Now, scientists face a new challenge: with terabytes, petabytes or exabytes of data at hand, stored in thousands of heterogeneous datasets, how can scientists find the datasets most relevant to their research interests? If they cannot find the data, then they may as well never have collected it; that data is lost to them. Our research addresses this challenge, using an existing scientific archive as our test-bed. We approach this problem in a new way: by adapting Information Retrieval techniques, developed for searching text documents, into the world of (primarily numeric) scientific data. We propose an approach that uses a blend of automated and ""semi-curated"" methods to extract metadata from large archives of scientific data. We then perform searches over the extracted metadata, returning results ranked by similarity to the query terms. We briefly describe an implementation performed at an ocean observatory to validate the proposed approach. We propose performance and scalability research to explore how continued archive growth will affect our goal of interactive response, no matter the scale.",2012,,10.1145/2389686.2389688
2410,"Netten, Niels and van den Braak, Susan and Choenni, Sunil and van Someren, Maarten",A Big Data Approach to Support Information Distribution in Crisis Response,"Big Data, Crisis Response for Public Safety, Information Distribution, Machine Learning, Relevance Assessments","Crisis response organizations operate in very dynamic environments, in which it is essential for responders to acquire all information critical to their task execution in time. In reality, the responders are often faced with information overload, incomplete information, or a combination of both. This hampers their decision-making process, workflow, situational awareness and, consequently, effective execution of collaborative crisis response. Therefore, getting the right information to the right person at the right time is of crucial importance.The task of processing all data during crisis response situations and determining for whom at a particular moment the information is relevant is not straightforward. When developing an information system to support this task, some important challenges have to be taken into account. These challenges relate to the structure and truthfulness of the used data, the assessment of information relevance, and the dissemination of relevant information in time. While methods and techniques from big data can be used to collect and integrate data, machine learning can be used to build a model for relevance assessments. An example implementation of such a framework of big data is the TAID software system that collects and integrates data communicated between first responders and may send information to crisis responders that were not addressed in the initial communication. As an example of the impact of TAID on crisis response, we show its effect in a simulated crisis response scenario.",2016,,10.1145/2910019.2910033
2411,"Sardjono, Wahyu and Retnowardhani, Astari and Emil Kaburuan, Robert and Rahmasari, Aninda",Artificial Intelligence and Big Data Analysis Implementation in Electronic Medical Records,"big data, electronic medical records, internet of things, artificial intelligence, Analysis","Industry 4.0 is the pioneer of the Internet of Things (IoT). The Internet of Things (IoT) are often heard and successful in business revolution from all sectors. The IoT are widely used in numerous sectors including medical services and have become the rise of Internet of Medical Things (IoMT). One of the implementations is the Electronic Health Record (EHR) systems. Previously the health records were used in traditional manner such as print-out health record of a patient and stored to an archive room. With the innovation of EHR, patients’ health records are digitalized which provides advantages from space efficiency and paperless forms. EHR helps medical service management to provide better healthcare services. With the integration of Artificial Intelligence (AI) and Big Data Analysis in EHR, healthcare services provide more accurate and reliable diagnosis.",2021,,10.1145/3512576.3512618
2412,"Dong, Wei and Xiao, Litian and Niu, Shengfen and Niu, Jianjun and Wang, Fei",Application Research of Big Data for Launch Support System at Space Launch Site,"Application research, Big data, Space launch site, Launch support system","At the space launch site, the big data of the launch support system comes from the construction of the launch site, the ground service, the comprehensive support process, and launch mission organization and command. The big data is extensive sources, various types, large scale, and rapid growth. The big data application can improve the data processing and management efficiency for the launch support system. Then the application can enhance the support capability of flight mission and success rate. This paper analyzes the existing data application of launch support system. The challenges and requirements of big data application are studied by the construction of intelligent launch site. The application pattern and target are put forward from four aspects of launch mission organization and command, mission application, comprehensive support, and information security. The classification of big data is proposed for a launch support system. The architecture of big data application system is designed, which meets the application pattern and target. It lays a foundation for the future big data project at the launch site.",2019,,10.1145/3331453.3360973
2413,"Liu, He and Wang, Xiaohui and Lei, Shuya and Zhang, Xi and Liu, Weiwei and Qin, Ming",A Rule Based Data Quality Assessment Architecture and Application for Electrical Data,"outlier, data quality, quality assessment, electrical data","Data quality assessment plays an important role in electricity consumption big data. It can help business people master the overall data situation, which can provide a strong guarantee for subsequent data improvement, analysis and decision. According to the electrical data quality issues, we design a rule-based data quality assessment architecture for electrical big data. It includes six types of data quality assessment indexes (such as comprehensiveness, accuracy, completeness), and the related data quality rules (such as non-empty rule and range rule), which can be used to guide the electrical data quality inspection. Meanwhile, for the accuracy, we propose an outlier detection method based on time time-relevant k-means, which is used to detect the voltage, curve and power data issues in electricity data. The experimental and simulation results show that the proposed architecture and method can work well for the comprehensive data quality assessment of electrical data.",2019,,10.1145/3371425.3371435
2414,"Liu, Zhao-ge and Li, Xiang-yang",Full View Scenario Model of Big Data Governance in Community Safety Service,"community safety, big data governance, scenario model, safety service","In community safety service, big data governance is the prime mode to achieve community safety big data sharing and service value increasing. Although existing researches have preliminarily established the general big data governance framework, identification and governance of specific sharing problems lack comprehensive and systematic scenario description. Applying software engineering method, this paper proposes a kind of scenario expression model of big data governance in community safety service. Considering the common features of big data governance scenarios, construct the meta-scenario model of big data governance in community safety services. Considering the scenario expression difference under different levels, scales and particle sizes, construct the full view scenario model of big data governance in community safety services by meta-models nesting to complete the scenario expression under different applying situation. Finally, a use case is proposed to verify the rationality and effectiveness of the scenario expression models.",2018,,10.1145/3268891.3268892
2415,"Bertossi, Leopoldo and Milani, Mostafa",Ontological Multidimensional Data Models and Contextual Data Quality,"Ontology-based data access, Datalog±, query answering, weakly-sticky programs","Data quality assessment and data cleaning are context-dependent activities. Motivated by this observation, we propose the Ontological Multidimensional Data Model (OMD model), which can be used to model and represent contexts as logic-based ontologies. The data under assessment are mapped into the context for additional analysis, processing, and quality data extraction. The resulting contexts allow for the representation of dimensions, and multidimensional data quality assessment becomes possible. At the core of a multidimensional context, we include a generalized multidimensional data model and a Datalog± ontology with provably good properties in terms of query answering. These main components are used to represent dimension hierarchies, dimensional constraints, and dimensional rules and define predicates for quality data specification. Query answering relies on and triggers navigation through dimension hierarchies and becomes the basic tool for the extraction of quality data. The OMD model is interesting per se beyond applications to data quality. It allows for a logic-based and computationally tractable representation of multidimensional data, extending previous multidimensional data models with additional expressive power and functionalities.",2018,,10.1145/3148239
2416,"Ordonez, Carlos and Garc\'{\i}a-Garc\'{\i}a, Javier",Managing Big Data Analytics Workflows with a Database System,,"A big data analytics workflow is long and complex, with many programs, tools and scripts interacting together. In general, in modern organizations there is a significant amount of big data analytics processing performed outside a database system, which creates many issues to manage and process big data analytics workflows. In general, data preprocessing is the most time-consuming task in a big data analytics workflow. In this work, we defend the idea of preprocessing, computing models and scoring data sets inside a database system. In addition, we discuss recommendations and experiences to improve big data analytics workflows by pushing data preprocessing (i.e. data cleaning, aggregation and column transformation) into a database system. We present a discussion of practical issues and common solutions when transforming and preparing data sets to improve big data analytics workflows. As a case study validation, based on experience from real-life big data analytics projects, we compare pros and cons between running big data analytics workflows inside and outside the database system. We highlight which tasks in a big data analytics workflow are easier to manage and faster when processed by the database system, compared to external processing.",2016,,10.1109/CCGrid.2016.63
2417,"Francisco, Maritza M. C. and Alves-Souza, Solange N. and Campos, Edit G. L. and De Souza, Luiz S.",Total Data Quality Management and Total Information Quality Management Applied to Costumer Relationship Management,"data quality management, Data quality, data quality problems, data quality methodology, data quality dimensions","Data quality (DQ) is an important issue for modern organizations, mainly for decision-making based on information, using solutions such as CRM, Business Analytics, and Big Data. In order to obtain quality data, it is necessary to implement methods, processes, and specific techniques that handle information as a product, with well established, controlled, and managed production processes. The literature provides several types of quality data management methodologies that treat structured data, and few treating semi- and non-structured data. Choosing the methodology to be adopted is one the major issues faced by organizations, when challenged to treat the data quality in a systematic manner. This paper makes a comparative analysis between TDQM -- Total Data Quality Management and TIQM -- Total Information Quality Management approaches, focusing on data quality problems in the context of a CRM -- Costumer Relationship Management application. Such analysis identifies the strengths and weaknesses of each methodology and suggests the most suitable for the CRM scenario.",2017,,10.1145/3149572.3149575
2418,"Wang, Hongzhi and Li, Mingda and Bu, Yingyi and Li, Jianzhong and Gao, Hong and Zhang, Jiacheng",Cleanix: A Big Data Cleaning Parfait,"data quality, data cleaning, big data","In this demo, we present Cleanix, a prototype system for cleaning relational Big Data. Cleanix takes data integrated from multiple data sources and cleans them on a shared-nothing machine cluster. The backend system is built on-top-of an extensible and flexible data-parallel substrate - the Hyracks framework. Cleanix supports various data cleaning tasks such as abnormal value detection and correction, incomplete data filling, de-duplication, and conflict resolution. We demonstrate that Cleanix is a practical tool that supports effective and efficient data cleaning at the large scale.",2014,,10.1145/2661829.2661837
2419,"G.C., Paul Suganthan and Sun, Chong and K., Krishna Gayatri and Zhang, Haojun and Yang, Frank and Rampalli, Narasimhan and Prasad, Shishir and Arcaute, Esteban and Krishnan, Ganesh and Deep, Rohit and Raghavendra, Vijay and Doan, AnHai",Why Big Data Industrial Systems Need Rules and What We Can Do About It,"rule management, classification, big data","Big Data industrial systems that address problems such as classification, information extraction, and entity matching very commonly use hand-crafted rules. Today, however, little is understood about the usage of such rules. In this paper we explore this issue. We discuss how these systems differ from those considered in academia. We describe default solutions, their limitations, and reasons for using rules. We show examples of extensive rule usage in industry. Contrary to popular perceptions, we show that there is a rich set of research challenges in rule generation, evaluation, execution, optimization, and maintenance. We discuss ongoing work at WalmartLabs and UW-Madison that illustrate these challenges. Our main conclusions are (1) using rules (together with techniques such as learning and crowdsourcing) is fundamental to building semantics-intensive Big Data systems, and (2) it is increasingly critical to address rule management, given the tens of thousands of rules industrial systems often manage today in an ad-hoc fashion.",2015,,10.1145/2723372.2742784
2420,"Pengxi, Li",The Construction Study of College Informationization Teaching Service System under the Background of Big Data,"Big data, Service system, Teaching informatization","Under the background of the rapid development of big data technology, the construction of college informationization teaching service system is the basis of the informationization teaching in colleges and universities. That is very important for the success of the informatization in Universities what is service realization model, business logic, architecture and platform conform to the whole development strategy of universities. Information management organization supports the planning, implementation, operation, maintenance and management of business information system. This paper analyzes the reform mode of college education information service system supported by big data technology. Based on the analysis of the reform mode of college informationization teaching service system supported by big data technology, this paper puts forward the design idea of post system based on big data. At the same time, with the case of ""big data assisted employment"", the post design and adjustment were carried out. The results show that big data assisted employment has greatly improved the efficiency and quality of the school's employment department, providing students with better employment security. Finally, the problems that need to be solved in the informatization teaching service are sorted out.",2019,,10.1145/3341069.3341086
2421,"Zhang, Mingke and Guo, Danhuai and Hu, Jinyong and Jin, Wei",Risk Prediction and Assessment of Foodborne Disease Based on Big Data,"big data, foodborne disease, machine learning, risk assessment","In recent years, the outbreak of foodborne diseases has been on an upward trend clearly. It is of great significance for us to predict the outbreak of foodborne diseases accurately and conduct quantitative risk assessment timely. Traditional prediction methods based on a single data source have drawbacks such as complex prediction processes and inaccurate prediction results. In this article, we figure out the scientific issues of how to improve the temporal and spatial accuracy of foodborne disease outbreak risk prediction in Beijing. Firstly, we analyze the different foodborne disease risk factors caused by the spread of water pollution in Beijing, and study the methods of collecting and preprocessing multi-source data. Then, through the comparison of different regression models and parameters tuning, we propose the use of Gradient Boosting Regression (GBR) model as a multi-source data fusion model to predict the outbreak of foodborne disease. Finally, we use the risk map to detect and predict foodborne disease outbreak in different business districts of Beijing based on visualization techniques, aiming to provide prevention and control assessment for decision-makers quickly and precisely.",2019,,10.1145/3356998.3365776
2422,"Febiri, Frank and Yihum Amare, Meseret and Hub, Miloslav",Fusion from Big Data to Smart Data to Enhance Quality of Information Systems,"Big Data, Smart data, Quality measures, Information systems","The term “smartness” in the data framework indicates relevancy based on the intended purpose of data. The Internet of Things (IoT) and advancements in technology have resulted in an ever-increasing pool of data available to all institutions to derive meaning and make sound decisions from them. The research presented in this paper explored the role smart data play in information systems quality through a qualitative study of how using the large pool of data (big data) and fusing it to smart data organizations can make sound and intelligent decisions using the available techniques. We use an existing architecture for a public institution to analyze how data ingestion can be achieved with minimum challenges. The findings suggest that even though there is a large pool of data for most organizations, it is becoming more challenging to use this data to make organizational sense due to the challenges posed by such data. The realization of smart data and its benefits in information systems helps improve the quality of information systems, reducing cost and promoting the smartness agenda of today's organization.",2021,,10.1145/3483816.3483836
2423,"Rinaldi, Antonio M. and Russo, Cristiano",A Semantic-Based Model to Represent Multimedia Big Data,"semantics, semantic bigdata, multimedia ontologies","The use of formal representation is a key task in the era of big data. In the context of multimedia big data this issue is stressed due to the intrinsic complexity nature of this kind of data. Moreover, the relations among objects should be clearly expressed and formalized to give the right meaning of data correlation. For this reason the design of formal models to represent and manage information is a necessary task to implement intelligent information systems. In this latter some approaches related to the semantic web could be used to improve the data models which underlie the implementation of big data applications. Using these models the visualization of data and information become an intrinsic and strategic task for the analysis and exploration of multimedia BigData. In this paper we propose the use of a semantic approach to formalize the model structure of multimedia BigData. In addition, the recognition of multimodal features to represent concepts and linguistic properties to relate them are an effective way to bridge the gap between the target semantic classes and the available low-level multimedia descriptors. The proposed model has been implemented in a NoSQL graph database populated from different knowledge sources and a visualization of this very large knowledge base has been presented and discussed as a case study.",2018,,10.1145/3281375.3281386
2424,"Moumen, Aniss","Adoption of Big Data, Cloud Computing &amp; IoT in Morocco Perception of Public Administrations Collaborators","Big data, IoT, Information System, Cloud Computing","The integration of new technologies such as big data, cloud computing, and the Internet of Things (IoT), constitute a new challenge for existing information systems of local administrations.In this work, we present the results of interviews conducted with the public administrations in the South-eastern region of Morocco, and as conclusion we expose two proposed models induced by this study.",2020,,10.1145/3399205.3399228
2425,"Chen, Zhimin and Wang, Yue and Narasayya, Vivek and Chaudhuri, Surajit",Customizable and Scalable Fuzzy Join for Big Data,,"Fuzzy join is an important primitive for data cleaning. The ability to customize fuzzy join is crucial to allow applications to address domain-specific data quality issues such as synonyms and abbreviations. While efficient indexing techniques exist for single-node implementations of customizable fuzzy join, the state-of-the-art scale-out techniques do not support customization, and exhibit poor performance and scalability characteristics. We describe the design of a scale-out fuzzy join operator that supports customization. We use a locality-sensitive-hashing (LSH) based signature scheme, and introduce optimizations that result in significant speed up with negligible impact on recall. We evaluate our implementation on the Azure Databricks version of Spark using several real-world and synthetic data sets. We observe speedups exceeding 50X compared to the best-known prior scale-out technique, and close to linear scalability with data size and number of nodes.",2019,,10.14778/3352063.3352128
2426,"Shi, Bin and YabinXu",Research on Copyright Protection Method of Big Data Based on Nash Equilibrium and Constraint Optimization,"Copyright protection, Majority voting strategy, Constrained optimization, Particle swarm optimization algorithm, Nash equilibrium, Big data, Data watermarking","Data watermarking technology is an effective means to protect the copyright of big data. In order to embed robust and highly available data watermarks, firstly, based on the game theory, a Nash equilibrium model between watermark robustness and data quality is established to solve the optimal number of data partitioning. Then, the mapping relationship between data partitioning and watermark bit is established by using secure hash algorithm. Finally, under the constraint of data usability, the improved particle swarm optimization algorithm is used to calculate the optimal solution of data change for each data partitioning, and then the data is changed accordingly to complete the embedding of watermark bit. In order to verify the copyright ownership of big data, this paper also gives the corresponding watermark extraction method. Watermark extraction is the inverse process of watermark embedding. First, traverse all partitions and extract the possible embedded bit values in each data partitioning. Then, the actual embedded watermark bit is finally determined by majority voting strategies. The experimental results show that our proposed method can not only detect watermarks under different attack conditions, ensure the robustness of big data watermarks, but also achieve better data quality, and the comprehensive effect of data watermarks is better than the existing methods.",2020,,10.1145/3393527.3393532
2427,"Firmani, Donatella and Tanca, Letizia and Torlone, Riccardo",Ethical Dimensions for Data Quality,"Data integration, knowledge extraction, source selection",,2019,,10.1145/3362121
2428,"M\""{u}ller, Daniel and Jain, Pratiksha and Te, Yieh-Funk",Augmenting Data Quality through High-Precision Gender Categorization,"record completion, patenting, Data quality improvement, gender name mapping","Mappings of first name to gender have been widely recognized as a critical tool for the completion, study, and validation of data records in a range of areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 4 million people, which was provided by a car insurance company. Then, we study how naming conventions have changed over time and how they differ by nationality. Next, we build a probabilistic first-name-to-gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in two-label and three-label settings and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies’ outcomes and find that our mapping produces high-precision results. We validate that the additional information of nationality and year of birth improve the precision scores of name-to-gender mappings. Therefore, the proposed approach constitutes an efficient process for improving the data quality of organizations’ records, if the gender attribute is missing or unreliable.",2019,,10.1145/3297720
2429,"Silva, Rodrigo Dantas da and de Ara\'{u}jo, Jean Jar Pereira and de Paiva, \'{A}lvaro Ferreira Pires and de Medeiros Valentim, Ricardo Alexsandro and Coutinho, Karilany Dantas and de Paiva, Jailton Carlos and Roussanaly, Azim and Boyer, Anne",A Big Data Architecture to a Multiple Purpose in Healthcare Surveillance: The Brazilian Syphilis Case,"big data, epidemiology, healthcare surveillance, syphilis","For many decades society did need to monitor and assess the standard of living of the population. In the 1950s, the United Nations (UN) saw this need and proposed 12 areas that should be evaluated, the first of which is listed under ""Health and Demography"", which focuses on what is expressed as the level of a population's health. Decades have passed and great results have been gained from similar initiatives such as reducing mortality from infectious diseases and even eradicating some others. In the age of the digital society, needs have grown. Monitoring demands that once perished from data to become concrete now suffer from the opposite effect, the excess of data from everywhere. Healthcare systems around the world use many different information systems, collecting and generating hundreds of data at unimaginable speed. We are billions of people on the planet and most of us are connected to the virtual world, sharing information, experiences and events with some kind of cloud. In this information age, the ability to aggregate and process this data is a major factor in raising public health to a new level. The development of tools capable of analyzing a large volume of data in seconds and producing knowledge for targeted decision making can help in the fight against specific diseases, in the process of continuing education of professionals, in the formation of new professionals, in the elaboration of new policies. with the specific locoregional look, in the analysis of hidden trends in front of so much information faced in everyday life and other possibilities. The present work proposes an architecture capable of storing and manipulating seeking to standardize the variables in order to allow to correlate this large amount of data in a systematic way, providing to several services and researchers the possibility of consuming health, social, economic and educational data for the promotion of public health.",2020,,10.1145/3401895.3402092
2430,"Christophides, Vassilis and Efthymiou, Vasilis and Palpanas, Themis and Papadakis, George and Stefanidis, Kostas",An Overview of End-to-End Entity Resolution for Big Data,"deep learning, crowdsourcing, Entity blocking and matching, block processing, strongly and nearly similar entities, batch and incremental entity resolution workflows","One of the most critical tasks for improving data quality and increasing the reliability of data analytics is Entity Resolution (ER), which aims to identify different descriptions that refer to the same real-world entity. Despite several decades of research, ER remains a challenging problem. In this survey, we highlight the novel aspects of resolving Big Data entities when we should satisfy more than one of the Big Data characteristics simultaneously (i.e., Volume and Velocity with Variety). We present the basic concepts, processing steps, and execution strategies that have been proposed by database, semantic Web, and machine learning communities in order to cope with the loose structuredness, extreme diversity, high speed, and large scale of entity descriptions used by real-world applications. We provide an end-to-end view of ER workflows&nbsp;for&nbsp;Big Data, critically review the pros and cons of existing methods, and conclude with the main open research&nbsp;directions.",2020,,10.1145/3418896
2431,"Gong, Xiaowen and Shroff, Ness",Incentivizing Truthful Data Quality for Quality-Aware Mobile Data Crowdsourcing,"Mobile data crowdsourcing, incentive mechanism, data quality","Mobile data crowdsourcing has found a broad range of applications (e.g., spectrum sensing, environmental monitoring) by leveraging the ""wisdom"" of a potentially large crowd of ""workers"" (i.e., mobile users). A key metric of crowdsourcing is data accuracy, which relies on the quality of the participating workers' data (e.g., the probability that the data is equal to the ground truth). However, the data quality of a worker can be its own private information (which the worker learns, e.g., based on its location) that it may have incentive to misreport, which can in turn mislead the crowdsourcing requester about the accuracy of the data. This issue is further complicated by the fact that the worker can also manipulate its effort made in the crowdsourcing task and the data reported to the requester, which can also mislead the requester. In this paper, we devise truthful crowdsourcing mechanisms for Quality, Effort, and Data Elicitation (QEDE), which incentivize strategic workers to truthfully report their private worker quality and data to the requester, and make truthful effort as desired by the requester. The truthful design of the QEDE mechanisms overcomes the lack of ground truth and the coupling in the joint elicitation of worker quality, effort, and data. Under the QEDE mechanisms, we characterize the socially optimal and the requester's optimal task assignments, and analyze their performance. We show that the requester's optimal assignment is determined by the largest ""virtual valuation"" rather than the highest quality among workers, which depends on the worker's quality and the quality's distribution. We evaluate the QEDE mechanisms using simulations which demonstrate the truthfulness of the mechanisms and the performance of the optimal task assignments.",2018,,10.1145/3209582.3209599
2432,"Lv, Zhihan and Lou, Ranran and Feng, Hailin and Chen, Dongliang and Lv, Haibin",Novel Machine Learning for Big Data Analytics in Intelligent Support Information Management Systems,"intelligent support information system, lightGBM, big data analysis, accuracy rate, Machine learning","Two-dimensional1 arrays of bi-component structures made of cobalt and permalloy elliptical dots with thickness of 25 nm, length 1 mm and width of 225 nm, have been prepared by a self-aligned shadow deposition technique. Brillouin light scattering has been exploited to study the frequency dependence of thermally excited magnetic eigenmodes on the intensity of the external magnetic field, applied along the easy axis of the elements.Scientific information technology has been developed rapidly. Here, the purposes are to make people's lives more convenient and ensure information management and classification. The machine learning algorithm is improved to obtain the optimized Light Gradient Boosting Machine (LightGBM) algorithm. Then, an Android-based intelligent support information management system is designed based on LightGBM for the big data analysis and classification management of information in the intelligent support information management system. The system is designed with modules of employee registration and login, company announcement notice, attendance and attendance management, self-service, and daily tools with the company as the subject. Furthermore, the performance of the constructed information management system is analyzed through simulations. Results demonstrate that the training time of the optimized LightGBM algorithm can stabilize at about 100s, and the test time can stabilize at 0.68s. Besides, its accuracy rate can reach 89.24%, which is at least 3.6% higher than other machine learning algorithms. Moreover, the acceleration efficiency analysis of each algorithm suggests that the optimized LightGBM algorithm is suitable for processing large amounts of data; its acceleration effect is more apparent, and its acceleration ratio is higher than other algorithms. Hence, the constructed intelligent support information management system can reach a high accuracy while ensuring the error, with apparent acceleration effect. Therefore, this model can provide an experimental reference for information classification and management in various fields.",2021,,10.1145/3469890
2433,"Li, Yonghong and Zhang, Shuwen and Jia, Nan",Research on the Transformation and Upgrading Path and Selection of Traditional Industries from the Perspective of Big Data,"Transformation and upgrading, Big data, Traditional industries, The path","With the emergence of a new generation of information technology, big data has become an important driving force for current social development. Digitalization has become the main direction of the transformation and upgrading of traditional industries. As the product of current informatization, big data includes data quantity, data quality and data analysis ability. It is used as two different ways to interpret the value creation of big data, making it clear that it can promote the transformation and upgrading of traditional industries through value creation. Then, it puts forward the traditional industrial transformation and upgrading path from the perspective of big data, namely the linear path of ""traditional industry + digital"" and the transitional non-linear ""digital + traditional industry"". Its path selection will be analyzed by combining external and internal factors.",2018,,10.1145/3301551.3301610
2434,"Chen, Xin and Yang, Lirong and Sun, Yanzhi",Human Resource Information System Performance Test under Big Data Technology,,"The era of big data has quietly arrived, which is a revolution that determines the development and future destiny of enterprises. Any enterprises that are not ready for this revolution will be eliminated by the era. This paper mainly studies the construction, analysis and management of human resource system in the era of big data. Based on the actual needs, this paper analyzes the business process and functional requirements of human resource management, completes the system architecture design, function module design, database design, realizes the system function module, and completes the test of the system function. The functional modules realized in this paper include: core personnel management, salary management and comprehensive inquiry. The human resource information system designed in this paper ensures the scientific nature, security, availability and portability of the system, meets the demand of data sharing, and plays a positive role in the whole human resource management cycle.",2021,,10.1145/3495018.3495345
2435,"Aljawarneh, Shadi and Lara, Juan A.",Editorial: Special Issue on Quality Assessment and Management in Big Data—Part I,"big data, Quality assessment, quality management",,2021,,10.1145/3449052
2436,"Aljawarneh, Shadi and Lara, Juan A.",Editorial: Special Issue on Quality Assessment and Management in Big Data—Part II,"quality management, Quality assessment, big data",,2021,,10.1145/3449056
2437,"Casado-Vara, Roberto and de la Prieta, Fernando and Prieto, Javier and Corchado, Juan M.",Blockchain Framework for IoT Data Quality via Edge Computing,"WSN, data quality false data detection, Blockchain, IoT, edge computing, non linear control","Smart home presents a challenge in control and monitoring of its wireless sensors networks (WSN) and the internet of things (IoT) devices which form it. The current IoT architectures are centralized, complex, with poor security in its communications and with upstream communication channels mainly. As a result, there are problems with data reliability. These problems include data missing, malicious data inserted, communications network overload, and overload of computing power at the central node. In this paper a new architecture is presented. This architecture based in blockchain introduce the edge computing layer and a new algorithm to improve data quality and false data detection.",2018,,10.1145/3282278.3282282
2438,"Bibri, Simon Elias and Krogstie, John",The Big Data Deluge for Transforming the Knowledge of Smart Sustainable Cities: A Data Mining Framework for Urban Analytics,"data mining, Smart sustainable cities, big data analytics","There has recently been much enthusiasm about the possibilities created by the big data deluge to better understand, monitor, analyze, and plan modern cities to improve their contribution to the goals of sustainable development. Indeed, much of our knowledge of urban sustainability has been gleaned from studies that are characterized by data scarcity. Therefore, this paper endeavors to develop a systematic framework for urban sustainability analytics based on a cross-industry standard process for data mining. The intention is to enable well-informed decision-making and enhanced insights in relation to diverse urban domains. We argue that there is tremendous potential to transform and advance the knowledge of smart sustainable cities through the creation of a big data deluge that seeks to provide much more sophisticated, wider-scale, finer-grained, real-time understanding, and control of various aspects of urbanity in the undoubtedly upcoming Exabyte Age.",2018,,10.1145/3286606.3286788
2439,"Freudiger, Julien and Rane, Shantanu and Brito, Alejandro E. and Uzun, Ersin",Privacy Preserving Data Quality Assessment for High-Fidelity Data Sharing,"privacy and confidentiality, data quality assessment, cryptographic protocols","In a data-driven economy that struggles to cope with the volume and diversity of information, data quality assessment has become a necessary precursor to data analytics. Real-world data often contains inconsistencies, conflicts and errors. Such dirty data increases processing costs and has a negative impact on analytics. Assessing the quality of a dataset is especially important when a party is considering acquisition of data held by an untrusted entity. In this scenario, it is necessary to consider privacy risks of the stakeholders.This paper examines challenges in privacy-preserving data quality assessment. A two-party scenario is considered, consisting of a client that wishes to test data quality and a server that holds the dataset. Privacy-preserving protocols are presented for testing important data quality metrics: completeness, consistency, uniqueness, timeliness and validity. For semi-honest parties, the protocols ensure that the client does not discover any information about the data other than the value of the quality metric. The server does not discover the parameters of the client's query, the specific attributes being tested and the computed value of the data quality metric. The proposed protocols employ additively homomorphic encryption in conjunction with condensed data representations such as counting hash tables and histograms, serving as efficient alternatives to solutions based on private set intersection.",2014,,10.1145/2663876.2663885
2440,"Bawatna, Mohammed and Green, Bertram and Kovalev, Sergey and Deinert, Jan-Christoph and Knodel, Oliver and Spallek, Rainer G.",Research and Implementation of Efficient Parallel Processing of Big Data at TELBE User Facility,"big data, signal processing, data acquisition systems, data processing pipeline, data analytics","In recent years, improvements in high-speed Analog-to-Digital Converters (ADC) and sensor technology has encouraged researchers to improve the performance of Data Acquisition (DAQ) systems for scientific experiments which require high speed and continuous data measurements --- in particular, measuring the electronic and magnetic properties of materials using pump-probe experiments at high repetition rates. Experiments at TELBE are capable of acquiring almost 100 Gigabytes of raw data every ten minutes. The DAQ system used at TELBE partitions the raw data into various subdirectories for further parallel processing utilizing the multicore structure of modern CPUs.Furthermore, several other types of processors that accelerate data processing like the GPU and FPGA have emerged to solve the challenges of processing the massive amount of raw data. However, the memory and network bottlenecks become a significant challenge in big data processing, and new scalable programming techniques are needed to solve these challenges. In this contribution, we will outline the design and implementation of our practical software approach for efficient parallel processing of our large data sets at the TELBE user facility.",2019,,
2441,"Zhang, Yong",Human Resource Data Quality Management Based on Multiple Regression Analysis,"human resources, data quality, Multiple regression analysis","The essence of human resource management informatization is data. Firstly, human information is transformed into data, and then the data can be used. This requires that the comprehensive, complete, timely and effective human resource data is entered into the system, and then the data is analyzed by using the human resource management system to provide decision support for the management. Due to the complexity of the internal law of objective things and the limitation of people's cognition, it is impossible to analyze the internal causality of the actual object and establish a mathematical model in accordance with the mechanism law. Therefore, when some mathematical models cannot be established by mechanism analysis, we usually adopt the method of collecting a large amount of data, and establish the model based on the statistical analysis of the data. Among them, the most widely used random model is statistical regression model.",2020,,10.1145/3444370.3444614
2442,"Stonebraker, Michael and Madden, Sam and Dubey, Pradeep","Intel ""Big Data"" Science and Technology Center Vision and Execution Plan",,"Intel has moved to a collaboration model with universities consisting of ""Science and Technology Centers"" (ISTCs). These are located at a ""hub"" university with participation from other universities, contain embedded Intel personnel, and are focused on some research theme. Intel held a national competition for a 5th Science and Technology center in 2012 and selected a proposal from M.I.T. with a theme of ""Big Data"". This paper presents the big data vision of this technology center and the execution plan for the first few years.",2013,,10.1145/2481528.2481537
2443,"Androutsopoulou, Aggeliki and Charalabidis, Yannis","A Framework for Evidence Based Policy Making Combining Big Data, Dynamic Modelling and Machine Intelligence","Big data, data mining, behavioural patterns, dynamic simulation, evidence based policy making, impact assessment, policy Modelling","Governments and policy makers are striving to respond to contemporary socio-economic challenges, however, often neglecting the human factor and the multidimensionality of policy implications. In this chapter, a framework for evidence based policy making is proposed, which integrates the usage of open big data coming from a multiplicity of sources with policy simulations. It encompasses the application of dynamic modelling methodologies and data mining techniques to extract knowledge from two types of data. On the one hand, objective data such as governmental and statistical data, are used to capture the interlinked policy domains and their underlying casual mechanisms. On the other hand, behavioural patterns and citizens' opinions are extracted from Web 2.0 sources, social media posts, polls and statistical surveys. To combine this multimodal information, our approach suggests a modelling methodology that bases on big data acquisition and processing for the identification of significant factors and counterintuitive interrelations between them, which can be applied in any policy domain. Then, to allow the practical application of the framework an ICT architecture is designed, with the aim to overcome challenges related with big data management and processing. Finally, validation of the approach for driving policy design and implementation in the future in diverse policy domains, is suggested.",2018,,10.1145/3209415.3209427
2444,,Data Quality Rule Definition and Discovery,,"Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.",2019,,
2445,"Khalajzadeh, Hourieh and Simmons, Andrew and Abdelrazek, Mohamed and Grundy, John and Hosking, John and He, Qiang and Ratnakanthan, Prasanna and Zia, Adil and Law, Meng","A Practical, Collaborative Approach for Modeling Big Data Analytics Application Requirements",,"Data analytics application development introduces many challenges including: new roles not in traditional software engineering practices - e.g. data scientists and data engineers; use of sophisticated machine learning (ML) model-based approaches; uncertainty inherent in the models; interfacing with models to fulfill software functionalities; deploying models at scale and rapid evolution of business goals and data sources. We describe our Big Data Analytics Modeling Languages (BiDaML) toolset to bring all stakeholders around one tool to specify, model and document big data applications. We report on our experience applying BiDaML to three real-world large-scale applications. Our approach successfully supports complex data analytics application development in industrial settings.",2020,,
2446,"Orenga-Rogl\'{a}, Sergio and Chalmeta, Ricardo",Framework for Implementing a Big Data Ecosystem in Organizations,,"Featuring the various dimensions of data management, it guides organizations through implementation fundamentals.",2018,,10.1145/3210752
2447,"Jia, Dong-Ming and Yuan, Cun-Feng and Guo, Song and Jiang, Zu-Zhen and Xu, Ding and Wang, Da-An","Application of ""Artificial Intelligence and Big Data"" in Sports Rehabilitation for Chinese Judicial Administrative Drug Addicts","big data, rehabilitation training, artificial intelligence, judicial administrative, Sports rehabilitation","Under the background of ""Wisdom Drug Rehabilitation"", we introduced ""Artificial Intelligence and Big Data"" into ""exercise rehabilitation"" work of drug addicts in judicial administrative system. It is a practical innovation of drug treatment in China. This article will elaborate this innovation of the construction and application of ""Exercise Rehabilitation"" intelligence platform system. This system will improve mental status, alleviate physical and psychological symptoms, ensure safety in places, lighten the burden of professional police officers, make rapid analysis, make accurate decisions and improve the integrity rate of the addict.",2019,,10.1145/3358331.3358336
2448,"Fox, Geoffrey C. and Jha, Shantenu and Qiu, Judy and Luckow, Andre",Towards an Understanding of Facets and Exemplars of Big Data Applications,,We study many Big Data applications from a variety of research and commercial areas and suggest a set of characteristic features and possible kernel benchmarks that stress those features for data analytics. We draw conclusions for the hardware and software architectures that are suggested by this analysis.,2014,,10.1145/2737909.2737912
2449,"Abdullah, Tariq and Ahmet, Ahmed",Genomics Analyser: A Big Data Framework for Analysing Genomics Data,"population scale clustering, compute cluster, algorithms, big data, machine learning, in-memory computing, resource management, data analysis","Abstract Genomics data is unstructured and mostly stored on hard disks. It is both technically and culturally residing in big data domain due to the challenges of volume, velocity and variety. Huge volumes of data are generated from diverse sources in different formats and at a high frequency. Appropriate data models are required to accommodate these data formats for analysing and producing required results with a quick response time. Genomics data can be analysed for a variety of purposes. Existing genomics data analysis pipelines are disk I/O intensive and focus on optimizing data processing for individual analysis tasks. Intensive disk I/O operations and focus on optimizing individual analysis tasks are the biggest bottleneck of existing genomics analysis pipelines. Making any updates in genomics data require reading the whole data set again. In this paper, we present a genomics data analysis framework that addresses both the issues of existing genomics analysis pipelines. It reads unstructured genomics data from sources, transforms it in a structured format and stores this data into a NoSQL database. In this way, genomics data can be queried like any other data and an update in the genomics data does not require reading the whole data set. The framework also presents an efficient analysis pipeline for analysing the genomics data for a variety of purposes like genotype clustering, gene expression microarrays, chromosome variations or gene linkage analysis. A case study of genotype clustering is presented to demonstrate and evaluate the effectiveness of the presented framework. Our results show that the framework improves overall performance of the genomics data analysis pipeline by 49% from existing genomics data analysis pipelines. Furthermore, our approach is robust and is able sustain high performance with high system workloads.",2017,,10.1145/3148055.3148072
2450,"Chotvijit, Sarunkorn and Thiarai, Malkiat and Jarvis, Stephen",Big Data Analytics in Social Care Provision: Spatial and Temporal Evidence from Birmingham,"local authority, social care, data analytics, spatio-temporal analysis, service provision, Birmingham","There is significant national interest in tackling issues surrounding the needs of vulnerable children and adults. At the same time, UK local authorities face severe financial challenges as a result of decreasing financial settlements and increasing demands from growing urban populations. This research employs state-of-the-art data analytics and visualisation techniques to analyse six years of local government social care data for the city of Birmingham, the UK's second most populated city. This analysis identifies: (i) service cost profiles over time; (ii) geographical dimensions to service demand and delivery; (iii) patterns in the provision of services, and (iv) the extent to which data value and data protection interact. The research accesses data held by the local authority to discover patterns and insights that may assist in the understanding of service demand, support decision making and resource management, whilst protecting and safeguarding its most vulnerable citizens. The use of data in this manner could also inform the approach a local authority has to its data, its capture and use, and the potential for supporting data-led management and service improvements.",2018,,10.1145/3209281.3209300
2451,"Cabanban-Casem, Christianne Lynnette",Analytical Visualization of Higher Education Institutions' Big Data for Decision Making,"Knowledge Management, Higher Education Data, Data Science","Education is an important element towards learning and human development, thus, it is the key towards identifying competencies and better productivity for the workforce. As part of the Commission on Higher Education's (CHED) thrust for improving efficiency and effectiveness by simplifying the collection process for all the stakeholders, the developed system will drastically improve the availability of data for making informed decisions and efficient generation of reports.This research outlines opportunities and challenges associated with the implementation and governance of Big Data in higher education through development and implementation of data analytics tool.",2019,,10.1145/3314527.3314537
2452,"Weichselbraun, Albert and Kuntschik, Philipp",Mitigating Linked Data Quality Issues in Knowledge-Intense Information Extraction Methods,"named entity linking, information extraction, linked data quality, mitigation strategies, applications, semantic technologies","Advances in research areas such as named entity linking and sentiment analysis have triggered the emergence of knowledge-intensive information extraction methods that combine classical information extraction with background knowledge from the Web. Despite data quality concerns, linked data sources such as DBpedia, GeoNames and Wikidata which encode facts in a standardized structured format are particularly attractive for such applications.This paper addresses the problem of data quality by introducing a framework that elaborates on linked data quality issues relevant to different stages of the background knowledge acquisition process, their impact on information extraction performance and applicable mitigation strategies. Applying this framework to named entity linking and data enrichment demonstrates the potential of the introduced mitigation strategies to lessen the impact of different kinds of data quality problems. An industrial use case that aims at the automatic generation of image metadata from image descriptions illustrates the successful deployment of knowledge-intensive information extraction in real-world applications and constraints introduced by data quality concerns.",2017,,10.1145/3102254.3102272
2453,"Wu, Jinrong and Sinnott, Richard O. and Effendy, Jemie and Gl\""{o}ckner, Stephan and Hu, William and Li, Jiajie",Usage Patterns and Data Quality: A Case Study of a National Type-1 Diabetes Study,"log analysis, auditing, Cloud, Type-1 diabetes",The Environmental Determinants of Islet Auto- immunity (ENDIA) project is Australia's largest study into the causes of Type-1 Diabetes (T1D). The ENDIA study is supported by a Cloud-based software platform including a clinical registry comprising extensive longitudinal information on families at risk of having a child that might go on to develop T1D. This registry includes both demographic and clinical information on families and children as well as the environmental factors that might influence the onset of T1D. A multitude of samples are obtained through the study and used to support a diverse portfolio of bioinformatics data analytics. The quality of the data in the registry is essential to the overall success of the project. This paper presents a Cloud-based log-analytics platform that supports the detailed analysis of patterns of usage of the registry by the clinical centres and collaborators involved. We explore the impact that the usage patterns have on the overall data quality. We also consider ways of improving data quality by mothers entering their own data through targeted mobile applications that have been developed for dietary data collection.,2017,,10.1145/3107514.3107518
2454,"Nascimento, Dimas C. and Pires, Carlos Eduardo and Mestre, Demetrio Gomes",A Data Quality-Aware Cloud Service Based on Metaheuristic and Machine Learning Provisioning Algorithms,"metaheuristic, provisioning, cloud computing, data quality, machine learning","Cloud Computing as a service has become a topic of increasing interest. The outsourcing of duties and infrastructure to external parties became a crucial concept for many business models. In this paper we discuss the design and experimental evaluation of provisioning algorithms, in a Data Quality-aware Service (DQaS) context, that enables dynamic Data Quality Service Level Agreements (DQSLA) management and optimization of cloud resources. The DQaS has been designed to respond effectively to the DQSLA requirements of the service customers, by minimizing SLA penalties and provisioning the cloud infrastructure for the execution of data quality algorithms. An experimental evaluation of the proposed provisioning algorithms, carried out through simulation, has provided very encouraging results that confirm the adequacy of these algorithms in the DQaS context.",2015,,10.1145/2695664.2695753
2455,"Yu, Xiaomu and Yin, Yuelin",Application Strategies of Medical Big Data in Health Economic Management,,"NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the ICIMTech 2021 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.",2021,,
2456,"Papst, Franz and Saukh, Olga and R\""{o}mer, Kay and Grandl, Florian and Jakovljevic, Igor and Steininger, Franz and Mayerhofer, Martin and Duda, J\""{u}rgen and Egger-Danner, Christa",Embracing Opportunities of Livestock Big Data Integration with Privacy Constraints,"agriculture, privacy-preserving data analysis, data privacy","Today's herd management undergoes a major transformation triggered by the penetration of cheap sensor solutions into cattle farms, and the promise of predictive analytics to detect animal health issues and product-related problems before they occur. The latter is particularly important to prevent disease spread, ensure animal health, animal welfare and product quality. Sensor businesses entering the market tend to build their solutions as end-to-end pipelines spanning sensors, proprietary algorithms, cloud services, and mobile apps. Since data privacy is an important issue in this industry, as a result, disconnected data silos, heterogeneity of APIs, and lack of common standards limit the value the sensor technologies could provide for herd management. In the last few years, researchers and communities proposed a number of data integration architectures to enable exchange between streams of sensor data. This paper surveys the existing efforts and outlines the opportunities they fail to address by treating sensor data as a black box. We discuss alternative solutions to the problem based on privacy-preserving collaborative learning, and provide a set of scenarios to show their benefits for both farmers and businesses.",2019,,10.1145/3365871.3365900
2457,"Song, Zhendong",Application of Big Data and Intelligent Processing Technology in Modern Chinese Multi-Category Words Part of Speech Tagging Corpus,"Tagging, Intelligent processing, Big data, Multi-category words, Corpus","The application of modern Chinese multi-category words corpus is very wide. With the development of the Internet, data from the corpus is getting bigger and bigger during collection. The data gradually develops so big that the current relational database is difficult to deal with them. This article analyzes the important role of the big data technology in corpu",2018,,10.1145/3209914.3234639
2458,"Liu, Yu and Wang, Yangtao and Gao, Lianli and Guo, Chan and Xie, Yanzhao and Xiao, Zhili",Deep Hash-Based Relevance-Aware Data Quality Assessment for Image Dark Data,"Resource allocation, relevance, data quality assessment, CPR, GAH, data mining","Data mining can hardly solve but always faces a problem that there is little meaningful information within the dataset serving a given requirement. Faced with multiple unknown datasets, to allocate data mining resources to acquire more desired data, it is necessary to establish a data quality assessment framework based on the relevance between the dataset and requirements. This framework can help the user to judge the potential benefits in advance, so as to optimize the resource allocation to those candidates. However, the unstructured data (e.g., image data) often presents dark data states, which makes it tricky for the user to understand the relevance based on content of the dataset in real time. Even if all data have label descriptions, how to measure the relevance between data efficiently under semantic propagation remains an urgent problem. Based on this, we propose a Deep Hash-based Relevance-aware Data Quality Assessment framework, which contains off-line learning and relevance mining parts as well as an on-line assessing part. In the off-line part, we first design a Graph Convolution Network (GCN)-AutoEncoder hash (GAH) algorithm to recognize the data (i.e., lighten the dark data), then construct a graph with restricted Hamming distance, and finally design a Cluster PageRank (CPR) algorithm to calculate the importance score for each node (image) so as to obtain the relevance representation based on semantic propagation. In the on-line part, we first retrieve the importance score by hash codes and then quickly get the assessment conclusion in the importance list. On the one hand, the introduction of GCN and co-occurrence probability in the GAH promotes the perception ability for dark data. On the other hand, the design of CPR utilizes hash collision to reduce the scale of graph and iteration matrix, which greatly decreases the consumption of space and computing resources. We conduct extensive experiments on both single-label and multi-label datasets to assess the relevance between data and requirements as well as test the resources allocation. Experimental results show our framework can gain the most desired data with the same mining resources. Besides, the test results on Tencent1M dataset demonstrate the framework can complete the assessment with a stability for given different requirements.",2021,,10.1145/3420038
2459,"Hou, Hanfang and Fu, Qiang and Zhang, Yang","An Empirical Study on the Classification, Grading, Sharing and Opening of Healthcare Big Data Based on Current Policies and Standards","Sharing, Opening, Healthcare big data, Classification, Grading","This paper expounds the connotations and features of healthcare big data, as well as the concepts and logical relations for its classification, grading, sharing and opening. It also summarizes overseas policies related thereto and their status quo, and emphatically introduces main policies, laws, regulations and national standards regarding the classification, grading, sharing and opening of healthcare big data in China, as well as practices in three places represented by Shandong Province, Guangdong Province and Fuzhou. Finally, principles and practical suggestions for the classification, grading, sharing and opening of healthcare big data were proposed based on current polices, regulations and standards",2021,,10.1145/3468945.3468964
2460,"Xu, Gang and Wu, Shunyu and Xie, Pengfei",Integration and Exchange Method of Multi-Source Heterogeneous Big Data for Intelligent Power Distribution and Utilization,"intelligent power distribution and utilization, information model, data fusion and exchange, multi-source and heterogeneous","With the development of smart grid and big data technologies, the stability and economy of distribution network operation are enhanced effectively. Intelligent power distribution and utilization (IPDU) big data platform, which exchanges operation data with other related distribution network management systems, makes decisions for demand side management, power system and distributed energy operation strategies by analyzing the big data. In order to solve the data fusion and exchange problems among all information systems, we proposed a kind of general information model for multi-source heterogeneous big data. In addition, a data fusion and exchange mechanism is established based on circle buffer to ensure the data quality. Finally, this paper demonstrates the effective of the method of IPDU big data fusion method by the example of distribution network reconfiguration. The method proposed in this paper can satisfy the data exchanging demands of future smart grid and demand side management, and it also has good confluent and extensible feature.",2016,,10.1145/3018009.3018040
2461,"Markopoulos, Dimitris and Tsolakidis, Anastasios and N. Karanikolas, Nikitas and Skourlas, Christos",Towards the Design of a Conceptual Framework for the Operation of Intensive Care Units Based on Big Data Analysis,"Intensive Care Unit, Big Data Analysis, Machine Learning, Conceptual Framework","The development of Big Data Analytics (BDA) technology and the maturity of the Machine Learning (ML) sector offer great opportunities for applications in Intensive Care Units (ICUs). This paper describes a Conceptual Framework and proposes its use in designing architectures and big data applications in ICUs. The Conceptual Framework is based on BDA,MLNatural Language Processing (NLP) and consists of the following subsystems: The ""Big Data Integration and ICUs"" module, the ""ICUs and critical care services"" module, the ""Use of standards and ICUs"" module, the ""Machine Learning and ICUs"" module, and the “NLP and ICUs” module. The framework is developed using Soft System Methodology (SSM) and Design Science Research Methodology (DSRM).",2020,,10.1145/3437120.3437352
2462,"Sharma, Abhishek B. and Ivan\v{c}i\'{c}, Franjo and Niculescu-Mizil, Alexandru and Chen, Haifeng and Jiang, Guofei",Modeling and Analytics for Cyber-Physical Systems in the Age of Big Data,,"In this position paper we argue that the availability of ""big"" monitoring data on Cyber-Physical Systems (CPS) is challenging the traditional CPS modeling approaches by violating their fundamental assumptions. However, big data alsobrings unique opportunities in its wake by enabling new modeling and analytics approaches as well as facilitating novel applications. We highlight a few key challenges andopportunities, and outline research directions for addressing them. To provide a proper context, we also summarize CPS modeling approaches, and discuss how modeling and analytics for CPS differs from general purpose IT systems.",2014,,10.1145/2627534.2627558
2463,"Chen, Xiaoyu",Research on Visual Analysis Method of Food Safety Big Data Based on Artificial Intelligence,,"In the background of today's big data era, the popularization of computer network and information technology helps us to understand and disseminate the hot information in the current society more quickly. By quickly understanding these hot issues, we can better supervise and prevent these problems in our life. In recent years, the problem of food safety appears frequently in our field of vision, which makes people have to regard food safety as a hot issue in today's social development. The state and relevant food safety supervision departments are also paying attention to the food safety problems. In order to better supervise food safety issues in this era of big data, this paper will analyze and study food safety issues with the help of popular technologies in the new era, such as artificial intelligence technology and big data technology, so as to formulate a new scheme to meet the needs of people in the new era for food safety supervision. Through the research, it can be found that a series of methods proposed in this paper can effectively provide new ideas for food safety big data visual analysis research method based on artificial intelligence.",2021,,10.1145/3510858.3511394
2464,"Qin, Yana and Yang, Haolin and Guo, Mengjie and Guo, Meicheng",An Advanced Data Science Model Based on Big Data Analytics for Urban Driving Cycle Construction in China,"urban driving cycle construction, feature engineering, data preprocessing, PCA, Big data analytics","In recent years, with the rapid growth of car ownership, Chinese road traffic conditions have changed a lot. Governments, enterprises, and the public are increasingly finding that the increasing deviation between the actual fuel consumption and the results of the regulatory certification based on NEDC (New European Driving Cycle). In addition, this deviation has seriously affected the credibility of the government, energy saving and emission reduction of automobiles and environmental pollution. Thus, need to improve urban driving cycle construction methods to adapt the Chinese traffic and automobiles driving cycles.This paper proposes an advanced data science model based on big data analysis for accurate urban driving cycle construction in Chinese cities. In addition, we conduct a lot of data analysis and statistics. Then we design a data preprocessing method for cleaning the noise data to use in driving cycle construction. Extensive experiments and analysis on real-world datasets demonstrate that the proposed methods can significantly reduce the impact of missing and abnormal data on microtrips segmentation, and thus the proposed methods can be used for driving cycle construction in China more accurately.",2020,,10.1145/3398329.3398330
2465,"Collins, Graham and Varilly, Hugh and Yoshinori, Tanabe",Pedagogical Lessons from an International Collaborative Big Data Undergraduate Research Project,"contributing student pedagogy, software architecture, retrospectives, Data analytics, peer assessment","This experience report covers the collaboration between UCL and NII Tokyo students in development of data analytics research projects: the challenges, contributing student pedagogy and changes to teaching. Students are often taught technology management separate from other computing modules. The teaching team designed a more coherent learning experience linking the technology management teaching more closely to engineering processes, specifically to engage students whose interest lies more with computing. This project has given rise to a re-evaluation of how technology management is taught to undergraduate students, adoption of architecture as a key aspect and inclusion of students with different levels of academic attainment within a class.",2015,,10.1145/2797433.2797467
2466,"Hagen, Loni and Seon Yi, Hye and Pietri, Siana and E. Keller, Thomas","Processes, Potential Benefits, and Limitations of Big Data Analytics: A Case Analysis of 311 Data from City of Miami","big data analytics, e-government, 311 data, information visualization","As part of the open government movement, an increasing number of 311 call centers have made their datasets available to the public. Studies have found that 311 request patterns are associated with personal attributes and living conditions. Most of these studies used New York City 311 data. In this study, we use 311 data from the City of Miami, a smaller local government, as a case study. This study contributes to digital government research and practices by making suggestions on best practices regarding the use of big data analytics on 311 data. In addition, we discuss limitations of 311 data and analytics results. Finally, we expect our results to inform decision making within the City of Miami government and other local governments.",2019,,10.1145/3325112.3325212
2467,"Peng, Shaoliang and Liao, Xiangke and Yang, Canqun and Lu, Yutong and Liu, Jie and Cui, Yingbo and Wang, Heng and Wu, Chengkun and Wang, Bingqiang",The Challenge of Scaling Genome Big Data Analysis Software on TH-2 Supercomputer,"TH-2 supercomputer, parallel optimization, SNP detection, sequence alignment, whole genome re-sequencing","Whole genome re-sequencing plays a crucial role in biomedical studies. The emergence of genomic big data calls for an enormous amount of computing power. However, current computational methods are inefficient in utilizing available computational resources. In this paper, we address this challenge by optimizing the utilization of the fastest supercomputer in the world - TH-2 supercomputer. TH-2 is featured by its neo-heterogeneous architecture, in which each compute node is equipped with 2 Intel Xeon CPUs and 3 Intel Xeon Phi coprocessors. The heterogeneity and the massive amount of data to be processed pose great challenges for the deployment of the genome analysis software pipeline on TH-2. Runtime profiling shows that SOAP3-dp and SOAPsnp are the most time-consuming components (up to 70% of total runtime) in a typical genome-analyzing pipeline. To optimize the whole pipeline, we first devise a number of parallel and optimization strategies for SOAP3-dp and SOAPsnp, respectively targeting each node to fully utilize all sorts of hardware resources provided both by CPU and MIC. We also employ a few scaling methods to reduce communication between different nodes. We then scaled up our method on TH-2. With 8192 nodes, the whole analyzing procedure took 8.37 hours to finish the analysis of a 300 TB dataset of whole genome sequences from 2,000 human beings, which can take as long as 8 months on a commodity server. The speedup is about 700x.",2015,,10.1109/CCGrid.2015.46
2468,"Wu, Rui and Cheng, Qian and He, Lisong and Cao, Zhenyu",Environmental Big Data Model and Recognition of Abnormal Emission from Enterprise Data,,,2021,,10.1145/3482632.3484095
2469,"Gong, Xiaowen and Shroff, Ness B.",Truthful Mobile Crowdsensing for Strategic Users With Private Data Quality,,"Mobile crowdsensing has found a variety of applications e.g., spectrum sensing, environmental monitoring by leveraging the “wisdom” of a potentially large crowd of mobile users. An important metric of a crowdsensing task is data accuracy, which relies on the data quality of the participating users’ data e.g., users’ received SNRs for measuring a transmitter’s transmit signal strength. However, the quality of a user can be its private information which, e.g., may depend on the user’s location that it can manipulate to its own advantage, which can mislead the crowdsensing requester about the knowledge of the data’s accuracy. This issue is exacerbated by the fact that the user can also manipulate its effort made in the crowdsensing task, which is a hidden action that could result in the requester having incorrect knowledge of the data’s accuracy. In this paper, we devise truthful crowdsensing mechanisms for Quality and Effort Elicitation QEE, which incentivize strategic users to truthfully reveal their private quality and truthfully make efforts as desired by the requester. The QEE mechanisms achieve the truthful design by overcoming the intricate dependency of a user’s data on its private quality and hidden effort. Under the QEE mechanisms, we show that the crowdsensing requester’s optimal RO effort assignment assigns effort only to the best user that has the smallest “virtual valuation”, which depends on the user’s quality and the quality’s distribution. We also show that, as the number of users increases, the performance gap between the RO effort assignment and the socially optimal effort assignment decreases, and converges to 0 asymptotically. We further discuss some extensions of the QEE mechanisms. Simulation results demonstrate the truthfulness of the QEE mechanisms and the system efficiency of the RO effort assignment.",2019,,10.1109/TNET.2019.2934026
2470,"Li, Jicai and Liu, Dan",A Method of Constructing Distributed Big Data Analysis Model for Machine Learning Based on Cloud Computing,,"There are many big data analysis methods, and it is an effective method to build big data analysis model through machine learning. Big data is characterized by large data scale and long calculation cycle. In order to speed up the calculation speed and shorten the calculation cycle, distributed computing method is one of the effective methods to solve the above problems. With the wide application and rapid development of information technology, cloud computing as a new business computing model has attracted more and more attention. However, the security of cloud computing data storage model lacks reliability. Under the mainstream cloud computing and big data basic environment, building a better model from resource aggregation to analysis and mining, and modeling distributed big data analysis can provide high-reliability, high-security, high-efficiency analysis services for practical analysis and mining applications such as intelligence judgment, information deployment and control, stakeholder analysis and intelligent decision-making.",2021,,10.1145/3482632.3484007
2471,"Zhao, Yan and Megdiche, Imen and Ravat, Franck and Dang, Vincent-nam","A Zone-Based Data Lake Architecture for IoT, Small and Big Data","Stream IoT Data, Technical Architecture, Zone-based, Data Lake, Metadata"," Data lakes are supposed to enable analysts to perform more efficient and efficacious data analysis by crossing multiple existing data sources, processes and analyses. However, it is impossible to achieve that when a data lake does not have a metadata governance system that progressively capitalizes on all the performed analysis experiments. The objective of this paper is to have an easily accessible, reusable data lake that capitalizes on all user experiences. To meet this need, we propose an analysis-oriented metadata model for data lakes. This model includes the descriptive information of datasets and their attributes, as well as all metadata related to the machine learning analyzes performed on these datasets. To illustrate our metadata solution, we implemented a web application of data lake metadata management. This application allows users to find and use existing data, processes and analyses by searching relevant metadata stored in a NoSQL data store within the data lake. To demonstrate how to easily discover metadata with the application, we present two use cases, with real data, including datasets similarity detection and machine learning guidance.",2021,,10.1145/3472163.3472185
2472,"Podhoranyi, Michal and Vojacek, Lukas",Social Media Data Processing Infrastructure by Using Apache Spark Big Data Platform: Twitter Data Analysis,"social network data, data processing architecture, Twitter, Apache Spark","Social media provide continuous data streams that contain information with different level of sensitivity, validity and accuracy. Therefore, this type of information has to be properly filtered, extracted and processed to avoid noisy and inaccurate results. The main goal of this work is to propose architecture and workflow able to process Twitter social network data in near real-time. The primary design of the introduced modern architecture covers all processing aspects from data ingestion and storing to data processing and analysing. This paper presents Apache Spark and Hadoop implementation. The secondary objective is to analyse tweets with the defined topic --- floods. The word frequency method (Word Clouds) is shown as a major tool to analyse the content of the input dataset. The experimental architecture confirmed the usefulness of many well-known functions of Spark and Hadoop in the social data domain. The platforms which were used provided effective tools for optimal data ingesting, storing as well as processing and analysing. Based on the analytical part, it was observed that the word frequency method (n-grams) can effectively reveal the tweets content. According to the results of this study, the tweets proved their high informative potential regarding data quality and quantity.",2019,,10.1145/3361821.3361825
2473,"Ding, Gaohu",Optimization of Modern Teaching System with Computer Technology under the Background of Big Data,,"The rapid development of science and technology has promoted the rapid development and wide application of CT(computer Technology), especially the emergence of BDT(big data technology) in recent years. The integration and improvement of CT and it has promoted great changes in all aspects of society, and these changes are beneficial, and they have brought us great convenience and help Help. For the education industry, under the background of BDT, the integration of CT and modern teaching system can provide new development thinking and new direction for the reform of modern education. In order to study what effect the combination of the two will bring, this paper selects two universities and their students as the experimental research objects to explore how the modern teaching system will be innovated and developed under the effect of this new technology. The experimental results show that the students of a university who have applied CT in the modern teaching system have a high degree of satisfaction, and the percentage of those who are satisfied has reached 67%. Moreover, the score of teaching and research group of a university is relatively high, and the highest score is 95.",2021,,10.1145/3495018.3495364
2474,"Li, Huan and Lu, Hua and Jensen, Christian S. and Tang, Bo and Cheema, Muhammad Aamir","Spatial Data Quality in the Internet of Things: Management, Exploitation, and Prospects","Internet of Things, spatiotemporal dependencies, quality management, spatiotemporal data cleaning, geo-sensory data, location refinement, spatial computing, spatial queries","With the continued deployment of the Internet of Things (IoT), increasing volumes of devices are being deployed that emit massive spatially referenced data. Due in part to the dynamic, decentralized, and heterogeneous architecture of the IoT, the varying and often low quality of spatial IoT data (SID) presents challenges to applications built on top of this data. This survey aims to provide unique insight to practitioners who intend to develop IoT-enabled applications and to researchers who wish to conduct research that relates to data quality in the IoT setting. The survey offers an inventory analysis of major data quality dimensions in SID and covers significant data characteristics and associated quality considerations. The survey summarizes data quality related technologies from both task and technique perspectives. Organizing the technologies from the task perspective, it covers recent progress in SID quality management, encompassing location refinement, uncertainty elimination, outlier removal, fault correction, data integration, and data reduction; and it covers low-quality SID exploitation, encompassing querying, analysis, and decision-making techniques. Finally, the survey covers emerging trends and open issues concerning the quality of SID.",2022,,10.1145/3498338
2475,"Wang, Wenwen",Research on the Application of Big Data Cloud Cleaning System in Physical Function Sports Training Management,,"The main purpose of physical function training is to solve the problems of many injuries, incorrect movement patterns, movement compensation and so on. In the era of big data, the scientific research field of sports training is gradually beginning to adopt the big data model for development, which will better form a new insight after data generation, collection, analysis and transformation. Therefore, this paper analyzes the application of big data intelligent cleaning system based on cloud computing in physical function training management. Students can quickly query running results through WEB query system, and administrators can download and modify students' results through WEB system. By using big data analysis technology, students are grouped according to their physical fitness test scores and BMI index, and their sports plans are formulated accordingly. A high-reliability real-time wireless receiving and sending sports monitoring system for big data is adopted, which realizes the functions of networking, unmanned and intelligent sports management.",2021,,10.1145/3482632.3487514
2476,"Shen, Yanyan and Dinh, Anh and Jagadish, H. V.",Introduction to the Special Issue on Data Science for Next Generation Big Data,,,2022,,10.1145/3507467
2477,"De, Sushovan and Hu, Yuheng and Meduri, Venkata Vamsikrishna and Chen, Yi and Kambhampati, Subbarao",BayesWipe: A Scalable Probabilistic Framework for Improving Data Quality,"statistical data cleaning, Data quality, offline and online cleaning","Recent efforts in data cleaning of structured data have focused exclusively on problems like data deduplication, record matching, and data standardization; none of the approaches addressing these problems focus on fixing incorrect attribute values in tuples. Correcting values in tuples is typically performed by a minimum cost repair of tuples that violate static constraints like Conditional Functional Dependencies (which have to be provided by domain experts or learned from a clean sample of the database). In this article, we provide a method for correcting individual attribute values in a structured database using a Bayesian generative model and a statistical error model learned from the noisy database directly. We thus avoid the necessity for a domain expert or clean master data. We also show how to efficiently perform consistent query answering using this model over a dirty database, in case write permissions to the database are unavailable. We evaluate our methods over both synthetic and real data.",2016,,10.1145/2992787
2478,"Farhana Jamaludin, Ain and Najib Razali, Muhammad and jalil, Rohaya and Othman, Hajar and Adnan, Yasmin",Identification of Business Intelligence in Big Data Maintenance of Government Sector in Putrajaya,"Business Intelligence, Maintenance Management, Data Management,","This paper contributes significantly, which focuses on an intelligent system that lets the government make an integral part of decision-making and can be applied horizontally to solve the problems in maintenance practice through business intelligence. Accordingly, a real-time data management system for maintenance management is proposed in this paper. It looks at a real case study highlighting the need for proper data management in the government sector. Our findings bridge the gap of information technology inserted in government office buildings, with maintenance management being the domain. This paper demonstrates the underlying structure of the developed simulation model.",2021,,10.1145/3457784.3457816
2479,"Win, Thee Zin and Kham, Nang Saing Moon",Mutual Information-Based Feature Selection Approach to Reduce High Dimension of Big Data,"Redundant Features, Feature Selection, Mutual Information, High Dimensional Data","As increasing the massive amount of data demands effective and efficient mining strategies, practitioners and researchers are trying to develop scalable mining algorithms, machine learning algorithms and strategies to be successful data mining in turning mountains of data into nuggets. Data of high dimension significantly increases the memory storage requirements and computational costs for data analytics. Therefore, reducing dimension can mainly improve three data mining performance: speed of learning, predictive accuracy and simplicity and comprehensibility of mined result. Feature selection, data preprocessing technique, is effective and efficient in data mining, data analytics and machine learning problems particularly in high dimension reduction. Most feature selection algorithms can eliminate only irrelevant features but redundant features. Not only irrelevant features but also redundant features can degrade learning performance. Mutual information measured feature selection is proposed in this work to remove both irrelevant and redundant features.",2018,,10.1145/3278312.3278316
2480,"Wang, Zehao and Zhang, Haoxiang and Chen, Tse-Hsun (Peter) and Wang, Shaowei",Would You like a Quick Peek? Providing Logging Support to Monitor Data Processing in Big Data Applications,,"To analyze large-scale data efficiently, developers have created various big data processing frameworks (e.g., Apache Spark). These big data processing frameworks provide abstractions to developers so that they can focus on implementing the data analysis logic. In traditional software systems, developers leverage logging to monitor applications and record intermediate states to assist workload understanding and issue diagnosis. However, due to the abstraction and the peculiarity of big data frameworks, there is currently no effective monitoring approach for big data applications. In this paper, we first manually study 1,000 randomly sampled Spark-related questions on Stack Overflow to study their root causes and the type of information, if recorded, that can assist developers with motioning and diagnosis. Then, we design an approach, DPLOG, which assists developers with monitoring Spark applications. DPLOG leverages statistical sampling to minimize performance overhead and provides intermediate information and hint/warning messages for each data processing step of a chained method pipeline. We evaluate DPLOG on six benchmarking programs and find that DPLOG has a relatively small overhead (i.e., less than 10% increase in response time when processing 5GB data) compared to without using DPLOG, and reduce the overhead by over 500% compared to the baseline. Our user study with 20 developers shows that DPLOG can reduce the needed time to debug big data applications by 63% and the participants give DPLOG an average of 4.85/5 for its usefulness. The idea of DPLOG may be applied to other big data processing frameworks, and our study sheds light on future research opportunities in assisting developers with monitoring big data applications.",2021,,
2481,"Wang, Hongzhi and Ding, Xiaoou and Chen, Xiangying and Li, Jianzhong and Gao, Hong",CleanCloud: Cleaning Big Data on Cloud,"entity resolution, data cleaning, parallel computing","We describe CleanCloud, a system for cleaning big data based on Map-Reduce paradigm in cloud. Using Map-Reduce paradigm, the system detects and repairs various data quality problems in big data. We demonstrate the following features of CleanCloud: (a) the support for cleaning multiple data quality problems in big data; (b) a visual tool for watching the status of big data cleaning process and tuning the parameters for data cleaning; (c) the friendly interface for data input and setting as well as cleaned data collection for big data. CleanCloud is a promising system that provides scalable and effect data cleaning mechanism for big data in either files or databases.",2017,,10.1145/3132847.3133187
2482,"Han, Caibao",The Architecture and Security Design of Big Data Platform of the Physical Teaching Information System in Colleges and Universities,,"This paper constructs the logic structure of big data platform of physical teaching information system in Colleges and universities through Hadoop distributed system, which ensures the realization of management and monitoring function, data collection function, data storage and query function, data calculation function, security and management function. The security design is carried out from access control, data authorization and management, Hadoop security configuration to deal with the possible security problems of big data platform.",2021,,10.1145/3482632.3483061
2483,"Chien, Chen-Fu and Chen, Ying-Jen and Wu, Jei-Zheng",Big Data Analytics for Modeling WAT Parameter Variation Induced by Process Tool in Semiconductor Manufacturing and Empirical Study,,"With the feature size shrinkage in advanced technology nodes, the modeling of process variations has become more critical for troubleshooting and yield enhancement. Misalignment among equipment tools or chambers in process stages is a major source of process variations. Because a process flow contains hundreds of stages during semiconductor fabrication, tool/chamber misalignment may more significantly affect the variation of transistor parameters in a wafer acceptance test. This study proposes a big data analytic framework that simultaneously considers the mean difference between tools and wafer-to-wafer variation and identifies possible root causes for yield enhancement. An empirical study was conducted to demonstrate the effectiveness of proposed approach and obtained promising results.",2016,,
2484,"Beneventano, Domenico and Bergamaschi, Sonia and Gagliardelli, Luca and Simonini, Giovanni",<i>BLAST2</i>: An Efficient Technique for Loose Schema Information Extraction from Heterogeneous Big Data Sources,"big data, data integration, Entity resolution","We present BLAST2, a novel technique to efficiently extract loose schema information, i.e., metadata that can serve as a surrogate of the schema alignment task within the Entity Resolution (ER) process, to identify records that refer to the same real-world entity when integrating multiple, heterogeneous, and voluminous data sources. The loose schema information is exploited for reducing the overall complexity of ER, whose na\""{\i}ve solution would imply O(n2) comparisons, where n is the number of entity representations involved in the process and can be extracted by both structured and unstructured data sources. BLAST2 is completely unsupervised yet able to achieve almost the same precision and recall of supervised state-of-the-art schema alignment techniques when employed for Entity Resolution tasks, as shown in our experimental evaluation performed on two real-world datasets (composed of 7 and 10 data sources, respectively).",2020,,10.1145/3394957
2485,"Shankaranarayanan, G. and Blake, Roger",From Content to Context: The Evolution and Growth of Data Quality Research,"text mining, information quality, Data quality","Research in data and information quality has made significant strides over the last 20 years. It has become a unified body of knowledge incorporating techniques, methods, and applications from a variety of disciplines including information systems, computer science, operations management, organizational behavior, psychology, and statistics. With organizations viewing “Big Data”, social media data, data-driven decision-making, and analytics as critical, data quality has never been more important. We believe that data quality research is reaching the threshold of significant growth and a metamorphosis from focusing on measuring and assessing data quality—content—toward a focus on usage and context. At this stage, it is vital to understand the identity of this research area in order to recognize its current state and to effectively identify an increasing number of research opportunities within. Using Latent Semantic Analysis (LSA) to analyze the abstracts of 972 peer-reviewed journal and conference articles published over the past 20 years, this article contributes by identifying the core topics and themes that define the identity of data quality research. It further explores their trends over time, pointing to the data quality dimensions that have—and have not—been well-studied, and offering insights into topics that may provide significant opportunities in this area.",2017,,10.1145/2996198
2486,"Nguyen, Phu H. and Sen, Sagar and Jourdan, Nicolas and Cassoli, Beatriz and Myrseth, Per and Armendia, Mikel and Myklebust, Odd",Software Engineering and AI for Data Quality in Cyber- Physical Systems - SEA4DQ'21 Workshop Report,,"Cyber-physical systems (CPS) have been developed in many industrial sectors and application domains in which the quality requirements of data acquired are a common factor. Data quality in CPS can deteriorate because of several factors such as sensor faults and failures due to operating in harsh and uncertain environments. How can software engineering and artificial intelligence (AI) help manage and tame data quality issues in CPS? This is the question we aimed to investigate in the SEA4DQ workshop. Emerging trends in software engineering need to take data quality management seriously as CPS are increasingly datacentric in their approach to acquiring and processing data along the edge-fog-cloud continuum. This workshop provided researchers and practitioners a forum for exchanging ideas, experiences, understanding of the problems, visions for the future, and promising solutions to the problems in data quality in CPS. Examples of topics include software/hardware architectures and frameworks for data quality management in CPS; software engineering and AI to detect anomalies in CPS data or to repair erroneous CPS data. SEA4DQ 2021, which took place on August 24th, 2021 was a satellite event of the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC / FSE) 2021. The workshop attracted 35 international participants and was exciting with a great keynote, six excellent presentations, and concluded on a high note with a panel discussion. SEA4DQ was motivated by the common research interests from the EU projects for Zero-Defects Manufacturing such as InterQ and Dat4.Zero.",2022,,10.1145/3502771.3502781
2487,"Wang, Hongzhi and Li, Mingda and Bu, Yingyi and Li, Jianzhong and Gao, Hong and Zhang, Jiacheng",Cleanix: A Parallel Big Data Cleaning System,,"For big data, data quality problem is more serious. Big data cleaning system requires scalability and the abilityof handling mixed errors. Motivated by this, we develop Cleanix, a prototype system for cleaning relational Big Data. Cleanix takes data integrated from multiple data sources and cleans them on a shared-nothing machine cluster. The backend system is built on-top-of an extensible and flexible data-parallel substrate the Hyracks framework. Cleanix supports various data cleaning tasks such as abnormal value detection and correction, incomplete data filling, de-duplication, and conflict resolution. In this paper, we show the organization, data cleaning algorithms as well as the design of Cleanix.",2016,,10.1145/2935694.2935702
2488,"Smith, Ken and Seligman, Len and Rosenthal, Arnon and Kurcz, Chris and Greer, Mary and Macheret, Catherine and Sexton, Michael and Eckstein, Adric","""Big Metadata"": The Need for Principled Metadata Management in Big Data Ecosystems","data integration, data discovery, metadata, Big data analytics","Current big data ecosystems lack a principled approach to metadata management. This impedes large organizations' ability to share data and data preparation and analysis code, to integrate data, and to ensure that analytic code makes compatible assumptions with the data it uses. This use-case paper describes the challenges and an in-progress effort to address them. We present a real application example, discuss requirements for ""big metadata"" drawn from that example as well as other U.S. government analytic applications, and briefly describe an effort to adapt an existing open source metadata manager to support the needs of big data ecosystems.",2014,,10.1145/2627770.2627776
2489,"Bizer, Christian and Dong, Luna and Ilyas, Ihab and Vidal, Maria-Esther",Editorial: Special Issue on Web Data Quality,,,2016,,10.1145/3005395
2490,"Khayyat, Zuhair and Ilyas, Ihab F. and Jindal, Alekh and Madden, Samuel and Ouzzani, Mourad and Papotti, Paolo and Quian\'{e}-Ruiz, Jorge-Arnulfo and Tang, Nan and Yin, Si",BigDansing: A System for Big Data Cleansing,"schema constraints, distributed data repair, distributed data cleansing, cleansing abstraction","Data cleansing approaches have usually focused on detecting and fixing errors with little attention to scaling to big datasets. This presents a serious impediment since data cleansing often involves costly computations such as enumerating pairs of tuples, handling inequality joins, and dealing with user-defined functions. In this paper, we present BigDansing, a Big Data Cleansing system to tackle efficiency, scalability, and ease-of-use issues in data cleansing. The system can run on top of most common general purpose data processing platforms, ranging from DBMSs to MapReduce-like frameworks. A user-friendly programming interface allows users to express data quality rules both declaratively and procedurally, with no requirement of being aware of the underlying distributed platform. BigDansing takes these rules into a series of transformations that enable distributed computations and several optimizations, such as shared scans and specialized joins operators. Experimental results on both synthetic and real datasets show that BigDansing outperforms existing baseline systems up to more than two orders of magnitude without sacrificing the quality provided by the repair algorithms.",2015,,10.1145/2723372.2747646
2491,"Wu, Jian and Kim, Kunho and Giles, C. Lee",CiteSeerX: 20 Years of Service to Scholarly Big Data,"digital libraries, scholarly big data, CiteSeerX, search engines","We overview CiteSeerX, the pioneer digital library search engine, that has been serving academic communities for more than 20 years (first released in 1998), from three perspectives. The system perspective summarizes its architecture evolution in three phases over the past 20 years. The data perspective describes how CiteSeerX has created searchable scholarly big datasets and made them freely available for multiple purposes. In order to be scalable and effective, AI technologies are employed in all essential modules. To effectively train these models, a sufficient amount of data has been labeled, which can then be reused for training future models. Finally, we discuss the future of CiteSeerX. Our ongoing work is to make CiteSeerX more sustainable. To this end, we are working to ingest all open access scholarly papers, estimated to be 30-40 million. Part of the plan is to discover dataset mentions and metadata in scholarly articles and make them more accessible via search interfaces. Users will have more opportunities to explore and trace datasets that can be reused and discover other datasets for new research projects. We summarize what was learned to make a similar system more sustainable and useful.",2019,,10.1145/3359115.3359119
2492,"Sinthong, Phanwadee and Patel, Dhaval and Zhou, Nianjun and Shrivastava, Shrey and Iyengar, Arun and Bhamidipaty, Anuradha",DQDF: Data-Quality-Aware Dataframes,,"Data quality assessment is an essential process of any data analysis process including machine learning. The process is time-consuming as it involves multiple independent data quality checks that are performed iteratively at scale on evolving data resulting from exploratory data analysis (EDA). Existing solutions that provide computational optimizations for data quality assessment often separate the data structure from its data quality which then requires efforts from users to explicitly maintain state-like information. They demand a certain level of distributed system knowledge to ensure high-level pipeline optimizations from data analysts who should instead be focusing on analyzing the data. We, therefore, propose data-quality-aware dataframes, a data quality management system embedded as part of a data analyst's familiar data structure, such as a Python dataframe. The framework automatically detects changes in datasets' metadata and exploits the context of each of the quality checks to provide efficient data quality assessment on ever-changing data. We demonstrate in our experiment that our approach can reduce the overall data quality evaluation runtime by 40-80% in both local and distributed setups with less than 10% increase in memory usage.",2021,,10.14778/3503585.3503602
2493,"Tang, Yan",Computer Assisted Design and Practice of GNSS Survey Course of Typical Tasks through Cloud Computing and Big Data Technology,,"Under the background of typical task orientation in higher vocational education, it is an inevitable trend to carry out teaching reform of GNSS Survey for engineering survey major in higher vocational education. This paper breaks the traditional theoretical teaching system, reconstructs it from the aspects of teaching content selection, combination of theory and practice teaching system, considers the actual situation of students' weak theoretical foundation in higher vocational colleges, and the core technical ability needed for employment and post, optimizes the curriculum structure, integrates the theoretical knowledge system and practice teaching links, and meets the requirements of project-oriented curriculum standards.",2021,,10.1145/3495018.3501149
2494,"Vieira, Ant\'{o}nio A. C. and Dias, Lu\'{\i}s M. S. and Santos, Maribel Y. and Pereira, Guilherme A. B. and Oliveira, Jos\'{e} A.",Real-Time Supply Chain Simulation: A Big Data-Driven Approach,,"Simulation of Supply Chains comprises huge amounts of data, resulting in numerous entities flowing in the model. These networks are highly dynamic systems, where entities' relationships and other elements evolve with time, paving the way for real-time Supply Chain decision-support tools capable of using real data. In light of this, a solution comprising of a Big Data Warehouse to store relevant data and a simulation model of an automotive plant, are being developed. The purpose of this paper is to address the modelling approach, which allowed the simulation model to automatically adapt to the data stored in a Big Data Warehouse and thus adapt to new scenarios without manual intervention. The main characteristics of the conceived solution were demonstrated, with emphasis to the real-time and the ability to allow the model to load the state of the system from the Big Data Warehouse.",2019,,
2495,"Deze, Wang",Application of Large Data Mining Technology in Colleges and Universities,"data mining, college student management, big data","Data mining is an advanced science and technology to process data and information. It can be extracted from a large number of complex data, or find some valuable data rules and models. Education has entered the era of big data. However, the way of data processing in university educational administration is relatively backward. Aiming at this problem, this paper applies data mining technology to college teaching management, extracts useful information from the data collected by educational administration management system, and provides correct and powerful data support and guarantee for college teaching managers to make relevant decisions.",2018,,10.1145/3291801.3291834
2496,"Geerts, Floris and Missier, Paolo and Paton, Norman",Editorial: Special Issue on Improving the Veracity and Value of Big Data,,,2018,,10.1145/3174791
2497,"Sindhu, C. S. and Hegde, Nagaratna P.",TAF: Temporal Analysis Framework for Handling Data Velocity in Healthcare Analytics,"Real-Time Analysis, Healthcare Analytics, Medical Data, Big Data, Data Volume","We are inundated in a flood of data today. Data is being collected at a rapid scale from variety of sources like healthcare, e-commerce, social networking and so on. Decisions which were earlier made on assumptions can now be made on the data itself. It's a well known fact that volume, variety, velocity and veracity are the challenges associated in handling Big Data. The dynamic nature of the Internet and the velocity factor pose humongous challenges in retrieving patterns from the data. Coping up with noisy data which occurs at a rapid rate is still an open challenge. We have handled the issues associated with variety and veracity. After reviewing the existing system, it was found that there is no significant research model towards addressing data velocity problem exclusively taking case study of healthcare analytics.Hence, this paper presents a novel framework TAF or Temporal Analysis Framework that mainly targets at handling the incoming speed of data and redundancies in Healthcare Analytics. The proposed system uses real-time data analysis that significantly handles the data velocity along with retention of minimal error. The study outcome was assessed to find minimal algorithm complexities compared to any system that doesn't use this approach of self-adaptable real-time data analysis.",2016,,10.1145/3010089.3010095
2498,"Tounsi, Youssef and Anoun, Houda and Hassouni, Larbi",CSMAS: Improving Multi-Agent Credit Scoring System by Integrating Big Data and the New Generation of Gradient Boosting Algorithms,"Big Data, Multi-Agent System, CatBoost, XgBoost, Credit Scoring, LightGBM","Credit risk is one of the main risks facing banks and credit institutions, with the current progress in machine learning, artificial intelligence and big data. Recent research has proposed several systems for improving credit rating. In this paper, a new scalable credit scoring multi-agent system called ""CSMAS"" is introduced for the prediction of problems in data mining of credit scoring domain. This engine is built using a seven-layer multi-agent system architecture to generate a data mining process based on the coordination of intelligent agents. CSMAS performance is based on preprocessing and data forecasting. The first layer is designed to retrieve any data from various core banking systems, payment systems, credit Bureaus and external databases and data sources and to store it in big data platform. The second layer is devoted to three different subtasks; feature engineering, pre-processing data and integrating diverse datasets. While the third layer is dedicated to dealing with missing Values and treating outliers. In the fourth layer, the techniques of dimensionality reduction are used to reduce the number of features in the original set of features. The fifth layer is dedicated to build a model using the new generation of Gradient Boosting Algorithms (XGBoost, LightGBM and CatBoost) and make predictions. The sixth layer is designed for the model's evaluation. The seventh layer is made to perform the rating of new credit applicants. The performance of CSMAS is assessed using a large dataset of Home Credit Default Risk from Kaggle Challenge (307511 records) to evaluate the risk of a loan applicant as a major problem for banks. The results show that the CSMAS give relevant results. Therefore, the results indicated that the CSMAS can be further employed as a reliable tool to predict more complicated case in credit scoring.",2020,,10.1145/3386723.3387851
2499,"Zhang, Mingming and Wo, Tianyu and Xie, Tao and Lin, Xuelian and Liu, Yaxiao",CarStream: An Industrial System of Big Data Processing for Internet-of-Vehicles,,"As the Internet-of-Vehicles (IoV) technology becomes an increasingly important trend for future transportation, designing large-scale IoV systems has become a critical task that aims to process big data uploaded by fleet vehicles and to provide data-driven services. The IoV data, especially high-frequency vehicle statuses (e.g., location, engine parameters), are characterized as large volume with a low density of value and low data quality. Such characteristics pose challenges for developing real-time applications based on such data. In this paper, we address the challenges in designing a scalable IoV system by describing CarStream, an industrial system of big data processing for chauffeured car services. Connected with over 30,000 vehicles, CarStream collects and processes multiple types of driving data including vehicle status, driver activity, and passenger-trip information. Multiple services are provided based on the collected data. CarStream has been deployed and maintained for three years in industrial usage, collecting over 40 terabytes of driving data. This paper shares our experiences on designing CarStream based on large-scale driving-data streams, and the lessons learned from the process of addressing the challenges in designing and maintaining CarStream.",2017,,10.14778/3137765.3137781
2500,"Tikito, I. and Souissi, N.",Data Collect Requirements Model,"Creation/Reception Process, Big data, System of Systems, Data lifecycle, Requirement Model, Collect system, Seven views, BPMN","In Big data era, managing data requires sufficient tools, last computer science evolution and developed methodologies. To be able to satisfy customer and the big need of information, multiple methods are developed to handle the complexity as well as the huge amount of data in different phases of data lifecycle. We notice for each complicated situation in data lifecycle we focus more particularly to develop storage or Analysis processes. For this reason in this paper, we try to have a different approach to resolve basic issues on targeting the first phase of data lifecycle, which is data collect. We present it as a System of systems, since the complexity of each phase of data lifecycle. In this research, we are interested by the collect system and particularly the process of Creation/Reception of data for which we model the requirements in order to manage smart data at the first level of the cycle. To build this model, we follow a methodology that required three major steps. Starting with requirement identification to defining criterion for each requirement, and in the last step will provide requirement modeling. This research highlight the importance of managing data collect to identify and restrict the issues of big data era.",2017,,10.1145/3090354.3090358
